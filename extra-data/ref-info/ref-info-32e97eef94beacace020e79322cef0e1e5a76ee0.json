{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700974"
                        ],
                        "name": "Barak A. Pearlmutter",
                        "slug": "Barak-A.-Pearlmutter",
                        "structuredName": {
                            "firstName": "Barak",
                            "lastName": "Pearlmutter",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barak A. Pearlmutter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 100
                            }
                        ],
                        "text": "There are xedpoint learning algorithms (for details see [72], [73], [74], [75], or for a survey see [76]) 1Typically ( ) = (1+e ) 1, in which case 0( ) = ( )(1 ( )), or the scaled ( ) = tanh( ), in which case 0( ) = (1 + ( ))(1 ( )) = 1 2( )."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "The analogous derivations for RTRL are carried out in [76]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 367,
                                "start": 363
                            }
                        ],
                        "text": "Real Time Recurrent Learning An online, exact, and stable, but computationally expensive, procedure for determining the derivatives of functions of the states of a dynamic system with respect to that system's internal parameters has been discovered and applied to recurrent neural networks a number of times [100], [101], [102], [103]; for reviews see also [81], [76], [104]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 248,
                                "start": 244
                            }
                        ],
                        "text": "The other is to take the continuous time equations, approximate them by di erence equations, precisely calculate the adjoint equations for this discrete time system, and then approximate back to get the continuous time adjoint equations, as in [76]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 121
                            }
                        ],
                        "text": "[123] report that their teacher forcing technique radically reduced training time for their recurrent networks, although [76] reports di culties when teacher forcing was used networks with a larger number of hidden units."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 33186947,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "af86162f839480c615c253253348ab535a7fa10c",
            "isKey": false,
            "numCitedBy": 163,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": "We survey learning algorithms for recurrent neural networks with hidden units and attempt to put the various techniques into a common framework. We discuss fixpoint learning algorithms, namely recurrent backpropagation and deterministic Boltzmann Machines, and non-fixpoint algorithms, namely backpropagation through time, Elman's history cutoff nets, and Jordan's output feedback architecture. Forward propagation, an online technique that uses adjoint equations, is also discussed. In many cases, the unified presentation leads to generalizations of various sorts. Some simulations are presented, and at the end, issues of computational complexity are addressed."
            },
            "slug": "Dynamic-recurrent-neural-networks-Pearlmutter",
            "title": {
                "fragments": [],
                "text": "Dynamic recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "This work surveys learning algorithms for recurrent neural networks with hidden units and attempts to put the various techniques into a common framework, resulting in a unified presentation that leads to generalizations of various sorts."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2003303"
                        ],
                        "name": "M. Gherrity",
                        "slug": "M.-Gherrity",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Gherrity",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gherrity"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 28051649,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "006c42929dcd480490fdb367fd7478b2956dbc99",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "A learning algorithm for recurrent neural networks is derived. This algorithm allows a network to learn specified trajectories in state space in response to various input sequences. The network dynamics are described by a system of coupled differential equations that specify the continuous change of the unit activities and weights over time. The algorithm is nonlocal, in that a change in the connection weight between two units may depend on the values for some of the weights between different units. However, the operation of a learned network (fixed weights) is local. If the network units are specified to behave like electronic amplifiers, then an analog implementation of the learned network is straightforward. An example demonstrates the use of the algorithm in a completely connected network of four units. The network creates a limit cycle attractor in order to perform the specified task.<<ETX>>"
            },
            "slug": "A-learning-algorithm-for-analog,-fully-recurrent-Gherrity",
            "title": {
                "fragments": [],
                "text": "A learning algorithm for analog, fully recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A learning algorithm for recurrent neural networks is derived that allows a network to learn specified trajectories in state space in response to various input sequences."
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47055692"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 117
                            }
                        ],
                        "text": "As usual in backpropagation, let us de ne ~ zi(t) = @+E @~ yi(t) (20) where the @+ denotes the ordered derivative of [97], with variables ordered here by time and not unit index."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 205118721,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "266e07d0dd9a75b61e3632e9469993dbaf063f1c",
            "isKey": false,
            "numCitedBy": 882,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Generalization-of-backpropagation-with-application-Werbos",
            "title": {
                "fragments": [],
                "text": "Generalization of backpropagation with application to a recurrent gas market model"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1840458369"
                        ],
                        "name": "Yan Fang",
                        "slug": "Yan-Fang",
                        "structuredName": {
                            "firstName": "Yan",
                            "lastName": "Fang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yan Fang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13337396,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3a0de0ad4bf796e2506080d508f83205cf8d76fc",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The backpropagation learning algorithm for feedforward networks (Rumelhart et al. 1986) has recently been generalized to recurrent networks (Pineda 1989). The algorithm has been further generalized by Pearlmutter (1989) to recurrent networks that produce time-dependent trajectories. The latter method requires much more training time than the feedforward or static recurrent algorithms. Furthermore, the learning can be unstable and the asymptotic accuracy unacceptable for some problems. In this note, we report a modification of the delta weight update rule that significantly improves both the performance and the speed of the original Pearlmutter learning algorithm. Our modified updating rule, a variation on that originally proposed by Jacobs (1988), allows adaptable independent learning rates for individual parameters in the algorithm. The update rule for the ith weight, wi, is given by the delta-bar-delta rule: with the change in learning rate ~~(t) on each epoch given by if &(t l)&(t) > 0 if &(t - l)&(t) < 0"
            },
            "slug": "Faster-Learning-for-Dynamic-Recurrent-Fang-Sejnowski",
            "title": {
                "fragments": [],
                "text": "Faster Learning for Dynamic Recurrent Backpropagation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A modification of the delta weight update rule is reported that significantly improves both the performance and the speed of the original Pearlmutter learning algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "92409901"
                        ],
                        "name": "T. Karjala",
                        "slug": "T.-Karjala",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Karjala",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Karjala"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2294138"
                        ],
                        "name": "D. Himmelblau",
                        "slug": "D.-Himmelblau",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Himmelblau",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Himmelblau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686788"
                        ],
                        "name": "R. Miikkulainen",
                        "slug": "R.-Miikkulainen",
                        "structuredName": {
                            "firstName": "Risto",
                            "lastName": "Miikkulainen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Miikkulainen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62602026,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "976e3bb77b343d08f68063be5db2c1352458dbb1",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Nonlinear programming techniques are used to train Elman-type simple recurrent neural networks to reconcile simulated measurements for a simple dynamic system (a draining tank). The networks are trained in a batch mode using the BFGS quasi-Newton nonlinear optimization algorithm. This makes it possible to avoid the trial and error associated with tuning the learning rate and momentum terms required in the various backpropagation algorithms. The random measurement errors used are uncorrelated and purely Gaussian in nature. Noisy data are used for both training the networks and testing network performance. Recurrent Elman networks are able to significantly reduce the noise level in the process measurements without explicit knowledge of the nonlinear dynamics of the system.<<ETX>>"
            },
            "slug": "Data-rectification-using-recurrent-(Elman)-neural-Karjala-Himmelblau",
            "title": {
                "fragments": [],
                "text": "Data rectification using recurrent (Elman) neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Nonlinear programming techniques are used to train Elman-type simple recurrent neural networks to reconcile simulated measurements for a simple dynamic system (a draining tank)."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings 1992] IJCNN International Joint Conference on Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700974"
                        ],
                        "name": "Barak A. Pearlmutter",
                        "slug": "Barak-A.-Pearlmutter",
                        "structuredName": {
                            "firstName": "Barak",
                            "lastName": "Pearlmutter",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barak A. Pearlmutter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 88
                            }
                        ],
                        "text": "Here, we will derive the continuous time version of backpropagation through time, as in [96], and use it in two toy domains."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 97
                            }
                        ],
                        "text": "The rst is the obvious extension of backpropagation through time (BPTT) to continuous time [95], [96], [62]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16813485,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "34468c0aa95a7aea212d8738ab899a69b2fc14c6",
            "isKey": false,
            "numCitedBy": 744,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Many neural network learning procedures compute gradients of the errors on the output layer of units after they have settled to their final values. We describe a procedure for finding E/wij, where E is an error functional of the temporal trajectory of the states of a continuous recurrent network and wij are the weights of that network. Computing these quantities allows one to perform gradient descent in the weights to minimize E. Simulations in which networks are taught to move through limit cycles are shown. This type of recurrent network seems particularly suited for temporally continuous domains, such as signal processing, control, and speech."
            },
            "slug": "Learning-State-Space-Trajectories-in-Recurrent-Pearlmutter",
            "title": {
                "fragments": [],
                "text": "Learning State Space Trajectories in Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A procedure for finding E/wij, where E is an error functional of the temporal trajectory of the states of a continuous recurrent network and wij are the weights of that network, which seems particularly suited for temporally continuous domains."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895771"
                        ],
                        "name": "D. Zipser",
                        "slug": "D.-Zipser",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zipser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zipser"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18860367,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cccd3fd7a45e7643f26391bd539ffbede0690f36",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent connectionist networks are important because they can perform temporally extended tasks, giving them considerable power beyond the static mappings performed by the now-familiar multilayer feedforward networks. This ability to perform highly nonlinear dynamic mappings makes these networks particularly interesting to study and potentially quite useful in tasks which have an important temporal component not easily handled through the use of simple tapped delay lines. Some examples are tasks involving recognition or generation of sequential patterns and sensorimotor control. This report examines a number of learning procedures for adjusting the weights in recurrent networks in order to train such networks to produce desired temporal behaviors from input-output stream examples. The procedures are all based on the computation of the gradient of performance error with respect to network weights, and a number of strategies for computing the necessary gradient information are described. Included here are approaches which are familiar and have been rst described elsewhere, along with several novel approaches. One particular purpose of this report is to provide uniform and detailed descriptions and derivations of the various techniques in order to emphasize how they relate to one another. Another important contribution of this report is a detailed analysis of the computational requirements of the various approaches discussed."
            },
            "slug": "Gradient-based-learning-algorithms-for-recurrent-Williams-Zipser",
            "title": {
                "fragments": [],
                "text": "Gradient-based learning algorithms for recurrent connectionist networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This report examines a number of learning procedures for adjusting the weights in recurrent networks in order to train such networks to produce desired temporal behaviors from input-output stream examples."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110349627"
                        ],
                        "name": "R.J. Williams",
                        "slug": "R.J.-Williams",
                        "structuredName": {
                            "firstName": "R.J.",
                            "lastName": "Williams",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R.J. Williams"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 7007887,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54e34d0053b71d78cec26e8c29f57a3b9e85de49",
            "isKey": false,
            "numCitedBy": 124,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "The author describes some relationships between the extended Kalman filter (EKF) as applied to recurrent net learning and some simpler techniques that are more widely used. In particular, making certain simplifications to the EKF gives rise to an algorithm essentially identical to the real-time recurrent learning (RTRL) algorithm. Since the EKF involves adjusting unit activity in the network, it also provides a principled generalization of the teacher forcing technique. Preliminary simulation experiments on simple finite-state Boolean tasks indicated that the EKF can provide substantial speed-up in number of time steps required for training on such problems when compared with simpler online gradient algorithms. The computational requirements of the EKF are steep, but scale with network size at the same rate as RTRL.<<ETX>>"
            },
            "slug": "Training-recurrent-networks-using-the-extended-Williams",
            "title": {
                "fragments": [],
                "text": "Training recurrent networks using the extended Kalman filter"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "The author describes some relationships between the extended Kalman filter (EKF) as applied to recurrent net learning and some simpler techniques that are more widely used, and gives rise to an algorithm essentially identical to the real-time recurrent learning (RTRL) algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings 1992] IJCNN International Joint Conference on Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895771"
                        ],
                        "name": "D. Zipser",
                        "slug": "D.-Zipser",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zipser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zipser"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 334,
                                "start": 329
                            }
                        ],
                        "text": "Real Time Recurrent Learning An online, exact, and stable, but computationally expensive, procedure for determining the derivatives of functions of the states of a dynamic system with respect to that system's internal parameters has been discovered and applied to recurrent neural networks a number of times [100], [101], [102], [103]; for reviews see also [81], [76], [104]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14711886,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce9a21b93ba29d4145a8ef6bf401e77f261848de",
            "isKey": false,
            "numCitedBy": 3832,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks. These algorithms have (1) the advantage that they do not require a precisely defined training interval, operating while the network runs; and (2) the disadvantage that they require nonlocal communication in the network being trained and are computationally expensive. These algorithms allow networks having recurrent connections to learn complex tasks that require the retention of information over time periods having either fixed or indefinite length."
            },
            "slug": "A-Learning-Algorithm-for-Continually-Running-Fully-Williams-Zipser",
            "title": {
                "fragments": [],
                "text": "A Learning Algorithm for Continually Running Fully Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109535043"
                        ],
                        "name": "Sreerupa Das",
                        "slug": "Sreerupa-Das",
                        "structuredName": {
                            "firstName": "Sreerupa",
                            "lastName": "Das",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sreerupa Das"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144473519"
                        ],
                        "name": "M. Mozer",
                        "slug": "M.-Mozer",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Mozer",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mozer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15995549,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2478b7c665d8940bfaa0a5b58baba756d6e65df4",
            "isKey": false,
            "numCitedBy": 86,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Although recurrent neural nets have been moderately successful in learning to emulate finite-state machines (FSMs), the continuous internal state dynamics of a neural net are not well matched to the discrete behavior of an FSM. We describe an architecture, called DOLCE, that allows discrete states to evolve in a net as learning progresses. DOLCE consists of a standard recurrent neural net trained by gradient descent and an adaptive clustering technique that quantizes the state space. DOLCE is based on the assumption that a finite set of discrete internal states is required for the task, and that the actual network state belongs to this set but has been corrupted by noise due to inaccuracy in the weights. DOLCE learns to recover the discrete state with maximum a posteriori probability from the noisy state. Simulations show that DOLCE leads to a significant improvement in generalization performance over earlier neural net approaches to FSM induction."
            },
            "slug": "A-Unified-Gradient-Descent/Clustering-Architecture-Das-Mozer",
            "title": {
                "fragments": [],
                "text": "A Unified Gradient-Descent/Clustering Architecture for Finite State Machine Induction"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "DOLCE is a architecture that allows discrete states to evolve in a net as learning progresses and leads to a significant improvement in generalization performance over earlier neural net approaches to FSM induction."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47918185"
                        ],
                        "name": "Jing Peng",
                        "slug": "Jing-Peng",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Peng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Peng"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 12979634,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "26bc0449360d7016f684eafae5b5d2feded32041",
            "isKey": false,
            "numCitedBy": 634,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel variant of the familiar backpropagation-through-time approach to training recurrent networks is described. This algorithm is intended to be used on arbitrary recurrent networks that run continually without ever being reset to an initial state, and it is specifically designed for computationally efficient computer implementation. This algorithm can be viewed as a cross between epochwise backpropagation through time, which is not appropriate for continually running networks, and the widely used on-line gradient approximation technique of truncated backpropagation through time."
            },
            "slug": "An-Efficient-Gradient-Based-Algorithm-for-On-Line-Williams-Peng",
            "title": {
                "fragments": [],
                "text": "An Efficient Gradient-Based Algorithm for On-Line Training of Recurrent Network Trajectories"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A novel variant of the familiar backpropagation-through-time approach to training recurrent networks is described, intended to be used on arbitrary recurrent networks that run continually without ever being reset to an initial state."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47055692"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "The rst is the obvious extension of backpropagation through time (BPTT) to continuous time [95], [96], [62]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18470994,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "1a3d22599028a05669e884f3eaf19a342e190a87",
            "isKey": false,
            "numCitedBy": 4036,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Backpropagation is now the most widely used tool in the field of artificial neural networks. At the core of backpropagation is a method for calculating derivatives exactly and efficiently in any large system made up of elementary subsystems or calculations which are represented by known, differentiable functions; thus, backpropagation has many applications which do not involve neural networks as such. This paper first reviews basic backpropagation, a simple method which is now being widely used in areas like pattern recognition and fault diagnosis. Next, it presents the basic equations for backpropagation through time, and discusses applications to areas like pattern recognition involving dynamic systems, systems identification, and control. Finally, i t describes further extensions of this method, to deal with systems other than neural networks, systems involving simultaneous equations or true recurrent networks, and other practical issues which arise with this method. Pseudocode is provided to clarify the algorithms. The chain rule for ordered derivatives-the theorem which underlies backpropagation-is briefly discussed."
            },
            "slug": "Backpropagation-Through-Time:-What-It-Does-and-How-Werbos",
            "title": {
                "fragments": [],
                "text": "Backpropagation Through Time: What It Does and How to Do It"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper first reviews basic backpropagation, a simple method which is now being widely used in areas like pattern recognition and fault diagnosis, and describes further extensions of this method, to deal with systems other than neural networks, systems involving simultaneous equations or true recurrent networks, and other practical issues which arise with this method."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105353"
                        ],
                        "name": "K. Narendra",
                        "slug": "K.-Narendra",
                        "structuredName": {
                            "firstName": "Kumpati",
                            "lastName": "Narendra",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Narendra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145245016"
                        ],
                        "name": "K. Parthasarathy",
                        "slug": "K.-Parthasarathy",
                        "structuredName": {
                            "firstName": "Kannan",
                            "lastName": "Parthasarathy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Parthasarathy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 374,
                                "start": 369
                            }
                        ],
                        "text": "Real Time Recurrent Learning An online, exact, and stable, but computationally expensive, procedure for determining the derivatives of functions of the states of a dynamic system with respect to that system's internal parameters has been discovered and applied to recurrent neural networks a number of times [100], [101], [102], [103]; for reviews see also [81], [76], [104]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9267259,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "be36852f284e19f2c512acb296dcb713ac5c01e7",
            "isKey": false,
            "numCitedBy": 665,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "An extension of the backpropagation method, termed dynamic backpropagation, which can be applied in a straightforward manner for the optimization of the weights (parameters) of multilayer neural networks is discussed. The method is based on the fact that gradient methods used in linear dynamical systems can be combined with backpropagation methods for neural networks to obtain the gradient of a performance index of nonlinear dynamical systems. The method can be applied to any complex system which can be expressed as the interconnection of linear dynamical systems and multilayer neural networks. To facilitate the practical implementation of the proposed method, emphasis is placed on the diagrammatic representation of the system which generates the gradient of the performance function."
            },
            "slug": "Gradient-methods-for-the-optimization-of-dynamical-Narendra-Parthasarathy",
            "title": {
                "fragments": [],
                "text": "Gradient methods for the optimization of dynamical systems containing neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An extension of the backpropagation method, termed dynamic back Propagation, which can be applied in a straightforward manner for the optimization of the weights (parameters) of multilayer neural networks is discussed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38836409"
                        ],
                        "name": "S. P. Day",
                        "slug": "S.-P.-Day",
                        "structuredName": {
                            "firstName": "Shawn",
                            "lastName": "Day",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. P. Day"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32247107"
                        ],
                        "name": "M. Davenport",
                        "slug": "M.-Davenport",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Davenport",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Davenport"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 150
                            }
                        ],
                        "text": "A continuous time feedforward network with learned time delays was successfully applied to a di cult time-series prediction task by Day and Davenport [25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 347,
                                "start": 343
                            }
                        ],
                        "text": "There are continuous-time feed-forward learning algorithms that are as e cient in both time and space as algorithms for pure feedforward networks, but are applicable only when w is upper-triangular but not necessarily zero-diagonal, in other words, when the network is feedforward except for recurrent self-connections [77], [78], [79], [80], [25] or for a survey, [81]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 363,
                                "start": 359
                            }
                        ],
                        "text": "It should be noted by engineers that many real-world problems which one might think would require recurrent architectures for their solution turn out to be solvable with feedforward architectures, sometimes augmented with preprocessed inputs such as tapped delay lines, and various other architectural embellishments [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [10], [32], [33], [34], [35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11164470,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "26d2cf8425a63a9b9d82fba19fffda5a4b38dc40",
            "isKey": false,
            "numCitedBy": 104,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Backpropagation is extended to continuous-time feedforward networks with internal, adaptable time delays. The new technique is suitable for parallel hardware implementation, with continuous multidimensional training signals. The resulting networks can be used for signal prediction, signal production, and spatiotemporal pattern recognition tasks. Unlike conventional backpropagation networks, they can easily adapt while performing true signal prediction. Simulation results are presented for networks trained to predict future values of the Mackey-Glass chaotic signal, using its present value as an input. For this application, networks with adaptable delays had less than half the prediction error of networks with fixed delays, and about one-quarter the error of conventional networks. After training, the network can be operated in a signal production configuration, where it autonomously generates a close approximation to the Mackey-Glass signal."
            },
            "slug": "Continuous-time-temporal-back-propagation-with-time-Day-Davenport",
            "title": {
                "fragments": [],
                "text": "Continuous-time temporal back-propagation with adaptable time delays"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "Backpropagation is extended to continuous-time feedforward networks with internal, adaptable time delays, suitable for parallel hardware implementation, with continuous multidimensional training signals."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2797623"
                        ],
                        "name": "H. Siegelmann",
                        "slug": "H.-Siegelmann",
                        "structuredName": {
                            "firstName": "Hava",
                            "lastName": "Siegelmann",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Siegelmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790264"
                        ],
                        "name": "Eduardo Sontag",
                        "slug": "Eduardo-Sontag",
                        "structuredName": {
                            "firstName": "Eduardo",
                            "lastName": "Sontag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eduardo Sontag"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2456483,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9e905dffcd67efb4b1f049ac764cc25d0069820c",
            "isKey": false,
            "numCitedBy": 390,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors pursue a particular approach to analog computation, based on dynamical systems of the type used in neural networks research. The systems have a fixed structure, invariant in time, corresponding to an unchanging number of 'neurons'. If allowed exponential time for computation, they turn out to have unbounded power. However, under polynomial-time constraints there are limits on their capabilities, though being more powerful than Turing machines. These networks are not likely to solve polynomially-NP-hard problems, as the equality 'P=NP' implies the almost complete collapse of the standard polynomial hierarchy. In contrast to classical computational models, the models studied exhibit at least some robustness with respect to noise and implementation errors.<<ETX>>"
            },
            "slug": "Analog-computation-via-neural-networks-Siegelmann-Sontag",
            "title": {
                "fragments": [],
                "text": "Analog Computation via Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "The authors pursue a particular approach to analog computation, based on dynamical systems of the type used in neural networks research, which exhibit at least some robustness with respect to noise and implementation errors."
            },
            "venue": {
                "fragments": [],
                "text": "Theor. Comput. Sci."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 150
                            }
                        ],
                        "text": "Similarly, learning of multiscale phenomena, which again typically consists of larger systems containing recurrent networks as components [52], [53], [54], [55], will not be discussed."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18271205,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "50c770b425a5bb25c77387f687a9910a9d130722",
            "isKey": false,
            "numCitedBy": 428,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous neural network learning algorithms for sequence processing are computationally expensive and perform poorly when it comes to long time lags. This paper first introduces a simple principle for reducing the descriptions of event sequences without loss of information. A consequence of this principle is that only unexpected inputs can be relevant. This insight leads to the construction of neural architectures that learn to divide and conquer by recursively decomposing sequences. I describe two architectures. The first functions as a self-organizing multilevel hierarchy of recurrent networks. The second, involving only two recurrent networks, tries to collapse a multilevel predictor hierarchy into a single recurrent net. Experiments show that the system can require less computation per time step and many fewer training sequences than conventional training algorithms for recurrent nets."
            },
            "slug": "Learning-Complex,-Extended-Sequences-Using-the-of-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Learning Complex, Extended Sequences Using the Principle of History Compression"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A simple principle for reducing the descriptions of event sequences without loss of information is introduced and this insight leads to the construction of neural architectures that learn to divide and conquer by recursively decomposing sequences."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699108"
                        ],
                        "name": "G. Tesauro",
                        "slug": "G.-Tesauro",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Tesauro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Tesauro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2119287963"
                        ],
                        "name": "Yu He",
                        "slug": "Yu-He",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109973472"
                        ],
                        "name": "Subutai Ahmad",
                        "slug": "Subutai-Ahmad",
                        "structuredName": {
                            "firstName": "Subutai",
                            "lastName": "Ahmad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Subutai Ahmad"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 45075765,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4cc5683d13b6de2b9d9fb22cbc949a40dd1ef008",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We calculate analytically the rate of convergence at long times in the backpropagation learning algorithm for networks with and without hidden units. For networks without hidden units using the standard quadratic error function and a sigmoidal transfer function, we find that the error decreases as 1/t for large t, and the output states approach their target values as 1/t. It is possible to obtain a different convergence rate for certain error and transfer functions, but the convergence can never be faster than 1/t. These results are unaffected by a momentum term in the learning algorithm, but convergence can be substantially improved by an adaptive learning rate scheme. For networks with hidden units, we generally expect the same rate of convergence to be obtained as in the single-layer case; however, under certain circumstances one can obtain a polynomial speed-up for non sigmoidal units, or a logarithmic speed-up for sigmoidal units. Our analytic results are confirmed by empirical measurements of the convergence rate in numerical simulations."
            },
            "slug": "Asymptotic-Convergence-of-Backpropagation-Tesauro-He",
            "title": {
                "fragments": [],
                "text": "Asymptotic Convergence of Backpropagation"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "Analytically the rate of convergence at long times in the backpropagation learning algorithm for networks with and without hidden units is calculated and it is found that the error decreases as 1/t for large t, and the output states approach their target values as1/t."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1964787"
                        ],
                        "name": "Jean-Pierre Raysz",
                        "slug": "Jean-Pierre-Raysz",
                        "structuredName": {
                            "firstName": "Jean-Pierre",
                            "lastName": "Raysz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Pierre Raysz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2080632378"
                        ],
                        "name": "B. Victorri",
                        "slug": "B.-Victorri",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Victorri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Victorri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5128684,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9375b666b2bdedca71dbecc37a4cfd73005e2f08",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Fully recurrent (asymmetrical) networks can be thought of as dynamic systems. The dynamics can be shaped to perform content addressable memories, recognize sequences, or generate trajectories. Unfortunately several problems can arise: First, the convergence in the state space is not guaranteed. Second, the learned fixed points or trajectories are not necessarily stable. Finally, there might exist spurious fixed points and/or spurious \"attracting\" trajectories that do not correspond to any patterns. In this paper, we introduce a new energy function that presents solutions to all of these problems. We present an efficient gradient descent algorithm which directly acts on the stability of the fixed points and trajectories and on the size and shape of the corresponding basin and valley of attraction. The results are illustrated by the simulation of a small content addressable memory."
            },
            "slug": "Shaping-the-State-Space-Landscape-in-Recurrent-Simard-Raysz",
            "title": {
                "fragments": [],
                "text": "Shaping the State Space Landscape in Recurrent Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An efficient gradient descent algorithm is presented which directly acts on the stability of the fixed points and trajectories and on the size and shape of the corresponding basin and valley of attraction."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34922532"
                        ],
                        "name": "Guo-Zheng Sun",
                        "slug": "Guo-Zheng-Sun",
                        "structuredName": {
                            "firstName": "Guo-Zheng",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guo-Zheng Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115401300"
                        ],
                        "name": "Hsing-Hen Chen",
                        "slug": "Hsing-Hen-Chen",
                        "structuredName": {
                            "firstName": "Hsing-Hen",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hsing-Hen Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2552960"
                        ],
                        "name": "Yee-Chun Lee",
                        "slug": "Yee-Chun-Lee",
                        "structuredName": {
                            "firstName": "Yee-Chun",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yee-Chun Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6994714,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b15938351b87946418428abcb0707824d24ba7d5",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The two well known learning algorithms of recurrent neural networks are the back-propagation (Rumelhart & et. al., Werbos) and the forward propagation (Williams and Zipser). The main drawback of back-propagation is its off-line backward path in time for error cumulation. This violates the on-line requirement in many practical applications. Although the forward propagation algorithm can be used in an on-line manner, the annoying drawback is the heavy computation load required to update the high dimensional sensitivity matrix (O(N4) operations for each time step). Therefore, to develop a fast forward algorithm is a challenging task. In this paper we proposed a forward learning algorithm which is one order faster (only O(N3) operations for each time step) than the sensitivity matrix algorithm. The basic idea is that instead of integrating the high dimensional sensitivity dynamic equation we solve forward in time for its Green's function to avoid the redundant computations, and then update the weights whenever the error is to be corrected. \n \nA Numerical example for classifying state trajectories using a recurrent network is presented. It substantiated the faster speed of the proposed algorithm than the Williams and Zipser's algorithm."
            },
            "slug": "Green's-Function-Method-for-Fast-On-Line-Learning-Sun-Chen",
            "title": {
                "fragments": [],
                "text": "Green's Function Method for Fast On-Line Learning Algorithm of Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A forward learning algorithm is proposed which is one order faster (only O(N3) operations for each time step) than the sensitivity matrix algorithm, and instead of integrating the high dimensional sensitivity dynamic equation the authors solve forward in time for its Green's function to avoid the redundant computations."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2020074"
                        ],
                        "name": "A. Lapedes",
                        "slug": "A.-Lapedes",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Lapedes",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lapedes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2542113"
                        ],
                        "name": "R. Farber",
                        "slug": "R.-Farber",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Farber",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Farber"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 36
                            }
                        ],
                        "text": "However, Crutch eld et al. [70] and Lapedes and Farber [71] have had success with the identi cation of chaotic systems using models without hidden state, and there is no reason to believe that learning the dynamics of chaotic systems is more di cult than learning the dynamics of non-chaotic ones."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "[70] and Lapedes and Farber [71] have had success with the identi cation of chaotic systems using models without hidden state, and there is no reason to believe that learning the dynamics of chaotic systems is more di cult than learning the dynamics of non-chaotic ones."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60720876,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d981c7637fc39335cf53cfa792a0f8d5b66ec6e",
            "isKey": false,
            "numCitedBy": 626,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The backpropagation learning algorithm for neural networks is developed into a formalism for nonlinear signal processing. We illustrate the method by selecting two common topics in signal processing, prediction and system modelling, and show that nonlinear applications can be handled extremely well by using neural networks. The formalism is a natural, nonlinear extension of the linear Least Mean Squares algorithm commonly used in adaptive signal processing. Simulations are presented that document the additional performance achieved by using nonlinear neural networks. First, we demonstrate that the formalism may be used to predict points in a highly chaotic time series with orders of magnitude increase in accuracy over conventional methods including the Linear Predictive Method and the Gabor-Volterra-Weiner Polynomial Method. Deterministic chaos is thought to be involved in many physical situations including the onset of turbulence in fluids, chemical reactions and plasma physics. Secondly, we demonstrate the use of the formalism in nonlinear system modelling by providing a graphic example in which it is clear that the neural network has accurately modelled the nonlinear transfer function. It is interesting to note that the formalism provides explicit, analytic, global, approximations to the nonlinear maps underlying the various time series. Furthermore, the neural net more\u00a0\u00bb seems to be extremely parsimonious in its requirements for data points from the time series. We show that the neural net is able to perform well because it globally approximates the relevant maps by performing a kind of generalized mode decomposition of the maps. 24 refs., 13 figs. \u00ab\u00a0less"
            },
            "slug": "Nonlinear-signal-processing-using-neural-networks:-Lapedes-Farber",
            "title": {
                "fragments": [],
                "text": "Nonlinear signal processing using neural networks: Prediction and system modelling"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "It is demonstrated that the backpropagation learning algorithm for neural networks may be used to predict points in a highly chaotic time series with orders of magnitude increase in accuracy over conventional methods including the Linear Predictive Method and the Gabor-Volterra-Weiner Polynomial Method."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144902513"
                        ],
                        "name": "P. Baldi",
                        "slug": "P.-Baldi",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Baldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Baldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30320952"
                        ],
                        "name": "F. Pineda",
                        "slug": "F.-Pineda",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pineda",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Pineda"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14926573,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9b7861d28f653ead3e02f1ae5c07540b2d07346d",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The concept of Contrastive Learning (CL) is developed as a family of possible learning algorithms for neural networks. CL is an extension of Deterministic Boltzmann Machines to more general dynamical systems. During learning, the network oscillates between two phases. One phase has a teacher signal and one phase has no teacher signal. The weights are updated using a learning rule that corresponds to gradient descent on a contrast function that measures the discrepancy between the free network and the network with a teacher signal. The CL approach provides a general unified framework for developing new learning algorithms. It also shows that many different types of clamping and teacher signals are possible. Several examples are given and an analysis of the landscape of the contrast function is proposed with some relevant predictions for the CL curves. An approach that may be suitable for collective analog implementations is described. Simulation results and possible extensions are briefly discussed together with a new conjecture regarding the function of certain oscillations in the brain. In the appendix, we also examine two extensions of contrastive learning to time-dependent trajectories."
            },
            "slug": "Contrastive-Learning-and-Neural-Oscillations-Baldi-Pineda",
            "title": {
                "fragments": [],
                "text": "Contrastive Learning and Neural Oscillations"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The CL approach provides a general unified framework for developing new learning algorithms and shows that many different types of clamping and teacher signals are possible, as well as examining two extensions of contrastive learning to time-dependent trajectories."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1822055"
                        ],
                        "name": "Raymond L. Watrous",
                        "slug": "Raymond-L.-Watrous",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Watrous",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raymond L. Watrous"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15329984,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "934e49dac717a924bfda841bf6e54c32e900f0d1",
            "isKey": false,
            "numCitedBy": 294,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of learning using connectionist networks, in which network connection strengths are modified systematically so that the response of the network increasingly approximates the desired response can be structured as an optimization problem. The widely used back propagation method of connectionist learning [19, 21, 18] is set in the context of nonlinear optimization. In this framework, the issues of stability, convergence and parallelism are considered. As a form of gradient descent with fixed step size, back propagation is known to be unstable, which is illustrated using Rosenbrock's function. This is contrasted with stable methods which involve a line search in the gradient direction. The convergence criterion for connectionist problems involving binary functions is discussed relative to the behavior of gradient descent in the vicinity of local minima. A minimax criterion is compared with the least squares criterion. The contribution of the momentum term [19, 18] to more rapid convergence is interpreted relative to the geometry of the weight space. It is shown that in plateau regions of relatively constant gradient, the momentum term acts to increase the step size by a factor of 1/1-\u03bc, where \u03bc is the momentum term. In valley regions with steep sides, the momentum constant acts to focus the search direction toward the local minimum by averaging oscillations in the gradient. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-88-62. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/597 LEARNING ALGORITHMS FOR CONNECTIONIST NETWORKS: APPLIED GRADIENT METHODS OF NONLINEAR OPTIMIZATION"
            },
            "slug": "Learning-Algorithms-for-Connectionist-Networks:-of-Watrous",
            "title": {
                "fragments": [],
                "text": "Learning Algorithms for Connectionist Networks: Applied Gradient Methods of Nonlinear Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that in plateau regions of relatively constant gradient, the momentum term acts to increase the step size by a factor of 1/1-\u03bc, where \u03bc is the momentumTerm, and in valley regions with steep sides,The momentum constant acts to focus the search direction toward the local minimum by averaging oscillations in the gradient."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48429353"
                        ],
                        "name": "Pineda",
                        "slug": "Pineda",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Pineda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pineda"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "There are xedpoint learning algorithms (for details see [72], [73], [74], [75], or for a survey see [76]) 1Typically ( ) = (1+e ) 1, in which case 0( ) = ( )(1 ( )), or the scaled ( ) = tanh( ), in which case 0( ) = (1 + ( ))(1 ( )) = 1 2( )."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 30
                            }
                        ],
                        "text": "It was shown independently by Pineda [72] and Alemeida [73] that the error backpropagation algorithm [61], [59], [60] is a special case of a more general error gradient computation procedure."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 63
                            }
                        ],
                        "text": "Recurrent Backpropagation It was shown independently by Pineda [72] and Alemeida [73] that the error backpropagation algorithm [61], [59], [60] is a special case of a more general error gradient computation procedure."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 29
                            }
                        ],
                        "text": "[9] successfully applied the [72], [73] recurrent backprop3A unit with external input could be pushed outside the [0,1] bounds of the range of the ( ) used."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 40994937,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6602985bd326d9996c68627b56ed389e2c90fd08",
            "isKey": true,
            "numCitedBy": 905,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "An adaptive neural network with asymmetric connections is introduced. This network is related to the Hopfield network with graded neurons and uses a recurrent generalization of the \\ensuremath{\\delta} rule of Rumelhart, Hinton, and Williams to modify adaptively the synaptic weights. The new network bears a resemblance to the master/slave network of Lapedes and Farber but it is architecturally simpler."
            },
            "slug": "Generalization-of-back-propagation-to-recurrent-Pineda",
            "title": {
                "fragments": [],
                "text": "Generalization of back-propagation to recurrent neural networks."
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "An adaptive neural network with asymmetric connections is introduced that bears a resemblance to the master/slave network of Lapedes and Farber but it is architecturally simpler."
            },
            "venue": {
                "fragments": [],
                "text": "Physical review letters"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1848627"
                        ],
                        "name": "U. Bodenhausen",
                        "slug": "U.-Bodenhausen",
                        "structuredName": {
                            "firstName": "Ulrich",
                            "lastName": "Bodenhausen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Bodenhausen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 2228811,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e11112bbafca470c1b93764c879fa47bfbadbb5f",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The Tempo-Network is an artificial neural network with both adaptive weights and adaptive time delays. U Bodenhausen (see ibid., vol.1, p.597-600, 1990) showed that the network is able to work as a autoassociator for sequences of patterns which are fed into the network one after the other. The patterns of the sequence are stepped through in time. The learning rules and the internal representations in the hidden layer are discussed. Simulations with the Tempo-Network are discussed. Neural networks with adaptive time delays seem to be interesting for tasks where the sequence contains nonadjacent events"
            },
            "slug": "Learning-internal-representations-of-pattern-in-a-Bodenhausen",
            "title": {
                "fragments": [],
                "text": "Learning internal representations of pattern sequences in a neural network with adaptive time-delays"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "The Tempo-Network is an artificial neural network with both adaptive weights and adaptive time delays that is able to work as a autoassociator for sequences of patterns which are fed into the network one after the other."
            },
            "venue": {
                "fragments": [],
                "text": "1990 IJCNN International Joint Conference on Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14426348,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f7c4048a03281e976f28d35c2f9fef3a58346e6",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Do you want your neural net algorithm to learn sequences? Do not limit yourself to conventional gradient descent (or approximations thereof). Instead, use your sequence learning algorithm (any will do) to implement the following method for history compression. No matter what your final goals are, train a network to predict its next input from the previous ones. Since only unpredictable inputs convey new information, ignore all predictable inputs but let all unexpected inputs (plus information about the time step at which they occurred) become inputs to a higher-level network of the same kind (working on a slower, self-adjusting time scale). Go on building a hierarchy of such networks. This principle reduces the descriptions of event sequences without loss of information, thus easing supervised or reinforcement learning tasks. Alternatively, you may use two recurrent networks to collapse a multi-level predictor hierarchy into a single recurrent net. Experiments show that systems based on these principles can require less computation per time step and many fewer training sequences than conventional training algorithms for recurrent nets. Finally you can modify the above method such that predictability is not defined in a yes-or-no fashion but in a continuous fashion."
            },
            "slug": "Learning-Unambiguous-Reduced-Sequence-Descriptions-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Learning Unambiguous Reduced Sequence Descriptions"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Experiments show that systems based on these principles can require less computation per time step and many fewer training sequences than conventional training algorithms for recurrent nets."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157784"
                        ],
                        "name": "C. Lee Giles",
                        "slug": "C.-Lee-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee Giles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153170210"
                        ],
                        "name": "Clifford B. Miller",
                        "slug": "Clifford-B.-Miller",
                        "structuredName": {
                            "firstName": "Clifford",
                            "lastName": "Miller",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Clifford B. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2158193072"
                        ],
                        "name": "Dong Chen",
                        "slug": "Dong-Chen",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34922532"
                        ],
                        "name": "Guo-Zheng Sun",
                        "slug": "Guo-Zheng-Sun",
                        "structuredName": {
                            "firstName": "Guo-Zheng",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guo-Zheng Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115401300"
                        ],
                        "name": "Hsing-Hen Chen",
                        "slug": "Hsing-Hen-Chen",
                        "structuredName": {
                            "firstName": "Hsing-Hen",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hsing-Hen Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2552960"
                        ],
                        "name": "Yee-Chun Lee",
                        "slug": "Yee-Chun-Lee",
                        "structuredName": {
                            "firstName": "Yee-Chun",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yee-Chun Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2543653,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8bdba34dbd4940cafff419cf6430d03d79f21231",
            "isKey": false,
            "numCitedBy": 109,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Simple second-order recurrent networks are shown to readily learn small known regular grammars when trained with positive and negative strings examples. We show that similar methods are appropriate for learning unknown grammars from examples of their strings. The training algorithm is an incremental real-time, recurrent learning (RTRL) method that computes the complete gradient and updates the weights at the end of each string. After or during training, a dynamic clustering algorithm extracts the production rules that the neural network has learned. The methods are illustrated by extracting rules from unknown deterministic regular grammars. For many cases the extracted grammar outperforms the neural net from which it was extracted in correctly classifying unseen strings."
            },
            "slug": "Extracting-and-Learning-an-Unknown-Grammar-with-Giles-Miller",
            "title": {
                "fragments": [],
                "text": "Extracting and Learning an Unknown Grammar with Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "Simple second-order recurrent networks are shown to readily learn small known regular grammars when trained with positive and negative strings examples and it is shown that similar methods are appropriate for learning unknowngrammars from examples of their strings."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2392662"
                        ],
                        "name": "O. Nerrand",
                        "slug": "O.-Nerrand",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Nerrand",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Nerrand"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399659418"
                        ],
                        "name": "P. Roussel-Ragot",
                        "slug": "P.-Roussel-Ragot",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Roussel-Ragot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Roussel-Ragot"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3078169"
                        ],
                        "name": "L. Personnaz",
                        "slug": "L.-Personnaz",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Personnaz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Personnaz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144097910"
                        ],
                        "name": "G. Dreyfus",
                        "slug": "G.-Dreyfus",
                        "structuredName": {
                            "firstName": "G\u00e9rard",
                            "lastName": "Dreyfus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Dreyfus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060680777"
                        ],
                        "name": "S. Marcos",
                        "slug": "S.-Marcos",
                        "structuredName": {
                            "firstName": "Sylvie",
                            "lastName": "Marcos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Marcos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 405,
                                "start": 401
                            }
                        ],
                        "text": "It should be noted by engineers that many real-world problems which one might think would require recurrent architectures for their solution turn out to be solvable with feedforward architectures, sometimes augmented with preprocessed inputs such as tapped delay lines, and various other architectural embellishments [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [10], [32], [33], [34], [35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 317,
                                "start": 313
                            }
                        ],
                        "text": "Although algorithms suitable for building systems of this type are reviewed to some extent below, such as the algorithm used in [9], the bulk of this paper is concerned with the problem of causing networks to exhibit particular desired detailed temporal behavior, which has found application in signal processing [10], [11], speech and language processing [12], [13], [14], and neuroscience [15], [16], [17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1621201,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "065531e12839d10b01b582ff9428d538ceaa6af1",
            "isKey": false,
            "numCitedBy": 199,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper proposes a general framework that encompasses the training of neural networks and the adaptation of filters. We show that neural networks can be considered as general nonlinear filters that can be trained adaptively, that is, that can undergo continual training with a possibly infinite number of time-ordered examples. We introduce the canonical form of a neural network. This canonical form permits a unified presentation of network architectures and of gradient-based training algorithms for both feedforward networks (transversal filters) and feedback networks (recursive filters). We show that several algorithms used classically in linear adaptive filtering, and some algorithms suggested by other authors for training neural networks, are special cases in a general classification of training algorithms for feedback networks."
            },
            "slug": "Neural-Networks-and-Nonlinear-Adaptive-Filtering:-Nerrand-Roussel-Ragot",
            "title": {
                "fragments": [],
                "text": "Neural Networks and Nonlinear Adaptive Filtering: Unifying Concepts and New Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that neural networks can be considered as general nonlinear filters that can be trained adaptively, that is, that can undergo continual training with a possibly infinite number of time-ordered examples."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27092071"
                        ],
                        "name": "B. Baird",
                        "slug": "B.-Baird",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Baird",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Baird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2010432"
                        ],
                        "name": "F. Eeckman",
                        "slug": "F.-Eeckman",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Eeckman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Eeckman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8184337,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2942bccd646603453ba53c07d5091d9bcd644377",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "A simple architecture and algorithm for analytically guaranteed associative memory storage of analog patterns, continuous sequences, and chaotic attractors in the same network is described. A matrix inversion determines network weights, given prototype patterns to be stored. There are N units of capacity in an N node network with 3N2 weights. It costs one unit per static attractor, two per Fourier component of each sequence, and four per chaotic attractor. There are no spurious attractors, and there is a Liapunov function in a special coordinate system which governs the approach of transient states to stored trajectories. Unsupervised or supervised incremental learning algorithms for pattern classification, such as competitive learning or bootstrap Widrow-Hoff can easily be implemented. The architecture can be \"folded\" into a recurrent network with higher order weights that can be used as a model of cortex that stores oscillatory and chaotic attractors by a Hebb rule. Hierarchical sensory-motor control networks may be constructed of interconnected \"cortical patches\" of these network modules. Network performance is being investigated by application to the problem of real time handwritten digit recognition."
            },
            "slug": "CAM-Storage-of-Analog-Patterns-and-Continuous-with-Baird-Eeckman",
            "title": {
                "fragments": [],
                "text": "CAM Storage of Analog Patterns and Continuous Sequences with 3N2 Weights"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A simple architecture and algorithm for analytically guaranteed associative memory storage of analog patterns, continuous sequences, and chaotic attractors in the same network is described and is being investigated by application to the problem of real time handwritten digit recognition."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "There are xedpoint learning algorithms (for details see [72], [73], [74], [75], or for a survey see [76]) 1Typically ( ) = (1+e ) 1, in which case 0( ) = ( )(1 ( )), or the scaled ( ) = tanh( ), in which case 0( ) = (1 + ( ))(1 ( )) = 1 2( )."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 182
                            }
                        ],
                        "text": "Deterministic Boltzmann Machines The Mean Field form of the stochastic Boltzmann Machine learning rule, or MFT BoltzmannMachines, [91] have been shown to descend an error functional [74]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 19929736,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ea4a33a468958de14303daaaba2349d0ed07b73",
            "isKey": false,
            "numCitedBy": 175,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "The Boltzmann machine learning procedure has been successfully applied in deterministic networks of analog units that use a mean field approximation to efficiently simulate a truly stochastic system (Peterson and Anderson 1987). This type of deterministic Boltzmann machine (DBM) learns much faster than the equivalent stochastic Boltzmann machine (SBM), but since the learning procedure for DBM's is only based on an analogy with SBM's, there is no existing proof that it performs gradient descent in any function, and it has only been justified by simulations. By using the appropriate interpretation for the way in which a DBM represents the probability of an output vector given an input vector, it is shown that the DBM performs steepest descent in the same function as the original SBM, except at rare discontinuities. A very simple way of forcing the weights to become symmetrical is also described, and this makes the DBM more biologically plausible than back-propagation (Werbos 1974; Parker 1985; Rumelhart et al. 1986)."
            },
            "slug": "Deterministic-Boltzmann-Learning-Performs-Steepest-Hinton",
            "title": {
                "fragments": [],
                "text": "Deterministic Boltzmann Learning Performs Steepest Descent in Weight-Space"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "By using the appropriate interpretation for the way in which a DBM represents the probability of an output vector given an input vector, it is shown that the DBM performs steepest descent in the same function as the original SBM, except at rare discontinuities."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144473519"
                        ],
                        "name": "M. Mozer",
                        "slug": "M.-Mozer",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Mozer",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mozer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18036435,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7fb4d10f6d2ee3133135958aefd50bf22dcced9d",
            "isKey": false,
            "numCitedBy": 303,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "Time is at the heart of many pattern recognition tasks (e.g., speech recognition). However, connectionist learning algorithms to date are not well-suited for dealing with time-varying input patterns. This chapter introduces a specialized connectionist architecture and corresponding specialization of the back-propagation learning algorithm that operates efficiently, both in computational time and space requirements, on temporal sequences. The key feature of the architecture is a layer of selfconnected hidden units that integrate their current value with the new input at each time step to construct a static representation of the temporal input sequence. This architecture avoids two deficiencies found in the back-propagation unfolding-intime procedure (Rumelhart, Hinton, & Williams, 1986) for handing sequence recognition tasks: first, it reduces the difficulty of temporal credit assignment by focusing the back-propagated error signal; second, it eliminates the need for a buffer to hold the input sequence and/or intermediate activity levels. The latter property is due to the fact that during the forward (activation) phase, incremental activity traces can be locally computed that hold all information necessary for back propagation in time. It is argued that this architecture should scale better than conventional recurrent architectures with respect to sequence length. The architecture has been used to implement a temporal version of Rumelhart and McClelland's (1986) verb past-tense model. The hidden units learn to behave something like Rumelhart and McClelland's \"Wickelphones,\" a rich and flexible representation of temporal information."
            },
            "slug": "A-Focused-Backpropagation-Algorithm-for-Temporal-Mozer",
            "title": {
                "fragments": [],
                "text": "A Focused Backpropagation Algorithm for Temporal Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A specialized connectionist architecture and corre sponding specialization of the backpropagation learnin g algori thm th at opera tes efficiently on temporal sequences is introduced and should scale better than conventional recurrent architectures wit h respect to sequenc e length."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144215175"
                        ],
                        "name": "R. Jacobs",
                        "slug": "R.-Jacobs",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Jacobs",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jacobs"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 77
                            }
                        ],
                        "text": "One technique that has proven useful in this particular situation is that of [134] which was applied by Fang and Sejnowski to the single gure eight problem perturbed in gure 9 with great success by [135]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9947500,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a9ef2995e8e1bd57a74343073219364811c2ace0",
            "isKey": false,
            "numCitedBy": 1988,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Increased-rates-of-convergence-through-learning-Jacobs",
            "title": {
                "fragments": [],
                "text": "Increased rates of convergence through learning rate adaptation"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2819781"
                        ],
                        "name": "P. Rowat",
                        "slug": "P.-Rowat",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Rowat",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Rowat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734225"
                        ],
                        "name": "A. Selverston",
                        "slug": "A.-Selverston",
                        "structuredName": {
                            "firstName": "Allen",
                            "lastName": "Selverston",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Selverston"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15775763,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ca2c86980d5e93975abbd6fbb099a4f766966161",
            "isKey": false,
            "numCitedBy": 101,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the most important problems for studying neural network models is the adjustment of parametas. Here we show how to formulate the problem as the minimization of the dilferslee betwen two limit cycles. The backprop%ation method for leaming algorithms is described .ss the application of grsdient descent to an eror fundion that computes this difference. A mathematical formulation is given that is applicable to any type of network model, and is applied to several models. For example, when learning in a network in which dl cells have a common, adjustable, bias current, the value of the bias ie adjustcd at a rate proportional to the difference between the sum of the target outputs and the sum of the actual outputs. When learning in a network of n cells where a target output is given for every cell. the learning algorithm splits into R indepndent leaming algorithms, one per cell. For networks containing gap junctions, a gap junction is modelled D conductance times the potential difference between the two adjacent cells. The requirement that a conductme g munt be positive is enionzed by replacing g by a functbn pos(g*) whose value is always positive, for example cxp(O.lg*), and deriving an algorithm that adjusts the parameta g* in place of g. When target output is specified for every cell in a network with gap junctions. the learning algorithm splits into fewer independent componeds, one for each gap-mnneaed subset of the network. The lemming algorithm for II gspmnneeted set of cells cannot be paralklized further. As a find example, a leaming algwithm is derived for a mutually inhibitory twc- cell network in which each cell has a membrane current. This generalized approach to backpropagation allows one to derive a learning algorithm for alntoat any model neural network given in tem of differmtid equations. It will be an essential tool for adjusting parmetem in small but complex network models."
            },
            "slug": "Learning-algorithms-for-oscillatory-networks-with-Rowat-Selverston",
            "title": {
                "fragments": [],
                "text": "Learning algorithms for oscillatory networks with gap junctions and membrane currents"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This generalized approach to backpropagation allows one to derive a learning algorithm for alntoat any model neural network given in tem of differmtid equations, and will be an essential tool for adjusting parmetem in small but complex network models."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3302759"
                        ],
                        "name": "D. Albesano",
                        "slug": "D.-Albesano",
                        "structuredName": {
                            "firstName": "Dario",
                            "lastName": "Albesano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Albesano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3185776"
                        ],
                        "name": "R. Gemello",
                        "slug": "R.-Gemello",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Gemello",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gemello"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2560024"
                        ],
                        "name": "F. Mana",
                        "slug": "F.-Mana",
                        "structuredName": {
                            "firstName": "Franco",
                            "lastName": "Mana",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Mana"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60875504,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "48c0c08e6d0aaeee59e333a55ac01b931492b383",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors report a method to directly encode temporal information into a neural network by explicitly modeling that information with a left-to-right automaton, and teaching a recurrent network to identify the automaton states. The state length and position are adjusted with the usual train and re-segment iterative procedure. The global model is a hybrid of a recurrent neural network which implements the state transition models, and dynamic programming, which finds the best state sequence. The advantages achieved by using recurrent networks are outlined by applying the method to a speaker-independent digit recognition task.<<ETX>>"
            },
            "slug": "Word-recognition-with-recurrent-network-automata-Albesano-Gemello",
            "title": {
                "fragments": [],
                "text": "Word recognition with recurrent network automata"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The authors report a method to directly encode temporal information into a neural network by explicitly modeling that information with a left-to-right automaton, and teaching a recurrent network to identify the automaton states."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings 1992] IJCNN International Joint Conference on Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2537431"
                        ],
                        "name": "Axel Cleeremans",
                        "slug": "Axel-Cleeremans",
                        "structuredName": {
                            "firstName": "Axel",
                            "lastName": "Cleeremans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Axel Cleeremans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403615640"
                        ],
                        "name": "D. Servan-Schreiber",
                        "slug": "D.-Servan-Schreiber",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Servan-Schreiber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Servan-Schreiber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "In particular, grammar learning, although intriguing and progressing rapidly [43], [44], [45], [46], [47], [48], [49], typically involves recurrent neural networks as components of more complex systems, and also at present is inferior in practice to discrete algorithmic techniques [50], [51]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[43] describe a regular language token prediction task which is di cult for Elman nets when the transition probabili-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7741931,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd46c1b5948abe04e565a8bae6454da63a1b021e",
            "isKey": false,
            "numCitedBy": 513,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We explore a network architecture introduced by Elman (1988) for predicting successive elements of a sequence. The network uses the pattern of activation over a set of hidden units from time-step t1, together with element t, to predict element t 1. When the network is trained with strings from a particular finite-state grammar, it can learn to be a perfect finite-state recognizer for the grammar. When the network has a minimal number of hidden units, patterns on the hidden units come to correspond to the nodes of the grammar, although this correspondence is not necessary for the network to act as a perfect finite-state recognizer. We explore the conditions under which the network can carry information about distant sequential contingencies across intervening elements. Such information is maintained with relative ease if it is relevant at each intermediate step; it tends to be lost when intervening elements do not depend on it. At first glance this may suggest that such networks are not relevant to natural language, in which dependencies may span indefinite distances. However, embeddings in natural language are not completely independent of earlier information. The final simulation shows that long distance sequential contingencies can be encoded by the network even if only subtle statistical properties of embedded strings depend on the early information."
            },
            "slug": "Finite-State-Automata-and-Simple-Recurrent-Networks-Cleeremans-Servan-Schreiber",
            "title": {
                "fragments": [],
                "text": "Finite State Automata and Simple Recurrent Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A network architecture introduced by Elman (1988) for predicting successive elements of a sequence and shows that long distance sequential contingencies can be encoded by the network even if only subtle statistical properties of embedded strings depend on the early information."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761373"
                        ],
                        "name": "J. Crutchfield",
                        "slug": "J.-Crutchfield",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Crutchfield",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Crutchfield"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16621753"
                        ],
                        "name": "Bruce S. McNamara",
                        "slug": "Bruce-S.-McNamara",
                        "structuredName": {
                            "firstName": "Bruce",
                            "lastName": "McNamara",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bruce S. McNamara"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[70] and Lapedes and Farber [71] have had success with the identi cation of chaotic systems using models without hidden state, and there is no reason to believe that learning the dynamics of chaotic systems is more di cult than learning the dynamics of non-chaotic ones."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14493184,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "944bb9874f26671e1cd64225f1ab2fb01aaf1934",
            "isKey": false,
            "numCitedBy": 322,
            "numCiting": 105,
            "paperAbstract": {
                "fragments": [],
                "text": "Temporal pattern learning, control and prediction, and chaotic data analysis share a common problem: deducing optimal equations of motion from observations of time-dependent behavior. Each desires to obtain models of the physical world from limited information. We describe a method to reconstruct the deterministic portion of the equations of motion directly from a data series. These equations of motion represent a vast reduction of a chaotic data set\u2019s observed complexity to a compact, algorithmic specification. This approach employs an informational measure of model optimality to guide searching through the space of dynamical systems. As corollary results, we indicate how to estimate the minimum embedding dimension, extrinsic noise level, metric entropy, and Lyapunov spectrum. Numerical and experimental applications demonstrate the method\u2019s feasibility and limitations. Extensions to estimating parametrized families of dynamical systems from bifurcation data and to spatial pattern evolution are presented. Applications to predicting chaotic data and the design of forecasting, learning, and control systems, are discussed."
            },
            "slug": "Equations-of-Motion-from-a-Data-Series-Crutchfield-McNamara",
            "title": {
                "fragments": [],
                "text": "Equations of Motion from a Data Series"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A method to reconstruct the deterministic portion of the equations of motion directly from a data series to represent a vast reduction of a chaotic data set\u2019s observed complexity to a compact, algorithmic specification is described."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2442252"
                        ],
                        "name": "P. Poddar",
                        "slug": "P.-Poddar",
                        "structuredName": {
                            "firstName": "Pinaki",
                            "lastName": "Poddar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Poddar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143928513"
                        ],
                        "name": "K. Unnikrishnan",
                        "slug": "K.-Unnikrishnan",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Unnikrishnan",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Unnikrishnan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60512655,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "505ceb957fb88ba0bb1cdb4485572ad06e3bbfdb",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present a feed-forward neural network architecture that can be used for nonlinear autoregressive prediction of multivariate time-series. It uses specialized neurons (called memory neurons) to store past activations of the network in an efficient fashion. The network learns to be a nonlinear predictor of the appropriate order to model temporal waveforms of speech signals. Arrays of such networks can be used to build real-time classifiers of speech sounds. Experiments where memory-neuron networks are trained to predict speech waveforms and sequences of spectral frames are described. Performance of the network for prediction of time-series with minimal a priori assumptions of its statistical properties is shown to be better than linear autoregressive models.<<ETX>>"
            },
            "slug": "Nonlinear-prediction-of-speech-signals-using-memory-Poddar-Unnikrishnan",
            "title": {
                "fragments": [],
                "text": "Nonlinear prediction of speech signals using memory neuron networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Performance of the network for prediction of time-series with minimal a priori assumptions of its statistical properties is shown to be better than linear autoregressive models."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks for Signal Processing Proceedings of the 1991 IEEE Workshop"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308302"
                        ],
                        "name": "D. Ackley",
                        "slug": "D.-Ackley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Ackley",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ackley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12174018,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657",
            "isKey": false,
            "numCitedBy": 3393,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Learning-Algorithm-for-Boltzmann-Machines-Ackley-Hinton",
            "title": {
                "fragments": [],
                "text": "A Learning Algorithm for Boltzmann Machines"
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35908234"
                        ],
                        "name": "C. Galland",
                        "slug": "C.-Galland",
                        "structuredName": {
                            "firstName": "Conrad",
                            "lastName": "Galland",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Galland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 156
                            }
                        ],
                        "text": "Other empirical studies indicate that applying xedpoint learning algorithms stabilizes networks, causing them to exhibit asymptotic xedpoint behavior [87], [88]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 234
                            }
                        ],
                        "text": "Although weight symmetry is assumed in the de nition of energy which is used in the de nition of probability, and is thus fundamental to these mathematics, it seems that in practice weight asymmetry can be tolerated in large networks [88]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 117337365,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ceb5324414c72957ed4a7f9c857f765b456a1e42",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Deterministic-Boltzmann-Learning-in-Networks-with-Galland-Hinton",
            "title": {
                "fragments": [],
                "text": "Deterministic Boltzmann Learning in Networks with Asymmetric Connectivity"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754328"
                        ],
                        "name": "D. Hush",
                        "slug": "D.-Hush",
                        "structuredName": {
                            "firstName": "Don",
                            "lastName": "Hush",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hush"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35216199"
                        ],
                        "name": "B. Horne",
                        "slug": "B.-Horne",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Horne",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Horne"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 411,
                                "start": 407
                            }
                        ],
                        "text": "It should be noted by engineers that many real-world problems which one might think would require recurrent architectures for their solution turn out to be solvable with feedforward architectures, sometimes augmented with preprocessed inputs such as tapped delay lines, and various other architectural embellishments [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [10], [32], [33], [34], [35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3191120,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56e02786bad4cf8781950b5df615729a417b31d7",
            "isKey": false,
            "numCitedBy": 1261,
            "numCiting": 148,
            "paperAbstract": {
                "fragments": [],
                "text": "Theoretical results concerning the capabilities and limitations of various neural network models are summarized, and some of their extensions are discussed. The network models considered are divided into two basic categories: static networks and dynamic networks. Unlike static networks, dynamic networks have memory. They fall into three groups: networks with feedforward dynamics, networks with output feedback, and networks with state feedback, which are emphasized in this work. Most of the networks discussed are trained using supervised learning.<<ETX>>"
            },
            "slug": "Progress-in-supervised-neural-networks-Hush-Horne",
            "title": {
                "fragments": [],
                "text": "Progress in supervised neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Theoretical results concerning the capabilities and limitations of various neural network models are summarized, and some of their extensions are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Signal Processing Magazine"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34915378"
                        ],
                        "name": "R. Rohwer",
                        "slug": "R.-Rohwer",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Rohwer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rohwer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 736183,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "af7802a50a8c294ebfd539ad72158475e5ecd9f2",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A simple method for training the dynamical behavior of a neural network is derived. It is applicable to any training problem in discrete-time networks with arbitrary feedback. The algorithm resembles back-propagation in that an error function is minimized using a gradient-based method, but the optimization is carried out in the hidden part of state space either instead of, or in addition to weight space. Computational results are presented for some simple dynamical training problems, one of which requires response to a signal 100 time steps in the past."
            },
            "slug": "The-\"Moving-Targets\"-Training-Algorithm-Rohwer",
            "title": {
                "fragments": [],
                "text": "The \"Moving Targets\" Training Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A simple method for training the dynamical behavior of a neural network using a gradient-based method and the optimization is carried out in the hidden part of state space either instead of, or in addition to weight space."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3327108"
                        ],
                        "name": "B. D. Vries",
                        "slug": "B.-D.-Vries",
                        "structuredName": {
                            "firstName": "Bert",
                            "lastName": "Vries",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. D. Vries"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143961030"
                        ],
                        "name": "J. Pr\u00edncipe",
                        "slug": "J.-Pr\u00edncipe",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Pr\u00edncipe",
                            "middleNames": [
                                "Carlos"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pr\u00edncipe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 958138,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a00986a3a7385020d9c50cdb76cb6a6b106b217f",
            "isKey": false,
            "numCitedBy": 276,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-gamma-model--A-new-neural-model-for-temporal-Vries-Pr\u00edncipe",
            "title": {
                "fragments": [],
                "text": "The gamma model--A new neural model for temporal processing"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686506"
                        ],
                        "name": "A. Atiya",
                        "slug": "A.-Atiya",
                        "structuredName": {
                            "firstName": "Amir",
                            "lastName": "Atiya",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Atiya"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11168657,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ab033369446f669153b18b1fa7c907f2385cbdff",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper generalizes the backpropagation method to a general network containing feedback connections. The network model considered consists of interconnected groups of neurons, where each group could be fully interconnected (it could have feedback connections, with possibly asymmetric weights), but no loops between the groups are allowed. A stochastic descent algorithm is applied, under a certain inequality constraint on each intra-group weight matrix which ensures for the network to possess a unique equilibrium state for every input."
            },
            "slug": "Learning-on-a-General-Network-Atiya",
            "title": {
                "fragments": [],
                "text": "Learning on a General Network"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "The backpropagation method is generalized to a general network containing feedback connections, where each group could be fully interconnected (it could have feedback connections), but no loops between the groups are allowed."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895771"
                        ],
                        "name": "D. Zipser",
                        "slug": "D.-Zipser",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zipser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zipser"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 27482241,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6735e5bf04f8bc8428f5ef9d01ec8c12924f3a9",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent nets are more powerful than feedforward nets because they allow simulation of dynamical systems. Everything from sine wave generators through computers to the brain are potential candidates, but to use recurrent nets to emulate dynamical systems we need learning algorithms to program them. Here I describe a new twist on an old algorithm for recurrent nets and compare it to its predecessors."
            },
            "slug": "Subgrouping-Reduces-Complexity-and-Speeds-Up-in-Zipser",
            "title": {
                "fragments": [],
                "text": "Subgrouping Reduces Complexity and Speeds Up Learning in Recurrent Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "A new twist on an old algorithm for recurrent nets is described and compared to its predecessors, which allow simulation of dynamical systems."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11761172,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "89b9a181801f32bf62c4237c4265ba036a79f9dc",
            "isKey": false,
            "numCitedBy": 136,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The real-time recurrent learning (RTRL) algorithm (Robinson and Fallside 1987; Williams and Zipser 1989) requires O(n4) computations per time step, where n is the number of noninput units. I describe a method suited for on-line learning that computes exactly the same gradient and requires fixed-size storage of the same order but has an average time complexity per time step of O(n3)."
            },
            "slug": "A-Fixed-Size-Storage-O(n3)-Time-Complexity-Learning-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "A Fixed Size Storage O(n3) Time Complexity Learning Algorithm for Fully Recurrent Continually Running Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A method suited for on-line learning that computes exactly the same gradient and requires fixed-size storage of the same order but has an average time complexity per time step of O(n3)."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152819463"
                        ],
                        "name": "T. Maxwell",
                        "slug": "T.-Maxwell",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Maxwell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Maxwell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157784"
                        ],
                        "name": "C. Lee Giles",
                        "slug": "C.-Lee-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee Giles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "79443021"
                        ],
                        "name": "Y. C. Lee",
                        "slug": "Y.-C.-Lee",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Lee",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. C. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1769857"
                        ],
                        "name": "H. H. Chen",
                        "slug": "H.-H.-Chen",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Chen",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. H. Chen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 109031226,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8a392cb17eb8074e0c5e5d1b47264b8857c7b54a",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Now that significant progress has been made in developing algorithms for training hidden units, we suggest that it is time to reevaluate the nonlinear discriminate approach, which once fell into disfavor due to the problem of proliferation of high order terms. We show that there are many powerful techniques for reducing the number of spurious terms, and that the high order approach has many advantages over a cascaded slab approach in certain problem areas. Advantages include increased expressive ability, decreased architectural complexity, and dramatically increased learning rates."
            },
            "slug": "Nonlinear-dynamics-of-artificial-neural-systems-Maxwell-Giles",
            "title": {
                "fragments": [],
                "text": "Nonlinear dynamics of artificial neural systems"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that there are many powerful techniques for reducing the number of spurious terms, and that the high order approach has many advantages over a cascaded slab approach in certain problem areas."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144288586"
                        ],
                        "name": "A. Back",
                        "slug": "A.-Back",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Back",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Back"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733691"
                        ],
                        "name": "A. Tsoi",
                        "slug": "A.-Tsoi",
                        "structuredName": {
                            "firstName": "Ah",
                            "lastName": "Tsoi",
                            "middleNames": [
                                "Chung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tsoi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 393,
                                "start": 389
                            }
                        ],
                        "text": "It should be noted by engineers that many real-world problems which one might think would require recurrent architectures for their solution turn out to be solvable with feedforward architectures, sometimes augmented with preprocessed inputs such as tapped delay lines, and various other architectural embellishments [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [10], [32], [33], [34], [35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7792757,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "c9282390b4ebe10861d54a1066d5f3907a54608a",
            "isKey": false,
            "numCitedBy": 236,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "A new neural network architecture involving either local feedforward global feedforward, and/or local recurrent global feedforward structure is proposed. A learning rule minimizing a mean square error criterion is derived. The performance of this algorithm (local recurrent global feedforward architecture) is compared with a local-feedforward global-feedforward architecture. It is shown that the local-recurrent global-feedforward model performs better than the local-feedforward global-feedforward model."
            },
            "slug": "FIR-and-IIR-Synapses,-a-New-Neural-Network-for-Time-Back-Tsoi",
            "title": {
                "fragments": [],
                "text": "FIR and IIR Synapses, a New Neural Network Architecture for Time Series Modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "It is shown that the local-recurrent global-feedforward model performs better than the local/local recurrent global feedforward model and the learning rule minimizing a mean square error criterion is derived."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27092071"
                        ],
                        "name": "B. Baird",
                        "slug": "B.-Baird",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Baird",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Baird"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 20055230,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a66aa986e281bc2f4751fc1e5fb9cdf08830fea7",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "An analytic formula is used to set weights in recurrent analog networks with higher-order correlations to achieve the associative or content-addressable memory (CAM) storage of continuous pattern sequences as periodic trajectories. This learning rule allows programming of characteristics of the network vector field independently of the spatiotemporal patterns to be stored. Stability of sequences, basin geometry, and rates of convergence may be determined. A Lyapunov function in a special coordinate system governs the approach of initial conditions to the nearest stored trajectory"
            },
            "slug": "A-learning-rule-for-CAM-storage-of-continuous-Baird",
            "title": {
                "fragments": [],
                "text": "A learning rule for CAM storage of continuous periodic sequences"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An analytic formula is used to set weights in recurrent analog networks with higher-order correlations to achieve the associative or content-addressable memory (CAM) storage of continuous pattern sequences as periodic trajectories."
            },
            "venue": {
                "fragments": [],
                "text": "1990 IJCNN International Joint Conference on Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714997"
                        ],
                        "name": "K. Doya",
                        "slug": "K.-Doya",
                        "structuredName": {
                            "firstName": "Kenji",
                            "lastName": "Doya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Doya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734225"
                        ],
                        "name": "A. Selverston",
                        "slug": "A.-Selverston",
                        "structuredName": {
                            "firstName": "Allen",
                            "lastName": "Selverston",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Selverston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2819781"
                        ],
                        "name": "P. Rowat",
                        "slug": "P.-Rowat",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Rowat",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Rowat"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12443277,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "75b910a748c88e2f0119adfe36e3b2298e44428e",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "A gradient descent algorithm for parameter estimation which is similar to those used for continuous-time recurrent neural networks was derived for Hodgkin-Huxley type neuron models. Using membrane potential trajectories as targets, the parameters (maximal conductances, thresholds and slopes of activation curves, time constants) were successfully estimated. The algorithm was applied to modeling slow non-spike oscillation of an identified neuron in the lobster stomatogastric ganglion. A model with three ionic currents was trained with experimental data. It revealed a novel role of A-current for slow oscillation below -50 mV."
            },
            "slug": "A-Hodgkin-Huxley-Type-Neuron-Model-That-Learns-Slow-Doya-Selverston",
            "title": {
                "fragments": [],
                "text": "A Hodgkin-Huxley Type Neuron Model That Learns Slow Non-Spike Oscillations"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A gradient descent algorithm for parameter estimation which is similar to those used for continuous-time recurrent neural networks was derived for Hodgkin-Huxley type neuron models and revealed a novel role of A-current for slow oscillation below -50 mV."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144529123"
                        ],
                        "name": "N. J. Cohen",
                        "slug": "N.-J.-Cohen",
                        "structuredName": {
                            "firstName": "Neal",
                            "lastName": "Cohen",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. J. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38234690"
                        ],
                        "name": "N. Weinberger",
                        "slug": "N.-Weinberger",
                        "structuredName": {
                            "firstName": "Nir",
                            "lastName": "Weinberger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Weinberger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12960317,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01fbe65823f0cea82a001a5a4504a66ab73e68f8",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "T h e Boltzmann machine is a nonlinear network of stochastic binary processing units t ha t interact pairwise through symmetric connection strengths. In a third-order Boltzmann machine, triples of units interact through symmetric conjunctive interactions. The Boltzmann learning algorithm is generalized t o higher-order interactions. The rate of learning for internal representations in a higher-order Boltzmann machine should be much faster t han for a second-order Boltzmann machine based on pairwise interactions. I N T R O D U C T I O N Thousands of hours of practice are required by humans t o become experts in domains such as chess, mathematics and physics1. Learning in these domains requires the mastery of a large number of highly interrelated ideas, and a deep understanding requires generalization as well as memorization. There are two traditions in the literature on learning in neural network models. One class of models is based on the problem of content-addressable memory and emphasizes a fast, one-shot form of learning. The second class of models uses slow, incremental learning, which requires many repetitions of examples. I t is difficult in humans t o s tudy fast and slow learning in isolation. In some amnesics, however, the long-term retention of facts is severely impaired, but the slow acquisition of skills, including cognitive skills, is spared2. Thus, it is possible tha t separate memory mechanisms are used t o implement fast learning and slow learning. Long practice is required t o become a n expert, bu t expert performance is swift and difficult to analyze; with more practice there is faster performance1. Why is slow learning so slow? One possibility is t h a t the expert develops internal representations tha t allow fast parallel searches for solutions t o problems in the task domain, in contrast t o a novice who must apply knowledge piecemeal. An internal representation is a mental model of the task domain; t h a t is, internal degrees of freedom between the sensory inputs and motor outputs t h a t efficiently encode the variables relevant t o the solution of the problem. This approach can be made more precise by specifying neural network models and showing how they incorporate internal representations. L E A R N I N G IN NETWORK M O D E L S Network models of fast learning include linear correlation-matrix m o d e l ~ ~ ~ ~ ~ ~ l ~ and the more recent nonlinear autoassociative m o d e ~ s ~ ~ ~ ~ ~ ~ ' ~ . These models use the Hebb learning rule t o store information t h a t can be retrieved by the completion of partially specified input patterns. New patterns are stored by imposing the pattern on the network and altering the connection strengths between the pairs of units that are above threshold. The information that is stored therefore concerns the correlations, .or second-order relationships between the components of the pattern. The internal model is built from correlations. Network models of slow learning include the perceptronll and adaline12. These networks can classify input patterns given only examples of inputs and desired outputs. The connection strengths are changed incrementally during the training and the network gradually converges to a set of weights tha t solves the problem if such as set of weights exists. Unfortunately, there are many difficult problems that cannot be solved with these networks, such as the prediction of parity'3. The perceptron and adaline are limited because they have only one layer of modifiable connection strengths and can only implement linear discriminant functions. Higher-order problems like parity cannot be solved by storing the desired patterns using the class of contentaddressable algorithms based on the Hebb learning rule. These models are limited because the metric of similarity is based on Hamming distance and only correlations can be used t o access patterns. The first network model to demonstrably learn t o solve higher-order problems was the Boltzmann machine, which overcame the limitations of previous network models by introducing hidden units14*15116. Hidden units are added t o the network t o mediate between the input and output units; they provide the extra internal degrees of freedom needed t o form internal representations. The Boltzmann learning algorithm incrementally modifies internal connections in the network to build higher-order pattern detectors. The hidden units can be recruited t o form internal representations for any problem; however, the learning may require an extremely large number of training examples and can be excessively slow. One way t o speed up the learning is to use hidden units that have higher-order interactions with other units. THIRD-ORDER BOLTZMANN MACHINES Consider a Boltzmann machine with a cubic global energy function: where si is the state of the i t h binary unit and w;p is a weight between triples of units. This type of interaction generalizes the pairwise interactions in Hopfield networkslo and Boltzmann machines, which contribute a quadratic term t o the energy. Fig. 1 shows an interpretation of the cubic term as conjunctive synapses. Each unit in the network updates its binary state asynchronously with probability where T is a parameter the i t h unit is given by analagous t o the temperature and the total input to If wijk is symmetric on all pairs of indices then the energy of the network is nonincreasing. It can be shown that in equilibrium the probabilities of global states P, follow a Boltzmann distribution Fig. 1. Third-order interactions between three units. In the diagram the lines between units represent reciprocal interactions that are activated only when the third unit is in the on state. The third unit acts presynaptically to conjunctively control the painvise interactions. There are two forms of the Boltzmann learning algorithm, one for networks with inputs and outputs treated identically, and a second for networks where the input units are always clamped15. The former learning algorithm will be generalized for third-order interactions. The learning metric on weight space remains the same: where P , is the probability of a global state with both the inputs and outputs clamped, and Pd, is the probability of a global state when the network is allowed to run freely. I t can be shown that the gradient of G is given by where p i jk is the ensemble average probability of three units all being in the on state when the input and output units are clamped, and pi;k is the corresponding probability when the network is running freely. T o minimize G , it is sufficient to measure the time averaged triple co-occurence probabilities when the network is in equilibrium under the two conditions and t o change each weight according to where c scales the size of each weight change. HIGHER-ORDER BOLTZMANN MACHINES Define the energy of a k -th order Boltzmann machine as where w 7172 . . . 7t is a k -dimensional weight indices. The G matrix can be minimized by matrix symmetric on all pairs of gradient descent: where P ~ , ~ . . . is the probability of the k-tuple co-occurence of the I (s 71 ,S 72 , . . s 7r ) when the inputs and outputs are clamped, and p 7172. . . 7L is the corresponding probability when the network is freely running. In general, the energy for a Boltzmann machine is the sum over all orders of interaction and the learning algorithm is a linear combination of terms from each order. This is a Markov random field with polynomial interactions17."
            },
            "slug": "Higher-Order-Boltzmann-Machines-Cohen-Weinberger",
            "title": {
                "fragments": [],
                "text": "Higher-Order Boltzmann Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This work presents the Boltzmann machine, a nonlinear network of stochastic binary processing units, which overcame the limitations of previous network models by introducing hidden units and shows how they incorporate internal representations."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2583597"
                        ],
                        "name": "J. Kolen",
                        "slug": "J.-Kolen",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Kolen",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kolen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6512171,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "979e6000391c1739104581b295ae5c9461a539b7",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Several recurrent networks have been proposed as representations for the task of formal language learning. After training a recurrent network recognize a formal language or predict the next symbol of a sequence, the next logical step is to understand the information processing carried out by the network. Some researchers have begun to extracting finite state machines from the internal state trajectories of their recurrent networks. This paper describes how sensitivity to initial conditions and discrete measurements can trick these extraction methods to return illusory finite state descriptions."
            },
            "slug": "Fool's-Gold:-Extracting-Finite-State-Machines-from-Kolen",
            "title": {
                "fragments": [],
                "text": "Fool's Gold: Extracting Finite State Machines from Recurrent Network Dynamics"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "How sensitivity to initial conditions and discrete measurements can trick these extraction methods to return illusory finite state descriptions is described."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103064895"
                        ],
                        "name": "M. Tugay",
                        "slug": "M.-Tugay",
                        "structuredName": {
                            "firstName": "Mehmet",
                            "lastName": "Tugay",
                            "middleNames": [
                                "Ali"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Tugay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2699636"
                        ],
                        "name": "Y. Tanik",
                        "slug": "Y.-Tanik",
                        "structuredName": {
                            "firstName": "Yal\u00e7in",
                            "lastName": "Tanik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Tanik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 119850024,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aefc5dd15cf6513e80c40ac65eec42b60d8969c1",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "The momentum least-mean square (MLMS) algorithm, a modified version of the well-known LMS algorithm, has recently been proposed, and an analysis of its basic convergence properties has been given. The authors revise the ranges of the MLMS algorithm's parameters, for which convergence is guaranteed, and provide precise expressions of convergence rate and steady-state performance of the algorithm under slow learning conditions. As a result, it is shown that, with Gaussian inputs and a low adaptation rate, the LMS and MLMS algorithms are equivalent, but, with inputs incorporating impulse noise components, the MLMS algorithm performs better. Due to its increased inertia, the MLMS algorithm becomes preferable for systems with inputs containing impulse noise components. At the expense of increased computational complexity, the MLMS algorithm is more stable against short-term disturbances exhibited by the filter input.<<ETX>>"
            },
            "slug": "Properties-of-the-momentum-LMS-algorithm-Tugay-Tanik",
            "title": {
                "fragments": [],
                "text": "Properties of the momentum LMS algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that, with Gaussian inputs and a low adaptation rate, the LMS and MLMS algorithms are equivalent, but, with inputs incorporating impulse noise components, the MLMS algorithm performs better and becomes preferable for systems with inputs containing impulses noise components."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144473519"
                        ],
                        "name": "M. Mozer",
                        "slug": "M.-Mozer",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Mozer",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mozer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5355536,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e141d68065ce638f9fc4f006eab2f66711e89768",
            "isKey": false,
            "numCitedBy": 160,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning structure in temporally-extended sequences is a difficult computational problem because only a fraction of the relevant information is available at any instant. Although variants of back propagation can in principle be used to find structure in sequences, in practice they are not sufficiently powerful to discover arbitrary contingencies, especially those spanning long temporal intervals or involving high order statistics. For example, in designing a connectionist network for music composition, we have encountered the problem that the net is able to learn musical structure that occurs locally in time--e.g., relations among notes within a musical phrase--but not structure that occurs over longer time periods--e.g., relations among phrases. To address this problem, we require a means of constructing a reduced description of the sequence that makes global aspects more explicit or more readily detectable. I propose to achieve this using hidden units that operate with different time constants. Simulation experiments indicate that slower time-scale hidden units are able to pick up global structure, structure that simply can not be learned by standard back propagation."
            },
            "slug": "Induction-of-Multiscale-Temporal-Structure-Mozer",
            "title": {
                "fragments": [],
                "text": "Induction of Multiscale Temporal Structure"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Simulation experiments indicate that slower time-scale hidden units are able to pick up global structure, structure that simply can not be learned by standard back propagation, using hidden units that operate with different time constants."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 771841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c2dd697bbe99c2ec71c807580a00f7e723cc20ae",
            "isKey": false,
            "numCitedBy": 233,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Appropriate bias is widely viewed as the key to efficient learning and generalization. I present a new algorithm, the Incremental Delta-Bar-Delta (IDBD) algorithm, for the learning of appropriate biases based on previous learning experience. The IDBD algorithm is developed for the case of a simple, linear learning system--the LMS or delta rule with a separate learning-rate parameter for each input. The IDBD algorithm adjusts the learning-rate parameters, which are an important form of bias for this system. Because bias in this approach is adapted based on previous learning experience, the appropriate test beds are drifting or non-stationary learning tasks. For particular tasks of this type, I show that the IDBD algorithm performs better than ordinary LMS and in fact finds the optimal learning rates. The IDBD algorithm extends and improves over prior work by Jacobs and by me in that it is fully incremental and has only a single free parameter. This paper also extends previous work by presenting a derivation of the IDBD algorithm as gradient descent in the space of learning-rate parameters. Finally, I offer a novel interpretation of the IDBD algorithm as an incremental form of hold-one-out cross validation."
            },
            "slug": "Adapting-Bias-by-Gradient-Descent:-An-Incremental-Sutton",
            "title": {
                "fragments": [],
                "text": "Adapting Bias by Gradient Descent: An Incremental Version of Delta-Bar-Delta"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A new algorithm, the Incremental Delta-Bar-Delta (IDBD) algorithm, for the learning of appropriate biases based on previous learning experience, and a novel interpretation of the IDBD algorithm as an incremental form of hold-one-out cross validation."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109535043"
                        ],
                        "name": "Sreerupa Das",
                        "slug": "Sreerupa-Das",
                        "structuredName": {
                            "firstName": "Sreerupa",
                            "lastName": "Das",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sreerupa Das"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157784"
                        ],
                        "name": "C. Lee Giles",
                        "slug": "C.-Lee-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee Giles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34922532"
                        ],
                        "name": "Guo-Zheng Sun",
                        "slug": "Guo-Zheng-Sun",
                        "structuredName": {
                            "firstName": "Guo-Zheng",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guo-Zheng Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17770068,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1811f708b8b7456a3708fabd2fd638da36bd7ba0",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Although considerable interest has been shown in language inference and automata induction using recurrent neural networks, success of these models has mostly been limited to regular languages. We have previously demonstrated that Neural Network Pushdown Automaton (NNPDA) model is capable of learning deterministic context-free languages (e.g., anbn and parenthesis languages) from examples. However, the learning task is computationally intensive. In this paper we discus some ways in which a priori knowledge about the task and data could be used for efficient learning. We also observe that such knowledge is often an experimental prerequisite for learning nontrivial languages (eg. anbncbmam)."
            },
            "slug": "Using-Prior-Knowledge-in-a-{NNPDA}-to-Learn-Das-Giles",
            "title": {
                "fragments": [],
                "text": "Using Prior Knowledge in a {NNPDA} to Learn Context-Free Languages"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Some ways in which a priori knowledge about the task and data could be used for efficient learning arediscussed, noting that such knowledge is often an experimental prerequisite for learning nontrivial languages."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467467"
                        ],
                        "name": "M. Gori",
                        "slug": "M.-Gori",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Gori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8899009"
                        ],
                        "name": "R. de Mori",
                        "slug": "R.-de-Mori",
                        "structuredName": {
                            "firstName": "Renato",
                            "lastName": "de Mori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. de Mori"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 13927858,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7ab73467f48f9c453be07c389c5dd1d1bfa70f25",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel backpropagation learning algorithm for a particular class of dynamic neural networks in which some units have a local feedback is proposed. Hence these networks can be trained to respond to sequences of input patterns. This algorithm has the same order of space and time requirements as backpropagation applied to feedforward networks. The authors present experimental results and comparisons with a speech recognition problem.<<ETX>>"
            },
            "slug": "BPS:-a-learning-algorithm-for-capturing-the-dynamic-Gori-Bengio",
            "title": {
                "fragments": [],
                "text": "BPS: a learning algorithm for capturing the dynamic nature of speech"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A novel backpropagation learning algorithm for a particular class of dynamic neural networks in which some units have a local feedback is proposed, which can be trained to respond to sequences of input patterns."
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2884373"
                        ],
                        "name": "J. Elman",
                        "slug": "J.-Elman",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Elman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Elman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[118] considers a version of backpropagation through time in discrete time in which the temporal history is cut"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2763403,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "668087f0ae7ce1de6e0bd0965dbb480c08103260",
            "isKey": false,
            "numCitedBy": 9858,
            "numCiting": 111,
            "paperAbstract": {
                "fragments": [],
                "text": "Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction."
            },
            "slug": "Finding-Structure-in-Time-Elman",
            "title": {
                "fragments": [],
                "text": "Finding Structure in Time"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory and suggests a method for representing lexical categories and the type/token distinction is developed."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1822055"
                        ],
                        "name": "Raymond L. Watrous",
                        "slug": "Raymond-L.-Watrous",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Watrous",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raymond L. Watrous"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48462607"
                        ],
                        "name": "L. Shastri",
                        "slug": "L.-Shastri",
                        "structuredName": {
                            "firstName": "Lokendra",
                            "lastName": "Shastri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Shastri"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 107
                            }
                        ],
                        "text": "Such \\time delay neural networks\" have proven useful in the domain of speech recognition [20], [22], [21], [115]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61357811,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c1e72bf04739f34fe332ddaa3835780fa9fba8d7",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of connectionist networks for speech recognition is assessed using a set of representative phonetic discrimination problems. The problems are chosen with respect to the physiological theory of phonetics in order to give broad coverage to the space of articulatory phonetics. Separate network solutions are sought to each phonetic discrimination problem. \nA connectionist network model called the Temporal Flow Model is defined which consists of simple processing units with single valued outputs interconnected by links of variable weight. The model represents temporal relationships using delay links and permits general patterns of connectivity including feedback. It is argued that the model has properties appropriate for time varying signals such as speech. \nMethods for selecting network architectures for different recognition problems are presented. The architectures discussed include random networks, minimally structured networks, hand crafted networks and networks automatically generated based on samples of speech data. \nNetworks are trained by modifying their weight parameters so as to minimize the mean squared error between the actual and the desired response of the output units. The desired output unit response is specified by a target function. Training is accomplished by a second order method of iterative nonlinear optimization by gradient descent which incorporates a method for computing the complete gradient of recurrent networks. \nNetwork solutions are demonstrated for all eight phonetic discrimination problems for one male speaker. The network solutions are analyzed carefully and are shown in every case to make use of known acoustic phonetic cues. The network solutions vary in the degree to which they make use of context dependent cues to achieve phoneme recognition. \nThe network solutions were tested on data not used for training and achieved an average accuracy of 99.5 $\\pm$ 0.4%. \nMethods for extending these results to a single network for recognizing the complete phoneme set from continuous speech obtained from different speakers are outlined. \nIt is concluded that acoustic phonetic speech recognition can be accomplished using connectionist networks."
            },
            "slug": "Speech-recognition-using-connectionist-networks-Watrous-Shastri",
            "title": {
                "fragments": [],
                "text": "Speech recognition using connectionist networks"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "It is concluded that acoustic phonetic speech recognition can be accomplished using connectionist networks."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3034179"
                        ],
                        "name": "T. Grossman",
                        "slug": "T.-Grossman",
                        "structuredName": {
                            "firstName": "Tal",
                            "lastName": "Grossman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Grossman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766683"
                        ],
                        "name": "R. Meir",
                        "slug": "R.-Meir",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Meir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Meir"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2426224"
                        ],
                        "name": "E. Domany",
                        "slug": "E.-Domany",
                        "structuredName": {
                            "firstName": "Eytan",
                            "lastName": "Domany",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Domany"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 7
                            }
                        ],
                        "text": "[120], [121], [122] propose a moving targets learning algorithm."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16511611,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d6deb1ddc764259fbdc7733ef80473081bff31d5",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a learning algorithm for multilayer neural networks composed of binary linear threshold elements. Whereas existing algorithms reduce the learning process to minimizing a cost function over the weights, our method treats the internal representations as the fundamental entities to be determined. Once a correct set of internal representations is arrived at, the weights are found by the local and biologically plausible Perceptron Learning Rule (PLR). We tested our learning algorithm on four problems: adjacency, symmetry, parity and combined symmetry-parity."
            },
            "slug": "Learning-by-Choice-of-Internal-Representations-Grossman-Meir",
            "title": {
                "fragments": [],
                "text": "Learning by Choice of Internal Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A learning algorithm for multilayer neural networks composed of binary linear threshold elements that treats the internal representations as the fundamental entities to be determined by the local and biologically plausible Perceptron Learning Rule."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40396597"
                        ],
                        "name": "Toshiyuki Hanazawa",
                        "slug": "Toshiyuki-Hanazawa",
                        "structuredName": {
                            "firstName": "Toshiyuki",
                            "lastName": "Hanazawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Toshiyuki Hanazawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9243990"
                        ],
                        "name": "K. Shikano",
                        "slug": "K.-Shikano",
                        "structuredName": {
                            "firstName": "Kiyohiro",
                            "lastName": "Shikano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Shikano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49464494"
                        ],
                        "name": "Kevin J. Lang",
                        "slug": "Kevin-J.-Lang",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Lang",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin J. Lang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9563026,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd62c9976534a6a2096a38244f6cbb03635a127e",
            "isKey": false,
            "numCitedBy": 2786,
            "numCiting": 92,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: (1) using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation; and (2) the time-delay arrangement enables the network to discover acoustic-phonetic features and the temporal relationships between them independently of position in time and therefore not blurred by temporal shifts in the input. As a recognition task, the speaker-dependent recognition of the phonemes B, D, and G in varying phonetic contexts was chosen. For comparison, several discrete hidden Markov models (HMM) were trained to perform the same task. Performance evaluation over 1946 testing tokens from three speakers showed that the TDNN achieves a recognition rate of 98.5% correct while the rate obtained by the best of the HMMs was only 93.7%. >"
            },
            "slug": "Phoneme-recognition-using-time-delay-neural-Waibel-Hanazawa",
            "title": {
                "fragments": [],
                "text": "Phoneme recognition using time-delay neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3286081"
                        ],
                        "name": "S. Lockery",
                        "slug": "S.-Lockery",
                        "structuredName": {
                            "firstName": "Shawn",
                            "lastName": "Lockery",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lockery"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1840458369"
                        ],
                        "name": "Yan Fang",
                        "slug": "Yan-Fang",
                        "structuredName": {
                            "firstName": "Yan",
                            "lastName": "Fang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yan Fang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 395,
                                "start": 391
                            }
                        ],
                        "text": "Although algorithms suitable for building systems of this type are reviewed to some extent below, such as the algorithm used in [9], the bulk of this paper is concerned with the problem of causing networks to exhibit particular desired detailed temporal behavior, which has found application in signal processing [10], [11], speech and language processing [12], [13], [14], and neuroscience [15], [16], [17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13096984,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "f8c5359e841a71480eb7436b897f51610486fde5",
            "isKey": false,
            "numCitedBy": 88,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Interneurons in leech ganglia receive multiple sensory inputs and make synaptic contacts with many motor neurons. These hidden units coordinate several different behaviors. We used physiological and anatomical constraints to construct a model of the local bending reflex. Dynamic networks were trained on experimentally derived input-output patterns using recurrent backpropagation. Units in the model were modified to include electrical synapses and multiple synaptic time constants. The properties of the hidden units that emerged in the simulations matched those in the leech. The model and data support distributed rather than localist representations in the local bending reflex. These results also explain counterintuitive aspects of the local bending circuitry."
            },
            "slug": "A-Dynamic-Neural-Network-Model-of-Sensorimotor-in-Lockery-Fang",
            "title": {
                "fragments": [],
                "text": "A Dynamic Neural Network Model of Sensorimotor Transformations in the Leech"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A model of the local bending reflex was constructed using physiological and anatomical constraints to construct a model of interneurons in leech ganglia and the properties of the hidden units that emerged in the simulations matched those in the leech."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3327108"
                        ],
                        "name": "B. D. Vries",
                        "slug": "B.-D.-Vries",
                        "structuredName": {
                            "firstName": "Bert",
                            "lastName": "Vries",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. D. Vries"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143961030"
                        ],
                        "name": "J. Pr\u00edncipe",
                        "slug": "J.-Pr\u00edncipe",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Pr\u00edncipe",
                            "middleNames": [
                                "Carlos"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pr\u00edncipe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7830172,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c115f0d793225c515ebce6be91521fcb8374ad6b",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new neural network model for processing of temporal patterns. This model, the gamma neural model, is as general as a convolution delay model with arbitrary weight kernels w(t). We show that the gamma model can be formulated as a (partially prewired) additive model. A temporal hebbian learning rule is derived and we establish links to related existing models for temporal processing."
            },
            "slug": "A-Theory-for-Neural-Networks-with-Time-Delays-Vries-Pr\u00edncipe",
            "title": {
                "fragments": [],
                "text": "A Theory for Neural Networks with Time Delays"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "This model, the gamma neural model, is as general as a convolution delay model with arbitrary weight kernels w(t) and it is shown that the gamma model can be formulated as a (partially prewired) additive model."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516142"
                        ],
                        "name": "D. Tank",
                        "slug": "D.-Tank",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Tank",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Tank"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219867"
                        ],
                        "name": "J. Hopfield",
                        "slug": "J.-Hopfield",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hopfield",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hopfield"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 32465715,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e69606729837aa1d0168c47f812cbccaba09dc83",
            "isKey": false,
            "numCitedBy": 344,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "An analog model neural network that can solve a general problem of recognizing patterns in a time-dependent signal is presented. The networks use a patterned set of delays to collectively focus stimulus sequence information to a neural state at a future time. The computational capabilities of the circuit are demonstrated on tasks somewhat similar to those necessary for the recognition of words in a continuous stream of speech. The network architecture can be understood from consideration of an energy function that is being minimized as the circuit computes. Neurobiological mechanisms are known for the generation of appropriate delays."
            },
            "slug": "Neural-computation-by-concentrating-information-in-Tank-Hopfield",
            "title": {
                "fragments": [],
                "text": "Neural computation by concentrating information in time."
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "An analog model neural network that can solve a general problem of recognizing patterns in a time-dependent signal is presented and can be understood from consideration of an energy function that is being minimized as the circuit computes."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1822055"
                        ],
                        "name": "Raymond L. Watrous",
                        "slug": "Raymond-L.-Watrous",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Watrous",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raymond L. Watrous"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "18612099"
                        ],
                        "name": "B. Ladendorf",
                        "slug": "B.-Ladendorf",
                        "structuredName": {
                            "firstName": "Bruce",
                            "lastName": "Ladendorf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ladendorf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2091434"
                        ],
                        "name": "G. Kuhn",
                        "slug": "G.-Kuhn",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Kuhn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kuhn"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 222252385,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2bed713424942fa3cb4e8916d2220c1e0a2b534d",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A complete gradient optimization technique for connectionist networks with recurrent links is presented. A truncated gradient technique is available in the literature. A network with recurrent links for discrimination of /b/, /d/, and /g/ in the context of following /i/, /a/, or /u/ is designed. The performance of this network as optimized with either of the two techniques is then compared. It will be shown that the complete gradient is exact and simpler, and it will be demonstrated that it leads to superior performance."
            },
            "slug": "Complete-gradient-optimization-of-a-recurrent-to-Watrous-Ladendorf",
            "title": {
                "fragments": [],
                "text": "Complete gradient optimization of a recurrent network applied to /b/,/d/,/g/ discrimination"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It will be shown that the complete gradient is exact and simpler, and it will be demonstrated that it leads to superior performance."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802785"
                        ],
                        "name": "S. Nowlan",
                        "slug": "S.-Nowlan",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Nowlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nowlan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2898975,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a41ca65a80b5644d23649043f2f625b4002a225",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Geoffrey E. Hinton Department of Computer Science . U ni versi ty of Toran to Toronto, Canada M5S lA4 One way of simplifying neural networks so they generalize better is to add an extra t.erm 10 the error fUll c tion that will penalize complexit.y. \\Ve propose a new penalt.y t.erm in which the dist rihution of weight values is modelled as a mixture of multiple gaussians . C nder this model, a set of weights is simple if the weights can be clustered into subsets so that the weights in each cluster have similar values . We allow the parameters of the mixture model to adapt at t.he same time as t.he network learns. Simulations demonstrate that this complexity term is more effective than previous complexity terms."
            },
            "slug": "Adaptive-Soft-Weight-Tying-using-Gaussian-Mixtures-Nowlan-Hinton",
            "title": {
                "fragments": [],
                "text": "Adaptive Soft Weight Tying using Gaussian Mixtures"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Simulations demonstrate that this complexity term is more effective than previous complexity terms."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50394648"
                        ],
                        "name": "N. Qian",
                        "slug": "N.-Qian",
                        "structuredName": {
                            "firstName": "Ning",
                            "lastName": "Qian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Qian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[9] successfully applied the [72], [73] recurrent backprop-"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 128
                            }
                        ],
                        "text": "Although algorithms suitable for building systems of this type are reviewed to some extent below, such as the algorithm used in [9], the bulk of this paper is concerned with the problem of causing networks to exhibit particular desired detailed temporal behavior, which has found application in signal processing [10], [11], speech and language processing [12], [13], [14], and neuroscience [15], [16], [17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6134807,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "437d76bd1f0348c568786c690f364d72bc14eb4d",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Binocular depth perception, or stereopsis, depends on matching corresponding points in two images taken from two vantage points. In random-dot stereograms the features to be matched are individual pixels. We have used the recurrent backpropagation learning algorithm of Pineda (1987) to construct network models with lateral and feedback connections that can solve the correspondence problem for random-dot stereograms. The network learned the uniqueness and continuity constraints originally proposed by Marr and Poggio (1976) from a training set of dense random-dot stereograms. We also constructed networks that can solve sparse random-dot stereograms of transparent surfaces. The success of the learning algorithm depended on taking advantage of translation invariance and restrictions on the range of interactions."
            },
            "slug": "Learning-to-Solve-Random-Dot-Stereograms-of-Dense-Qian-Sejnowski",
            "title": {
                "fragments": [],
                "text": "Learning to Solve Random-Dot Stereograms of Dense and Transparent Surfaces with Recurrent Backpropagation"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The recurrent backpropagation learning algorithm of Pineda (1987) is used to construct network models with lateral and feedback connections that can solve the correspondence problem for random-dot stereograms."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1913418"
                        ],
                        "name": "B. Widrow",
                        "slug": "B.-Widrow",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Widrow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Widrow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46790239"
                        ],
                        "name": "J. McCool",
                        "slug": "J.-McCool",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "McCool",
                            "middleNames": [
                                "M."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. McCool"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2186837"
                        ],
                        "name": "M. Larimore",
                        "slug": "M.-Larimore",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Larimore",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Larimore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145120448"
                        ],
                        "name": "C. Johnson",
                        "slug": "C.-Johnson",
                        "structuredName": {
                            "firstName": "C.R.",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Johnson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16404647,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44c46f0bad2c9d9aa1ecf7b09e845d15bbb0bf80",
            "isKey": false,
            "numCitedBy": 1401,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the performance characteristics of the LMS adaptive filter, a digital filter composed of a tapped delay line and adjustable weights, whose impulse response is controlled by an adaptive algorithm. For stationary stochastic inputs, the mean-square error, the difference between the filter output and an externally supplied input called the \"desired response,\" is a quadratic function of the weights, a paraboloid with a single fixed minimum point that can be sought by gradient techniques. The gradient estimation process is shown to introduce noise into the weight vector that is proportional to the speed of adaptation and number of weights. The effect of this noise is expressed in terms of a dimensionless quantity \"misadjustment\" that is a measure of the deviation from optimal Wiener performance. Analysis of a simple nonstationary case, in which the minimum point of the error surface is moving according to an assumed first-order Markov process, shows that an additional contribution to misadjustment arises from \"lag\" of the adaptive process in tracking the moving minimum point. This contribution, which is additive, is proportional to the number of weights but inversely proportional to the speed of adaptation. The sum of the misadjustments can be minimized by choosing the speed of adaptation to make equal the two contributions. It is further shown, in Appendix A, that for stationary inputs the LMS adaptive algorithm, based on the method of steepest descent, approaches the theoretical limit of efficiency in terms of misadjustment and speed of adaptation when the eigenvalues of the input correlation matrix are equal or close in value. When the eigenvalues are highly disparate (\u03bbmax/\u03bbmin> 10), an algorithm similar to LMS but based on Newton's method would approach this theoretical limit very closely."
            },
            "slug": "Stationary-and-nonstationary-learning-of-the-LMS-Widrow-McCool",
            "title": {
                "fragments": [],
                "text": "Stationary and nonstationary learning characteristics of the LMS adaptive filter"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that for stationary inputs the LMS adaptive algorithm, based on the method of steepest descent, approaches the theoretical limit of efficiency in terms of misadjustment and speed of adaptation when the eigenvalues of the input correlation matrix are equal or close in value."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE"
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143956638"
                        ],
                        "name": "R. Allen",
                        "slug": "R.-Allen",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Allen",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Allen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3280194"
                        ],
                        "name": "J. Alspector",
                        "slug": "J.-Alspector",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Alspector",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Alspector"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 150
                            }
                        ],
                        "text": "Other empirical studies indicate that applying xedpoint learning algorithms stabilizes networks, causing them to exhibit asymptotic xedpoint behavior [87], [88]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7453,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "059b6440f2502496d0edabe1624558b6d0a52905",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Boltzmann-based models with asymmetric connections are investigated. Although they are initially unstable, these networks spontaneously self-stabilize as a result of learning. Moreover, pairs of weights symmetrize during learning; however, the symmetry is not enough to account for the observed stability. To characterize the system it is useful to consider how its entropy is affected by learning and the entropy of the information stream. The stability of an asymmetric network is confirmed with an electronic model."
            },
            "slug": "Learning-of-stable-states-in-stochastic-asymmetric-Allen-Alspector",
            "title": {
                "fragments": [],
                "text": "Learning of stable states in stochastic asymmetric networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The stability of an asymmetric network is confirmed with an electronic model and pairs of weights symmetrize during learning; however, the symmetry is not enough to account for the observed stability."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144391220"
                        ],
                        "name": "M. Kawato",
                        "slug": "M.-Kawato",
                        "structuredName": {
                            "firstName": "Mitsuo",
                            "lastName": "Kawato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kawato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48756419"
                        ],
                        "name": "Tohru Setoyama",
                        "slug": "Tohru-Setoyama",
                        "structuredName": {
                            "firstName": "Tohru",
                            "lastName": "Setoyama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tohru Setoyama"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50465418"
                        ],
                        "name": "R. Suzuki",
                        "slug": "R.-Suzuki",
                        "structuredName": {
                            "firstName": "Ryoji",
                            "lastName": "Suzuki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Suzuki"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 197
                            }
                        ],
                        "text": "com This paper is concerned with learning algorithms for recurrent networks themselves, and not with recurrent networks as elements of larger systems, such as specialized architectures for control [36], [37], [38], [39]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 30034637,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "aa44c5e67bb44a416ea235a154c904bc76173b38",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Feedback-error-learning-of-movement-by-multi-layer-Kawato-Setoyama",
            "title": {
                "fragments": [],
                "text": "Feedback error learning of movement by multi-layer neural network"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145556695"
                        ],
                        "name": "I. Kanter",
                        "slug": "I.-Kanter",
                        "structuredName": {
                            "firstName": "Ido",
                            "lastName": "Kanter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Kanter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759839"
                        ],
                        "name": "S. Solla",
                        "slug": "S.-Solla",
                        "structuredName": {
                            "firstName": "Sara",
                            "lastName": "Solla",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Solla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18303822,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "0c43153a3627c7d98cc09f909c232f3899597204",
            "isKey": false,
            "numCitedBy": 92,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "The learning time of a simple neural network model is obtained through an analytic computation of the eigenvalue spectrum for the Hessian matrix, which describes the second order properties of the cost function in the space of coupling coefficients. The form of the eigenvalue distribution suggests new techniques for accelerating the learning process, and provides a theoretical justification for the choice of centered versus biased state variables."
            },
            "slug": "Second-Order-Properties-of-Error-Surfaces:-Learning-LeCun-Kanter",
            "title": {
                "fragments": [],
                "text": "Second Order Properties of Error Surfaces: Learning Time and Generalization"
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 1990"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51902284"
                        ],
                        "name": "C. Skarda",
                        "slug": "C.-Skarda",
                        "structuredName": {
                            "firstName": "Christine",
                            "lastName": "Skarda",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Skarda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144784302"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Freeman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18498339,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "fa18cf2efa4c454da93178180258524f9a0add17",
            "isKey": false,
            "numCitedBy": 1933,
            "numCiting": 206,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Recent \u201cconnectionist\u201d models provide a new explanatory alternative to the digital computer as a model for brain function. Evidence from our EEG research on the olfactory bulb suggests that the brain may indeed use computational mechanisms like those found in connectionist models. In the present paper we discuss our data and develop a model to describe the neural dynamics responsible for odor recognition and discrimination. The results indicate the existence of sensory- and motor-specific information in the spatial dimension of EEG activity and call for new physiological metaphors and techniques of analysis. Special emphasis is placed in our model on chaotic neural activity. We hypothesize that chaotic behavior serves as the essential ground state for the neural perceptual apparatus, and we propose a mechanism for acquiring new forms of patterned activity corresponding to new learned odors. Finally, some of the implications of our neural model for behavioral theories are briefly discussed. Our research, in concert with the connectionist work, encourages a reevaluation of explanatory models that are based only on the digital computer metaphor."
            },
            "slug": "How-brains-make-chaos-in-order-to-make-sense-of-the-Skarda-Freeman",
            "title": {
                "fragments": [],
                "text": "How brains make chaos in order to make sense of the world"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A model to describe the neural dynamics responsible for odor recognition and discrimination is developed and it is hypothesized that chaotic behavior serves as the essential ground state for the neural perceptual apparatus and a mechanism for acquiring new forms of patterned activity corresponding to new learned odors is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Behavioral and Brain Sciences"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708309"
                        ],
                        "name": "J. Shynk",
                        "slug": "J.-Shynk",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shynk",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shynk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39093506"
                        ],
                        "name": "Subhasish Subhasish",
                        "slug": "Subhasish-Subhasish",
                        "structuredName": {
                            "firstName": "Subhasish",
                            "lastName": "Subhasish",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Subhasish Subhasish"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 367,
                                "start": 362
                            }
                        ],
                        "text": "Since the acceleration of convergence in these gradient systems is such an important issue, it can be helpful to know some of the techniques used to analyze the limitations of convergence under various conditions in systems of this sort, and of some other techniques for accelerating their convergence; see [139, page 304] and [140], [141], [142], [143], [144], [145], [146], [147], [148]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122604038,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "44fee1b297f5b31a7f2d873643b8f2268b2fa456",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Several modifications of the well-known least-mean-square (LMS) algorithm have been proposed for improved operation. The authors analyze one such recent innovation that corresponds to the ordinary LMS algorithm with an additional momentum term, parameterized by the factor alpha . The analysis of convergence in the mean yields some novel behavior insofar that it leads to complex eigenvalues of the transition matrix for the mean weight vector. The convergence in the mean-square analysis demonstrates that instability will occur as alpha tends closer to 1, a result not predicted by the analysis of convergence in the mean.<<ETX>>"
            },
            "slug": "The-LMS-algorithm-with-momentum-updating-Shynk-Subhasish",
            "title": {
                "fragments": [],
                "text": "The LMS algorithm with momentum updating"
            },
            "venue": {
                "fragments": [],
                "text": "1988., IEEE International Symposium on Circuits and Systems"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6075523"
                        ],
                        "name": "P. Kienker",
                        "slug": "P.-Kienker",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Kienker",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Kienker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 150
                            }
                        ],
                        "text": "Hidden units make it possible for networks to discover and exploit regularities of the task at hand, such as symmetries or replicated structure [56], [57], and training procedures capable of exploiting hidden units, such as the Boltzmann machine learning procedure [58] and backpropagation [59], [60], [61], [62], are behind much of the current excitement in the neural network eld [63]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5660814,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b300e1bbb6ad0d513db2eeb64a2508b4fafb9da6",
            "isKey": false,
            "numCitedBy": 107,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-symmetry-groups-with-hidden-units:-beyond-Sejnowski-Kienker",
            "title": {
                "fragments": [],
                "text": "Learning symmetry groups with hidden units: beyond the perceptron"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2024710"
                        ],
                        "name": "A. Weigend",
                        "slug": "A.-Weigend",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Weigend",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Weigend"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794321"
                        ],
                        "name": "B. Huberman",
                        "slug": "B.-Huberman",
                        "structuredName": {
                            "firstName": "Bernardo",
                            "lastName": "Huberman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Huberman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 217236,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f707a81a278d1598cd0a4493ba73f22dcdf90639",
            "isKey": false,
            "numCitedBy": 655,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Inspired by the information theoretic idea of minimum description length, we add a term to the back propagation cost function that penalizes network complexity. We give the details of the procedure, called weight-elimination, describe its dynamics, and clarify the meaning of the parameters involved. From a Bayesian perspective, the complexity term can be usefully interpreted as an assumption about prior distribution of the weights. We use this procedure to predict the sunspot time series and the notoriously noisy series of currency exchange rates."
            },
            "slug": "Generalization-by-Weight-Elimination-with-to-Weigend-Rumelhart",
            "title": {
                "fragments": [],
                "text": "Generalization by Weight-Elimination with Application to Forecasting"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This work adds a term to the back propagation cost function that penalizes network complexity, called weight-elimination, and uses this procedure to predict the sunspot time series and the notoriously noisy series of currency exchange rates."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145020250"
                        ],
                        "name": "R. P. Gorman",
                        "slug": "R.-P.-Gorman",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Gorman",
                            "middleNames": [
                                "Paul"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. P. Gorman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6956398,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7bdaefdd9954b75fdf305135b27105214c8eac66",
            "isKey": false,
            "numCitedBy": 1149,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Analysis-of-hidden-units-in-a-layered-network-to-Gorman-Sejnowski",
            "title": {
                "fragments": [],
                "text": "Analysis of hidden units in a layered network trained to classify sonar targets"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40268570"
                        ],
                        "name": "A. J. Robinson",
                        "slug": "A.-J.-Robinson",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Robinson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Robinson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2086268"
                        ],
                        "name": "F. Failside",
                        "slug": "F.-Failside",
                        "structuredName": {
                            "firstName": "F.",
                            "lastName": "Failside",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Failside"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10802530,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b247fe6efc9e1011555428647f390dd98bdd3446",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Error propagation nets have been shown to be able to learn a variety of tasks in which a static input pattern is mapped onto a static output pattern. This paper presents a generalisation of these nets to deal with time varying, or dynamic patterns, and three possible architectures are explored. As an example, dynamic nets are applied to the problem of speech coding, in which a time sequence of speech data are coded by one net and decoded by another. The use of dynamic nets gives a better signal to noise ratio than that achieved using static nets."
            },
            "slug": "Static-and-Dynamic-Error-Propagation-Networks-with-Robinson-Failside",
            "title": {
                "fragments": [],
                "text": "Static and Dynamic Error Propagation Networks with Application to Speech Coding"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper presents a generalisation of error propagation nets to deal with time varying, or dynamic patterns, and three possible architectures are explored."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111403674"
                        ],
                        "name": "Michael A. Cohen",
                        "slug": "Michael-A.-Cohen",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Cohen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael A. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682174"
                        ],
                        "name": "S. Grossberg",
                        "slug": "S.-Grossberg",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Grossberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Grossberg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 233,
                                "start": 229
                            }
                        ],
                        "text": "Some simple linear conditions on the weights, such as zero-diagonal symmetry (wij = wji, wii = 0) guarantee that the Lyopunov function L = Xi;j wijyiyj +Xi (yi log yi + (1 yi) log(1 yi)) (5) decreases until a xedpoint is reached [83]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2215551,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "00e6b6ea28c0217d7c7e90824c17b37528f69104",
            "isKey": false,
            "numCitedBy": 2238,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Systems that are competitive and possess symmetric interactions admit a global Lyapunov function. However, a global Lyapunov function whose equilibrium set can be effectively analyzed has not yet been discovered. It remains an open question whether the Lyapunov function approach, which requires a study of equilibrium points, or an alternative global approach, such as the Lyapunov functional approach, which sidesteps a direct study of equilibrium points will ultimately handle all of the physically important cases."
            },
            "slug": "Absolute-stability-of-global-pattern-formation-and-Cohen-Grossberg",
            "title": {
                "fragments": [],
                "text": "Absolute stability of global pattern formation and parallel memory storage by competitive neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It remains an open question whether the Lyapunov function approach, which requires a study of equilibrium points, or an alternative global approach, such as the LyAPunov functional approach, will ultimately handle all of the physically important cases."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Systems, Man, and Cybernetics"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1822055"
                        ],
                        "name": "Raymond L. Watrous",
                        "slug": "Raymond-L.-Watrous",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Watrous",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raymond L. Watrous"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2091434"
                        ],
                        "name": "G. Kuhn",
                        "slug": "G.-Kuhn",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Kuhn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kuhn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16369582,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2d86ff53e0cbf244eb0aac8189ced50b39196185",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Second-order recurrent networks that recognize simple finite state languages over {0,1}* are induced from positive and negative examples. Using the complete gradient of the recurrent network and sufficient training examples to constrain the definition of the language to be induced, solutions are obtained that correctly recognize strings of arbitrary length. A method for extracting a finite state automaton corresponding to an optimized network is demonstrated."
            },
            "slug": "Induction-of-Finite-State-Automata-Using-Recurrent-Watrous-Kuhn",
            "title": {
                "fragments": [],
                "text": "Induction of Finite-State Automata Using Second-Order Recurrent Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "Second-order recurrent networks that recognize simple finite state languages over {0,1}* are induced from positive and negative examples and solutions are obtained that correctly recognize strings of arbitrary length."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5721376"
                        ],
                        "name": "A. Bryson",
                        "slug": "A.-Bryson",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Bryson",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bryson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70473089"
                        ],
                        "name": "W. Denham",
                        "slug": "W.-Denham",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Denham",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Denham"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "One is direct, using the calculus of variations [98]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 308,
                                "start": 304
                            }
                        ],
                        "text": "Since we wish to know the e ect of making this in nitesimal change to wij throughout time, we integrate over the entire interval, yielding @E @wij = Z t1 t0 yi 0(xj)zjdt: (27) One can also derive (26), (27) and (37) using the calculus of variations and Lagrange multipliers, as in optimal control theory [98], [99]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 120635375,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "90470d612cc0d749b3ecdcfd914bdd592d9500bf",
            "isKey": false,
            "numCitedBy": 423,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "SYSTEMATIC and rapid steepest-ascent numerical procedure is described for determining optimum programs for nonlinear systems with terminal constraints. The procedure uses the concept of local linearization around a nominal (nonoptimum) path. The effect on the terminal conditions of a small change in the control variable program is determined by numerical integration of the adjoint differential equations for small perturbations about the nominal path. Having these adjoint (or influence) functions, it is then possible to determine the change in the control variable program that gives maximum increase in the pay-off function for a given mean-square perturbation of the control variable program while simultaneously changing the terminal quantities by desired amounts. By repeating this process in small steps, a control variable program that minimizes one quantity and yields specified values of other terminal quantities can be approached as closely as desired. Three numerical examples are presented: (a) The angle-of-attack program for a typical supersonic interceptor to climb to altitude in minimum time is determined with and without specified terminal velocity and heading. (6) The angle-of-attack program for the same interceptor to climb to maximum altitude is determined, (c) The angle-of-attack program is determined for a hypersonic orbital glider to obtain maximum surface range starting from satellite speed at 300,000 ft altitude."
            },
            "slug": "A-Steepest-Ascent-Method-for-Solving-Optimum-Bryson-Denham",
            "title": {
                "fragments": [],
                "text": "A Steepest-Ascent Method for Solving Optimum Programming Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "By repeating this process in small steps, a control variable program that minimizes one quantity and yields specified values of other terminal quantities can be approached as closely as desired."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144473519"
                        ],
                        "name": "M. Mozer",
                        "slug": "M.-Mozer",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Mozer",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mozer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109535043"
                        ],
                        "name": "Sreerupa Das",
                        "slug": "Sreerupa-Das",
                        "structuredName": {
                            "firstName": "Sreerupa",
                            "lastName": "Das",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sreerupa Das"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9847661,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ef873b940a5acfeb45796fb6d98163300f8903e6",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a neural net architecture that can discover hierarchical and recursive structure in symbol strings. To detect structure at multiple levels, the architecture has the capability of reducing symbols substrings to single symbols, and makes use of an external stack memory. In terms of formal languages, the architecture can learn to parse strings in an LR(O) context-free grammar. Given training sets of positive and negative exemplars, the architecture has been trained to recognize many different grammars. The architecture has only one layer of modifiable weights, allowing for a straightforward interpretation of its behavior."
            },
            "slug": "A-Connectionist-Symbol-Manipulator-that-Discovers-Mozer-Das",
            "title": {
                "fragments": [],
                "text": "A Connectionist Symbol Manipulator that Discovers the Structure of Context-Free Languages"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A neural net architecture that can discover hierarchical and recursive structure in symbol strings and has the capability of reducing symbols substrings to single symbols, and makes use of an external stack memory."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69848015"
                        ],
                        "name": "T. Uchiyama",
                        "slug": "T.-Uchiyama",
                        "structuredName": {
                            "firstName": "Tadasu",
                            "lastName": "Uchiyama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Uchiyama"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143643764"
                        ],
                        "name": "K. Shimohara",
                        "slug": "K.-Shimohara",
                        "structuredName": {
                            "firstName": "Katsunori",
                            "lastName": "Shimohara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Shimohara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060277030"
                        ],
                        "name": "Y. Tokunaga",
                        "slug": "Y.-Tokunaga",
                        "structuredName": {
                            "firstName": "Yukio",
                            "lastName": "Tokunaga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Tokunaga"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18520687,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3344845a50571543d029a772a9f88c8a5151e678",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "A modified leaky integrator, that is, a neuron model that generates continuous output according to its activation history, is proposed for temporal pattern processing. A modified leaky integrator network is described, and its applicability to temporal pattern processing is discussed theoretically. The learning algorithm for the modified leaky integrator network is presented, and the efficiency in temporal pattern processing is shown through simulations.<<ETX>>"
            },
            "slug": "A-modified-leaky-integrator-network-for-temporal-Uchiyama-Shimohara",
            "title": {
                "fragments": [],
                "text": "A modified leaky integrator network for temporal pattern processing"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A modified leaky integrator, that is, a neuron model that generates continuous output according to its activation history, is proposed for temporal pattern processing."
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2547681"
                        ],
                        "name": "S. Dreyfus",
                        "slug": "S.-Dreyfus",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Dreyfus",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dreyfus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 122712889,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "492d70a8b53ea6ee98c4b84964fe0a859fb67bef",
            "isKey": false,
            "numCitedBy": 272,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Dynamic-Programming-and-the-Calculus-of-Variations-Dreyfus",
            "title": {
                "fragments": [],
                "text": "Dynamic Programming and the Calculus of Variations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1960
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700974"
                        ],
                        "name": "Barak A. Pearlmutter",
                        "slug": "Barak-A.-Pearlmutter",
                        "structuredName": {
                            "firstName": "Barak",
                            "lastName": "Pearlmutter",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barak A. Pearlmutter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9490,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "debfde8d6cd86cf61b50e9824cb2ff6bafecd507",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Batch gradient descent, \u0394w(t) = -\u03bddE/dw(t), converges to a minimum of quadratic form with a time constant no better than 1/4\u03bbmax/\u03bbmin where \u03bbmin and \u03bbmax are the minimum and maximum eigenvalues of the Hessian matrix of E with respect to w. It was recently shown that adding a momentum term \u0394w(t) = -\u03bddE/dw(t) + \u03b1\u0394w(t - 1) improves this to 1/4\u221a\u03bbmax/\u03bbmin, although only in the batch case. Here we show that second-order momentum, \u0394w(t) = -\u03bddE/dw(t) + \u03b1\u0394w(t -1) + \u03b2\u0394w(t - 2), can lower this no further. We then regard gradient descent with momentum as a dynamic system and explore a non quadratic error surface, showing that saturation of the error accounts for a variety of effects observed in simulations and justifies some popular heuristics."
            },
            "slug": "Gradient-Descent:-Second-Order-Momentum-and-Error-Pearlmutter",
            "title": {
                "fragments": [],
                "text": "Gradient Descent: Second Order Momentum and Saturating Error"
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37274089"
                        ],
                        "name": "D. Henderson",
                        "slug": "D.-Henderson",
                        "structuredName": {
                            "firstName": "Donnie",
                            "lastName": "Henderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Henderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2799635"
                        ],
                        "name": "R. Howard",
                        "slug": "R.-Howard",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Howard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Howard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34859193"
                        ],
                        "name": "W. Hubbard",
                        "slug": "W.-Hubbard",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "Hubbard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Hubbard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041866"
                        ],
                        "name": "L. Jackel",
                        "slug": "L.-Jackel",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Jackel",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jackel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 41312633,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8e8f3c8d4418c8d62e306538c9c1292635e9d27",
            "isKey": false,
            "numCitedBy": 7828,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification."
            },
            "slug": "Backpropagation-Applied-to-Handwritten-Zip-Code-LeCun-Boser",
            "title": {
                "fragments": [],
                "text": "Backpropagation Applied to Handwritten Zip Code Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This paper demonstrates how constraints from the task domain can be integrated into a backpropagation network through the architecture of the network, successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1456777230"
                        ],
                        "name": "E. Rios-Patron",
                        "slug": "E.-Rios-Patron",
                        "structuredName": {
                            "firstName": "Ernesto",
                            "lastName": "Rios-Patron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Rios-Patron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144960825"
                        ],
                        "name": "R. Braatz",
                        "slug": "R.-Braatz",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Braatz",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Braatz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7259480,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "10d3578f425f42c6115c69f8404fc720fb76c305",
            "isKey": false,
            "numCitedBy": 1821,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Referring to the above said paper by Narendra-Parthasarathy (ibid., vol.1, p4-27 (1990)), it is noted that the given Example 2 (p.15) has a third equilibrium state corresponding to the point (0.5, 0.5)."
            },
            "slug": "On-the-\"Identification-and-control-of-dynamical-Rios-Patron-Braatz",
            "title": {
                "fragments": [],
                "text": "On the \"Identification and control of dynamical systems using neural networks\""
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47576611"
                        ],
                        "name": "G. Heidbreder",
                        "slug": "G.-Heidbreder",
                        "structuredName": {
                            "firstName": "Glenn",
                            "lastName": "Heidbreder",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Heidbreder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110727029"
                        ],
                        "name": "Hua Lee",
                        "slug": "Hua-Lee",
                        "structuredName": {
                            "firstName": "Hua",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hua Lee"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 187
                            }
                        ],
                        "text": "All these problems can be solved in a single stroke by noting that the correct zero-knowledge hypothesis for scale parameters is not at in their values, but rather at in their log values [131]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61052698,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5b3561e46b35c4ac351f3822d96f1634c3ba7d10",
            "isKey": false,
            "numCitedBy": 310,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : This volume contains selections from among the presentations at the Thirteenth International Workshop on Maximum Entropy and Bayesian Methods- MAXENT93 for short- held at the University of California, Santa Barbara (UCSB), August 1-5, 1993. This annual workshop is devoted to the theory and practice of Bayesian probability and the use of the maximum entropy principle in assigning prior probabilities. Like its predecessors, MAXENT93 attracted researchers and scholars representing a wide diversity of disciplines and applications. These included physicists, geophysicists, astronomers, statisticians, engineers, and economists, among others. Indeed Bayesian methods increasingly compel the interest of any who would apply scientific inference. The impressive successes, so evident in the proceedings of the past workshops, when adherence to Bayesian principles replaces popular ad hoc approaches in problems of inference, continue. Many are reported in this volume. It is perhaps indicative of the growing acceptance of Bayesian methods that the most prominent controversy at the thirteenth workshop was not a Bayesian- frequents confrontation but rather a disagreement over the suitability of using an approximation in the Bayesian formalism."
            },
            "slug": "Maximum-Entropy-and-Bayesian-Methods.-Heidbreder-Lee",
            "title": {
                "fragments": [],
                "text": "Maximum Entropy and Bayesian Methods."
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This volume contains selections from among the presentations at the Thirteenth International Workshop on Maximum Entropy and Bayesian Methods- MAXENT93 for short- held at the University of California, Santa Barbara (UCSB), August 1-5, 1993."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145031933"
                        ],
                        "name": "J. Blom",
                        "slug": "J.-Blom",
                        "structuredName": {
                            "firstName": "Joke",
                            "lastName": "Blom",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Blom"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399510610"
                        ],
                        "name": "J. M. Sanz-Serna",
                        "slug": "J.-M.-Sanz-Serna",
                        "structuredName": {
                            "firstName": "Jes\u00fas",
                            "lastName": "Sanz-Serna",
                            "middleNames": [
                                "Mar\u00eda"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. M. Sanz-Serna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690674"
                        ],
                        "name": "J. Verwer",
                        "slug": "J.-Verwer",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Verwer",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Verwer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 122729649,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f05debe71ab89dc5bd612cc4e9f498b09419d532",
            "isKey": false,
            "numCitedBy": 76,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-simple-moving-grid-methods-for-one-dimensional-Blom-Sanz-Serna",
            "title": {
                "fragments": [],
                "text": "On simple moving grid methods for one-dimensional evolutionary partial differential equations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144450891"
                        ],
                        "name": "A. Gelb",
                        "slug": "A.-Gelb",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Gelb",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gelb"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60189955,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "22b125851acd9ab6d16485104eb554a13367b7a7",
            "isKey": false,
            "numCitedBy": 4854,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This is the first book on the optimal estimation that places its major emphasis on practical applications, treating the subject more from an engineering than a mathematical orientation. Even so, theoretical and mathematical concepts are introduced and developed sufficiently to make the book a self-contained source of instruction for readers without prior knowledge of the basic principles of the field. The work is the product of the technical staff of the The Analytic Sciences Corporation (TASC), an organization whose success has resulted largely from its applications of optimal estimation techniques to a wide variety of real situations involving large-scale systemsArthur Gelb writes in the Foreword that \"It is our intent throughout to provide a simple and interesting picture of the central issues underlying modern estimation theory and practice. Heuristic, rather than theoretically elegant, arguments are used extensively, with emphasis on physical insights and key questions of practical importance.\"Numerous illustrative examples, many based on actual applications, have been interspersed throughout the text to lead the student to a concrete understanding of the theoretical material. The inclusion of problems with \"built-in\" answers at the end of each of the nine chapters further enhances the self-study potential of the text.After a brief historical prelude, the book introduces the mathematics underlying random process theory and state-space characterization of linear dynamic systems. The theory and practice of optimal estimation is them presented, including filtering, smoothing, and prediction. Both linear and non-linear systems, and continuous- and discrete-time cases, are covered in considerable detail. New results are described concerning the application of covariance analysis to non-linear systems and the connection between observers and optimal estimators. The final chapters treat such practical and often pivotal issues as suboptimal structure, and computer loading considerations.This book is an outgrowth of a course given by TASC at a number of US Government facilities. Virtually all of the members of the TASC technical staff have, at one time and in one way or another, contributed to the material contained in the work"
            },
            "slug": "Applied-Optimal-Estimation-Gelb",
            "title": {
                "fragments": [],
                "text": "Applied Optimal Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "This is the first book on the optimal estimation that places its major emphasis on practical applications, treating the subject more from an engineering than a mathematical orientation, and the theory and practice of optimal estimation is presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788375"
                        ],
                        "name": "D. Waltz",
                        "slug": "D.-Waltz",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Waltz",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Waltz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145539951"
                        ],
                        "name": "J. Pollack",
                        "slug": "J.-Pollack",
                        "structuredName": {
                            "firstName": "Jordan",
                            "lastName": "Pollack",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pollack"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 374,
                                "start": 371
                            }
                        ],
                        "text": "First, recurrent networks are capable of settling to a solution that satis es many constraints [1], as in a vision system which relaxes to an interpretation of an image which maximally satis es a complex set of con icting constraints [2], [3], [4], [5], [6], a system which relaxes to nd a posture for a robot satisfying many criteria [7], and models of language parsing [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17836106,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c96fe25817c5fca96719cfa56cdaeeb2d17c93d7",
            "isKey": false,
            "numCitedBy": 518,
            "numCiting": 86,
            "paperAbstract": {
                "fragments": [],
                "text": "This is a description of research in developing a natural language processing system with modular knowledge sources but strongly interactive processing. The system offers insights into a variety of linguistic phenomena and allows easy testing of a variety of hypotheses. Language interpretation takes place on a activation network which is dynamically created from input, recent context, and long-term knowledge. Initially ambiguous and unstable, the network settles on a single interpretation, using a parallel, analog relaxation process. We also describe a parallel model for the representation of context and of the priming of concepts. Examples illustrating contextual influence on meaning interpretation and \u201csemantic garden path\u201d sentence processing, among other issues, are included."
            },
            "slug": "Massively-Parallel-Parsing:-A-Strongly-Interactive-Waltz-Pollack",
            "title": {
                "fragments": [],
                "text": "Massively Parallel Parsing: A Strongly Interactive Model of Natural Language Interpretation"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This work describes a parallel model for the representation of context and of the priming of concepts in a natural language processing system with modular knowledge sources but strongly interactive processing."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20589514"
                        ],
                        "name": "R. Mehra",
                        "slug": "R.-Mehra",
                        "structuredName": {
                            "firstName": "Raman",
                            "lastName": "Mehra",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mehra"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 238574860,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "70bde22ec9f30e6b107db1c4ac827e63282302c8",
            "isKey": false,
            "numCitedBy": 1166,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "A Kalman filter requires an exact knowledge of the process noise covariance matrix Q and the measurement noise covariance matrix R . Here we consider the case in which the true values of Q and R are unknown. The system is assumed to be constant, and the random inputs are stationary. First, a correlation test is given which checks whether a particular Kalman filter is working optimally or not. If the filter is suboptimal, a technique is given to obtain asymptotically normal, unbiased, and consistent estimates of Q and R . This technique works only for the case in which the form of Q is known and the number of unknown elements in Q is less than n \\times r where n is the dimension of the state vector and r is the dimension of the measurement vector. For other cases, the optimal steady-state gain K op is obtained directly by an iterative procedure without identifying Q . As a corollary, it is shown that the steady-state optimal Kalman filter gain K op depends only on n \\times r linear functionals of Q . The results are first derived for discrete systems. They are then extended to continuous systems. A numerical example is given to show the usefulness of the approach."
            },
            "slug": "On-the-identification-of-variances-and-adaptive-Mehra",
            "title": {
                "fragments": [],
                "text": "On the identification of variances and adaptive Kalman filtering"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47055692"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59694629,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fa277dfe3645463a25432282563fca4891d846ea",
            "isKey": false,
            "numCitedBy": 261,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The following paper summarizes the major properties and applications of a collection of algorithms involving differentiation and optimization at minimum cost. The areas of application include the sensitivity analysis of models, new work in statistical or econometric estimation, optimization, artificial intelligence and neuron modelling. The details, references and derivations can be obtained by requesting \u201eSensitivity Analysis Methods for Nonlinear Systems\u201c from Forecast Analysis and Evaluation Team, Quality Assurance, OSS/EIA, Room 7413, Department of Energy, Washington, DC 20461."
            },
            "slug": "Applications-of-advances-in-nonlinear-sensitivity-Werbos",
            "title": {
                "fragments": [],
                "text": "Applications of advances in nonlinear sensitivity analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "The following paper summarizes the major properties and applications of a collection of algorithms involving differentiation and optimization at minimum cost, including the sensitivity analysis of models, new work in statistical or econometric estimation, optimization, artificial intelligence and neuron modelling."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695714"
                        ],
                        "name": "B. Anderson",
                        "slug": "B.-Anderson",
                        "structuredName": {
                            "firstName": "Brian.",
                            "lastName": "Anderson",
                            "middleNames": [
                                "D.",
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Anderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109008954"
                        ],
                        "name": "J. Moore",
                        "slug": "J.-Moore",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Moore",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Moore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48841496"
                        ],
                        "name": "M. Eslami",
                        "slug": "M.-Eslami",
                        "structuredName": {
                            "firstName": "Mansour",
                            "lastName": "Eslami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Eslami"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 198
                            }
                        ],
                        "text": "[125], [126] have pointed out that RTRL is related to a version of the [127] lter, in the extension that allows it to apply to nonlinear systems, namely the extended Kalman lter (EKF) [128], [107], [129]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 39637449,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a544d9c589367a470e96f9fd0a01587b4cdec2ce",
            "isKey": false,
            "numCitedBy": 3564,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Estimation theory has had a tremendous impact on many problem areas over the past two decades. Beginning with its original use in the aerospace industry, its applications can now be found in many different areas such as control and communjcations, power systems, transportation systems, bioengineering, image processing, etc. Along with linear system theory and optimal control, a course in estimation theorycan be found in the graduate system and control curriculum,of most schools in the country. In fact, it is probably one of the most,salable courses as far as employment is concerned. However, despite its economic value and the amount of activities in the field, very few books on estimation theory have appeared recently. This book helps to fill the void in the market and does that in a superb manner. Although the book is called OptimalFiltering, the coverage is restricted to discrete time filtering. A more appropriate title would thus be Optimal Discrete Time ,Filtering. The authors\u2019 decision to concentrate on discrete time f lters is due to \u201crecent technological developments as well as the easier path offered students and instructors.\u201d This is probably a wise move since a thorough treatment of continuous time filtering will require a better knowledge o f stochastic processes than most graduate students or engineers will have. As it stands now, the text requires little background beyond that of linear system theory and probability theory. Written by active researchers, in the area, the book covers the  standard  topics  such  as  Kalman filtering, innovations processes, smoothing, and adaptive and nonlinear estimation. Much of the material in the book has been around for a long time and has been widely used, by practitioners in the area: Some results are more recent. However,-it .has been difficult to locate all of them presented in a n organized manner within a single text. This is especially true of the chapters dealing with the computation aspects and nonlinear and adaptive estimation. After a short introductory chapter, Chapter 2 introduces the mathematical model to be used throughout most of the book. The discrete time Kalman filter is 1 hen presented in Chapter 3, along with some applications. Chapter 4 contains a treatment"
            },
            "slug": "Optimal-Filtering-Anderson-Moore",
            "title": {
                "fragments": [],
                "text": "Optimal Filtering"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This book helps to fill the void in the market and does that in a superb manner by covering the standard topics such as Kalman filtering, innovations processes, smoothing, and adaptive and nonlinear estimation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Systems, Man, and Cybernetics"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87253279"
                        ],
                        "name": "J. Sejnowski",
                        "slug": "J.-Sejnowski",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Sejnowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10379672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1718965f492d4e9fe1d98a3fb83efe671a4aed2c",
            "isKey": false,
            "numCitedBy": 557,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "When a vision system creates an interpretation of some input datn, it assigns truth values or probabilities to intcrnal hypothcses about the world. We present a non-dctcrministic method for assigning truth values that avoids many of the problcms encountered by existing relaxation methods. Instead of rcprcscnting probabilitics with realnumbers, we usc a more dircct encoding in which thc probability \\ associated with a hypotlmis is rcprcscntcd by the probability hat it is in one of two states, true or false. Wc give a particular nondeterministic operator, based on statistical mechanics, for updating the truth values of hypothcses. The operator ensures that the probability of discovering a particular combination of hypothcscs is a simplc function of how good that combination is. Wc show that thcrc is a simple relationship bctween this operator and Bayesian inference, and we describe a learning rule which allows a parallel system to converge on a set ofweights that optimizes its perccptt~al inferences."
            },
            "slug": "OPTIMAL-PERCEPTUAL-INFERENCE-Hinton-Sejnowski",
            "title": {
                "fragments": [],
                "text": "OPTIMAL PERCEPTUAL INFERENCE"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A particular nondeterministic operator is given, based on statistical mechanics, for updating the truth values of hypothcses, and a learning rule is described which allows a parallel system to converge on a set ofweights that optimizes its perccptt~al inferences."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2797623"
                        ],
                        "name": "H. Siegelmann",
                        "slug": "H.-Siegelmann",
                        "structuredName": {
                            "firstName": "Hava",
                            "lastName": "Siegelmann",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Siegelmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790264"
                        ],
                        "name": "Eduardo Sontag",
                        "slug": "Eduardo-Sontag",
                        "structuredName": {
                            "firstName": "Eduardo",
                            "lastName": "Sontag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eduardo Sontag"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5909565,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7509b472cbe7b1fe71a8fccf60f34cc873d1ab63",
            "isKey": false,
            "numCitedBy": 365,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Turing-computability-with-neural-nets-Siegelmann-Sontag",
            "title": {
                "fragments": [],
                "text": "Turing computability with neural nets"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49464494"
                        ],
                        "name": "Kevin J. Lang",
                        "slug": "Kevin-J.-Lang",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Lang",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin J. Lang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 345,
                                "start": 341
                            }
                        ],
                        "text": "It should be noted by engineers that many real-world problems which one might think would require recurrent architectures for their solution turn out to be solvable with feedforward architectures, sometimes augmented with preprocessed inputs such as tapped delay lines, and various other architectural embellishments [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [10], [32], [33], [34], [35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 95
                            }
                        ],
                        "text": "Such \\time delay neural networks\" have proven useful in the domain of speech recognition [20], [22], [21], [115]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1234937,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e08d090d1e586610d636a46004876e9f3ded8209",
            "isKey": false,
            "numCitedBy": 640,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-time-delay-neural-network-architecture-for-word-Lang-Waibel",
            "title": {
                "fragments": [],
                "text": "A time-delay neural network architecture for isolated word recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50381028"
                        ],
                        "name": "L. Cooper",
                        "slug": "L.-Cooper",
                        "structuredName": {
                            "firstName": "Leon",
                            "lastName": "Cooper",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Cooper"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70230929"
                        ],
                        "name": "M. W. Cooper",
                        "slug": "M.-W.-Cooper",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Cooper",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. W. Cooper"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118354964,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "47750bef0ab131c622dda6bc6e030650f60ec823",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Dynamic-Programming-and-the-Calculus-of-Variations-Cooper-Cooper",
            "title": {
                "fragments": [],
                "text": "Dynamic Programming and the Calculus of Variations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18686894,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "909226ce00fbb74306da00911d48651383bf1ae8",
            "isKey": false,
            "numCitedBy": 164,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "Although general network learning rules are of undeniable interest, it is generally agreed that successful accounts of learning must incorporate domain-specific, a priori knowledge. Such knowledge might be used, for example, to determine the structure of a network or its initial weights. The author discusses a third possibility in which domain-specific knowledge is incorporated directly in a network learning rule via a set of constraints on activations. The approach uses the notion of a forward model to give constraints a domain-specific interpretation. This approach is demonstrated with several examples from the domain of motor learning.<<ETX>>"
            },
            "slug": "Generic-constraints-on-underspecified-target-Jordan",
            "title": {
                "fragments": [],
                "text": "Generic constraints on underspecified target trajectories"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The author discusses a third possibility in which domain-specific knowledge is incorporated directly in a network learning rule via a set of constraints on activations, which uses the notion of a forward model to give constraints a domain- specific interpretation."
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49464494"
                        ],
                        "name": "Kevin J. Lang",
                        "slug": "Kevin-J.-Lang",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Lang",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin J. Lang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14625328,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5affc896bcb291bca6e3ba60d34eeac28214e2e",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "It is well known that when an automatic learning algorithm is applied to a fixed corpus of data, the size of the corpus places an upper bound on the number of degrees of freedom that the model can contain if it is to generalize well. Because the amount of hardware in a neural network typically increases with the dimensionality of its inputs, it can be challenging to build a high-performance network for classifying large input patterns. In this paper, several techniques for addressing this problem are discussed in the context of an isolated word recognition task."
            },
            "slug": "Dimensionality-Reduction-and-Prior-Knowledge-in-Lang-Hinton",
            "title": {
                "fragments": [],
                "text": "Dimensionality Reduction and Prior Knowledge in E-Set Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "Several techniques for addressing the problem of the size of the corpus and the number of degrees of freedom that the model can contain if it is to generalize well are discussed in the context of an isolated word recognition task."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2911738"
                        ],
                        "name": "D. Angluin",
                        "slug": "D.-Angluin",
                        "structuredName": {
                            "firstName": "Dana",
                            "lastName": "Angluin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Angluin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 286,
                                "start": 282
                            }
                        ],
                        "text": "In particular, grammar learning, although intriguing and progressing rapidly [43], [44], [45], [46], [47], [48], [49], typically involves recurrent neural networks as components of more complex systems, and also at present is inferior in practice to discrete algorithmic techniques [50], [51]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11873053,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "225331d1700a9544545cc7c54a63c1b485269ce7",
            "isKey": false,
            "numCitedBy": 2035,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-Regular-Sets-from-Queries-and-Angluin",
            "title": {
                "fragments": [],
                "text": "Learning Regular Sets from Queries and Counterexamples"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Comput."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 117579368,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "id": "2ab4641653283557000c94bc444b1be3fbc6ee78",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "The Boltzmann machine is a nonlinear network of stochastic binary processing units that interact pairwise through symmetric connection strengths. In a third\u2010order Boltzmann machine, triples of units interact through symmetric conjunctive interactions. The Boltzmann learning algorithm is generalized to higher\u2010order Boltzmann machine should be much faster than for a second\u2010order Boltzmann machine based on pairwise interactions."
            },
            "slug": "Higher\u2010order-Boltzmann-machines-Sejnowski",
            "title": {
                "fragments": [],
                "text": "Higher\u2010order Boltzmann machines"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The Boltzmann learning algorithm is generalized to higher\u2010order Boltzman machine, which should be much faster than for a second\u2010order Bolzmann machine based on pairwise interactions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145847131"
                        ],
                        "name": "S. Kirkpatrick",
                        "slug": "S.-Kirkpatrick",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Kirkpatrick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kirkpatrick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5882723"
                        ],
                        "name": "C. D. Gelatt",
                        "slug": "C.-D.-Gelatt",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Gelatt",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. D. Gelatt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "88645967"
                        ],
                        "name": "M. Vecchi",
                        "slug": "M.-Vecchi",
                        "structuredName": {
                            "firstName": "Michelle",
                            "lastName": "Vecchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Vecchi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 205939,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "dd5061631a4d11fa394f4421700ebf7e78dcbc59",
            "isKey": false,
            "numCitedBy": 39631,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "There is a deep and useful connection between statistical mechanics (the behavior of systems with many degrees of freedom in thermal equilibrium at a finite temperature) and multivariate or combinatorial optimization (finding the minimum of a given function depending on many parameters). A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems. This connection to statistical mechanics exposes new information and provides an unfamiliar perspective on traditional optimization problems and methods."
            },
            "slug": "Optimization-by-Simulated-Annealing-Kirkpatrick-Gelatt",
            "title": {
                "fragments": [],
                "text": "Optimization by Simulated Annealing"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717841"
                        ],
                        "name": "R. Szeliski",
                        "slug": "R.-Szeliski",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Szeliski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Szeliski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 257,
                                "start": 254
                            }
                        ],
                        "text": "First, recurrent networks are capable of settling to a solution that satis es many constraints [1], as in a vision system which relaxes to an interpretation of an image which maximally satis es a complex set of con icting constraints [2], [3], [4], [5], [6], a system which relaxes to nd a posture for a robot satisfying many criteria [7], and models of language parsing [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 38712359,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6cede4bbe7a04732c99c1fd20d9326d0015f5ab9",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "This report examines a number of parallel . algorithms for solving random-dot stereograms. A new class of algorithms based on the Boltzmann Machine is introduced and compared to previously developed algorithms. The report includes a review of the stereo correspondence problem and of cooperative techniques for solving this problem. The use of energy functions for characterizing the computational problem, and the use of stochastic optimization techniques for solving the problem are explained."
            },
            "slug": "Cooperative-algorithms-for-solving-random-dot-Szeliski",
            "title": {
                "fragments": [],
                "text": "Cooperative algorithms for solving random-dot stereograms"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "This report examines a number of parallel algorithms for solving random-dot stereograms and introduces a new class of algorithms based on the Boltzmann Machine, which is compared to previously developed algorithms."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790264"
                        ],
                        "name": "Eduardo Sontag",
                        "slug": "Eduardo-Sontag",
                        "structuredName": {
                            "firstName": "Eduardo",
                            "lastName": "Sontag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eduardo Sontag"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12704725,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "da22a85b3e859139ff4aa40f85597e69ba3ff76d",
            "isKey": false,
            "numCitedBy": 573,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper starts by placing neural net techniques in a general nonlinear control framework. After that, several basic theoretical results on networks are surveyed."
            },
            "slug": "Neural-Networks-for-Control-Sontag",
            "title": {
                "fragments": [],
                "text": "Neural Networks for Control"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "This paper starts by placing neural net techniques in a general nonlinear control framework, and several basic theoretical results on networks are surveyed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32133325"
                        ],
                        "name": "D. Jacobson",
                        "slug": "D.-Jacobson",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Jacobson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Jacobson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 122366806,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "773a8899401199d045e3e4b9138bf9fcf7dcd408",
            "isKey": false,
            "numCitedBy": 82,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, the notion of differential dynamic programming is used to develop new second-order and first-order successive-approximation methods for determining optimal control.The unconstrained, nonlinear control problem is first considered, and a second-order algorithm is developed which has wider application then existing second-variation and second-order algorithms. A new first-order algorithm emerges as a special case of the second-order one. Control inequality constraints are introduced into the problem and a second-order algorithm is devised which is able to solve this constrained problem. It is believed that control constraints have not been handled previously in this way. Again, a first-order algorithm emerges as a special case. The usefulness of the second-order algorithms is illustrated by the computer solution of three control problems.The methods presented in this paper have been extended by the author to solve problems with terminal constraints and implicitly given final time. Details of these procedures are not given in this paper, but the relevant references are cited."
            },
            "slug": "New-second-order-and-first-order-algorithms-for-A-Jacobson",
            "title": {
                "fragments": [],
                "text": "New second-order and first-order algorithms for determining optimal control: A differential dynamic programming approach"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "The notion of differential dynamic programming is used to develop new second-order and first-order successive-approximation methods for determining optimal control and these methods have been extended to solve problems with terminal constraints and implicitly given final time."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49464494"
                        ],
                        "name": "Kevin J. Lang",
                        "slug": "Kevin-J.-Lang",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Lang",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin J. Lang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1010499,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "6b88f41738085c1a2bffe6123541755b1118e5e2",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "One way to achieve viewpoint-invariant shape recognition is to impose a canonical, object-based frame of reference on a shape and to describe the positions, sizes and orientations of the shape's features relative to the imposed frame. This compulation can be implemented in a parallel network of neuron-like processors, but the network has a tendency to make errors of a peculiar kind: When presented with several shapes it sometimes perceives one shape in the position of another. The parameters can be carefully tuned to avoid these \"illusory conjunctions\" in normal circumstances, but they reappear if the visual input is replaced by a random mask before the network has settled down. Treisman and Schmidt (1982) have shown that people make similar errors."
            },
            "slug": "Shape-Recognition-and-Illusory-Conjunctions-Hinton-Lang",
            "title": {
                "fragments": [],
                "text": "Shape Recognition and Illusory Conjunctions"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "One way to achieve viewpoint-invariant shape recognition is to impose a canonical, object-based frame of reference on a shape and to describe the positions, sizes and orientations of the shape's features relative to the imposed frame."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 62245742,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "111fd833a4ae576cfdbb27d87d2f8fc0640af355",
            "isKey": false,
            "numCitedBy": 19355,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-internal-representations-by-error-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning internal representations by error propagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145317070"
                        ],
                        "name": "A. Haddad",
                        "slug": "A.-Haddad",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Haddad",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Haddad"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 36107084,
            "fieldsOfStudy": [
                "Engineering",
                "Mathematics"
            ],
            "id": "d6fb0f67eb2cc76bc88b8422918c165d6dd55890",
            "isKey": false,
            "numCitedBy": 1896,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Optimal FilteringIntroduction to Random Signals and Applied Kalman Filtering with Matlab Exercises and SolutionsApplied Optimal ControlInverse Problem Theory and Methods for Model Parameter EstimationBayesian Estimation and TrackingInverse Methods for Atmospheric SoundingOptimal ControlBayesian Filtering and SmoothingStatistical Orbit DeterminationEstimation and Control of Dynamical SystemsPractical Methods for Optimal Control and Estimation Using Nonlinear ProgrammingApplied Optimal ControlOptimal Control and EstimationOptimal Estimation of Dynamic SystemsOptimal State EstimationOptimal Estimation of Dynamic SystemsBeyond the Kalman Filter: Particle Filters for Tracking ApplicationsEstimation with Applications to Tracking and NavigationApplied Optimal EstimationKalman FilteringProgress in Astronautics and AeronauticsMethods of Model Based Process ControlAn Engineering Approach to Optimal Control and Estimation TheoryOptimal and Robust EstimationApplied Optimal EstimationOptimal Control and EstimationStochastic Processes and Filtering TheoryOptimal Estimation of Dynamic Systems, Second EditionAn Introduction to Optimal Estimation of Dynamical SystemsState Estimation for Dynamic SystemsState Estimation for RoboticsApplied State Estimation and AssociationContinuous Time Dynamical SystemsApplied Optimal Control & EstimationOptimal State Estimation for Process Monitoring, Fault Diagnosis and ControlEstimation, Control, and the Discrete Kalman FilterKalman FiltersState Estimation in ChemometricsStochastic Models, Estimation, and ControlApplied optimal estimation"
            },
            "slug": "Applied-optimal-estimation-Haddad",
            "title": {
                "fragments": [],
                "text": "Applied optimal estimation"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE"
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145478495"
                        ],
                        "name": "D. Sanghi",
                        "slug": "D.-Sanghi",
                        "structuredName": {
                            "firstName": "Dheeraj",
                            "lastName": "Sanghi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Sanghi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41224761"
                        ],
                        "name": "\u00d3. Gudmundsson",
                        "slug": "\u00d3.-Gudmundsson",
                        "structuredName": {
                            "firstName": "\u00d3lafur",
                            "lastName": "Gudmundsson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00d3. Gudmundsson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770467"
                        ],
                        "name": "A. Agrawala",
                        "slug": "A.-Agrawala",
                        "structuredName": {
                            "firstName": "Ashok",
                            "lastName": "Agrawala",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Agrawala"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 290
                            }
                        ],
                        "text": "A unique xedpoint is reached regardless of initial conditions if P ij w 2 ij < max( ) where max( ) is the maximal value of (x) for any x [85], but in practice much weaker bounds on the weights seem to su ce, as indicated by empirical studies of the dynamics of networks with random weights [86]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12307877,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e241eb4ed5fe7cd18edd003c32ac4e853a1a2b26",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Study-of-Network-Dynamics-Sanghi-Gudmundsson",
            "title": {
                "fragments": [],
                "text": "Study of Network Dynamics"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Networks ISDN Syst."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144215175"
                        ],
                        "name": "R. Jacobs",
                        "slug": "R.-Jacobs",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Jacobs",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jacobs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17147798,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9b2842e8db11fa2ec716fc4c4c7de0fae17b0eac",
            "isKey": false,
            "numCitedBy": 133,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The forward modeling approach is a methodology for learning control when data is available in distal coordinate systems. We extend previous work by considering how this methodology can be applied to the optimization of quantities that are distal not only in space but also in time."
            },
            "slug": "Learning-to-Control-an-Unstable-System-with-Forward-Jordan-Jacobs",
            "title": {
                "fragments": [],
                "text": "Learning to Control an Unstable System with Forward Modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work extends previous work by considering how this methodology can be applied to the optimization of quantities that are distal not only in space but also in time."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49464494"
                        ],
                        "name": "Kevin J. Lang",
                        "slug": "Kevin-J.-Lang",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Lang",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin J. Lang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 292,
                                "start": 288
                            }
                        ],
                        "text": "In particular, grammar learning, although intriguing and progressing rapidly [43], [44], [45], [46], [47], [48], [49], typically involves recurrent neural networks as components of more complex systems, and also at present is inferior in practice to discrete algorithmic techniques [50], [51]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7480497,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5d7b922d40b00c9dd7ec3d3de196f21fc4cfbad1",
            "isKey": false,
            "numCitedBy": 175,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Approximate inference of finite state machines from sparse labeled examples has been proved NP-hard when an adversary chooses the target machine and the training set [Ang78, KV89, PW89]. We have, however, empirically found that DFA's are approximately learnable from sparse data when the target machine and training set are selected at random."
            },
            "slug": "Random-DFA's-can-be-approximately-learned-from-Lang",
            "title": {
                "fragments": [],
                "text": "Random DFA's can be approximately learned from sparse uniform examples"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Empirically found that DFA's are approximately learnable from sparse data when the target machine and training set are selected at random."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '92"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1861837498"
                        ],
                        "name": "G. G. Stokes",
                        "slug": "G.-G.-Stokes",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Stokes",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. G. Stokes"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 221060727,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "90006064cafcb0a9ad8a30cffeb56efe7e14129b",
            "isKey": false,
            "numCitedBy": 673075,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "however (for it was the literal soul of the life of the Redeemer, John xv. io), is the peculiar token of fellowship with the Redeemer. That love to God (what is meant here is not God\u2019s love to men) is described in such a case as a perfect love (love that has been perfected), involves no difficulty, for the simple reason that the proposition is purely hypothetical. We must, of course, also take the &dquo;keeping&dquo; in all its stringency. John knows right well that the case supposed here ncver becomes full reality. &dquo; Hereb)\u2019,&dquo; i.e. from the actual realization of love to God. &dquo; TIli7i 7e)e are ill Hinz &dquo;"
            },
            "slug": "\"J.\"-Stokes",
            "title": {
                "fragments": [],
                "text": "\"J.\""
            },
            "venue": {
                "fragments": [],
                "text": "The New Yale Book of Quotations"
            },
            "year": 2021
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 56940469,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "1bd9ac8414a9fbe08c5322381bc3a67041184b95",
            "isKey": false,
            "numCitedBy": 590,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-appeal-of-parallel-distributed-processing-McClelland-Rumelhart",
            "title": {
                "fragments": [],
                "text": "The appeal of parallel distributed processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143766793"
                        ],
                        "name": "A. Rosenfeld",
                        "slug": "A.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Azriel",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Rosenfeld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 441278,
            "fieldsOfStudy": [
                "Computer Science",
                "Art"
            ],
            "id": "b441dd8fb25eddbaf92bc9938afda69627a281ab",
            "isKey": false,
            "numCitedBy": 3876,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Computer-Vision-Rosenfeld",
            "title": {
                "fragments": [],
                "text": "Computer Vision"
            },
            "venue": {
                "fragments": [],
                "text": "Adv. Comput."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714997"
                        ],
                        "name": "K. Doya",
                        "slug": "K.-Doya",
                        "structuredName": {
                            "firstName": "Kenji",
                            "lastName": "Doya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Doya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144074411"
                        ],
                        "name": "M. E. Boyle",
                        "slug": "M.-E.-Boyle",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Boyle",
                            "middleNames": [
                                "E.",
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. E. Boyle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734225"
                        ],
                        "name": "A. Selverston",
                        "slug": "A.-Selverston",
                        "structuredName": {
                            "firstName": "Allen",
                            "lastName": "Selverston",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Selverston"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17238575,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "aa1d17d8bba598ebd71d692e5b553aba7ff16e1c",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "A computer model of the musculoskeletal system of the lobster gastric mill was constructed in order to provide a behavioral interpretation of the rhythmic patterns obtained from isolated stomatogastric ganglion. The model was based on Hill's muscle model and quasi-static approximation of the skeletal dynamics and could simulate the change of chewing patterns by the effect of neuromodulators."
            },
            "slug": "Maaping-Between-Neural-and-Physical-Activities-of-Doya-Boyle",
            "title": {
                "fragments": [],
                "text": "Maaping Between Neural and Physical Activities of the Lobster Gastric Mill"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A computer model of the musculoskeletal system of the lobster gastric mill was constructed in order to provide a behavioral interpretation of the rhythmic patterns obtained from isolated stomatogastric ganglion."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693428"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Davis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143766793"
                        ],
                        "name": "A. Rosenfeld",
                        "slug": "A.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Azriel",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Rosenfeld"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 249
                            }
                        ],
                        "text": "First, recurrent networks are capable of settling to a solution that satis es many constraints [1], as in a vision system which relaxes to an interpretation of an image which maximally satis es a complex set of con icting constraints [2], [3], [4], [5], [6], a system which relaxes to nd a posture for a robot satisfying many criteria [7], and models of language parsing [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3287021,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a44112dbb9a08fc45949607ac76ae6efff749694",
            "isKey": false,
            "numCitedBy": 160,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Cooperating-Processes-for-Low-Level-Vision:-A-Davis-Rosenfeld",
            "title": {
                "fragments": [],
                "text": "Cooperating Processes for Low-Level Vision: A Survey"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153281777"
                        ],
                        "name": "D. Marr",
                        "slug": "D.-Marr",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Marr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Marr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 237,
                                "start": 234
                            }
                        ],
                        "text": "First, recurrent networks are capable of settling to a solution that satis es many constraints [1], as in a vision system which relaxes to an interpretation of an image which maximally satis es a complex set of con icting constraints [2], [3], [4], [5], [6], a system which relaxes to nd a posture for a robot satisfying many criteria [7], and models of language parsing [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 767650,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6bb6eba248b6ef8d228dc33e6113c3e12dd48fee",
            "isKey": false,
            "numCitedBy": 1166,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "The extraction of stereo-disparity information from two images depends upon establishing a correspondence between them. In this article we analyze the nature of the correspondence computation and derive a cooperative algorithm that implements it. We show that this algorithm successfully extracts information from random-dot stereograms, and its implications for the psychophysics and neurophysiology of the visual system are briefly discussed."
            },
            "slug": "Cooperative-computation-of-stereo-disparity.-Marr-Poggio",
            "title": {
                "fragments": [],
                "text": "Cooperative computation of stereo disparity."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that this algorithm successfully extracts information from random-dot stereograms, and its implications for the psychophysics and neurophysiology of the visual system are briefly discussed."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52075048"
                        ],
                        "name": "P. Strevens",
                        "slug": "P.-Strevens",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Strevens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Strevens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 215
                            }
                        ],
                        "text": "com This paper is concerned with learning algorithms for recurrent networks themselves, and not with recurrent networks as elements of larger systems, such as specialized architectures for control [36], [37], [38], [39]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 220781652,
            "fieldsOfStudy": [
                "Education",
                "Linguistics"
            ],
            "id": "09830e7210f5e9d4f204ebad2a2a8def4e9de9f1",
            "isKey": false,
            "numCitedBy": 4178,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "over native-speaking users of English. Secondly, the numerical preponderance of non-native speakers means that it is their communication which is increasing more rapidly and thus dominating the development and evolution of English. Thirdly, it is therefore becoming inescapably necessary for native speakers to accept unfamiliarities in the effective use of English. Fourthly, acceptance of these unfamiliarities will be easier if there is a basis for understanding them."
            },
            "slug": "Iii-Strevens",
            "title": {
                "fragments": [],
                "text": "Iii"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 33244130,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "f6afa2a788e79f180a0591f441caf34a355732cb",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of finding a puppet in a configuration of overlapping, transparent rectangles is used to show how a relaxation algorithm can extract the globally best figure from a network of conflicting local interpretations."
            },
            "slug": "Using-Relaxation-to-find-a-Puppet-Hinton",
            "title": {
                "fragments": [],
                "text": "Using Relaxation to find a Puppet"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The problem of finding a puppet in a configuration of overlapping, transparent rectangles is used to show how a relaxation algorithm can extract the globally best figure from a network of conflicting local interpretations."
            },
            "venue": {
                "fragments": [],
                "text": "AISB"
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2071749017"
                        ],
                        "name": "\u5c0f\u8c37 \u5b66",
                        "slug": "\u5c0f\u8c37-\u5b66",
                        "structuredName": {
                            "firstName": "\u5c0f\u8c37",
                            "lastName": "\u5b66",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u5c0f\u8c37 \u5b66"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 174945071,
            "fieldsOfStudy": [],
            "id": "01b43fb5cee78aa02d1f672fef9a9ceccbedb8b3",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "1993-International-Joint-Conference-on-Neural-\u5c0f\u8c37",
            "title": {
                "fragments": [],
                "text": "1993 International Joint Conference on Neural Networks\u306b\u51fa\u5e2d\u3057\u3066"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "26459113"
                        ],
                        "name": "M. Caudill",
                        "slug": "M.-Caudill",
                        "structuredName": {
                            "firstName": "Maureen",
                            "lastName": "Caudill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Caudill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153925248"
                        ],
                        "name": "C. Butler",
                        "slug": "C.-Butler",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Butler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Butler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 127853816,
            "fieldsOfStudy": [
                "Geography"
            ],
            "id": "70a0ed1d60bdccee721ec6437e7d1c55c96d887a",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "IEEE-First-International-Conference-on-Neural-:-San-Caudill-Butler",
            "title": {
                "fragments": [],
                "text": "IEEE First International Conference on Neural Networks : Sheraton Harbor Island East, San Diego, California, June 21-24, 1987"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2810659"
                        ],
                        "name": "K. Cios",
                        "slug": "K.-Cios",
                        "structuredName": {
                            "firstName": "Krzysztof",
                            "lastName": "Cios",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Cios"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145895848"
                        ],
                        "name": "M. E. Shields",
                        "slug": "M.-E.-Shields",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Shields",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. E. Shields"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120130418,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "13c1b1f6dcad68d92b15ce87333c2cc6593c9263",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Advances-in-neural-information-processing-systems-7-Cios-Shields",
            "title": {
                "fragments": [],
                "text": "Advances in neural information processing systems 7"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2319833"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 113
                            }
                        ],
                        "text": "It was shown independently by Pineda [72] and Alemeida [73] that the error backpropagation algorithm [61], [59], [60] is a special case of a more general error gradient computation procedure."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 80
                            }
                        ],
                        "text": "where zi is the ordered partial derivative of E with respect to yi as de ned in [60], E is an error measure over y(t1), and ei = @E=@yi(t1) is the simple derivative of E with respect to the nal state of a unit."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 300,
                                "start": 296
                            }
                        ],
                        "text": "Hidden units make it possible for networks to discover and exploit regularities of the task at hand, such as symmetries or replicated structure [56], [57], and training procedures capable of exploiting hidden units, such as the Boltzmann machine learning procedure [58] and backpropagation [59], [60], [61], [62], are behind much of the current excitement in the neural network eld [63]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207975157,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "56623a496727d5c71491850e04512ddf4152b487",
            "isKey": false,
            "numCitedBy": 4468,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Beyond-Regression-:-\"New-Tools-for-Prediction-and-Werbos",
            "title": {
                "fragments": [],
                "text": "Beyond Regression : \"New Tools for Prediction and Analysis in the Behavioral Sciences"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1913418"
                        ],
                        "name": "B. Widrow",
                        "slug": "B.-Widrow",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Widrow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Widrow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1976925"
                        ],
                        "name": "M. Hoff",
                        "slug": "M.-Hoff",
                        "structuredName": {
                            "firstName": "Marcian",
                            "lastName": "Hoff",
                            "middleNames": [
                                "E."
                            ],
                            "suffix": "Jr."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hoff"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60830585,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e14b2ff9dc2234df94fc24d89fc25e797d0e9e7",
            "isKey": false,
            "numCitedBy": 2623,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Adaptive-switching-circuits-Widrow-Hoff",
            "title": {
                "fragments": [],
                "text": "Adaptive switching circuits"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516142"
                        ],
                        "name": "D. Tank",
                        "slug": "D.-Tank",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Tank",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Tank"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219867"
                        ],
                        "name": "J. Hopfield",
                        "slug": "J.-Hopfield",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hopfield",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hopfield"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 203175244,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e570de6e8f260804a673dcb53f13b33c13a3014",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "CONCENTRATION-INFORMATION-IN-TIME:-ANALOG-NEURAL-TO-Tank-Hopfield",
            "title": {
                "fragments": [],
                "text": "CONCENTRATION INFORMATION IN TIME: ANALOG NEURAL NETWORKS WITH APPLICATIONS TO SPEECH RECOGNITION PROBLEMS."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144410963"
                        ],
                        "name": "S. Kung",
                        "slug": "S.-Kung",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Kung",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1998157"
                        ],
                        "name": "F. Fallside",
                        "slug": "F.-Fallside",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Fallside",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Fallside"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073062163"
                        ],
                        "name": "J. A. Sorenson",
                        "slug": "J.-A.-Sorenson",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Sorenson",
                            "middleNames": [
                                "A.A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. A. Sorenson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1769629"
                        ],
                        "name": "C. Kamm",
                        "slug": "C.-Kamm",
                        "structuredName": {
                            "firstName": "Candace",
                            "lastName": "Kamm",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Kamm"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60115478,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "c77a39a18bf6d73e78c0e1e6eaa560aebeb0511a",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-networks-for-signal-processing-:-proceedings-Kung-Fallside",
            "title": {
                "fragments": [],
                "text": "Neural networks for signal processing : proceedings of the 1991 IEEE workshop"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066667188"
                        ],
                        "name": "Sepp Hochreiter",
                        "slug": "Sepp-Hochreiter",
                        "structuredName": {
                            "firstName": "Sepp",
                            "lastName": "Hochreiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sepp Hochreiter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "I. Introduction"
                    },
                    "intents": []
                }
            ],
            "corpusId": 60091947,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3f3d13e95c25a8f6a753e38dfce88885097cbd43",
            "isKey": false,
            "numCitedBy": 603,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Untersuchungen-zu-dynamischen-neuronalen-Netzen-Hochreiter",
            "title": {
                "fragments": [],
                "text": "Untersuchungen zu dynamischen neuronalen Netzen"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2952703"
                        ],
                        "name": "Y. Chauvin",
                        "slug": "Y.-Chauvin",
                        "structuredName": {
                            "firstName": "Yves",
                            "lastName": "Chauvin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Chauvin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59638996,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04683566afd0bedc05063cfe04fe65a465354b67",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Back-Propagation:-Theory,-Architecture,-and-Chauvin-Rumelhart",
            "title": {
                "fragments": [],
                "text": "Back-Propagation: Theory, Architecture, and Applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1516909860"
                        ],
                        "name": "D. Touretzky",
                        "slug": "D.-Touretzky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Touretzky",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Touretzky"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 207977638,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "91dbb78ef704fc4369a3d9e37f15af734a4e8328",
            "isKey": false,
            "numCitedBy": 334,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Advances-in-neural-information-processing-systems-2-Touretzky",
            "title": {
                "fragments": [],
                "text": "Advances in neural information processing systems 2"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 12
                            }
                        ],
                        "text": "Jordan Nets [124] used a backpropagation network with the outputs clocked back to the inputs to generate temporal sequences."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 56723681,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d76aafbeb54575859441a442376766c597f6bb52",
            "isKey": false,
            "numCitedBy": 1102,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Attractor-dynamics-and-parallelism-in-a-sequential-Jordan",
            "title": {
                "fragments": [],
                "text": "Attractor dynamics and parallelism in a connectionist sequential machine"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068289963"
                        ],
                        "name": "L. B. Almeida",
                        "slug": "L.-B.-Almeida",
                        "structuredName": {
                            "firstName": "Lu\u00eds",
                            "lastName": "Almeida",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. B. Almeida"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58820035,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8be3f21ab796bd9811382b560507c1c679fae37f",
            "isKey": false,
            "numCitedBy": 325,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-learning-rule-for-asynchronous-perceptrons-with-a-Almeida",
            "title": {
                "fragments": [],
                "text": "A learning rule for asynchronous perceptrons with feedback in a combinatorial environment"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1516909860"
                        ],
                        "name": "D. Touretzky",
                        "slug": "D.-Touretzky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Touretzky",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Touretzky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48855558"
                        ],
                        "name": "D. Pomerleau",
                        "slug": "D.-Pomerleau",
                        "structuredName": {
                            "firstName": "Dean",
                            "lastName": "Pomerleau",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pomerleau"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 386,
                                "start": 382
                            }
                        ],
                        "text": "Hidden units make it possible for networks to discover and exploit regularities of the task at hand, such as symmetries or replicated structure [56], [57], and training procedures capable of exploiting hidden units, such as the Boltzmann machine learning procedure [58] and backpropagation [59], [60], [61], [62], are behind much of the current excitement in the neural network eld [63]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 58789552,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5f71bc2fe361b441b363a6a3fb0577c61fc8926",
            "isKey": false,
            "numCitedBy": 107,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "What's-hidden-in-the-hidden-layers-Touretzky-Pomerleau",
            "title": {
                "fragments": [],
                "text": "What's hidden in the hidden layers?"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32681919"
                        ],
                        "name": "D. Wilde",
                        "slug": "D.-Wilde",
                        "structuredName": {
                            "firstName": "Douglass",
                            "lastName": "Wilde",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Wilde"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16590890"
                        ],
                        "name": "C. Beightler",
                        "slug": "C.-Beightler",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Beightler",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Beightler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58760835,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ea88b5ae74011fb69eea26f8cf6930676ab5e94e",
            "isKey": false,
            "numCitedBy": 790,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Foundations-of-Optimization.-Wilde-Beightler",
            "title": {
                "fragments": [],
                "text": "Foundations of Optimization."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 53796860,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4ade4934db522fe6d634ff6f48887da46eedb4d1",
            "isKey": false,
            "numCitedBy": 903,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-distributed-representations-of-concepts.-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning distributed representations of concepts."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "114788213"
                        ],
                        "name": "D. Signorini",
                        "slug": "D.-Signorini",
                        "structuredName": {
                            "firstName": "DavidF.",
                            "lastName": "Signorini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Signorini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4137433"
                        ],
                        "name": "J. Slattery",
                        "slug": "J.-Slattery",
                        "structuredName": {
                            "firstName": "Jim",
                            "lastName": "Slattery",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Slattery"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2058057862"
                        ],
                        "name": "S. Dodds",
                        "slug": "S.-Dodds",
                        "structuredName": {
                            "firstName": "Sally",
                            "lastName": "Dodds",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dodds"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51934565"
                        ],
                        "name": "V. Lane",
                        "slug": "V.-Lane",
                        "structuredName": {
                            "firstName": "V",
                            "lastName": "Lane",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lane"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2095059657"
                        ],
                        "name": "P. Littlejohns",
                        "slug": "P.-Littlejohns",
                        "structuredName": {
                            "firstName": "P",
                            "lastName": "Littlejohns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Littlejohns"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 2878979,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "20b844e395355b40fa5940c61362ec40e56027aa",
            "isKey": false,
            "numCitedBy": 4703,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-networks-Signorini-Slattery",
            "title": {
                "fragments": [],
                "text": "Neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "The Lancet"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055557361"
                        ],
                        "name": "C. Peterson",
                        "slug": "C.-Peterson",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Peterson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Peterson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110869996"
                        ],
                        "name": "James R. Anderson",
                        "slug": "James-R.-Anderson",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Anderson",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James R. Anderson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "Two competing techniques for such problems are simulated annealing [150], [58] and mean eld theory [92]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 122
                            }
                        ],
                        "text": "Even before the learning rule was rigorously justi ed, deterministic Boltzmann Machines were applied to a number of tasks [92], [91]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3851750,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "607802a4067cb7738bac85d3ca3386f859e637b9",
            "isKey": false,
            "numCitedBy": 500,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Mean-Field-Theory-Learning-Algorithm-for-Neural-Peterson-Anderson",
            "title": {
                "fragments": [],
                "text": "A Mean Field Theory Learning Algorithm for Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 30261890,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f41872eac65a9ff218ae8a75b97532c4654f9c71",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Relaxation-and-its-role-in-vision-Hinton",
            "title": {
                "fragments": [],
                "text": "Relaxation and its role in vision"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145847131"
                        ],
                        "name": "S. Kirkpatrick",
                        "slug": "S.-Kirkpatrick",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Kirkpatrick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kirkpatrick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "146492386"
                        ],
                        "name": "D. Gelatt",
                        "slug": "D.-Gelatt",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Gelatt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gelatt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1855550"
                        ],
                        "name": "M. Vecchi",
                        "slug": "M.-Vecchi",
                        "structuredName": {
                            "firstName": "Mario",
                            "lastName": "Vecchi",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Vecchi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 40427348,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "0a0c026b2b6c04baaf1fa2933d5998519bc9c5fa",
            "isKey": false,
            "numCitedBy": 850,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Optimization-by-Simmulated-Annealing-Kirkpatrick-Gelatt",
            "title": {
                "fragments": [],
                "text": "Optimization by Simmulated Annealing"
            },
            "venue": {
                "fragments": [],
                "text": "Sci."
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802785"
                        ],
                        "name": "S. Nowlan",
                        "slug": "S.-Nowlan",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Nowlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nowlan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 63
                            }
                        ],
                        "text": "network without hidden units to exhibit the desired attractors [89]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 111
                            }
                        ],
                        "text": "Backpropagation through time has been used to train discrete time networks to perform a variety of tasks [59], [89]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 38524960,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "018cd98e224712b318e1c1c88ec182bfdbfd6e42",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Gain-Variation-in-Recurrent-Error-Propagation-Nowlan",
            "title": {
                "fragments": [],
                "text": "Gain Variation in Recurrent Error Propagation Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144323963"
                        ],
                        "name": "S. Alexander",
                        "slug": "S.-Alexander",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Alexander",
                            "middleNames": [
                                "Thomas"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Alexander"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 12374910,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7d78b005150b873a1b72423cdc045267e03daa7",
            "isKey": false,
            "numCitedBy": 3370,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Adaptive-Signal-Processing-Alexander",
            "title": {
                "fragments": [],
                "text": "Adaptive Signal Processing"
            },
            "venue": {
                "fragments": [],
                "text": "Texts and Monographs in Computer Science"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35224059"
                        ],
                        "name": "S. Golowich",
                        "slug": "S.-Golowich",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Golowich",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Golowich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58731351,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "18c8c66e14d5276e5435376d67b9c9d16d7b58b7",
            "isKey": false,
            "numCitedBy": 283,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-Information-Processing-Systems-Vapnik-Golowich",
            "title": {
                "fragments": [],
                "text": "Neural Information Processing Systems"
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 1997"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895771"
                        ],
                        "name": "D. Zipser",
                        "slug": "D.-Zipser",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zipser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zipser"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14792754,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10dae7fca6b65b61d155a622f0c6ca2bc3922251",
            "isKey": false,
            "numCitedBy": 567,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Gradient-based-learning-algorithms-for-recurrent-Williams-Zipser",
            "title": {
                "fragments": [],
                "text": "Gradient-based learning algorithms for recurrent networks and their computational complexity"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimiza-  tion by simulated annealing"
            },
            "venue": {
                "fragments": [],
                "text": "Science, vol"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "Stochastic Boltzmann Machines themselves [58] are beyond our scope here; instead, we give only the probabilistic interpretation of MFT Boltzmann Machines, without derivation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 269,
                                "start": 265
                            }
                        ],
                        "text": "Hidden units make it possible for networks to discover and exploit regularities of the task at hand, such as symmetries or replicated structure [56], [57], and training procedures capable of exploiting hidden units, such as the Boltzmann machine learning procedure [58] and backpropagation [59], [60], [61], [62], are behind much of the current excitement in the neural network eld [63]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "jnowski, \\A learning algorithm for BoltzmannMachines"
            },
            "venue": {
                "fragments": [],
                "text": "Cog-  nitive Science,"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\A rst look at phonetic discrimination using connectionist models with recurrent links\", SCIMP working paper 82018, Institute for Defense Analysis"
            },
            "venue": {
                "fragments": [],
                "text": "\\A rst look at phonetic discrimination using connectionist models with recurrent links\", SCIMP working paper 82018, Institute for Defense Analysis"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 249
                            }
                        ],
                        "text": "First, recurrent networks are capable of settling to a solution that satis es many constraints [1], as in a vision system which relaxes to an interpretation of an image which maximally satis es a complex set of con icting constraints [2], [3], [4], [5], [6], a system which relaxes to nd a posture for a robot satisfying many criteria [7], and models of language parsing [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cooperating processes for lowlevel vision: A survey"
            },
            "venue": {
                "fragments": [],
                "text": "Arti cial Intelligence, vol. 3, pp. 245{ 264"
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning algorithms for os-  cillatory networks with gap junctions and membrane currents\",  Network: Computation"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Systems,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 130
                            }
                        ],
                        "text": "Deterministic Boltzmann Machines The Mean Field form of the stochastic Boltzmann Machine learning rule, or MFT BoltzmannMachines, [91] have been shown to descend an error functional [74]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 128
                            }
                        ],
                        "text": "Even before the learning rule was rigorously justi ed, deterministic Boltzmann Machines were applied to a number of tasks [92], [91]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 74
                            }
                        ],
                        "text": "The Mean Field form of the stochastic Boltzmann Machine learning rule, or MFT BoltzmannMachines, [91] have been shown to descend an error functional [74]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A mean eld theory learning  algorithm for neural nets"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Systems,"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Higher-order Boltzmann machines A learning rule for CAM storage of continuous periodic Con$ Neural Networks Computing, Amer. Institute Physics, no. 151, sequences"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Snowbird B. Baird Proc. IJCNN '90 I1 (Int. Joint Con$ Neural Networks)"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The development of the time-delay neural network architecture for speech recognition Phoneme recognition using time-delay networks"
            },
            "venue": {
                "fragments": [],
                "text": "Dep. Comput. Sci. IEEE Trans"
            },
            "year": 1211
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 399,
                                "start": 395
                            }
                        ],
                        "text": "It should be noted by engineers that many real-world problems which one might think would require recurrent architectures for their solution turn out to be solvable with feedforward architectures, sometimes augmented with preprocessed inputs such as tapped delay lines, and various other architectural embellishments [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [10], [32], [33], [34], [35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and C A"
            },
            "venue": {
                "fragments": [],
                "text": "Camm, Eds., Neural Networks for Signal Processing: Proceedings of the 1991 IEEE Workshop. IEEE Press"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 20
                            }
                        ],
                        "text": "(For a simple task, [90] reports that reaching the precise xedpoint is not crucial to learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fixed point analysis for recurrent neural networks\", in Ad-  vances in Neural Information Processing Systems I, David S"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "pearlmutter, \u2018.Two new learning procedures for recurrent net- [IO91"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Nehvork Rev.,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\The development o f t h e time-delay neural network architecture for speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": "\\The development o f t h e time-delay neural network architecture for speech recognition"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fixed point analysis landscape in recurrent networks , \u201c in Advances in Neural Information for recurrent neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information ProProcessing Systems"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimal algorithms for adaptive networks: Secondorder backpropagation, second-order direct propagation and secondorder Hebbian learning"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE 1st Int. Con$ Neural Networks"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 393,
                                "start": 389
                            }
                        ],
                        "text": "It should be noted by engineers that many real-world problems which one might think would require recurrent architectures for their solution turn out to be solvable with feedforward architectures, sometimes augmented with preprocessed inputs such as tapped delay lines, and various other architectural embellishments [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [10], [32], [33], [34], [35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "FIR and IIR synapses"
            },
            "venue": {
                "fragments": [],
                "text": "a new neural network architecture for time series modelling\", Neural Computation, vol. 3, no. 3, pp. 337{350"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Faster learning for dy-  namic recurrent backpropagation"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Parallel distributed p r ocessing: Explorations in the microstructure o f c ognition"
            },
            "venue": {
                "fragments": [],
                "text": "Parallel distributed p r ocessing: Explorations in the microstructure o f c ognition"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dimensionality reduction and prior knowledge in eset recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Deterministic Boltzmann learning performs steepest"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Properties of the mo-  mentum LMS algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Processing,"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 184
                            }
                        ],
                        "text": "[125], [126] have pointed out that RTRL is related to a version of the [127] lter, in the extension that allows it to apply to nonlinear systems, namely the extended Kalman lter (EKF) [128], [107], [129]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mahra, \\On the identi cation of variances and adaptive Kalman ltering"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Automatic Control, vol. AC-15,"
            },
            "year": 1970
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning state space trajectories in recurrent neural Processing Systems"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computa"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sontag, \\Analog computation v i a neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "The Second Israel Symposium on Theory of Computing and Systems"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 154
                            }
                        ],
                        "text": "Also, since we are concerned with learning, we will not discuss the computational power of recurrent networks considered as abstract machines [40], [41], [42]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Analog computation via  neural networks\", in The Second Israel Symposium on Theory  of Computing and Systems, Natanya, Israel"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 99
                            }
                        ],
                        "text": "Consider a network with n units and m weights which is run for s time steps (variable grid methods [133] would re-"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and Jan G"
            },
            "venue": {
                "fragments": [],
                "text": "Verwer, On Simple Moving Grid Methods for One-Dimensional Evolutionary Partial Di erential Equations, Stichting Mathematisch Centrum, Amsterdam, The Netherlands"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Eds., Applied Optimal Estimation"
            },
            "venue": {
                "fragments": [],
                "text": "Eds., Applied Optimal Estimation"
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 279
                            }
                        ],
                        "text": "An online, exact, and stable, but computationally expensive, procedure for determining the derivatives of functions of the states of a dynamic system with respect to that system's internal parameters has been discovered and applied to recurrent neural networks a number of times [100], [101], [102], [103]; for reviews see also [81], [76], [104]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Applicationsof advances in nonlinear sensitivity analysis\", in System Modeling and Optimization"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 10th IFIP Conference,"
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "DRAFT OF July IEEE TRANSACTIONS ON NEURAL NETWORKS"
            },
            "venue": {
                "fragments": [],
                "text": "DRAFT OF July IEEE TRANSACTIONS ON NEURAL NETWORKS"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 333,
                                "start": 329
                            }
                        ],
                        "text": "It should be noted by engineers that many real-world problems which one might think would require recurrent architectures for their solution turn out to be solvable with feedforward architectures, sometimes augmented with preprocessed inputs such as tapped delay lines, and various other architectural embellishments [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [10], [32], [33], [34], [35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "Such \\time delay neural networks\" have proven useful in the domain of speech recognition [20], [22], [21], [115]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The development of the  time-delay neural network architecture for speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Tech. Rep. CMU-CS-88-152,"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Selverston, \\Mapping between neural and physical activities of the lobster gastric mill system"
            },
            "venue": {
                "fragments": [],
                "text": "Selverston, \\Mapping between neural and physical activities of the lobster gastric mill system"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Shape recognition and illusory conjunc Advances in Neural Information tions"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. 9th Int. Joint Con$ Artificial Intell., Los Angeles, Aug. Processing Systems"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 107
                            }
                        ],
                        "text": "Such \\time delay neural networks\" have proven useful in the domain of speech recognition [20], [22], [21], [115]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Speech Recognition Using Connectionist Net-  works"
            },
            "venue": {
                "fragments": [],
                "text": "PhD thesis, University of Pennsylvania,"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\New second order and rst order algorithm for determining optimal control: A diierential dynamic programming approach"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Optimization Theory and Applications"
            },
            "year": 1968
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Adaptation of cue-speciic learning rates in network models of human category learning"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fourteenth DRAFT OF July IEEE TRANSACTIONS ON NEURAL NETWORKS Annual Meeting of the Cognitive Science Society"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Methods of nonlinear Analysis: Volume 11. New York: models with recurrent links"
            },
            "venue": {
                "fragments": [],
                "text": "Institute Defense Anal"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Back-propagation: Theory, Architectures and Applications, L a wrence Erlbaum Associates"
            },
            "venue": {
                "fragments": [],
                "text": "Back-propagation: Theory, Architectures and Applications, L a wrence Erlbaum Associates"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sontag, \\Analog computation via neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "The Second Israel Symposium on Theory of Computing and Systems"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A focused backpropagation algorithm for temporal Academic"
            },
            "venue": {
                "fragments": [],
                "text": "A focused backpropagation algorithm for temporal Academic"
            },
            "year": 1973
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Amsterdam: Stichting Mathematisch Centrum"
            },
            "venue": {
                "fragments": [],
                "text": "Amsterdam: Stichting Mathematisch Centrum"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Applfcations of Artificial Neural Networks, number 1294 in A P E Proceedings Series"
            },
            "venue": {
                "fragments": [],
                "text": "Applfcations of Artificial Neural Networks, number 1294 in A P E Proceedings Series"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Two new learning procedures for recurrent net An efficient gradient-based algorithm for works on-line training of recurrent network trajectories"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Nehvork Rev"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Applicationsof advancesin nonlinearsensitivity analysis"
            },
            "venue": {
                "fragments": [],
                "text": "System Modeling and Optimization: Proceedings of the 10th IFIP Conference 38 in Lecture Notes in Control and Information Sciences"
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sejnowski, \\Higher-order Boltzmann machines"
            },
            "venue": {
                "fragments": [],
                "text": "Denker 160]"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 78
                            }
                        ],
                        "text": "technique which is suitable to online pattern presentation, see [136], [137], [138]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adaptation of cue-speci c learning rates in network models of human category learning"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fourteenth"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Adaptive s w i t c hing circuits\", in Western Electronic Show and Convention, Convention Record"
            },
            "venue": {
                "fragments": [],
                "text": "96{104, Institute of Radio Engineers"
            },
            "year": 1960
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 192
                            }
                        ],
                        "text": "Similar synaptic architectures using alpha function envelopes (which obviate the need for a history bu er) whose parameters were learned were proposed and used in systems without hidden units [114], [29]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hop eld, \\Neural computation by time  compression"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sci-  ences,"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 20
                            }
                        ],
                        "text": "(For a simple task, [90] reports that reaching the precise xedpoint is not crucial to learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fixed point analysis for recurrent neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 126
                            }
                        ],
                        "text": "Like BPTT, the technique was known and applied to other sorts of systems since the 1950s; for a hook into this literature see [105], [106] or the closely related Extended Kalman Filter [107]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "New second order and rst order algorithm  for determining optimal control: A di erential dynamic pro-  gramming approach"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Optimization Theory and  Applications,"
            },
            "year": 1968
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalization by weightelimination with application to forecasting"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 209
                            }
                        ],
                        "text": "com This paper is concerned with learning algorithms for recurrent networks themselves, and not with recurrent networks as elements of larger systems, such as specialized architectures for control [36], [37], [38], [39]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Identi cation and con-  trol of dynamical systems using neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans-  actions on Neural Networks,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 126
                            }
                        ],
                        "text": "Like BPTT, the technique was known and applied to other sorts of systems since the 1950s; for a hook into this literature see [105], [106] or the closely related Extended Kalman Filter [107]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "New second order and rst order algorithm  for determining optimal control: A di erential dynamic pro-  gramming approach"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Optimization Theory and  Applications,"
            },
            "year": 1968
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalization by weightelimination with application to forecasting"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Computer Vision, P r i n tice- Hall"
            },
            "venue": {
                "fragments": [],
                "text": "Computer Vision, P r i n tice- Hall"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Neural computation by time compression"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 74
                            }
                        ],
                        "text": "Two competing techniques for such problems are simulated annealing [150], [58] and mean eld theory [92]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "Stochastic Boltzmann Machines themselves [58] are beyond our scope here; instead, we give only the probabilistic interpretation of MFT Boltzmann Machines, without derivation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 269,
                                "start": 265
                            }
                        ],
                        "text": "Hidden units make it possible for networks to discover and exploit regularities of the task at hand, such as symmetries or replicated structure [56], [57], and training procedures capable of exploiting hidden units, such as the Boltzmann machine learning procedure [58] and backpropagation [59], [60], [61], [62], are behind much of the current excitement in the neural network eld [63]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A learning algorithm for BoltzmannMachines"
            },
            "venue": {
                "fragments": [],
                "text": "Cognitive Science,"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Learning by c hoice of internal representations"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Systems"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "27] Applications of Artiicial Neural Networks, n umber 1294 in APIE Proceedings Series"
            },
            "venue": {
                "fragments": [],
                "text": "27] Applications of Artiicial Neural Networks, n umber 1294 in APIE Proceedings Series"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Generic constraints on underspeciied target trajectories"
            },
            "venue": {
                "fragments": [],
                "text": "IJCNN89 158"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Shaping the state space Fixed point analysis landscape in recurrent networks"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information for recurrent neural networks, \" in Advances in Neural Information Pro- Processing Systems"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Proc. 1991 IEEE Workshop Neural Networks Signal Process"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. 1991 IEEE Workshop Neural Networks Signal Process"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\A hodgkinhuxley type neuron model that learns slow non-spike oscillation"
            },
            "venue": {
                "fragments": [],
                "text": "\\A hodgkinhuxley type neuron model that learns slow non-spike oscillation"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A HodgkinHuxley type neuron model that learns slow nonspike oscillation"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mapping between neural and physical activities of the lobster gastric mill system"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adaptive Signal Pro-  cessing, Prentice-Hall signal processing series"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Generalization o f b a c k-propagation to recurrent neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Physical Review Letters"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "McClelland, and the PDP research  group., Eds., Parallel distributed processing: Explorations in  the microstructure of cognition, Volume 1: Foundations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Identiication and control of dynamical systems using neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "How brains make chaos to make a recurrent gas market model"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adaptation of cuespecific learning rates in network models of human category learning"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. 14thAnnu. Meeting Cognitive Sci. Soc"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dynamic recurrent neural networks,, Carnegie running fully recurrent neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computa"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 154
                            }
                        ],
                        "text": "Also, since we are concerned with learning, we will not discuss the computational power of recurrent networks considered as abstract machines [40], [41], [42]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Analog computation via neural networks\", in The Second Israel"
            },
            "venue": {
                "fragments": [],
                "text": "Symposium on Theory of Computing and Systems,"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 321,
                                "start": 317
                            }
                        ],
                        "text": "It should be noted by engineers that many real-world problems which one might think would require recurrent architectures for their solution turn out to be solvable with feedforward architectures, sometimes augmented with preprocessed inputs such as tapped delay lines, and various other architectural embellishments [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [10], [32], [33], [34], [35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Back-propagation: Theory,  Architectures and Applications, Lawrence ErlbaumAssociates"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A learning for asynchronous perceptrons with"
            },
            "venue": {
                "fragments": [],
                "text": "A learning for asynchronous perceptrons with"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\A uniied gradientdescent/clustering a r c hitecture for nite state machine induction"
            },
            "venue": {
                "fragments": [],
                "text": "\\A uniied gradientdescent/clustering a r c hitecture for nite state machine induction"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dynamic Programming and the Calculus of Variations, v ol. 21 of Mathematics in science and engineering"
            },
            "venue": {
                "fragments": [],
                "text": "Dynamic Programming and the Calculus of Variations, v ol. 21 of Mathematics in science and engineering"
            },
            "year": 1965
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 332,
                                "start": 327
                            }
                        ],
                        "text": "Since the acceleration of convergence in these gradient systems is such an important issue, it can be helpful to know some of the techniques used to analyze the limitations of convergence under various conditions in systems of this sort, and of some other techniques for accelerating their convergence; see [139, page 304] and [140], [141], [142], [143], [144], [145], [146], [147], [148]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and C"
            },
            "venue": {
                "fragments": [],
                "text": "R. Johnson, Jr., \\Stationary and nonstationary learning characteristics of the LMS adaptive lter\", Proceedings of the IEEE, vol. 64, pp. 1151{1162"
            },
            "year": 1976
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[120], [121], [122] propose a moving targets learning algorithm."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Une proc edure d'apprentissage pour r eseau a seuil assym etrique\", in Cognitiva"
            },
            "venue": {
                "fragments": [],
                "text": "A la Fronti ere de l'Intelligence Arti cielle des Sciences de la Connaissance des Neurosciences,"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On Simple Moving Grid Methods for One-Dimensional Evolutionary Partial Diierential Equations, S t i c hting Mathematisch Centrum"
            },
            "venue": {
                "fragments": [],
                "text": "On Simple Moving Grid Methods for One-Dimensional Evolutionary Partial Diierential Equations, S t i c hting Mathematisch Centrum"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\On the identiication of variances and adaptive Kalman ltering"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Automatic Control"
            },
            "year": 1970
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Adaptive switching circuits\", in Western Electronic Show and Convention, Convention Record"
            },
            "venue": {
                "fragments": [],
                "text": "96{104, Institute of Radio Engineers"
            },
            "year": 1960
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 26
                            }
                        ],
                        "text": "The Moving Targets Method [120], [121], [122] propose a moving targets learning algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Une proc edure d'apprentissage pour r eseau  a seuil assym etrique\", in Cognitiva 85: A la Fronti ere de  l'Intelligence"
            },
            "venue": {
                "fragments": [],
                "text": "Arti cielle des Sciences de la Connaissance des  Neurosciences,"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 153
                            }
                        ],
                        "text": "This was intended to model a controlled modulation of a central pattern generator from tonic modulatory input, as in the lobster stomatagastric gangleon [116]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Distinct re-  sponses of electrically-coupled pacemaker neurons to activation  of a modulatory projection neuron"
            },
            "venue": {
                "fragments": [],
                "text": "Society for Neuroscience  Abstracts,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 126
                            }
                        ],
                        "text": "Like BPTT, the technique was known and applied to other sorts of systems since the 1950s; for a hook into this literature see [105], [106] or the closely related Extended Kalman Filter [107]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "New second order and rst order algorithm for determining optimal control: A di erential dynamic programming approach"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Optimization Theory and Applications, vol. 2"
            },
            "year": 1968
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Two new learning procedures for recurrent n e t works"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Network Review"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "`Neural' computation of de-  cisions in optimization problems"
            },
            "venue": {
                "fragments": [],
                "text": "Biological Cybernetics,"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The gamma model-A new neural network for temporal processing"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\A learning algorithm for continuallyrunning fully recurrentneural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Yann LeCun, \\Une proc edure d'apprentissage pour r eseau a seuil assym etrique\", in Cognitiva 85: A la Fronti ere d e l'Intelligence A rtiicielle des Sciences de la Connaissance d e s Neurosciences"
            },
            "venue": {
                "fragments": [],
                "text": "Yann LeCun, \\Une proc edure d'apprentissage pour r eseau a seuil assym etrique\", in Cognitiva 85: A la Fronti ere d e l'Intelligence A rtiicielle des Sciences de la Connaissance d e s Neurosciences"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An efficient gradientbased algorithm for works"
            },
            "venue": {
                "fragments": [],
                "text": "Proc . IJCNN Systems 2"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Yann LeCun, \\Une proc edure d'apprentissage pour r eseau a seuil assym etrique"
            },
            "venue": {
                "fragments": [],
                "text": "Cognitiva 85: A la Fronti ere de l'Intelligence Artiicielle des Sciences de la Connaissance des Neurosciences"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\A xed size storage O(n 3 ) time complexity learning algorithm for fully recurrent c o n tinually running networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hoppeld, \\Neural computation by t i m e compression"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bcckpropagation; Theory, Architectures and Applications"
            },
            "venue": {
                "fragments": [],
                "text": "Bcckpropagation; Theory, Architectures and Applications"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 369,
                                "start": 365
                            }
                        ],
                        "text": "There are continuous-time feed-forward learning algorithms that are as e cient in both time and space as algorithms for pure feedforward networks, but are applicable only when w is upper-triangular but not necessarily zero-diagonal, in other words, when the network is feedforward except for recurrent self-connections [77], [78], [79], [80], [25] or for a survey, [81]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 332,
                                "start": 328
                            }
                        ],
                        "text": "An online, exact, and stable, but computationally expensive, procedure for determining the derivatives of functions of the states of a dynamic system with respect to that system's internal parameters has been discovered and applied to recurrent neural networks a number of times [100], [101], [102], [103]; for reviews see also [81], [76], [104]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Two new learning procedures for recurrent networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Network Review,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Snowbird Conference on Neural Networks for Computing, n umber 151 in AIP conference proceedings"
            },
            "venue": {
                "fragments": [],
                "text": "Snowbird Conference on Neural Networks for Computing, n umber 151 in AIP conference proceedings"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Barak Pearlmutter, \\Learning state space trajectories in recurrent neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 291
                            }
                        ],
                        "text": "A unique xedpoint is reached regardless of initial conditions if Pij w2 ij < max( 0) where max( 0) is the maximal value of 0(x) for any x [85], but in practice much weaker bounds on the weights seem to su ce, as indicated by empirical studies of the dynamics of networks with random weights [86]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A study of network dy-  namics"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Statistical Physics,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bryson , Jr . , \\ A steepest ascent method for solving optimumprogrammingproblems \""
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Applied Mechan"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 127
                            }
                        ],
                        "text": "Recurrent Backpropagation It was shown independently by Pineda [72] and Alemeida [73] that the error backpropagation algorithm [61], [59], [60] is a special case of a more general error gradient computation procedure."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 306,
                                "start": 302
                            }
                        ],
                        "text": "Hidden units make it possible for networks to discover and exploit regularities of the task at hand, such as symmetries or replicated structure [56], [57], and training procedures capable of exploiting hidden units, such as the Boltzmann machine learning procedure [58] and backpropagation [59], [60], [61], [62], are behind much of the current excitement in the neural network eld [63]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "TR-47, MIT  Center for Research in Computational Economics and Man"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 209
                            }
                        ],
                        "text": "com This paper is concerned with learning algorithms for recurrent networks themselves, and not with recurrent networks as elements of larger systems, such as specialized architectures for control [36], [37], [38], [39]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Identi cation and control of dynamical systems using neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Neural Networks,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Deterministic Boltzmann learning"
            },
            "venue": {
                "fragments": [],
                "text": "(In?. Joint Con$ Neural Networks),"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Richard Rohwer, \\The \\moving targets\" training algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Touretzky 156]"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 71
                            }
                        ],
                        "text": "[125], [126] have pointed out that RTRL is related to a version of the [127] lter, in the extension that allows it to apply to nonlinear systems, namely the extended Kalman lter (EKF) [128], [107], [129]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Kalman, \\A new approach to linear ltering and prediction problems"
            },
            "venue": {
                "fragments": [],
                "text": "Trans. ASME Journal of Basic Engineering,"
            },
            "year": 1960
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Phoneme recognition using time-delay n e t works"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Acoustics, Speech, and Signal Processing"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Computability with the classical sigmoid"
            },
            "venue": {
                "fragments": [],
                "text": "Proc . 6 th Annu . ACM Workshop Computational Leaming Theory"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\An eecient gradient-based algorithm for on-line training of recurrent n e t work trajectories"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Snowbird Conference on Neural Networks for Computing, number 151 in AIP conference proceedings"
            },
            "venue": {
                "fragments": [],
                "text": "Snowbird Conference on Neural Networks for Computing, number 151 in AIP conference proceedings"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Applications of advancesin nonlinearsensitivity analysis"
            },
            "venue": {
                "fragments": [],
                "text": "System Modeling and Optimization: Proceedings of the 10th IFIP Conference 38 in Lecture Notes in Control and Information Sciences"
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 417,
                                "start": 413
                            }
                        ],
                        "text": "It should be noted by engineers that many real-world problems which one might think would require recurrent architectures for their solution turn out to be solvable with feedforward architectures, sometimes augmented with preprocessed inputs such as tapped delay lines, and various other architectural embellishments [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [10], [32], [33], [34], [35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The gammamodel|a new neural  network for temporal processing"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks,"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Deterministic Boltzmann learning performs steepest algorithm for analog, fully recurrent neu. ral networks"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IJCNN '89 Int. Joint Con$ Neural Networks, descent in weight-space"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On Simple  Moving Grid Methods for One-Dimensional Evolutionary Par-  tial Di erential Equations, Stichting Mathematisch Centrum"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 133
                            }
                        ],
                        "text": "Like BPTT, the technique was known and applied to other sorts of systems since the 1950s; for a hook into this literature see [105], [106] or the closely related Extended Kalman Filter [107]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Methods of Nonlinear Analysis: Volume II"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Siegelmann, \\Computability with the classical sigmoid"
            },
            "venue": {
                "fragments": [],
                "text": "Sixth Annual ACM Workshop on Computational Learning Theory"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Finding structurein time"
            },
            "venue": {
                "fragments": [],
                "text": "\\Finding structurein time"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mozer, \\Induction of multiscale temporal structure"
            },
            "venue": {
                "fragments": [],
                "text": "Mozer, \\Induction of multiscale temporal structure"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalization of backpropagation with application to C How brains make chaos to make a recurrent gas market model"
            },
            "venue": {
                "fragments": [],
                "text": "sense of the world"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "CS-90-196"
            },
            "venue": {
                "fragments": [],
                "text": "CS-90-196"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Nonlinear dynamics of artiicial neural systems"
            },
            "venue": {
                "fragments": [],
                "text": "\\Nonlinear dynamics of artiicial neural systems"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 153
                            }
                        ],
                        "text": "This was intended to model a controlled modulation of a central pattern generator from tonic modulatory input, as in the lobster stomatagastric gangleon [116]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Distinct responses of electrically-coupled pacemaker neurons to activation of a modulatory projection neuron"
            },
            "venue": {
                "fragments": [],
                "text": "Society for Neuroscience Abstracts, vol. 20, no. 18.6, pp. 23"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Two new learning procedures for recurrent networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Network Review"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "27] Applications of Artiicial Neural Networks, number 1294 in APIE Proceedings Series"
            },
            "venue": {
                "fragments": [],
                "text": "27] Applications of Artiicial Neural Networks, number 1294 in APIE Proceedings Series"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mapping between neural and physical activities of the lobster gastric mill system"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems 5"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 103
                            }
                        ],
                        "text": "The rst is the obvious extension of backpropagation through time (BPTT) to continuous time [95], [96], [62]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 312,
                                "start": 308
                            }
                        ],
                        "text": "Hidden units make it possible for networks to discover and exploit regularities of the task at hand, such as symmetries or replicated structure [56], [57], and training procedures capable of exploiting hidden units, such as the Boltzmann machine learning procedure [58] and backpropagation [59], [60], [61], [62], are behind much of the current excitement in the neural network eld [63]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dynamic Programming and Karkhov Processes"
            },
            "venue": {
                "fragments": [],
                "text": "Dynamic Programming and Karkhov Processes"
            },
            "year": 1960
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning of stable states in stochastic [I 131 U. Bodenhausen Learning internal representations of pattern sequences asymmetric networks Rep. in a neural network with adaptive time-delays"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IJCNN '90 I1 TM-ARE-015240?. Joint Con$ Neural Networks)"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 148
                            }
                        ],
                        "text": "Also, since we are concerned with learning, we will not discuss the computational power of recurrent networks considered as abstract machines [40], [41], [42]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Computability with the classical sigmoid"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. 6th Annu. ACM Workshop Computational Leaming Theory"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning internal representations of pattern sequences asymmetric networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 338,
                                "start": 335
                            }
                        ],
                        "text": "First, recurrent networks are capable of settling to a solution that satis es many constraints [1], as in a vision system which relaxes to an interpretation of an image which maximally satis es a complex set of con icting constraints [2], [3], [4], [5], [6], a system which relaxes to nd a posture for a robot satisfying many criteria [7], and models of language parsing [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Using relaxation to nd a puppet"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the A.I.S.B. Summer Conference,"
            },
            "year": 1976
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 233,
                                "start": 229
                            }
                        ],
                        "text": "Feedforward Networks with State It is noteworthy that that the same basic mathematical technique of forward propagation can be applied to networks with a restricted architecture, feedforward networks whose units have state [77], [78], [80]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 329,
                                "start": 325
                            }
                        ],
                        "text": "There are continuous-time feed-forward learning algorithms that are as e cient in both time and space as algorithms for pure feedforward networks, but are applicable only when w is upper-triangular but not necessarily zero-diagonal, in other words, when the network is feedforward except for recurrent self-connections [77], [78], [79], [80], [25] or for a survey, [81]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A rst look at phonetic discrimination using  connectionist models with recurrent links\", SCIMP working  paper 82018, Institute for Defense"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 369,
                                "start": 365
                            }
                        ],
                        "text": "There are continuous-time feed-forward learning algorithms that are as e cient in both time and space as algorithms for pure feedforward networks, but are applicable only when w is upper-triangular but not necessarily zero-diagonal, in other words, when the network is feedforward except for recurrent self-connections [77], [78], [79], [80], [25] or for a survey, [81]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 361,
                                "start": 357
                            }
                        ],
                        "text": "Real Time Recurrent Learning An online, exact, and stable, but computationally expensive, procedure for determining the derivatives of functions of the states of a dynamic system with respect to that system's internal parameters has been discovered and applied to recurrent neural networks a number of times [100], [101], [102], [103]; for reviews see also [81], [76], [104]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Two new learning procedures for re-  current networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Network Review,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\A modiied leaky integrator network for temporal pattern recognition"
            },
            "venue": {
                "fragments": [],
                "text": "IJCNN89 158]"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 190
                            }
                        ],
                        "text": "\u2026algorithm used in [9], the bulk of this paper is concerned with the problem of causing networks to exhibit particular desired detailed temporal behavior, which has found application in signal processing [10], [11], speech and language processing [12], [13], [14], and neuroscience [15], [16], [17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "What's hidden in the hidden Cambridge"
            },
            "venue": {
                "fragments": [],
                "text": "What's hidden in the hidden Cambridge"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Complex Syst"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalization of backpropagation to recurrent neural netAug. 31-Sep"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 148
                            }
                        ],
                        "text": "Also, since we are concerned with learning, we will not discuss the computational power of recurrent networks considered as abstract machines [40], [41], [42]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Computability with the clas-  sical sigmoid"
            },
            "venue": {
                "fragments": [],
                "text": "Sixth Annual ACM Workshop on Computa-  tional Learning"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 99
                            }
                        ],
                        "text": "Another is to use BPTT with a history cuto of k units of time, termed BPTT(k) by Williams and Peng [109], and make a small weight change each timestep."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An e cient gradient-based  algorithm for on-line training of recurrent network trajecto-  ries"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 142
                            }
                        ],
                        "text": "Also, since we are concerned with learning, we will not discuss the computational power of recurrent networks considered as abstract machines [40], [41], [42]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Turing computability with neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Appl. Math. Lett"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Backpropagation through time: What it does and how to B. Baird and F. Eeckman, \" CAM storage of analog patterns and contindo it"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE Advances in Neural Information"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mellon Univ. School ComPut. SCi"
            },
            "venue": {
                "fragments": [],
                "text": "Mellon Univ. School ComPut. SCi"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Deterministic Boltzmann learning in Neural computation by time compresnetworks with asymmetric connectivity"
            },
            "venue": {
                "fragments": [],
                "text": "Univ. Toronto Dep. Comput. sion. \" Proc. National Academy Sci"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Home, \u201cProgress in supervised neural networks,"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Signal Process. Mag., vol. 10,"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\A learning algorithm for continually running fully recurrent n e u r a l n e t works"
            },
            "venue": {
                "fragments": [],
                "text": "Tech. Rep. ICS Report"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 329,
                                "start": 325
                            }
                        ],
                        "text": "There are continuous-time feed-forward learning algorithms that are as e cient in both time and space as algorithms for pure feedforward networks, but are applicable only when w is upper-triangular but not necessarily zero-diagonal, in other words, when the network is feedforward except for recurrent self-connections [77], [78], [79], [80], [25] or for a survey, [81]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 197
                            }
                        ],
                        "text": "It is noteworthy that that the same basic mathematical technique of forward propagation can be applied to networks with a restricted architecture, feedforward networks whose units have state [77], [78], [80]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A rst look at phonetic discrimination using connectionist models with recurrent links"
            },
            "venue": {
                "fragments": [],
                "text": "SCIMP working paper 82018,"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 339,
                                "start": 335
                            }
                        ],
                        "text": "It should be noted by engineers that many real-world problems which one might think would require recurrent architectures for their solution turn out to be solvable with feedforward architectures, sometimes augmented with preprocessed inputs such as tapped delay lines, and various other architectural embellishments [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [10], [32], [33], [34], [35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 101
                            }
                        ],
                        "text": "Such \\time delay neural networks\" have proven useful in the domain of speech recognition [20], [22], [21], [115]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Phoneme recognition using time-delay networks"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Acoustics, Speech, and Signal Process-  ing,"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and the PDP research group., Eds., Parallel distributed processing: Explorations in the microstructure of cognition"
            },
            "venue": {
                "fragments": [],
                "text": "Foundations"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Home, \u201cProgress in supervised neural networks,"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Signal Process. Mag., vol. 10,"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Complete gradient optimizationof a recurrent n e t work applied to BDG descrimination"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Acoustical Society of America"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Training recurrent n e t works using the extended Kalman lter"
            },
            "venue": {
                "fragments": [],
                "text": "IJCNN92'Baltimore 153]"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 74
                            }
                        ],
                        "text": "This technique has been discovered independently a number of times [110], [111]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A xed size storageO(n3) time com-  plexity learning algorithm for fully recurrent continually run-  ning networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation,"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 314,
                                "start": 310
                            }
                        ],
                        "text": "Since we wish to know the e ect of making this in nitesimal change to wij throughout time, we integrate over the entire interval, yielding @E @wij = Z t1 t0 yi 0(xj)zjdt: (27) One can also derive (26), (27) and (37) using the calculus of variations and Lagrange multipliers, as in optimal control theory [98], [99]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dynamic Programming and the Calculus of  Variations, vol. 21 of Mathematics in science and engineering"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1965
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Elman, \\Finding structure in time"
            },
            "venue": {
                "fragments": [],
                "text": "Cognitive Science"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Using relaxation to nd a puppet"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the A.I.S.B. Summer Conference"
            },
            "year": 1976
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning unambiguous reduced sequence descriptions Advances in Neural Information Processing Systems 4in"
            },
            "venue": {
                "fragments": [],
                "text": "Learning unambiguous reduced sequence descriptions Advances in Neural Information Processing Systems 4in"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Contrastive learning and neural oscillations learning algorithm for continually Neural Computa"
            },
            "venue": {
                "fragments": [],
                "text": "Contrastive learning and neural oscillations learning algorithm for continually Neural Computa"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Lee Giles, and Guo-Zheng Sun, \\Using prior knowledge in a NNPDA to learn context-free languages"
            },
            "venue": {
                "fragments": [],
                "text": "Lee Giles, and Guo-Zheng Sun, \\Using prior knowledge in a NNPDA to learn context-free languages"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 345,
                                "start": 341
                            }
                        ],
                        "text": "It should be noted by engineers that many real-world problems which one might think would require recurrent architectures for their solution turn out to be solvable with feedforward architectures, sometimes augmented with preprocessed inputs such as tapped delay lines, and various other architectural embellishments [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [10], [32], [33], [34], [35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 95
                            }
                        ],
                        "text": "Such \\time delay neural networks\" have proven useful in the domain of speech recognition [20], [22], [21], [115]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A timedelay neural network architecture for isolated word recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "BPS : A learning algorithm for timization of dynamical systems containing neural networks , \u201d IEEE capturing the dynamic nature of speech"
            },
            "venue": {
                "fragments": [],
                "text": "Proc . IJCNN \u2019 89 Int . Joint Trans"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[125], [126] have pointed out that RTRL is related to a version of the [127] lter, in the extension that allows it to apply to nonlinear systems, namely the extended Kalman lter (EKF) [128], [107], [129]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural network nonlinear adaptive ltering using the extended Kalman lter algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the International Neural Networks Conference,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[12] applied recurrent networks with immutable time delays in the domain of speech."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 360,
                                "start": 356
                            }
                        ],
                        "text": "Although algorithms suitable for building systems of this type are reviewed to some extent below, such as the algorithm used in [9], the bulk of this paper is concerned with the problem of causing networks to exhibit particular desired detailed temporal behavior, which has found application in signal processing [10], [11], speech and language processing [12], [13], [14], and neuroscience [15], [16], [17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Complete gradient optimizationof a recurrentnetwork applied to BDG descrimination"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Acoustical Society of America,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Kalman, \\A new approach to linear ltering and predic-  tion problems"
            },
            "venue": {
                "fragments": [],
                "text": "Trans. ASME Journal of Basic Engineering,  vol. 82,"
            },
            "year": 1960
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Optimal algorithms for adaptive networks: Second order back propagation, second order direct propagation , and second order Hebbian learning"
            },
            "venue": {
                "fragments": [],
                "text": "\\Optimal algorithms for adaptive networks: Second order back propagation, second order direct propagation , and second order Hebbian learning"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A steepest ascent method for solving optimum model of the olfactory system programming problems Equations of motion from a 1962. data series Theoretical Division"
            },
            "venue": {
                "fragments": [],
                "text": "S. E. Dreyfus, Dynamic Programming and the Calculus of Variations nonlinear signal processing using neural networks: Prediction and system modeling"
            },
            "year": 1965
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Lee Giles, and Guo-Zheng Sun, \\Using prior knowledge in a NNPDA to learn context-free languages"
            },
            "venue": {
                "fragments": [],
                "text": "Lee Giles, and Guo-Zheng Sun, \\Using prior knowledge in a NNPDA to learn context-free languages"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 71,
            "methodology": 34
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 302,
        "totalPages": 31
    },
    "page_url": "https://www.semanticscholar.org/paper/Gradient-calculations-for-dynamic-recurrent-neural-Pearlmutter/32e97eef94beacace020e79322cef0e1e5a76ee0?sort=total-citations"
}