{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120684987"
                        ],
                        "name": "Muhammad Usman Ghani Khan",
                        "slug": "Muhammad-Usman-Ghani-Khan",
                        "structuredName": {
                            "firstName": "Muhammad",
                            "lastName": "Khan",
                            "middleNames": [
                                "Usman",
                                "Ghani"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Muhammad Usman Ghani Khan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152830261"
                        ],
                        "name": "Lei Zhang",
                        "slug": "Lei-Zhang",
                        "structuredName": {
                            "firstName": "Lei",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lei Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703592"
                        ],
                        "name": "Y. Gotoh",
                        "slug": "Y.-Gotoh",
                        "structuredName": {
                            "firstName": "Yoshihiko",
                            "lastName": "Gotoh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Gotoh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 84
                            }
                        ],
                        "text": "The first task, multi-sentence description, has been explored for videos previously [2, 8, 22], but open challenges remain, namely finding a segmentation of appropriate granDetailed: A woman turned on stove."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[8] produces multiple sentences and use paraphrasing and merging to get the minimum needed number of sentences."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16173722,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3fbb3e7dc882f060bb2adf8aa0cf9f5bd6968dd7",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "This contribution addresses the approach to creating smooth and coherent description of video streams. Firstly conventional image processing techniques are applied to extract high level features from individual video frames. Natural language description of the frame contents is produced based on high level features. In order to extend the approach to description of video streams, we introduce units of features and overview how units can be used to present coherent, smooth and well phrased descriptions by incorporating spatial and temporal information. The approach is evaluated by calculating overlap similarity score between human authored and machine generated descriptions."
            },
            "slug": "Towards-coherent-natural-language-description-of-Khan-Zhang",
            "title": {
                "fragments": [],
                "text": "Towards coherent natural language description of video streams"
            },
            "venue": {
                "fragments": [],
                "text": "2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34849128"
                        ],
                        "name": "Marcus Rohrbach",
                        "slug": "Marcus-Rohrbach",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Rohrbach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcus Rohrbach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144369161"
                        ],
                        "name": "Wei Qiu",
                        "slug": "Wei-Qiu",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Qiu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Qiu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144889265"
                        ],
                        "name": "Ivan Titov",
                        "slug": "Ivan-Titov",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Titov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ivan Titov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727272"
                        ],
                        "name": "Stefan Thater",
                        "slug": "Stefan-Thater",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Thater",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefan Thater"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717560"
                        ],
                        "name": "Manfred Pinkal",
                        "slug": "Manfred-Pinkal",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Pinkal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred Pinkal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "As mentioned above, the approach of [19] uses visual attributes to obtain the features for CRF unaries."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 24
                            }
                        ],
                        "text": "We base our approach on [19] which uses an intermediate SR modeled with a CRF."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": "In addition to [19], to enable a consistent prediction within a video, we introduce a high level topic node t in the graph, which is also connected to all nodes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "We base our approach for a video snippet on the translation approach proposed in [19]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [19] the features are quantized in a codebook and used to train the visual attribute classifiers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 83
                            }
                        ],
                        "text": "Recently, this task has received an increased interest both in the computer vision [4, 12, 5, 2, 19] and computational linguistic communities [13, 25, 11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 129
                            }
                        ],
                        "text": "An alternative is to retrieve sentences from a training corpus [4, 2] or to compose novel descriptions based on a language model [12, 13, 15, 19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "One of the conclusions drawn in [19] is that the noisy visual recognition is a main limitation of the suggested translation approach."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "The visual recognition approach of [19] is based on dense trajectory features [24]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 60
                            }
                        ],
                        "text": "While this paper builds on the semantic representation from [19], our idea of consistency is applicable to other semantic representations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5775821,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e8cd37fbd8bd5e690eef5861cf92af8e002d4533",
            "isKey": false,
            "numCitedBy": 313,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Humans use rich natural language to describe and communicate visual perceptions. In order to provide natural language descriptions for visual content, this paper combines two important ingredients. First, we generate a rich semantic representation of the visual content including e.g. object and activity labels. To predict the semantic representation we learn a CRF to model the relationships between different components of the visual input. And second, we propose to formulate the generation of natural language as a machine translation problem using the semantic representation as source language and the generated sentences as target language. For this we exploit the power of a parallel corpus of videos and textual descriptions and adapt statistical machine translation to translate between our two languages. We evaluate our video descriptions on the TACoS dataset, which contains video snippets aligned with sentence descriptions. Using automatic evaluation and human judgments we show significant improvements over several baseline approaches, motivated by prior work. Our translation approach also shows improvements over related work on an image description task."
            },
            "slug": "Translating-Video-Content-to-Natural-Language-Rohrbach-Qiu",
            "title": {
                "fragments": [],
                "text": "Translating Video Content to Natural Language Descriptions"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper generates a rich semantic representation of the visual content including e.g. object and activity labels and proposes to formulate the generation of natural language as a machine translation problem using the semantic representation as source language and the generated sentences as target language."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Computer Vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2363529"
                        ],
                        "name": "P. Das",
                        "slug": "P.-Das",
                        "structuredName": {
                            "firstName": "Pradipto",
                            "lastName": "Das",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Das"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2026123"
                        ],
                        "name": "Chenliang Xu",
                        "slug": "Chenliang-Xu",
                        "structuredName": {
                            "firstName": "Chenliang",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chenliang Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38972663"
                        ],
                        "name": "Richard F. Doell",
                        "slug": "Richard-F.-Doell",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Doell",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard F. Doell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3587688"
                        ],
                        "name": "Jason J. Corso",
                        "slug": "Jason-J.-Corso",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Corso",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason J. Corso"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "Next, [2] ensures that their low level detections are consistent with their concept prediction and retrieve the most likely training sentence."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 44
                            }
                        ],
                        "text": "To generate multiple sentences for a video, [2] segments the video based on the similarity of concept detections in neighboring frames."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 83
                            }
                        ],
                        "text": "Recently, this task has received an increased interest both in the computer vision [4, 12, 5, 2, 19] and computational linguistic communities [13, 25, 11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 84
                            }
                        ],
                        "text": "The first task, multi-sentence description, has been explored for videos previously [2, 8, 22], but open challenges remain, namely finding a segmentation of appropriate granDetailed: A woman turned on stove."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 63
                            }
                        ],
                        "text": "An alternative is to retrieve sentences from a training corpus [4, 2] or to compose novel descriptions based on a language model [12, 13, 15, 19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12284555,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a23ab0fb7d9e9961e92d704ed71e3dbc15c0d908",
            "isKey": true,
            "numCitedBy": 225,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of describing images through natural language has gained importance in the computer vision community. Solutions to image description have either focused on a top-down approach of generating language through combinations of object detections and language models or bottom-up propagation of keyword tags from training images to test images through probabilistic or nearest neighbor techniques. In contrast, describing videos with natural language is a less studied problem. In this paper, we combine ideas from the bottom-up and top-down approaches to image description and propose a method for video description that captures the most relevant contents of a video in a natural language description. We propose a hybrid system consisting of a low level multimodal latent topic model for initial keyword annotation, a middle level of concept detectors and a high level module to produce final lingual descriptions. We compare the results of our system to human descriptions in both short and long forms on two datasets, and demonstrate that final system output has greater agreement with the human descriptions than any single level."
            },
            "slug": "A-Thousand-Frames-in-Just-a-Few-Words:-Lingual-of-Das-Xu",
            "title": {
                "fragments": [],
                "text": "A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes a hybrid system consisting of a low level multimodal latent topic model for initial keyword annotation, a middle level of concept detectors and a high level module to produce final lingual descriptions that captures the most relevant contents of a video in a natural language description."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3006928"
                        ],
                        "name": "N. Krishnamoorthy",
                        "slug": "N.-Krishnamoorthy",
                        "structuredName": {
                            "firstName": "Niveda",
                            "lastName": "Krishnamoorthy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Krishnamoorthy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3163967"
                        ],
                        "name": "Girish Malkarnenkar",
                        "slug": "Girish-Malkarnenkar",
                        "structuredName": {
                            "firstName": "Girish",
                            "lastName": "Malkarnenkar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Girish Malkarnenkar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2903226"
                        ],
                        "name": "Kate Saenko",
                        "slug": "Kate-Saenko",
                        "structuredName": {
                            "firstName": "Kate",
                            "lastName": "Saenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kate Saenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687120"
                        ],
                        "name": "S. Guadarrama",
                        "slug": "S.-Guadarrama",
                        "structuredName": {
                            "firstName": "Sergio",
                            "lastName": "Guadarrama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Guadarrama"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 119
                            }
                        ],
                        "text": "To generate descriptions for videos and images, rules or templates are a powerful tool but need to be manually defined [12, 22, 6, 11, 5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 142
                            }
                        ],
                        "text": "Recently, this task has received an increased interest both in the computer vision [4, 12, 5, 2, 19] and computational linguistic communities [13, 25, 11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8740928,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3b9f8101c61b415f946625b69f69fc9e3d0d6fc4",
            "isKey": false,
            "numCitedBy": 190,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "\n \n We present a holistic data-driven technique that generates natural-language descriptions for videos. We combine the output of state-of-the-art object and activity detectors with \"real-world' knowledge to select the most probable subject-verb-object triplet for describing a video. We show that this knowledge, automatically mined from web-scale text corpora, enhances the triplet selection algorithm by providing it contextual information and leads to a four-fold increase in activity identification. Unlike previous methods, our approach can annotate arbitrary videos without requiring the expensive collection and annotation of a similar training video corpus. We evaluate our technique against a baseline that does not use text-mined knowledge and show that humans prefer our descriptions 61% of the time.\n \n"
            },
            "slug": "Generating-Natural-Language-Video-Descriptions-Krishnamoorthy-Malkarnenkar",
            "title": {
                "fragments": [],
                "text": "Generating Natural-Language Video Descriptions Using Text-Mined Knowledge"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This work combines the output of state-of-the-art object and activity detectors with \"real-world' knowledge to select the most probable subject-verb-object triplet for describing a video, and shows that this knowledge, automatically mined from web-scale text corpora, enhances the triplet selection algorithm by providing it contextual information and leads to a four-fold increase in activity identification."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2485529"
                        ],
                        "name": "Michaela Regneri",
                        "slug": "Michaela-Regneri",
                        "structuredName": {
                            "firstName": "Michaela",
                            "lastName": "Regneri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michaela Regneri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34849128"
                        ],
                        "name": "Marcus Rohrbach",
                        "slug": "Marcus-Rohrbach",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Rohrbach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcus Rohrbach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24138684"
                        ],
                        "name": "Dominikus Wetzel",
                        "slug": "Dominikus-Wetzel",
                        "structuredName": {
                            "firstName": "Dominikus",
                            "lastName": "Wetzel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dominikus Wetzel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727272"
                        ],
                        "name": "Stefan Thater",
                        "slug": "Stefan-Thater",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Thater",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefan Thater"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717560"
                        ],
                        "name": "Manfred Pinkal",
                        "slug": "Manfred-Pinkal",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Pinkal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred Pinkal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 7
                            }
                        ],
                        "text": "Unlike [17], workers could freely describe videos without aligning each sentence"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 95
                            }
                        ],
                        "text": "Data collection The data was collected via Amazon Mechanical Turk (AMT) using the TACoS corpus [17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 57
                            }
                        ],
                        "text": "We start by using the ground truth intervals provided by TACoS."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 95
                            }
                        ],
                        "text": "We use additional videos from the MPII Composite dataset [20] that correspond to dishes of the TACoS subset."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 120
                            }
                        ],
                        "text": "First line shows the result of extracting sentences from the detailed description generated by model of [19] trained on TACoS."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 70
                            }
                        ],
                        "text": "This is an increase from 22.1% reported by [19] due to converting the TACoS corpus to past tense, making it more uniform."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 47
                            }
                        ],
                        "text": "We augment the video-description dataset TACoS [17] with short and single sentence descriptions (see Sec."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 56
                            }
                        ],
                        "text": "Due to the relative simplicity of the dishes present in TACoS (e.g. as preparing a carrot), we do not observe much variation in the degree of abstractness of the used words."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 66
                            }
                        ],
                        "text": "Overall, we have collected about 2600 triples of descriptions for TACoS videos."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 91
                            }
                        ],
                        "text": "We assign part-of-speech (POS) tags to the collected descriptions and the ones provided by TACoS using the Stanford POS tagger [23]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 106
                            }
                        ],
                        "text": "The possible states of the CRF nodes are based on the provided video segment labels for the TACoS dataset [17] as well as the topic (dish) labels of the videos."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 52
                            }
                        ],
                        "text": "The detailed descriptions and the descriptions from TACoS are similar except in the APPLIANCE category."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 22
                            }
                        ],
                        "text": "The descriptions from TACoS, the detailed descriptions and the short description mainly use verbs describing specific activities, such as cut or take, see Table 1."
                    },
                    "intents": []
                }
            ],
            "corpusId": 438559,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "21b3007f967d39e1346bc91e0fc8b3f16121300c",
            "isKey": false,
            "numCitedBy": 274,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work has shown that the integration of visual information into text-based models can substantially improve model predictions, but so far only visual information extracted from static images has been used. In this paper, we consider the problem of grounding sentences describing actions in visual information extracted from videos. We present a general purpose corpus that aligns high quality videos with multiple natural language descriptions of the actions portrayed in the videos, together with an annotation of how similar the action descriptions are to each other. Experimental results demonstrate that a text-based model of similarity between actions improves substantially when combined with visual information from videos depicting the described actions."
            },
            "slug": "Grounding-Action-Descriptions-in-Videos-Regneri-Rohrbach",
            "title": {
                "fragments": [],
                "text": "Grounding Action Descriptions in Videos"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A general purpose corpus is presented that aligns high quality videos with multiple natural language descriptions of the actions portrayed in the videos, together with an annotation of how similar the action descriptions are to each other."
            },
            "venue": {
                "fragments": [],
                "text": "Transactions of the Association for Computational Linguistics"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143787583"
                        ],
                        "name": "Ali Farhadi",
                        "slug": "Ali-Farhadi",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Farhadi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ali Farhadi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1888731"
                        ],
                        "name": "Mohsen Hejrati",
                        "slug": "Mohsen-Hejrati",
                        "structuredName": {
                            "firstName": "Mohsen",
                            "lastName": "Hejrati",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohsen Hejrati"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21160985"
                        ],
                        "name": "M. Sadeghi",
                        "slug": "M.-Sadeghi",
                        "structuredName": {
                            "firstName": "Mohammad",
                            "lastName": "Sadeghi",
                            "middleNames": [
                                "Amin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sadeghi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052690705"
                        ],
                        "name": "Peter Young",
                        "slug": "Peter-Young",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Young",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Young"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3125805"
                        ],
                        "name": "Cyrus Rashtchian",
                        "slug": "Cyrus-Rashtchian",
                        "structuredName": {
                            "firstName": "Cyrus",
                            "lastName": "Rashtchian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cyrus Rashtchian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3118681"
                        ],
                        "name": "J. Hockenmaier",
                        "slug": "J.-Hockenmaier",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Hockenmaier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hockenmaier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 83
                            }
                        ],
                        "text": "Recently, this task has received an increased interest both in the computer vision [4, 12, 5, 2, 19] and computational linguistic communities [13, 25, 11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 63
                            }
                        ],
                        "text": "An alternative is to retrieve sentences from a training corpus [4, 2] or to compose novel descriptions based on a language model [12, 13, 15, 19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13272863,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eaaed23a2d94feb2f1c3ff22a25777c7a78f3141",
            "isKey": false,
            "numCitedBy": 986,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Humans can prepare concise descriptions of pictures, focusing on what they find important. We demonstrate that automatic methods can do so too. We describe a system that can compute a score linking an image to a sentence. This score can be used to attach a descriptive sentence to a given image, or to obtain images that illustrate a given sentence. The score is obtained by comparing an estimate of meaning obtained from the image to one obtained from the sentence. Each estimate of meaning comes from a discriminative procedure that is learned us-ingdata. We evaluate on a novel dataset consisting of human-annotated images. While our underlying estimate of meaning is impoverished, it is sufficient to produce very good quantitative results, evaluated with a novel score that can account for synecdoche."
            },
            "slug": "Every-Picture-Tells-a-Story:-Generating-Sentences-Farhadi-Hejrati",
            "title": {
                "fragments": [],
                "text": "Every Picture Tells a Story: Generating Sentences from Images"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A system that can compute a score linking an image to a sentence, which can be used to attach a descriptive sentence to a given image, or to obtain images that illustrate a given sentence."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145809603"
                        ],
                        "name": "N. Siddharth",
                        "slug": "N.-Siddharth",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Siddharth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Siddharth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21570451"
                        ],
                        "name": "Andrei Barbu",
                        "slug": "Andrei-Barbu",
                        "structuredName": {
                            "firstName": "Andrei",
                            "lastName": "Barbu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrei Barbu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737754"
                        ],
                        "name": "J. Siskind",
                        "slug": "J.-Siskind",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Siskind",
                            "middleNames": [
                                "Mark"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Siskind"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1202699,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c8653e7d237891044fb7b9f886db5b57bbc94cf",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a system that demonstrates how the compositional structure of events, in concert with the compositional structure of language, can interplay with the underlying focusing mechanisms in video action recognition, providing a medium for top-down and bottom-up integration as well as multi-modal integration between vision and language. We show how the roles played by participants (nouns), their characteristics (adjectives), the actions performed (verbs), the manner of such actions (adverbs), and changing spatial relations between participants (prepositions), in the form of whole-sentence descriptions mediated by a grammar, guides the activity-recognition process. Further, the utility and expressiveness of our framework is demonstrated by performing three separate tasks in the domain of multi-activity video: sentence-guided focus of attention, generation of sentential description, and query-based search, simply by leveraging the framework in different manners."
            },
            "slug": "Seeing-What-You're-Told:-Sentence-Guided-Activity-Siddharth-Barbu",
            "title": {
                "fragments": [],
                "text": "Seeing What You're Told: Sentence-Guided Activity Recognition in Video"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A system that demonstrates how the compositional structure of events can interplay with the underlying focusing mechanisms in video action recognition, providing a medium for top-down and bottom-up integration as well as multi-modal integration between vision and language."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120684987"
                        ],
                        "name": "Muhammad Usman Ghani Khan",
                        "slug": "Muhammad-Usman-Ghani-Khan",
                        "structuredName": {
                            "firstName": "Muhammad",
                            "lastName": "Khan",
                            "middleNames": [
                                "Usman",
                                "Ghani"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Muhammad Usman Ghani Khan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152830261"
                        ],
                        "name": "Lei Zhang",
                        "slug": "Lei-Zhang",
                        "structuredName": {
                            "firstName": "Lei",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lei Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703592"
                        ],
                        "name": "Y. Gotoh",
                        "slug": "Y.-Gotoh",
                        "structuredName": {
                            "firstName": "Yoshihiko",
                            "lastName": "Gotoh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Gotoh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10083843,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fbadf89b990acedf23e1df03d4869010d2dbc59e",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This contribution addresses generation of natural language descriptions for human actions and behaviour observed in video streams. The work starts with implementation of conventional image processing techniques to extract high-level features from video. Because human is often the most important and also interesting feature, description focuses on humans and their activities. Although feature extraction processes are erroneous at various levels, we explore approaches to put them together to produce a coherent description. Evaluation is made by calculating the overlap similarity score between human authored and machine generated descriptions."
            },
            "slug": "Human-Focused-Video-Description-Khan-Zhang",
            "title": {
                "fragments": [],
                "text": "Human Focused Video Description"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "This contribution addresses generation of natural language descriptions for human actions and behaviour observed in video streams by implementing conventional image processing techniques to extract high-level features from video."
            },
            "venue": {
                "fragments": [],
                "text": "2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2910174"
                        ],
                        "name": "Haonan Yu",
                        "slug": "Haonan-Yu",
                        "structuredName": {
                            "firstName": "Haonan",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haonan Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737754"
                        ],
                        "name": "J. Siskind",
                        "slug": "J.-Siskind",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Siskind",
                            "middleNames": [
                                "Mark"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Siskind"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 142
                            }
                        ],
                        "text": "Recently, this task has received an increased interest both in the computer vision [4, 12, 5, 2, 19] and computational linguistic communities [13, 25, 11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5061155,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96a0320ef14877038906947b684011cf7378c440",
            "isKey": false,
            "numCitedBy": 138,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method that learns representations for word meanings from short video clips paired with sentences. Unlike prior work on learning language from symbolic input, our input consists of video of people interacting with multiple complex objects in outdoor environments. Unlike prior computer-vision approaches that learn from videos with verb labels or images with noun labels, our labels are sentences containing nouns, verbs, prepositions, adjectives, and adverbs. The correspondence between words and concepts in the video is learned in an unsupervised fashion, even when the video depicts simultaneous events described by multiple sentences or when different aspects of a single event are described with multiple sentences. The learned word meanings can be subsequently used to automatically generate description of new video."
            },
            "slug": "Grounded-Language-Learning-from-Video-Described-Yu-Siskind",
            "title": {
                "fragments": [],
                "text": "Grounded Language Learning from Video Described with Sentences"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A method that learns representations for word meanings from short video clips paired with sentences that can be subsequently used to automatically generate description of new video."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71208033"
                        ],
                        "name": "Chun-Chet Tan",
                        "slug": "Chun-Chet-Tan",
                        "structuredName": {
                            "firstName": "Chun-Chet",
                            "lastName": "Tan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chun-Chet Tan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717861"
                        ],
                        "name": "Yu-Gang Jiang",
                        "slug": "Yu-Gang-Jiang",
                        "structuredName": {
                            "firstName": "Yu-Gang",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu-Gang Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143977389"
                        ],
                        "name": "C. Ngo",
                        "slug": "C.-Ngo",
                        "structuredName": {
                            "firstName": "Chong-Wah",
                            "lastName": "Ngo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Ngo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "Using a simple template, [22] generates a sentence every 10 seconds based on concept detection."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 119
                            }
                        ],
                        "text": "To generate descriptions for videos and images, rules or templates are a powerful tool but need to be manually defined [12, 22, 6, 11, 5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 84
                            }
                        ],
                        "text": "The first task, multi-sentence description, has been explored for videos previously [2, 8, 22], but open challenges remain, namely finding a segmentation of appropriate granDetailed: A woman turned on stove."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17627359,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "261ae2f6f426b5efd32ea5b981751d1c63c5309d",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatically generating compact textual descriptions of complex video contents has wide applications. With the recent advancements in automatic audio-visual content recognition, in this paper we explore the technical feasibility of the challenging issue of precisely recounting video contents. Based on cutting-edge automatic recognition techniques, we start from classifying a variety of visual and audio concepts in video contents. According to the classification results, we apply simple rule-based methods to generate textual descriptions of video contents. Results are evaluated by conducting carefully designed user studies. We find that the state-of-the-art visual and audio concept classification, although far from perfect, is able to provide very useful clues indicating what is happening in the videos. Most users involved in the evaluation confirmed the informativeness of our machine-generated descriptions."
            },
            "slug": "Towards-textually-describing-complex-video-contents-Tan-Jiang",
            "title": {
                "fragments": [],
                "text": "Towards textually describing complex video contents with audio-visual concept classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper explores the technical feasibility of the challenging issue of precisely recounting video contents using cutting-edge automatic recognition techniques and finds that the state-of-the-art visual and audio concept classification, although far from perfect, is able to provide very useful clues indicating what is happening in the videos."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Multimedia"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49693392"
                        ],
                        "name": "A. Kojima",
                        "slug": "A.-Kojima",
                        "structuredName": {
                            "firstName": "Atsuhiro",
                            "lastName": "Kojima",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kojima"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114145871"
                        ],
                        "name": "Takeshi Tamura",
                        "slug": "Takeshi-Tamura",
                        "structuredName": {
                            "firstName": "Takeshi",
                            "lastName": "Tamura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Takeshi Tamura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145950023"
                        ],
                        "name": "K. Fukunaga",
                        "slug": "K.-Fukunaga",
                        "structuredName": {
                            "firstName": "Kunio",
                            "lastName": "Fukunaga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukunaga"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16139212,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d53a97a3dd7760b193c0d9a5293b60feff239059",
            "isKey": false,
            "numCitedBy": 279,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a method for describing human activities from video images based on concept hierarchies of actions. Major difficulty in transforming video images into textual descriptions is how to bridge a semantic gap between them, which is also known as inverse Hollywood problem. In general, the concepts of events or actions of human can be classified by semantic primitives. By associating these concepts with the semantic features extracted from video images, appropriate syntactic components such as verbs, objects, etc. are determined and then translated into natural language sentences. We also demonstrate the performance of the proposed method by several experiments."
            },
            "slug": "Natural-Language-Description-of-Human-Activities-on-Kojima-Tamura",
            "title": {
                "fragments": [],
                "text": "Natural Language Description of Human Activities from Video Images Based on Concept Hierarchy of Actions"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "A method for describing human activities from video images based on concept hierarchies of actions based on semantic primitives, which demonstrates the performance of the proposed method by several experiments."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145592791"
                        ],
                        "name": "Polina Kuznetsova",
                        "slug": "Polina-Kuznetsova",
                        "structuredName": {
                            "firstName": "Polina",
                            "lastName": "Kuznetsova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Polina Kuznetsova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2004053"
                        ],
                        "name": "Vicente Ordonez",
                        "slug": "Vicente-Ordonez",
                        "structuredName": {
                            "firstName": "Vicente",
                            "lastName": "Ordonez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vicente Ordonez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685538"
                        ],
                        "name": "Tamara L. Berg",
                        "slug": "Tamara-L.-Berg",
                        "structuredName": {
                            "firstName": "Tamara",
                            "lastName": "Berg",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tamara L. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699545"
                        ],
                        "name": "Yejin Choi",
                        "slug": "Yejin-Choi",
                        "structuredName": {
                            "firstName": "Yejin",
                            "lastName": "Choi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yejin Choi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[13] models discourse constraints, content planning, linguistic cohesion, and is able to reduce redundancy using ILP."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 142
                            }
                        ],
                        "text": "Recently, this task has received an increased interest both in the computer vision [4, 12, 5, 2, 19] and computational linguistic communities [13, 25, 11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 93
                            }
                        ],
                        "text": "In contrast we model a global semantic topic to allow descriptions with many sentences while [13] generates in most cases only 1-3 sentences."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 129
                            }
                        ],
                        "text": "An alternative is to retrieve sentences from a training corpus [4, 2] or to compose novel descriptions based on a language model [12, 13, 15, 19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10315654,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2a0d0f6c5a69b264710df0230696f47c5918e2f2",
            "isKey": true,
            "numCitedBy": 319,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a holistic data-driven approach to image description generation, exploiting the vast amount of (noisy) parallel image data and associated natural language descriptions available on the web. More specifically, given a query image, we retrieve existing human-composed phrases used to describe visually similar images, then selectively combine those phrases to generate a novel description for the query image. We cast the generation process as constraint optimization problems, collectively incorporating multiple interconnected aspects of language composition for content planning, surface realization and discourse structure. Evaluation by human annotators indicates that our final system generates more semantically correct and linguistically appealing descriptions than two nontrivial baselines."
            },
            "slug": "Collective-Generation-of-Natural-Image-Descriptions-Kuznetsova-Ordonez",
            "title": {
                "fragments": [],
                "text": "Collective Generation of Natural Image Descriptions"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A holistic data-driven approach to image description generation, exploiting the vast amount of (noisy) parallel image data and associated natural language descriptions available on the web to generate novel descriptions for query images."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687120"
                        ],
                        "name": "S. Guadarrama",
                        "slug": "S.-Guadarrama",
                        "structuredName": {
                            "firstName": "Sergio",
                            "lastName": "Guadarrama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Guadarrama"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3006928"
                        ],
                        "name": "N. Krishnamoorthy",
                        "slug": "N.-Krishnamoorthy",
                        "structuredName": {
                            "firstName": "Niveda",
                            "lastName": "Krishnamoorthy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Krishnamoorthy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3163967"
                        ],
                        "name": "Girish Malkarnenkar",
                        "slug": "Girish-Malkarnenkar",
                        "structuredName": {
                            "firstName": "Girish",
                            "lastName": "Malkarnenkar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Girish Malkarnenkar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1811430"
                        ],
                        "name": "Subhashini Venugopalan",
                        "slug": "Subhashini-Venugopalan",
                        "structuredName": {
                            "firstName": "Subhashini",
                            "lastName": "Venugopalan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Subhashini Venugopalan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2903226"
                        ],
                        "name": "Kate Saenko",
                        "slug": "Kate-Saenko",
                        "structuredName": {
                            "firstName": "Kate",
                            "lastName": "Saenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kate Saenko"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 11
                            }
                        ],
                        "text": "Closest is [5], which predicts more abstract verbs and nouns if the uncertainty is too high for a more specific prediction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 119
                            }
                        ],
                        "text": "To generate descriptions for videos and images, rules or templates are a powerful tool but need to be manually defined [12, 22, 6, 11, 5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 83
                            }
                        ],
                        "text": "Recently, this task has received an increased interest both in the computer vision [4, 12, 5, 2, 19] and computational linguistic communities [13, 25, 11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11418612,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d6a7a563640bf53953c4fda0997e4db176488510",
            "isKey": false,
            "numCitedBy": 405,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Despite a recent push towards large-scale object recognition, activity recognition remains limited to narrow domains and small vocabularies of actions. In this paper, we tackle the challenge of recognizing and describing activities ``in-the-wild''. We present a solution that takes a short video clip and outputs a brief sentence that sums up the main activity in the video, such as the actor, the action and its object. Unlike previous work, our approach works on out-of-domain actions: it does not require training videos of the exact activity. If it cannot find an accurate prediction for a pre-trained model, it finds a less specific answer that is also plausible from a pragmatic standpoint. We use semantic hierarchies learned from the data to help to choose an appropriate level of generalization, and priors learned from Web-scale natural language corpora to penalize unlikely combinations of actors/actions/objects, we also use a Web-scale language model to ``fill in'' novel verbs, i.e. when the verb does not appear in the training set. We evaluate our method on a large YouTube corpus and demonstrate it is able to generate short sentence descriptions of video clips better than baseline approaches."
            },
            "slug": "YouTube2Text:-Recognizing-and-Describing-Arbitrary-Guadarrama-Krishnamoorthy",
            "title": {
                "fragments": [],
                "text": "YouTube2Text: Recognizing and Describing Arbitrary Activities Using Semantic Hierarchies and Zero-Shot Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper presents a solution that takes a short video clip and outputs a brief sentence that sums up the main activity in the video, such as the actor, the action and its object, and uses a Web-scale language model to ``fill in'' novel verbs."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Computer Vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145564333"
                        ],
                        "name": "Girish Kulkarni",
                        "slug": "Girish-Kulkarni",
                        "structuredName": {
                            "firstName": "Girish",
                            "lastName": "Kulkarni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Girish Kulkarni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3128210"
                        ],
                        "name": "Visruth Premraj",
                        "slug": "Visruth-Premraj",
                        "structuredName": {
                            "firstName": "Visruth",
                            "lastName": "Premraj",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Visruth Premraj"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2985883"
                        ],
                        "name": "S. Dhar",
                        "slug": "S.-Dhar",
                        "structuredName": {
                            "firstName": "Sagnik",
                            "lastName": "Dhar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dhar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50341924"
                        ],
                        "name": "Siming Li",
                        "slug": "Siming-Li",
                        "structuredName": {
                            "firstName": "Siming",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Siming Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699545"
                        ],
                        "name": "Yejin Choi",
                        "slug": "Yejin-Choi",
                        "structuredName": {
                            "firstName": "Yejin",
                            "lastName": "Choi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yejin Choi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685538"
                        ],
                        "name": "Tamara L. Berg",
                        "slug": "Tamara-L.-Berg",
                        "structuredName": {
                            "firstName": "Tamara",
                            "lastName": "Berg",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tamara L. Berg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[12] connects different object detection with prepositions using a CRF and generates a sentence for each pair."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 119
                            }
                        ],
                        "text": "To generate descriptions for videos and images, rules or templates are a powerful tool but need to be manually defined [12, 22, 6, 11, 5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 83
                            }
                        ],
                        "text": "Recently, this task has received an increased interest both in the computer vision [4, 12, 5, 2, 19] and computational linguistic communities [13, 25, 11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 129
                            }
                        ],
                        "text": "An alternative is to retrieve sentences from a training corpus [4, 2] or to compose novel descriptions based on a language model [12, 13, 15, 19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10116609,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "169b847e69c35cfd475eb4dcc561a24de11762ca",
            "isKey": true,
            "numCitedBy": 483,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "We posit that visually descriptive language offers computer vision researchers both information about the world, and information about how people describe the world. The potential benefit from this source is made more significant due to the enormous amount of language data easily available today. We present a system to automatically generate natural language descriptions from images that exploits both statistics gleaned from parsing large quantities of text data and recognition algorithms from computer vision. The system is very effective at producing relevant sentences for images. It also generates descriptions that are notably more true to the specific image content than previous work."
            },
            "slug": "Baby-talk:-Understanding-and-generating-simple-Kulkarni-Premraj",
            "title": {
                "fragments": [],
                "text": "Baby talk: Understanding and generating simple image descriptions"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A system to automatically generate natural language descriptions from images that exploits both statistics gleaned from parsing large quantities of text data and recognition algorithms from computer vision that is very effective at producing relevant sentences for images."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726095131"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Gupta",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2577358"
                        ],
                        "name": "P. Srinivasan",
                        "slug": "P.-Srinivasan",
                        "structuredName": {
                            "firstName": "Praveen",
                            "lastName": "Srinivasan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Srinivasan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46865129"
                        ],
                        "name": "Jianbo Shi",
                        "slug": "Jianbo-Shi",
                        "structuredName": {
                            "firstName": "Jianbo",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianbo Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693428"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Davis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 119
                            }
                        ],
                        "text": "To generate descriptions for videos and images, rules or templates are a powerful tool but need to be manually defined [12, 22, 6, 11, 5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 12
                            }
                        ],
                        "text": "For videos, [6] learns AND/OR graphs to capture the causal relationships of actions given visual and textual data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16635607,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "49927656ede0c75af22ca73dcf4abba028839650",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Analyzing videos of human activities involves not only recognizing actions (typically based on their appearances), but also determining the story/plot of the video. The storyline of a video describes causal relationships between actions. Beyond recognition of individual actions, discovering causal relationships helps to better understand the semantic meaning of the activities. We present an approach to learn a visually grounded storyline model of videos directly from weakly labeled data. The storyline model is represented as an AND-OR graph, a structure that can compactly encode storyline variation across videos. The edges in the AND-OR graph correspond to causal relationships which are represented in terms of spatio-temporal constraints. We formulate an Integer Programming framework for action recognition and storyline extraction using the storyline model and visual groundings learned from training data."
            },
            "slug": "Understanding-videos,-constructing-plots-learning-a-Gupta-Srinivasan",
            "title": {
                "fragments": [],
                "text": "Understanding videos, constructing plots learning a visually grounded storyline model from annotated videos"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An Integer Programming framework for action recognition and storyline extraction using the storyline model and visual groundings learned from training data is formulated."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3259253"
                        ],
                        "name": "Kristina Toutanova",
                        "slug": "Kristina-Toutanova",
                        "structuredName": {
                            "firstName": "Kristina",
                            "lastName": "Toutanova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kristina Toutanova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38666915"
                        ],
                        "name": "D. Klein",
                        "slug": "D.-Klein",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Klein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 127
                            }
                        ],
                        "text": "We assign part-of-speech (POS) tags to the collected descriptions and the ones provided by TACoS using the Stanford POS tagger [23]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14835360,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb42a490cf4f186d3383c92963817d100afd81e2",
            "isKey": false,
            "numCitedBy": 3438,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) fine-grained modeling of unknown word features. Using these ideas together, the resulting tagger gives a 97.24% accuracy on the Penn Treebank WSJ, an error reduction of 4.4% on the best previous single automatically learned tagging result."
            },
            "slug": "Feature-Rich-Part-of-Speech-Tagging-with-a-Cyclic-Toutanova-Klein",
            "title": {
                "fragments": [],
                "text": "Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A new part-of-speech tagger is presented that demonstrates the following ideas: explicit use of both preceding and following tag contexts via a dependency network representation, broad use of lexical features, and effective use of priors in conditional loglinear models."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34849128"
                        ],
                        "name": "Marcus Rohrbach",
                        "slug": "Marcus-Rohrbach",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Rohrbach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcus Rohrbach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2485529"
                        ],
                        "name": "Michaela Regneri",
                        "slug": "Michaela-Regneri",
                        "structuredName": {
                            "firstName": "Michaela",
                            "lastName": "Regneri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michaela Regneri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1906895"
                        ],
                        "name": "M. Andriluka",
                        "slug": "M.-Andriluka",
                        "structuredName": {
                            "firstName": "Mykhaylo",
                            "lastName": "Andriluka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Andriluka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40404576"
                        ],
                        "name": "S. Amin",
                        "slug": "S.-Amin",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Amin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717560"
                        ],
                        "name": "Manfred Pinkal",
                        "slug": "Manfred-Pinkal",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Pinkal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred Pinkal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 57
                            }
                        ],
                        "text": "We use additional videos from the MPII Composite dataset [20] that correspond to dishes of the TACoS subset."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 104
                            }
                        ],
                        "text": "The topic node unary E(st|v) is defined similarly, based on the composite activity recognition features [20] as visual descriptors of video v."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 103
                            }
                        ],
                        "text": "Following the experimental setup of [19] we use videos and dense trajectory features [24] published by [20]; we use the same test split for visual recognition and video description."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 69
                            }
                        ],
                        "text": "Here we use the features proposed for composite activity recognition [20], training a specific SVM for each state of the dish node."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10175213,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8623fe8b087cedcaac276e313f8fed6f0dfccc33",
            "isKey": false,
            "numCitedBy": 131,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "State-of-the-art human activity recognition methods build on discriminative learning which requires a representative training set for good performance. This leads to scalability issues for the recognition of large sets of highly diverse activities. In this paper we leverage the fact that many human activities are compositional and that the essential components of the activities can be obtained from textual descriptions or scripts. To share and transfer knowledge between composite activities we model them by a common set of attributes corresponding to basic actions and object participants. This attribute representation allows to incorporate script data that delivers new variations of a composite activity or even to unseen composite activities. In our experiments on 41 composite cooking tasks, we found that script data to successfully capture the high variability of composite activities. We show improvements in a supervised case where training data for all composite cooking tasks is available, but we are also able to recognize unseen composites by just using script data and without any manual video annotation."
            },
            "slug": "Script-Data-for-Attribute-Based-Recognition-of-Rohrbach-Regneri",
            "title": {
                "fragments": [],
                "text": "Script Data for Attribute-Based Recognition of Composite Activities"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper leverage the fact that many human activities are compositional and that the essential components of the activities can be obtained from textual descriptions or scripts to incorporate script data that delivers new variations of a composite activity or even to unseen composite activities."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2579449"
                        ],
                        "name": "Christian Hardmeier",
                        "slug": "Christian-Hardmeier",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Hardmeier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Hardmeier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713395"
                        ],
                        "name": "Sara Stymne",
                        "slug": "Sara-Stymne",
                        "structuredName": {
                            "firstName": "Sara",
                            "lastName": "Stymne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sara Stymne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143675545"
                        ],
                        "name": "J. Tiedemann",
                        "slug": "J.-Tiedemann",
                        "structuredName": {
                            "firstName": "J\u00f6rg",
                            "lastName": "Tiedemann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tiedemann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720988"
                        ],
                        "name": "Joakim Nivre",
                        "slug": "Joakim-Nivre",
                        "structuredName": {
                            "firstName": "Joakim",
                            "lastName": "Nivre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joakim Nivre"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 163
                            }
                        ],
                        "text": "While SMT has mostly focused on the translation of single sentences, recent approaches aim to optimize the entire translation in order to generate consistent text [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 162171,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2546f710ddaddbcf576552f728fb9b57cbeb179c",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe Docent, an open-source decoder for statistical machine translation that breaks with the usual sentence-bysentence paradigm and translates complete documents as units. By taking translation to the document level, our decoder can handle feature models with arbitrary discourse-wide dependencies and constitutes an essential infrastructure component in the quest for discourse-aware SMT models. 1 Motivation"
            },
            "slug": "Docent:-A-Document-Level-Decoder-for-Phrase-Based-Hardmeier-Stymne",
            "title": {
                "fragments": [],
                "text": "Docent: A Document-Level Decoder for Phrase-Based Statistical Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "Docent is an open-source decoder for statistical machine translation that breaks with the usual sentence-bysentence paradigm and translates complete documents as units and constitutes an essential infrastructure component in the quest for discourse-aware SMT models."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1906895"
                        ],
                        "name": "M. Andriluka",
                        "slug": "M.-Andriluka",
                        "structuredName": {
                            "firstName": "Mykhaylo",
                            "lastName": "Andriluka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Andriluka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145920814"
                        ],
                        "name": "S. Roth",
                        "slug": "S.-Roth",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16956687,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2b8f87a454b80da5800337ccb5579cc7afbef45e",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we consider people detection and articulated pose estimation, two closely related and challenging problems in computer vision. Conceptually, both of these problems can be addressed within the pictorial structures framework (Felzenszwalb and Huttenlocher in Int. J. Comput. Vis. 61(1):55\u201379, 2005; Fischler and Elschlager in IEEE Trans. Comput. C-22(1):67\u201392, 1973), even though previous approaches have not shown such generality. A principal difficulty for such a general approach is to model the appearance of body parts. The model has to be discriminative enough to enable reliable detection in cluttered scenes and general enough to capture highly variable appearance. Therefore, as the first important component of our approach, we propose a discriminative appearance model based on densely sampled local descriptors and AdaBoost classifiers. Secondly, we interpret the normalized margin of each classifier as likelihood in a generative model and compute marginal posteriors for each part using belief propagation. Thirdly, non-Gaussian relationships between parts are represented as Gaussians in the coordinate system of the joint between the parts. Additionally, in order to cope with shortcomings of tree-based pictorial structures models, we augment our model with additional repulsive factors in order to discourage overcounting of image evidence. We demonstrate that the combination of these components within the pictorial structures framework results in a generic model that yields state-of-the-art performance for several datasets on a variety of tasks: people detection, upper body pose estimation, and full body pose estimation."
            },
            "slug": "Discriminative-Appearance-Models-for-Pictorial-Andriluka-Roth",
            "title": {
                "fragments": [],
                "text": "Discriminative Appearance Models for Pictorial Structures"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The combination of these components within the pictorial structures framework results in a generic model that yields state-of-the-art performance for several datasets on a variety of tasks: people detection, upper body pose estimation, and full body Pose estimation."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49501003"
                        ],
                        "name": "Margaret Mitchell",
                        "slug": "Margaret-Mitchell",
                        "structuredName": {
                            "firstName": "Margaret",
                            "lastName": "Mitchell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Margaret Mitchell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34176020"
                        ],
                        "name": "Jesse Dodge",
                        "slug": "Jesse-Dodge",
                        "structuredName": {
                            "firstName": "Jesse",
                            "lastName": "Dodge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jesse Dodge"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46479604"
                        ],
                        "name": "Amit Goyal",
                        "slug": "Amit-Goyal",
                        "structuredName": {
                            "firstName": "Amit",
                            "lastName": "Goyal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amit Goyal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721910"
                        ],
                        "name": "Kota Yamaguchi",
                        "slug": "Kota-Yamaguchi",
                        "structuredName": {
                            "firstName": "Kota",
                            "lastName": "Yamaguchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kota Yamaguchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714215"
                        ],
                        "name": "K. Stratos",
                        "slug": "K.-Stratos",
                        "structuredName": {
                            "firstName": "Karl",
                            "lastName": "Stratos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Stratos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682965"
                        ],
                        "name": "Xufeng Han",
                        "slug": "Xufeng-Han",
                        "structuredName": {
                            "firstName": "Xufeng",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xufeng Han"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40614240"
                        ],
                        "name": "Alyssa C. Mensch",
                        "slug": "Alyssa-C.-Mensch",
                        "structuredName": {
                            "firstName": "Alyssa",
                            "lastName": "Mensch",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alyssa C. Mensch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685538"
                        ],
                        "name": "Tamara L. Berg",
                        "slug": "Tamara-L.-Berg",
                        "structuredName": {
                            "firstName": "Tamara",
                            "lastName": "Berg",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tamara L. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722360"
                        ],
                        "name": "Hal Daum\u00e9",
                        "slug": "Hal-Daum\u00e9",
                        "structuredName": {
                            "firstName": "Hal",
                            "lastName": "Daum\u00e9",
                            "middleNames": [],
                            "suffix": "III"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hal Daum\u00e9"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 144
                            }
                        ],
                        "text": "An alternative is to retrieve sentences from a training corpus [4, 2] or to compose novel descriptions based on a language model [12, 13, 15, 19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2972357,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "355de7460120ddc1150d9ce3756f9848983f7ff4",
            "isKey": false,
            "numCitedBy": 383,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a novel generation system that composes humanlike descriptions of images from computer vision detections. By leveraging syntactically informed word co-occurrence statistics, the generator filters and constrains the noisy detections output from a vision system to generate syntactic trees that detail what the computer vision system sees. Results show that the generation system outperforms state-of-the-art systems, automatically generating some of the most natural image descriptions to date."
            },
            "slug": "Midge:-Generating-Image-Descriptions-From-Computer-Mitchell-Dodge",
            "title": {
                "fragments": [],
                "text": "Midge: Generating Image Descriptions From Computer Vision Detections"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "A novel generation system that composes humanlike descriptions of images from computer vision detections by leveraging syntactically informed word co-occurrence statistics and automatically generating some of the most natural image descriptions to date."
            },
            "venue": {
                "fragments": [],
                "text": "EACL"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46506697"
                        ],
                        "name": "Heng Wang",
                        "slug": "Heng-Wang",
                        "structuredName": {
                            "firstName": "Heng",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Heng Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2909350"
                        ],
                        "name": "Alexander Kl\u00e4ser",
                        "slug": "Alexander-Kl\u00e4ser",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Kl\u00e4ser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Kl\u00e4ser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689269"
                        ],
                        "name": "Cheng-Lin Liu",
                        "slug": "Cheng-Lin-Liu",
                        "structuredName": {
                            "firstName": "Cheng-Lin",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cheng-Lin Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 85
                            }
                        ],
                        "text": "Following the experimental setup of [19] we use videos and dense trajectory features [24] published by [20]; we use the same test split for visual recognition and video description."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 78
                            }
                        ],
                        "text": "The visual recognition approach of [19] is based on dense trajectory features [24]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5401052,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bbe0819a47a9f3f11dd34bb3ab44a997ef111088",
            "isKey": false,
            "numCitedBy": 1551,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a video representation based on dense trajectories and motion boundary descriptors. Trajectories capture the local motion information of the video. A dense representation guarantees a good coverage of foreground motion as well as of the surrounding context. A state-of-the-art optical flow algorithm enables a robust and efficient extraction of dense trajectories. As descriptors we extract features aligned with the trajectories to characterize shape (point coordinates), appearance (histograms of oriented gradients) and motion (histograms of optical flow). Additionally, we introduce a descriptor based on motion boundary histograms (MBH) which rely on differential optical flow. The MBH descriptor shows to consistently outperform other state-of-the-art descriptors, in particular on real-world videos that contain a significant amount of camera motion. We evaluate our video representation in the context of action classification on nine datasets, namely KTH, YouTube, Hollywood2, UCF sports, IXMAS, UIUC, Olympic Sports, UCF50 and HMDB51. On all datasets our approach outperforms current state-of-the-art results."
            },
            "slug": "Dense-Trajectories-and-Motion-Boundary-Descriptors-Wang-Kl\u00e4ser",
            "title": {
                "fragments": [],
                "text": "Dense Trajectories and Motion Boundary Descriptors for Action Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The MBH descriptor shows to consistently outperform other state-of-the-art descriptors, in particular on real-world videos that contain a significant amount of camera motion."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053229833"
                        ],
                        "name": "Christopher Dyer",
                        "slug": "Christopher-Dyer",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Dyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher Dyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2295928"
                        ],
                        "name": "S. Muresan",
                        "slug": "S.-Muresan",
                        "structuredName": {
                            "firstName": "Smaranda",
                            "lastName": "Muresan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Muresan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680292"
                        ],
                        "name": "P. Resnik",
                        "slug": "P.-Resnik",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Resnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Resnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 111
                            }
                        ],
                        "text": "Furthermore, we exploit the probabilistic output of the CRF and incorporate it in the SMT using a word-lattice [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 12
                            }
                        ],
                        "text": "The work of [3] shows that a probabilistic input encoded in a word lattice can improve the performance Output of system (a) the person got out a cucumber from the fridge (b) the person washed the cucumber (c) the person got out a peeler from the drawer (d) the person got out a knife from the drawer (e) the person peeled the cucumber (f) the person sliced the cucumber"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1261609,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "860c1880c5a98cc6a5aeb331113eeda0d84e725c",
            "isKey": false,
            "numCitedBy": 190,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Word lattice decoding has proven useful in spoken language translation; we argue that it provides a compelling model for translation of text genres, as well. We show that prior work in translating lattices using finite state techniques can be naturally extended to more expressive synchronous context-free grammarbased models. Additionally, we resolve a significant complication that non-linear word lattice inputs introduce in reordering models. Our experiments evaluating the approach demonstrate substantial gains for ChineseEnglish and Arabic-English translation."
            },
            "slug": "Generalizing-Word-Lattice-Translation-Dyer-Muresan",
            "title": {
                "fragments": [],
                "text": "Generalizing Word Lattice Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "It is argued that word lattice decoding provides a compelling model for translation of text genres, as well, and that prior work in translating lattices using finite state techniques can be naturally extended to more expressive synchronous context-free grammarbased models."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48127057"
                        ],
                        "name": "Ingrid Zukerman",
                        "slug": "Ingrid-Zukerman",
                        "structuredName": {
                            "firstName": "Ingrid",
                            "lastName": "Zukerman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ingrid Zukerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737616"
                        ],
                        "name": "D. Litman",
                        "slug": "D.-Litman",
                        "structuredName": {
                            "firstName": "Diane",
                            "lastName": "Litman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Litman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "in context of user models [26], we are not aware of any work in computer vision which studies how to select the desired amount of information to be recognized."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2576858,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb1b1fadceb96dc45391da449c5e40ded87f5b03",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 156,
            "paperAbstract": {
                "fragments": [],
                "text": "The fields of user modeling and natural language processing have been closely linked since the early days of user modeling. Natural language systems consult user models in order to improve their understanding of users' requirements and to generate appropriate and relevant responses. At the same time, the information natural language systems obtain from their users is expected to increase the accuracy of their user models. In this paper, we review natural language systems for generation, understanding and dialogue, focusing on the requirements and limitations these systems and user models place on each other. We then propose avenues for future research."
            },
            "slug": "Natural-Language-Processing-and-User-Modeling:-and-Zukerman-Litman",
            "title": {
                "fragments": [],
                "text": "Natural Language Processing and User Modeling: Synergies and Limitations"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper reviews natural language systems for generation, understanding and dialogue, focusing on the requirements and limitations these systems and user models place on each other and proposes avenues for future research."
            },
            "venue": {
                "fragments": [],
                "text": "User Modeling and User-Adapted Interaction"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2008596"
                        ],
                        "name": "Arpit Mittal",
                        "slug": "Arpit-Mittal",
                        "structuredName": {
                            "firstName": "Arpit",
                            "lastName": "Mittal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Arpit Mittal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143635539"
                        ],
                        "name": "P. Torr",
                        "slug": "P.-Torr",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Torr",
                            "middleNames": [
                                "H.",
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Torr"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8162487,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "12c6f3ae8f20a1473a89b9cbb82d0f02275ea62b",
            "isKey": false,
            "numCitedBy": 234,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a two-stage method for detecting hands and their orientation in unconstrained images. The first stage uses three complementary detectors to propose hand bounding boxes. Each bounding box is then scored by the three detectors independently, and a second stage classifier learnt to compute a final confidence score for the proposals using these features. We make the following contributions: (i) we add context-based and skin-based proposals to a sliding window shape based detector to increase recall; (ii) we develop a new method of non-maximum suppression based on super-pixels; and (iii) we introduce a fully annotated hand dataset for training and testing. We show that the hand detector exceeds the state of the art on two public datasets, including the PASCAL VOC 2010 human layout challenge."
            },
            "slug": "Hand-detection-using-multiple-proposals-Mittal-Zisserman",
            "title": {
                "fragments": [],
                "text": "Hand detection using multiple proposals"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The hand detector exceeds the state of the art on two public datasets, including the PASCAL VOC 2010 human layout challenge and is introduced with a fully annotated hand dataset for training and testing."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38666915"
                        ],
                        "name": "D. Klein",
                        "slug": "D.-Klein",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Klein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 205
                            }
                        ],
                        "text": "Hence, we automatically post-process the descriptions such that they are more cohesive using a set of domain-independent rules: (1) we fix punctuation and create syntactic parses using the Stanford parser [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11495042,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a600850ac0120cb09a0b7de7da80bb6a7a76de06",
            "isKey": false,
            "numCitedBy": 3370,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We demonstrate that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar. Indeed, its performance of 86.36% (LP/LR F1) is better than that of early lexicalized PCFG models, and surprisingly close to the current state-of-the-art. This result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models: an unlexicalized PCFG is much more compact, easier to replicate, and easier to interpret than more complex lexical models, and the parsing algorithms are simpler, more widely understood, of lower asymptotic complexity, and easier to optimize."
            },
            "slug": "Accurate-Unlexicalized-Parsing-Klein-Manning",
            "title": {
                "fragments": [],
                "text": "Accurate Unlexicalized Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "It is demonstrated that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144096985"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Miller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1671874,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "68c03788224000794d5491ab459be0b2a2c38677",
            "isKey": false,
            "numCitedBy": 13890,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4]."
            },
            "slug": "WordNet:-A-Lexical-Database-for-English-Miller",
            "title": {
                "fragments": [],
                "text": "WordNet: A Lexical Database for English"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "WordNet1 provides a more effective combination of traditional lexicographic information and modern computing, and is an online lexical database designed for use under program control."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34849128"
                        ],
                        "name": "Marcus Rohrbach",
                        "slug": "Marcus-Rohrbach",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Rohrbach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcus Rohrbach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40404576"
                        ],
                        "name": "S. Amin",
                        "slug": "S.-Amin",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Amin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1906895"
                        ],
                        "name": "M. Andriluka",
                        "slug": "M.-Andriluka",
                        "structuredName": {
                            "firstName": "Mykhaylo",
                            "lastName": "Andriluka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Andriluka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "Our hand detector alone significantly improves over the state-of-the-art FPS approach of [18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 88
                            }
                        ],
                        "text": "Hand Detection Evaluation We evaluate our hand detector on the \u201cPose Challenge\u201d dataset [18] that contains 1277 test images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9349950,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "49435aab7cdf259335725acc96691f755e436f55",
            "isKey": false,
            "numCitedBy": 479,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "While activity recognition is a current focus of research the challenging problem of fine-grained activity recognition is largely overlooked. We thus propose a novel database of 65 cooking activities, continuously recorded in a realistic setting. Activities are distinguished by fine-grained body motions that have low inter-class variability and high intra-class variability due to diverse subjects and ingredients. We benchmark two approaches on our dataset, one based on articulated pose tracks and the second using holistic video features. While the holistic approach outperforms the pose-based approach, our evaluation suggests that fine-grained activities are more difficult to detect and the body model can help in those cases. Providing high-resolution videos as well as an intermediate pose representation we hope to foster research in fine-grained activity recognition."
            },
            "slug": "A-database-for-fine-grained-activity-detection-of-Rohrbach-Amin",
            "title": {
                "fragments": [],
                "text": "A database for fine grained activity detection of cooking activities"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A novel database of 65 cooking activities, continuously recorded in a realistic setting, is proposed, suggesting that fine-grained activities are more difficult to detect and the body model can help in those cases."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755162"
                        ],
                        "name": "Philipp Koehn",
                        "slug": "Philipp-Koehn",
                        "structuredName": {
                            "firstName": "Philipp",
                            "lastName": "Koehn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philipp Koehn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152378023"
                        ],
                        "name": "Hieu T. Hoang",
                        "slug": "Hieu-T.-Hoang",
                        "structuredName": {
                            "firstName": "Hieu",
                            "lastName": "Hoang",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hieu T. Hoang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2539211"
                        ],
                        "name": "Alexandra Birch",
                        "slug": "Alexandra-Birch",
                        "structuredName": {
                            "firstName": "Alexandra",
                            "lastName": "Birch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexandra Birch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763608"
                        ],
                        "name": "Chris Callison-Burch",
                        "slug": "Chris-Callison-Burch",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Callison-Burch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Callison-Burch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102811815"
                        ],
                        "name": "Marcello Federico",
                        "slug": "Marcello-Federico",
                        "structuredName": {
                            "firstName": "Marcello",
                            "lastName": "Federico",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcello Federico"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895952"
                        ],
                        "name": "N. Bertoldi",
                        "slug": "N.-Bertoldi",
                        "structuredName": {
                            "firstName": "Nicola",
                            "lastName": "Bertoldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Bertoldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46898156"
                        ],
                        "name": "Brooke Cowan",
                        "slug": "Brooke-Cowan",
                        "structuredName": {
                            "firstName": "Brooke",
                            "lastName": "Cowan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brooke Cowan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2529583"
                        ],
                        "name": "Wade Shen",
                        "slug": "Wade-Shen",
                        "structuredName": {
                            "firstName": "Wade",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wade Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055137469"
                        ],
                        "name": "C. Moran",
                        "slug": "C.-Moran",
                        "structuredName": {
                            "firstName": "Christine",
                            "lastName": "Moran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Moran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1983801"
                        ],
                        "name": "R. Zens",
                        "slug": "R.-Zens",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745899"
                        ],
                        "name": "Chris Dyer",
                        "slug": "Chris-Dyer",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Dyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Dyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143832874"
                        ],
                        "name": "Ondrej Bojar",
                        "slug": "Ondrej-Bojar",
                        "structuredName": {
                            "firstName": "Ondrej",
                            "lastName": "Bojar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ondrej Bojar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057195055"
                        ],
                        "name": "Alexandra Constantin",
                        "slug": "Alexandra-Constantin",
                        "structuredName": {
                            "firstName": "Alexandra",
                            "lastName": "Constantin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexandra Constantin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082901914"
                        ],
                        "name": "Evan Herbst",
                        "slug": "Evan-Herbst",
                        "structuredName": {
                            "firstName": "Evan",
                            "lastName": "Herbst",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Evan Herbst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 46
                            }
                        ],
                        "text": "It uses statistical machine translation (SMT) [10] to translate the SR to a single sentence for a manually segmented video-snippet."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 794019,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4ee2eab4c298c1824a9fb8799ad8eed21be38d21",
            "isKey": false,
            "numCitedBy": 5929,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks."
            },
            "slug": "Moses:-Open-Source-Toolkit-for-Statistical-Machine-Koehn-Hoang",
            "title": {
                "fragments": [],
                "text": "Moses: Open Source Toolkit for Statistical Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "An open-source toolkit for statistical machine translation whose novel contributions are support for linguistically motivated factors, confusion network decoding, and efficient data formats for translation models and language models."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144096985"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Miller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 52886585,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3bc9f8eb5ba303816fd5f642f2e7408f0752d3c4",
            "isKey": false,
            "numCitedBy": 301,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of this project is to provide lexical resources for natural language research. The primary emphases are on the further development and dissemination of the on-line lexical database, WordNet. A secondary goal is to learn how to develop contextual representations for different senses of a polysemous word, where a contextual representation is comprised of topical and local context for each sense."
            },
            "slug": "WordNet:-A-Lexical-Database-for-English-Miller",
            "title": {
                "fragments": [],
                "text": "WordNet: A Lexical Database for English"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "The goal of this project is to provide lexical resources for natural language research and to learn how to develop contextual representations for different senses of a polysemous word."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687524"
                        ],
                        "name": "A. Vedaldi",
                        "slug": "A.-Vedaldi",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Vedaldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vedaldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2487006"
                        ],
                        "name": "B. Fulkerson",
                        "slug": "B.-Fulkerson",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Fulkerson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Fulkerson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1458265,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d720a95e1501922ea17ee31f299f43b2db5e15ef",
            "isKey": false,
            "numCitedBy": 3386,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "VLFeat is an open and portable library of computer vision algorithms. It aims at facilitating fast prototyping and reproducible research for computer vision scientists and students. It includes rigorous implementations of common building blocks such as feature detectors, feature extractors, (hierarchical) k-means clustering, randomized kd-tree matching, and super-pixelization. The source code and interfaces are fully documented. The library integrates directly with MATLAB, a popular language for computer vision research."
            },
            "slug": "Vlfeat:-an-open-and-portable-library-of-computer-Vedaldi-Fulkerson",
            "title": {
                "fragments": [],
                "text": "Vlfeat: an open and portable library of computer vision algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "VLFeat is an open and portable library of computer vision algorithms that includes rigorous implementations of common building blocks such as feature detectors, feature extractors, (hierarchical) k-means clustering, randomized kd-tree matching, and super-pixelization."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Multimedia"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49604675"
                        ],
                        "name": "Philipp Koehn",
                        "slug": "Philipp-Koehn",
                        "structuredName": {
                            "firstName": "Philipp",
                            "lastName": "Koehn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philipp Koehn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102811815"
                        ],
                        "name": "Marcello Federico",
                        "slug": "Marcello-Federico",
                        "structuredName": {
                            "firstName": "Marcello",
                            "lastName": "Federico",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcello Federico"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2529583"
                        ],
                        "name": "Wade Shen",
                        "slug": "Wade-Shen",
                        "structuredName": {
                            "firstName": "Wade",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wade Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895952"
                        ],
                        "name": "N. Bertoldi",
                        "slug": "N.-Bertoldi",
                        "structuredName": {
                            "firstName": "Nicola",
                            "lastName": "Bertoldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Bertoldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763608"
                        ],
                        "name": "Chris Callison-Burch",
                        "slug": "Chris-Callison-Burch",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Callison-Burch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Callison-Burch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143832874"
                        ],
                        "name": "Ondrej Bojar",
                        "slug": "Ondrej-Bojar",
                        "structuredName": {
                            "firstName": "Ondrej",
                            "lastName": "Bojar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ondrej Bojar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46898156"
                        ],
                        "name": "Brooke Cowan",
                        "slug": "Brooke-Cowan",
                        "structuredName": {
                            "firstName": "Brooke",
                            "lastName": "Cowan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brooke Cowan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745899"
                        ],
                        "name": "Chris Dyer",
                        "slug": "Chris-Dyer",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Dyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Dyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152378023"
                        ],
                        "name": "Hieu T. Hoang",
                        "slug": "Hieu-T.-Hoang",
                        "structuredName": {
                            "firstName": "Hieu",
                            "lastName": "Hoang",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hieu T. Hoang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1983801"
                        ],
                        "name": "R. Zens",
                        "slug": "R.-Zens",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057195055"
                        ],
                        "name": "Alexandra Constantin",
                        "slug": "Alexandra-Constantin",
                        "structuredName": {
                            "firstName": "Alexandra",
                            "lastName": "Constantin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexandra Constantin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082901914"
                        ],
                        "name": "Evan Herbst",
                        "slug": "Evan-Herbst",
                        "structuredName": {
                            "firstName": "Evan",
                            "lastName": "Herbst",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Evan Herbst"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055137469"
                        ],
                        "name": "C. Moran",
                        "slug": "C.-Moran",
                        "structuredName": {
                            "firstName": "Christine",
                            "lastName": "Moran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Moran"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 61651780,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "99e8d34817ae10d7304521e89c5fbf908b9d856b",
            "isKey": false,
            "numCitedBy": 543,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Open-Source-Toolkit-for-Statistical-Machine-Models-Koehn-Federico",
            "title": {
                "fragments": [],
                "text": "Open Source Toolkit for Statistical Machine Translation: Factored Translation Models and Lattice Decoding"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 36670487,
            "fieldsOfStudy": [],
            "id": "9885ab16e7b3834563c148d7e14f7465dc6e1918",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "User Modeling and User-Adapted Interaction"
            },
            "venue": {
                "fragments": [],
                "text": "User Modeling and User-Adapted Interaction"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Coherent Multi-Sentence Video Description with Variable Level of Detail"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 57
                            }
                        ],
                        "text": ", sN , st\u3009 using loopy belief propagation implemented in [21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "UGM: Matlab code for undirected graphical models"
            },
            "venue": {
                "fragments": [],
                "text": "di.ens.fr/\u223cmschmidt/Software/UGM.html,"
            },
            "year": 2013
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 18,
            "methodology": 16
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 34,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Coherent-Multi-sentence-Video-Description-with-of-Rohrbach-Rohrbach/889e723cd6d581e120ee6776b231fdf69707ab50?sort=total-citations"
}