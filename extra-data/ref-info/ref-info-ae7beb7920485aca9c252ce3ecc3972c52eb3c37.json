{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3337260"
                        ],
                        "name": "G. Langdon",
                        "slug": "G.-Langdon",
                        "structuredName": {
                            "firstName": "Glen",
                            "lastName": "Langdon",
                            "middleNames": [
                                "G."
                            ],
                            "suffix": "Jr."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Langdon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Indeed, we may regard the binary logarithm, -log P(x I 9), otherwise known in information theory as the self-information and in statistics just as the negative log likelihood, to be the number of bits it takes to redescribe or encode x with an ideal code relative to the assumed statistical model of the data, see Rissanen and Langdon [28]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Rissanen and Langdon [28], the measure -log P(x I 9) for the ideal code length is more appropriate."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 28270470,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8f54bc3a24a1f01808e6e8479a11e5b0244f5523",
            "isKey": false,
            "numCitedBy": 465,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The problems arising in the modeling and coding of strings for compression purposes are discussed. The notion of an information source that simplifies and sharpens the traditional one is axiomatized, and adaptive and nonadaptive models are defined. With a measure of complexity assigned to the models, a fundamental theorem is proved which states that models that use any kind of alphabet extension are inferior to the best models using no alphabet extensions at all. A general class of so-called first-in first-out (FIFO) arithmetic codes is described which require no alphabet extension devices and which therefore can be used in conjunction with the best models. Because the coding parameters are the probabilities that define the model, their design is easy, and the application of the code is straightforward even with adaptively changing source models."
            },
            "slug": "Universal-modeling-and-coding-Rissanen-Langdon",
            "title": {
                "fragments": [],
                "text": "Universal modeling and coding"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A general class of so-called first-in first-out (FIFO) arithmetic codes is described which require no alphabet extension devices and which therefore can be used in conjunction with the best models."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35206065"
                        ],
                        "name": "E. Jaynes",
                        "slug": "E.-Jaynes",
                        "structuredName": {
                            "firstName": "Edwin",
                            "lastName": "Jaynes",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Jaynes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 30406857,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "42f93dad279f0d38594d6269f475ec1e415dbbdf",
            "isKey": false,
            "numCitedBy": 639,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "In decision theory, mathematical analysis shows that once the sampling distribution, loss function, and sample are specified, the only remaining basis for a choice among different admissible decisions lies in the prior probabilities. Therefore, the logical foundations of decision theory cannot be put in fully satisfactory form until the old problem of arbitrariness (sometimes called \"subjectiveness\") in assigning prior probabilities is resolved. The principle of maximum entropy represents one step in this direction. Its use is illustrated, and a correspondence property between maximum-entropy probabilities and frequencies is demonstrated. The consistency of this principle with the principles of conventional \"direct probability\" analysis is illustrated by showing that many known results may be derived by either method. However, an ambiguity remains in setting up a prior on a continuous parameter space because the results lack invariance under a change of parameters; thus a further principle is needed. It is shown that in many problems, including some of the most important in practice, this ambiguity can be removed by applying methods of group theoretical reasoning which have long been used in theoretical physics. By finding the group of transformations on the parameter space which convert the problem into an equivalent one, a basic desideratum of consistency can be stated in the form of functional equations which impose conditions on, and in some cases fully determine, an \"invariant measure\" on the parameter space."
            },
            "slug": "Prior-Probabilities-Jaynes",
            "title": {
                "fragments": [],
                "text": "Prior Probabilities"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that in many problems, including some of the most important in practice, this ambiguity can be removed by applying methods of group theoretical reasoning which have long been used in theoretical physics."
            },
            "venue": {
                "fragments": [],
                "text": "Encyclopedia of Machine Learning"
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727567"
                        ],
                        "name": "R. Solomonoff",
                        "slug": "R.-Solomonoff",
                        "structuredName": {
                            "firstName": "Ray",
                            "lastName": "Solomonoff",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Solomonoff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1898103,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bef2ae523cd4447af687fae13bfbb606e4a4a5ca",
            "isKey": false,
            "numCitedBy": 1585,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Formal-Theory-of-Inductive-Inference.-Part-II-Solomonoff",
            "title": {
                "fragments": [],
                "text": "A Formal Theory of Inductive Inference. Part II"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Control."
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069357329"
                        ],
                        "name": "P. Elias",
                        "slug": "P.-Elias",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Elias",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Elias"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Such codes were called representations of integers by Elias [5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Elias [5] and Chaitin [2], and with a little thought easily appreciated, that if the decoder always reads the code string from left to right, then a both necessary and sufficient condition for the decoder to be able to separate the codeword from whatever binary string follows it, is that the codewords for the integers form a prefix set."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In this section we draw on the work of Elias [5], where several coding systems for integers are described."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Now, by Theorem 4 in Elias [5], the right hand side ratio tends to 1 as N grows."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "(The description as given is not quite complete; for a complete description, see Elias [5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6752631,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ecd86899c752f1e41ec4792cff22a4c7fbfc048",
            "isKey": true,
            "numCitedBy": 1206,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Countable prefix codeword sets are constructed with the universal property that assigning messages in order of decreasing probability to codewords in order of increasing length gives an average code-word length, for any message set with positive entropy, less than a constant times the optimal average codeword length for that source. Some of the sets also have the asymptotically optimal property that the ratio of average codeword length to entropy approaches one uniformly as entropy increases. An application is the construction of a uniformly universal sequence of codes for countable memoryless sources, in which the n th code has a ratio of average codeword length to source rate bounded by a function of n for all sources with positive rate; the bound is less than two for n = 0 and approaches one as n increases."
            },
            "slug": "Universal-codeword-sets-and-representations-of-the-Elias",
            "title": {
                "fragments": [],
                "text": "Universal codeword sets and representations of the integers"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "An application is the construction of a uniformly universal sequence of codes for countable memoryless sources, in which the n th code has a ratio of average codeword length to source rate bounded by a function of n for all sources with positive rate."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102248985"
                        ],
                        "name": "I. J. Goodd",
                        "slug": "I.-J.-Goodd",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Goodd",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. J. Goodd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102716982"
                        ],
                        "name": "R. A. Gaskins",
                        "slug": "R.-A.-Gaskins",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Gaskins",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. A. Gaskins"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121071360,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d4ceb4f1653941aea73ac3b6f6c7972bc48fb332",
            "isKey": false,
            "numCitedBy": 380,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Given a number of observations xlt ...,xN, a nonparametric method is suggested for estimating the entire probability density curve. The method is to subtract a roughness penalty from the log likelihood, where the roughness penalty is a certain functional of the assumed density function / . Those used are linear combinations of J y' dx and J y\" dx, where y = *]f. The method appears to be consistent under wide conditions, although consistent methods can be rough. Numerical examples are given and show that for certain values of the coefficients in this linear expression the density function turns out to be very smooth even when N is small. Multivariate extensions are proposed, including one to distributions having some continuous and some discrete components, but numerical examples of these have not been tried. Some of the techniques are borrowed from quantum mechanics and tensor calculus. 1. INTBODUOTION A fundamental problem in statistics is the estimation of a density function from a sample of observations. An example of a useful but not very familiar application would be for the estimation of probability densities of discriminant functions. This could beused, for example, for choosing between treatments when a discriminant function is available for success and failure in regard to each treatment. Another kind of application is for deciding whether a bump in the apparent density function is genuinely in the population, or more generally for deciding whether one density function is a better estimate than another one and by how much. According to Orear & Cassel (1971), 'bump hunting' is one of the major current activities of experimental physicists, and they requested aid from statisticians. When a bump is not due to random variation it is usually due to a new elementary particle or resonance. The method of estimating a density function to be discussed in this paper arose from a proposal of Good (1971 a, b) and is here modified, developed further, and exemplified numerically and graphically. A multivariate extension is proposed but no multivariate numerical calculations have yet been done."
            },
            "slug": "Nonparametric-roughness-penalties-for-probability-Goodd-Gaskins",
            "title": {
                "fragments": [],
                "text": "Nonparametric roughness penalties for probability densities"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727567"
                        ],
                        "name": "R. Solomonoff",
                        "slug": "R.-Solomonoff",
                        "structuredName": {
                            "firstName": "Ray",
                            "lastName": "Solomonoff",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Solomonoff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "While the models involved in the algorithmic theory of information are non-probabilistic, in the work of Solomonoff [30, 31], a very general apparatus is proposed for the generation of probabilities for all finite strings."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1673415,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0453028e68581624ec68cfec70214231da8dbce7",
            "isKey": false,
            "numCitedBy": 397,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "In 1964 the author proposed as an explication of {\\em a priori} probability the probability measure induced on output strings by a universal Turing machine with unidirectional output tape and a randomly coded unidirectional input tape. Levin has shown that if tilde{P}'_{M}(x) is an unnormalized form of this measure, and P(x) is any computable probability measure on strings, x , then \\tilde{P}'_{M}\\geqCP(x) where C is a constant independent of x . The corresponding result for the normalized form of this measure, P'_{M} , is directly derivable from Willis' probability measures on nonuniversal machines. If the conditional probabilities of P'_{M} are used to approximate those of P , then the expected value of the total squared error in these conditional probabilities is bounded by -(1/2) \\ln C . With this error criterion, and when used as the basis of a universal gambling scheme, P'_{M} is superior to Cover's measure b\\ast . When H\\ast\\equiv -\\log_{2} P'_{M} is used to define the entropy of a rmite sequence, the equation H\\ast(x,y)= H\\ast(x)+H^{\\ast}_{x}(y) holds exactly, in contrast to Chaitin's entropy definition, which has a nonvanishing error term in this equation."
            },
            "slug": "Complexity-based-induction-systems:-Comparisons-and-Solomonoff",
            "title": {
                "fragments": [],
                "text": "Complexity-based induction systems: Comparisons and convergence theorems"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Levin has shown that if tilde{P}'_{M}(x) is an unnormalized form of this measure, and P( x) is any computable probability measure on strings, x, then \\tilde{M}'_M}\\geqCP (x) where C is a constant independent of x ."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1402910087"
                        ],
                        "name": "S. K. Leung-Yan-Cheong",
                        "slug": "S.-K.-Leung-Yan-Cheong",
                        "structuredName": {
                            "firstName": "Sik",
                            "lastName": "Leung-Yan-Cheong",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. K. Leung-Yan-Cheong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Define for any positive number x, x> 1, see Leung-Yan-Cheong and Cover [22], who used the construct to obtain lower bounds for the mean code length, log*(x) = log x + log log x + *."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We calculate the sum c - E 2 -l0g'n by sharpening the arguments given in [22]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7970875,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c6aece82c4c2076eb22db65a16e6c376b94df37b",
            "isKey": false,
            "numCitedBy": 116,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "It is known that the expected codeword length L_{UD} of the best uniquely decodable (UD) code satisfies H(X)\\leqL_{UD} . Let X be a random variable which can take on n values. Then it is shown that the average codeword length L_{1:1} for the best one-to-one (not necessarily uniquely decodable) code for X is shorter than the average codeword length L_{UD} for the best uniquely decodable code by no more than (\\log_{2} \\log_{2} n)+ 3 . Let Y be a random variable taking on a finite or countable number of values and having entropy H . Then it is proved that L_{1:1}\\geq H-\\log_{2}(H + 1)-\\log_{2}\\log_{2}(H + 1 )-\\cdots -6 . Some relations are established among the Kolmogorov, Chaitin, and extension complexities. Finally it is shown that, for all computable probability distributions, the universal prefix codes associated with the conditional Chaitin complexity have expected codeword length within a constant of the Shannon entropy."
            },
            "slug": "Some-equivalences-between-Shannon-entropy-and-Leung-Yan-Cheong-Cover",
            "title": {
                "fragments": [],
                "text": "Some equivalences between Shannon entropy and Kolmogorov complexity"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that, for all computable probability distributions, the universal prefix codes associated with the conditional Chaitin complexity have expected codeword length within a constant of the Shannon entropy."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8635127"
                        ],
                        "name": "C. S. Wallace",
                        "slug": "C.-S.-Wallace",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Wallace",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. S. Wallace"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4110643"
                        ],
                        "name": "D. Boulton",
                        "slug": "D.-Boulton",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Boulton",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Boulton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In [25], following a suggestion by Wallace and Boulton [32], we solved the optimum precision problem in a manner which initially appeared to be adequate but which now can"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61324799,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a1d76ee9b6fb4d441e31abcd4dadc4e44c576017",
            "isKey": false,
            "numCitedBy": 1100,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "1. The class to which each thing belongs. 2. The average properties of each class. 3. The deviations of each thing from the average properties of its parent class. If the things are found to be concentrated in a small area of the region of each class in the measurement space then the deviations will be small, and with reference to the average class properties most of the information about a thing is given by naming the class to which it belongs. In this case the information may be recorded much more briefly than if a classification had not been used. We suggest that the best classification is that which results in the briefest recording of all the attribute information. In this context, we will regard the measurements of each thing as being a message about that thing. Shannon (1948) showed that where messages may be regarded as each nominating the occurrence of a particular event among a universe of possible events, the information needed to record a series of such messages is minimised if the messages are encoded so that the length of each message is proportional to minus the logarithm of the relative frequency of occurrence of the event which it nominates. The information required is greatest when all frequencies are equal. The messages here nominate the positions in measurement space of the 5 1 points representing the attributes of the things. If the expected density of points in the measurement space is everywhere uniform, the positions of the points cannot be encoded more briefly than by a simple list of the measured values. However, if the expected density is markedly non-uniform, application"
            },
            "slug": "An-Information-Measure-for-Classification-Wallace-Boulton",
            "title": {
                "fragments": [],
                "text": "An Information Measure for Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is suggested that the best classification is that which results in the briefest recording of all the attribute information, and the measurements of each thing are regarded as being a message about that thing."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. J."
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35206065"
                        ],
                        "name": "E. Jaynes",
                        "slug": "E.-Jaynes",
                        "structuredName": {
                            "firstName": "Edwin",
                            "lastName": "Jaynes",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Jaynes"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In the final Section 5, we show that Jaynes's principle of maximum entropy [14, 15, 16, 17] may be viewed as a particular instance of ours."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This is of some significance because there are a number of important applications where the ML principle fails but where the maximum entropy formalism has been highly successful [16, 17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 42335268,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8c32c4eda83a9e464ab01ee74f243127c38d8662",
            "isKey": false,
            "numCitedBy": 1521,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We discuss the relations between maximum-entropy (MAXENT) and other methods of spectral analysis such as the Schuster, Blackman-Tukey, maximum-likelihood, Bayesian, and Autoregressive (AR, ARMA, or ARIMA) models, emphasizing that they are not in conflict, but rather are appropriate in different problems. We conclude that: 1) \"Orthodox\" sampling theory methods are useful in problems where we have a known model (sampling distribution) for the properties of the noise, but no appreciable prior information about the quantities being estimated. 2) MAXENT is optimal in problems where we have prior information about multiplicities, but no noise. 3) The full Bayesian solution includes both of these as special cases and is needed in problems where we have both prior information and noise. 4) AR models are in one sense a special case of MAXENT, but in another sense they are ubiquitous in all spectral analysis problems with discrete time series. 5) Empirical methods such as Blackman-Tukey, which do not invoke even a likelihood function, are useful in the preliminary, exploratory phase of a problem where our knowledge is sufficient to permit intuitive judgments about how to organize a calculation (smoothing, decimation, windows, prewhitening, padding with zeroes, etc.) but insufficient to set up a quantitative model which would do the proper things for us automatically and optimally."
            },
            "slug": "On-the-rationale-of-maximum-entropy-methods-Jaynes",
            "title": {
                "fragments": [],
                "text": "On the rationale of maximum-entropy methods"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "The relations between maximum-entropy (MAXENT) and other methods of spectral analysis such as the Schuster, Blackman-Tukey, maximum-likelihood, Bayesian, and Autoregressive models are discussed, emphasizing that they are not in conflict, but rather are appropriate in different problems."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7544534"
                        ],
                        "name": "H. Jeffreys",
                        "slug": "H.-Jeffreys",
                        "structuredName": {
                            "firstName": "Harold",
                            "lastName": "Jeffreys",
                            "middleNames": [
                                "Sir"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Jeffreys"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123496360,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "63d459348fb37e972cb3c98e4563ed89c1d901b2",
            "isKey": false,
            "numCitedBy": 1427,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "DR. KEYNES'S book is a searching analysis of the fundamental principles of the theory of probability and of the particular judgments involved in its application to concrete problems. He adopts the view that knowledge may be relevant to our rational belief of a proposition without amounting to complete proof or disproof of it, and treats the probability as a measure of this relevance. NO.Otherwise he does not attempt to define \u201cprobability,\u201d regarding it as a concept intelligible without further definition. In this respect, as in several others, he is in agreement with the views expressed by Dr. Wrinch and the present reviewer (Philosophical Magazine, vol. 38, 1919, pp. 715-31), and some comparison of the two presentations may not be out of place.A Treatise on ProbabilityBy J. M. Keynes. Pp. xi + 466. (London: Macmillan and Co., Ltd., 1921.) 18s. net."
            },
            "slug": "The-Theory-of-Probability-Jeffreys",
            "title": {
                "fragments": [],
                "text": "The Theory of Probability"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1896
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2550392"
                        ],
                        "name": "B. Efron",
                        "slug": "B.-Efron",
                        "structuredName": {
                            "firstName": "Bradley",
                            "lastName": "Efron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Efron"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "By and large, we will end up in a similar code length for 9 no matter which norm we use, but we will et a articularly satisfying invariance property if we pick the natural norm, 910 IIM(O = (9, M(0)0), which also has to do with statistical \"curvature,\" Efron [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120162246,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "221d44cd364bfa4f6d789eba44b55243f70fdd4b",
            "isKey": false,
            "numCitedBy": 240,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "There are two important spaces connected with every multivariate exponential family, the natural parameter space and the expectation parameter space. We describe some geometric results relating the two. (In the simplest case, that of a normal translation family, the two spaces coincide and the geometry is the familiar Euclidean one.) Maximum likelihood estimation, within one-parameter curved subfamilies of the multivariate family, has two simple and useful geometric interpretations. The geometry also relates to the Fisherian question: to what extent can the Fisher information be replaced by $-\\partial^2/\\partial\\theta^2\\lbrack\\log f_\\theta(x)\\rbrack\\mid_{\\theta=\\hat{\\theta}}$ in the variance bound for $\\hat{\\theta}$, the maximum likelihood estimator?"
            },
            "slug": "THE-GEOMETRY-OF-EXPONENTIAL-FAMILIES-Efron",
            "title": {
                "fragments": [],
                "text": "THE GEOMETRY OF EXPONENTIAL FAMILIES"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 30140639,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d382b9c11e5c6a8e173fbeb442545e3be8d3e3a5",
            "isKey": false,
            "numCitedBy": 6261,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Modeling-By-Shortest-Data-Description*-Rissanen",
            "title": {
                "fragments": [],
                "text": "Modeling By Shortest Data Description*"
            },
            "venue": {
                "fragments": [],
                "text": "Autom."
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49413185"
                        ],
                        "name": "E. Hannan",
                        "slug": "E.-Hannan",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "Hannan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Hannan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The criterion has been shown to lead to strongly consistent estimates of the parameters, including their number, in AR-processes, Hannan and Quinn [12] and Rissanen [26], and in ARMA-processes, Hannan [11] and Hannan and Rissanen [13]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 121631656,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "fa13d6a3f7267b635dd35f3eab3d5740a42eccfb",
            "isKey": false,
            "numCitedBy": 498,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": ">2LK(j)E(n -j), Y,0?K(j)zi = k(z) = gh, where the K(j) decrease to zero at a geometric rate, and that the E(n) are the linear innovations. It has been assumed that &{x(n)} = 0 but this is immaterial to all of the results presented below, which would continue to hold if the various statistics involved were computed from (sample) mean corrected quantities. Our purpose is to study the estimation of the true order, which shall be called po, qo. A zero subscript shall be used throughout for true quantities, e.g., aofj), g0 and ko. This estimation problem is considerably more complex than that for the case q = 0, which has been studied, for example in [7], [14]. This is because when p > po, q > qo the estimates o(j),/3(j), obtained by maximising the Gaussian likelihood, do not converge in any reasonable sense because the likelihood is constant along the \"line\" where k = ko, so that as N (the sample size) increases the sample point will search up and down that line. Though the estimation method used will be based on the maximisation of the Gaussian likelihood the assumption of normality will not be made. It will always be assumed that"
            },
            "slug": "The-Estimation-of-the-Order-of-an-ARMA-Process-Hannan",
            "title": {
                "fragments": [],
                "text": "The Estimation of the Order of an ARMA Process"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145179124"
                        ],
                        "name": "I. Good",
                        "slug": "I.-Good",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Good",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Good"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Further, we mention the works of Good [8, 9], where information-like measures for \"explicativity\" and weight of evidence are studied."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Although the proposed measures do not generally admit a description length interpretation, no more than the penalty term added to the likelihood in Good and Gaskins [10], the deep and philosophical observations in [8] and [9] do have some relevance to our basic notions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121804290,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "2d9d50d8b1371adac5aeeb25297ea080efc8d50c",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "My aim is to discuss (degrees of) corroboration, explanatory power, and complexity, and the relationships between them, and to suggest a new form of Ockham's razor which is applicable in particular to practical statistical problems. It will be found necessary to distinguish between two meanings of 'explanatory power' but nearly unique explicata will be suggested in terms of probability. These explicata will be derived from reasonable desiderata or axioms. Corroboration, explanatory power, and complexity can be expressed in terms of one another, and probability can be defined in terms of them, but I believe that the best practical method for estimating any of these things is to make judgments about all of them, and perhaps other things as well, and then to make deductions and modify the judgments in the light of inconsistencies that emerge. Among the conclusions are (a) (degree of) corroboration can be identified with 'weight of evidence' (in a sense not to be confused with the sense in which it was used, for example, by Popper, 1959), where the identification preserves all ordering relations (inequalities) between degrees of corroboration; (b) the explanatory power in the 'weak sense' of a hypothesis H, for explaining evidence E, namely the explanatory power that is unaffected by cluttering up H with irrelevancies, can be identified with the amount of mutual information between H and E; (c) the explanatory power in the strong sense, that is affected by the cluttering mentioned, can be obtained by subtracting from the weak explanatory power a term proportional to the information in H; (d) the complexity of a proposition can be identified with the amount of information in it, that is, minus the logarithm of its probability; (e) a sharpened form of 'Ockham's razor' is proposed which is that if our primary purpose is explanation we should select the hypothesis (among those we know) which has the maximum strong explanatory power; (f) this improvement of Ockham's razor can be applied to the estimation of statistical parameters. It will typically give rise to interval estimates, but it would give point estimates if weak explanatory power Received 27.xii.67"
            },
            "slug": "Corroboration,-Explanation,-Evolving-Probability,-a-Good",
            "title": {
                "fragments": [],
                "text": "Corroboration, Explanation, Evolving Probability, Simplicity and a Sharpened Razor"
            },
            "venue": {
                "fragments": [],
                "text": "The British Journal for the Philosophy of Science"
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121752527,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "92c0dcf47bc774aef0a55cdd27b032db977d24cf",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "A theorem is proved which demonstrates that an earlier derived minimum description length estimation criterion is capable of distinguishing between structures in linear models for vector processes. A fairly simple algorithm is described for the estimation of the best model, including its structure and the number of its parameters."
            },
            "slug": "Estimation-of-structure-by-minimum-description-Rissanen",
            "title": {
                "fragments": [],
                "text": "Estimation of structure by minimum description length"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734614"
                        ],
                        "name": "A. Wyner",
                        "slug": "A.-Wyner",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Wyner",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Wyner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1065570,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "90baca80f2d3b8c06beafec493f186d29653abde",
            "isKey": false,
            "numCitedBy": 82,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-Upper-Bound-on-the-Entropy-Series-Wyner",
            "title": {
                "fragments": [],
                "text": "An Upper Bound on the Entropy Series"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Control."
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2748598"
                        ],
                        "name": "S. Kullback",
                        "slug": "S.-Kullback",
                        "structuredName": {
                            "firstName": "Solomon",
                            "lastName": "Kullback",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kullback"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We wish to emphasize that our information theoretic view of estimation differs in an essential manner from the earlier approaches that depend on the mean of the self information, the entropy, Jaynes [14], or on the so-called mutual information, Kullback [21]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 86412308,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "51739712c9b795f9533131122698cd5d01699f9d",
            "isKey": false,
            "numCitedBy": 82,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "information theory and statistics. Book lovers, when you need a new book to read, find the book here. Never worry not to find what you need. Is the information theory and statistics your needed book now? That's true; you are really a good reader. This is a perfect book that comes from great author to share with you. The book offers the best experience and lesson to take, not only take, but also learn."
            },
            "slug": "Information-Theory-and-Statistics-Kullback",
            "title": {
                "fragments": [],
                "text": "Information Theory and Statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1959
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145179124"
                        ],
                        "name": "I. Good",
                        "slug": "I.-Good",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Good",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Good"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, Good in [9] maintains that the number of dimensionless parameters in a \"law\" is not an adequate measure of its complexity, because a parameter such as 5."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Further, we mention the works of Good [8, 9], where information-like measures for \"explicativity\" and weight of evidence are studied."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Although the proposed measures do not generally admit a description length interpretation, no more than the penalty term added to the likelihood in Good and Gaskins [10], the deep and philosophical observations in [8] and [9] do have some relevance to our basic notions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61860305,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "b3e953505fe68b4dec2044c6d7e8b16f077160b0",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "By explicativity is meant the extent to which one proposition or event explains why another one should be believed. Detailed mathematical and philosophical arguments are given for accepting a specific formula for explicativity that was previously proposed by the author with much less complete discussion. Some implications of the formula are discussed, and it is applied to several problems of statistical estimation and significance testing with intuitively appealing results. The work is intended to be a contribution to both philosophy and statistics."
            },
            "slug": "Explicativity:-a-mathematical-theory-of-explanation-Good",
            "title": {
                "fragments": [],
                "text": "Explicativity: a mathematical theory of explanation with statistical applications"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "Detailed mathematical and philosophical arguments are given for accepting a specific formula for explicativity that was previously proposed by the author with much less complete discussion and applied to several problems of statistical estimation and significance testing with intuitively appealing results."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Royal Society of London. A. Mathematical and Physical Sciences"
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49413185"
                        ],
                        "name": "E. Hannan",
                        "slug": "E.-Hannan",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "Hannan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Hannan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1909917"
                        ],
                        "name": "B. Quinn",
                        "slug": "B.-Quinn",
                        "structuredName": {
                            "firstName": "Barry",
                            "lastName": "Quinn",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Quinn"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The criterion has been shown to lead to strongly consistent estimates of the parameters, including their number, in AR-processes, Hannan and Quinn [12] and Rissanen [26], and in ARMA-processes, Hannan [11] and Hannan and Rissanen [13]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 117985009,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "a2c7e73413f2426446de4b4c00b03d1713fca4f6",
            "isKey": false,
            "numCitedBy": 2557,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY It is shown that a strongly consistent estimation procedure for the order of an autoregression can be based on the law of the iterated logarithm for the partial autocorrelations. As compared to other strongly consistent procedures this procedure will underestimate the order to a lesser degree."
            },
            "slug": "The-determination-of-the-order-of-an-autoregression-Hannan-Quinn",
            "title": {
                "fragments": [],
                "text": "The determination of the order of an autoregression"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49353544"
                        ],
                        "name": "R. Fisher",
                        "slug": "R.-Fisher",
                        "structuredName": {
                            "firstName": "Rory",
                            "lastName": "Fisher",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fisher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "principle of the maximum likelihood (ML) due to Gauss [7] and Fisher [6] prescribes as the estimate such a parameter value 9 which makes the probability P(x I 0) of the observed sequence maximum."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15354499,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "bd2e048c676ad778351bd7d7660240a978422117",
            "isKey": false,
            "numCitedBy": 3233,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Centre of Location. That abscissa of a frequency curve for which the sampling errors of optimum location are uncorrelated with those of optimum scaling. (9.)"
            },
            "slug": "On-the-Mathematical-Foundations-of-Theoretical-Fisher",
            "title": {
                "fragments": [],
                "text": "On the Mathematical Foundations of Theoretical Statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1922
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2135764"
                        ],
                        "name": "A. Kolmogorov",
                        "slug": "A.-Kolmogorov",
                        "structuredName": {
                            "firstName": "Andrei",
                            "lastName": "Kolmogorov",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kolmogorov"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 119745517,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "83077865493592cd8ebc5c9a8b900521428491ad",
            "isKey": false,
            "numCitedBy": 2634,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Three-approaches-to-the-quantitative-definition-of-Kolmogorov",
            "title": {
                "fragments": [],
                "text": "Three approaches to the quantitative definition of information"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33447835"
                        ],
                        "name": "J. Kemeny",
                        "slug": "J.-Kemeny",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Kemeny",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kemeny"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Kemeny [19], which in spirit is perhaps the closest prior publication to ours."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 170623449,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9f15984b64e315dc500f96a9bb71ac3e9c7c2fc0",
            "isKey": false,
            "numCitedBy": 86,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Use-of-Simplicity-in-Induction-Kemeny",
            "title": {
                "fragments": [],
                "text": "The Use of Simplicity in Induction"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1953
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2737945"
                        ],
                        "name": "H. Akaike",
                        "slug": "H.-Akaike",
                        "structuredName": {
                            "firstName": "Hirotugu",
                            "lastName": "Akaike",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Akaike"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The first two terms define essentially the criteria of Schwarz [29] and Akaike [1] originally derived only for the Koopman-Darmois family of distributions, which now get extended to any smooth family of distributions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 117470511,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb5e8213a462581cb43286f5c5307fa5398576ea",
            "isKey": false,
            "numCitedBy": 693,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-entropy-maximization-principle-Akaike",
            "title": {
                "fragments": [],
                "text": "On entropy maximization principle"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We wish to emphasize that our information theoretic view of estimation differs in an essential manner from the earlier approaches that depend on the mean of the self information, the entropy, Jaynes [14], or on the so-called mutual information, Kullback [21]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 125523249,
            "fieldsOfStudy": [],
            "id": "11fbf06e4c1c4eddc91a68e434433a4fc5f7cfc4",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information Theory and Statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1959
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052516042"
                        ],
                        "name": "G. Schwarz",
                        "slug": "G.-Schwarz",
                        "structuredName": {
                            "firstName": "Gideon",
                            "lastName": "Schwarz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Schwarz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The first two terms define essentially the criteria of Schwarz [29] and Akaike [1] originally derived only for the Koopman-Darmois family of distributions, which now get extended to any smooth family of distributions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123722079,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "37e44d1de8003d8394d158ec6afd1ff0e87e595b",
            "isKey": false,
            "numCitedBy": 39572,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Estimating-the-Dimension-of-a-Model-Schwarz",
            "title": {
                "fragments": [],
                "text": "Estimating the Dimension of a Model"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756533"
                        ],
                        "name": "G. Chaitin",
                        "slug": "G.-Chaitin",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Chaitin",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Chaitin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The outlined minimum description length principle to do estimation has its roots in the algorithmic notion of information due to Solomonoff [30] and Kolmogorov [20]; see also Chaitin [2] and Levin [24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Elias [5] and Chaitin [2], and with a little thought easily appreciated, that if the decoder always reads the code string from left to right, then a both necessary and sufficient condition for the decoder to be able to separate the codeword from whatever binary string follows it, is that the codewords for the integers form a prefix set."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14133389,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "294ee566e35418f9257bb3477dd1502f8751f165",
            "isKey": false,
            "numCitedBy": 921,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "A new definition of program-size complexity is made. H(A,B/C,D) is defined to be the size in bits of the shortest self-delimiting program for calculating strings A and B if one is given a minimal-size self-delimiting program for calculating strings C and D. This differs from previous definitions: (1) programs are required to be self-delimiting, i.e. no program is a prefix of another, and (2) instead of being given C and D directly, one is given a program for calculating them that is minimal in size. Unlike previous definitions, this one has precisely the formal properties of the entropy concept of information theory. For example, H(A,B) = H(A) + H(B/A) -~ 0(1). Also, if a program of length k is assigned measure 2 -k, then H(A) = -log2 (the probability that the standard universal computer will calculate A) -{- 0(1)."
            },
            "slug": "A-Theory-of-Program-Size-Formally-Identical-to-Chaitin",
            "title": {
                "fragments": [],
                "text": "A Theory of Program Size Formally Identical to Information Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A new definition of program-size complexity is made, which has precisely the formal properties of the entropy concept of information theory."
            },
            "venue": {
                "fragments": [],
                "text": "JACM"
            },
            "year": 1975
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 11,
            "methodology": 7,
            "result": 4
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 26,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/A-UNIVERSAL-PRIOR-FOR-INTEGERS-AND-ESTIMATION-BY-Rissanen/ae7beb7920485aca9c252ce3ecc3972c52eb3c37?sort=total-citations"
}