{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781874"
                        ],
                        "name": "E. Osuna",
                        "slug": "E.-Osuna",
                        "structuredName": {
                            "firstName": "Edgar",
                            "lastName": "Osuna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Osuna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052797964"
                        ],
                        "name": "R. Freund",
                        "slug": "R.-Freund",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Freund",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 103
                            }
                        ],
                        "text": "For problems with very large numbers of SVs, a modi edtraining algorithm has recently been proposed by Osuna, Freund, and Girosi (1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 269,
                                "start": 238
                            }
                        ],
                        "text": "More invariances (for example, for the pattern recognition case, smallrotations, or varying ink thickness) could be incorporated, possibly combined withtechniques for dealing with optimization problems involving very large numbers ofSVs (Osuna, Freund, and Girosi, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5667586,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a61a3bf41fc770186a58fa34466af337e997ef6",
            "isKey": false,
            "numCitedBy": 1235,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the problem of training a support vector machine (SVM) on a very large database in the case in which the number of support vectors is also very large. Training a SVM is equivalent to solving a linearly constrained quadratic programming (QP) problem in a number of variables equal to the number of data points. This optimization problem is known to be challenging when the number of data points exceeds few thousands. In previous work done by us as well as by other researchers, the strategy used to solve the large scale QP problem takes advantage of the fact that the expected number of support vectors is small (<3,000). Therefore, the existing algorithms cannot deal with more than a few thousand support vectors. In this paper we present a decomposition algorithm that is guaranteed to solve the QP problem and that does not make assumptions on the expected number of support vectors. In order to present the feasibility of our approach we consider a foreign exchange rate time series database with 110,000 data points that generates 100,000 support vectors."
            },
            "slug": "An-improved-training-algorithm-for-support-vector-Osuna-Freund",
            "title": {
                "fragments": [],
                "text": "An improved training algorithm for support vector machines"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper presents a decomposition algorithm that is guaranteed to solve the QP problem and that does not make assumptions on the expected number of support vectors."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks for Signal Processing VII. Proceedings of the 1997 IEEE Signal Processing Society Workshop"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 55
                            }
                        ],
                        "text": "This canbe carried out by the reduced set technique of Burges (1996) (cf. Appendix D.1.1),proposed in the context of Support Vector machines."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 186
                            }
                        ],
                        "text": "To this end, we have to minimize = k 0k2: (D.3)The crucial point is that even if is not given explicitely, can be computed (andminimized) in terms of kernels, using ( (x) (y)) = k(x;y) (Burges, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 76
                            }
                        ],
                        "text": "However, the latter problem can be solved with the Reduced Set (RS) method (Burges,1996, see Appendix D.1.1), which reduces the complexity of the decision functionrepresentation by approximating it in terms of fewer vectors."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 69
                            }
                        ],
                        "text": "To compensate for the second point,we used the reduced set method of Burges (1996) for increasing speed."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 52810328,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1061ff8a216a8d00f5f189d7ea593c6f0703b771",
            "isKey": true,
            "numCitedBy": 511,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A Support Vector Machine SVM is a uni versal learning machine whose decision sur face is parameterized by a set of support vec tors and by a set of corresponding weights An SVM is also characterized by a kernel function Choice of the kernel determines whether the resulting SVM is a polynomial classi er a two layer neural network a ra dial basis function machine or some other learning machine SVMs are currently considerably slower in test phase than other approaches with sim ilar generalization performance To address this we present a general method to signif icantly decrease the complexity of the deci sion rule obtained using an SVM The pro posed method computes an approximation to the decision rule in terms of a reduced set of vectors These reduced set vectors are not support vectors and can in some cases be computed analytically We give ex perimental results for three pattern recogni tion problems The results show that the method can decrease the computational com plexity of the decision rule by a factor of ten with no loss in generalization perfor mance making the SVM test speed com petitive with that of other methods Fur ther the method allows the generalization performance complexity trade o to be di rectly controlled The proposed method is not speci c to pattern recognition and can be applied to any problem where the Sup port Vector algorithm is used for example regression INTRODUCTION SUPPORT VECTOR MACHINES Consider a two class classi er for which the decision rule takes the form"
            },
            "slug": "Simplified-Support-Vector-Decision-Rules-Burges",
            "title": {
                "fragments": [],
                "text": "Simplified Support Vector Decision Rules"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The results show that the method can decrease the computational complexity of the decision rule by a factor of ten with no loss in generalization perfor mance making the SVM test speed com petitive with that of other methods."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38817267"
                        ],
                        "name": "K. Sung",
                        "slug": "K.-Sung",
                        "structuredName": {
                            "firstName": "Kah",
                            "lastName": "Sung",
                            "middleNames": [
                                "Kay"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1900499,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4a422669ec9b6a60b05d2d2595314008a5fb419",
            "isKey": false,
            "numCitedBy": 1314,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "The support vector (SV) machine is a novel type of learning machine, based on statistical learning theory, which contains polynomial classifiers, neural networks, and radial basis function (RBF) networks as special cases. In the RBF case, the SV algorithm automatically determines centers, weights, and threshold that minimize an upper bound on the expected test error. The present study is devoted to an experimental comparison of these machines with a classical approach, where the centers are determined by X-means clustering, and the weights are computed using error backpropagation. We consider three machines, namely, a classical RBF machine, an SV machine with Gaussian kernel, and a hybrid system with the centers determined by the SV method and the weights trained by error backpropagation. Our results show that on the United States postal service database of handwritten digits, the SV machine achieves the highest recognition accuracy, followed by the hybrid system. The SV approach is thus not only theoretically well-founded but also superior in a practical application."
            },
            "slug": "Comparing-support-vector-machines-with-Gaussian-to-Sch\u00f6lkopf-Sung",
            "title": {
                "fragments": [],
                "text": "Comparing support vector machines with Gaussian kernels to radial basis function classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The results show that on the United States postal service database of handwritten digits, the SV machine achieves the highest recognition accuracy, followed by the hybrid system, and the SV approach is thus not only theoretically well-founded but also superior in a practical application."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703769"
                        ],
                        "name": "J. Karhunen",
                        "slug": "J.-Karhunen",
                        "structuredName": {
                            "firstName": "Juha",
                            "lastName": "Karhunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Karhunen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768650"
                        ],
                        "name": "J. Joutsensalo",
                        "slug": "J.-Joutsensalo",
                        "structuredName": {
                            "firstName": "Jyrki",
                            "lastName": "Joutsensalo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Joutsensalo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1578694,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "993ebaefaa0bc8cc201d9e2f5cfef346cb8881b9",
            "isKey": false,
            "numCitedBy": 299,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Generalizations-of-principal-component-analysis,-Karhunen-Joutsensalo",
            "title": {
                "fragments": [],
                "text": "Generalizations of principal component analysis, optimization problems, and neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 99
                            }
                        ],
                        "text": "The original treatments are due to Vapnik and Chervonenkis(1974), Boser, Guyon, and Vapnik (1992), Guyon, Boser, and Vapnik (1993), Cortesand Vapnik (1995), and Vapnik (1995b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12154028,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7020c2455b74deda5af696248cc41c53e32c00e2",
            "isKey": false,
            "numCitedBy": 229,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Large VC-dimension classifiers can learn difficult tasks, but are usually impractical because they generalize well only if they are trained with huge quantities of data. In this paper we show that even high-order polynomial classifiers in high dimensional spaces can be trained with a small amount of training data and yet generalize better than classifiers with a smaller VC-dimension. This is achieved with a maximum margin algorithm (the Generalized Portrait). The technique is applicable to a wide variety of classifiers, including Perceptrons, polynomial classifiers (sigma-pi unit networks) and Radial Basis Functions. The effective number of parameters is adjusted automatically by the training algorithm to match the complexity of the problem. It is shown to equal the number of those training patterns which are closest patterns to the decision boundary (supporting patterns). Bounds on the generalization error and the speed of convergence of the algorithm are given. Experimental results on handwritten digit recognition demonstrate good generalization compared to other algorithms."
            },
            "slug": "Automatic-Capacity-Tuning-of-Very-Large-Classifiers-Guyon-Boser",
            "title": {
                "fragments": [],
                "text": "Automatic Capacity Tuning of Very Large VC-Dimension Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that even high-order polynomial classifiers in high dimensional spaces can be trained with a small amount of training data and yet generalize better than classifiers with a smaller VC-dimension."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 149
                            }
                        ],
                        "text": "\u2026select values for free parametersin the SV algorithm, as for instance the degree of the polynomial kernel which willperform best on a test set (Sch olkopf, Burges, and Vapnik, 1995; Blanz, Sch olkopf,B ultho , Burges, Vapnik, and Vetter, 1996; Sch olkopf, Sung, Burges, Girosi, Niyogi,Poggio, and\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 40
                            }
                        ],
                        "text": "Sections 2.4 and 2.6.1 are based on Sch olkopf, Burges, and Vapnik(1995), AIII Press."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 165
                            }
                        ],
                        "text": "In the following, we go one step further and use the bound (1.5) also topredict the kernel degree which yields the best generalization for polynomial classi ers(Sch olkopf, Burges, and Vapnik, 1995)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6636078,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7ec8029e5855b6efbac161488a2e68f83298091c",
            "isKey": false,
            "numCitedBy": 650,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We report a novel possibility for extracting a small subset of a data base which contains all the information necessary to solve a given classification task: using the Support Vector Algorithm to train three different types of handwritten digit classifiers, we observed that these types of classifiers construct their decision surface from strongly overlapping small (\u2248 4%) subsets of the data base. This finding opens up the possibility of compressing data bases significantly by disposing of the data which is not important for the solution of a given task. \n \nIn addition, we show that the theory allows us to predict the classifier that will have the best generalization ability, based solely on performance on the training set and characteristics of the learning machines. This finding is important for cases where the amount of available data is limited."
            },
            "slug": "Extracting-Support-Data-for-a-Given-Task-Sch\u00f6lkopf-Burges",
            "title": {
                "fragments": [],
                "text": "Extracting Support Data for a Given Task"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is observed that three different types of handwritten digit classifiers construct their decision surface from strongly overlapping small subsets of the data base, which opens up the possibility of compressing data bases significantly by disposing of theData which is not important for the solution of a given task."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 11652139,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0366ce5be03f003f8b28078f8e154a79baa80987",
            "isKey": false,
            "numCitedBy": 322,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract. We present a kernel-based framework for pattern recognition, regression estimation, function approximation, and multiple operator inversion. Adopting a regularization-theoretic framework, the above are formulated as constrained optimization problems. Previous approaches such as ridge regression, support vector methods, and regularization networks are included as special cases. We show connections between the cost function and some properties up to now believed to apply to support vector machines only. For appropriately chosen cost functions, the optimal solution of all the problems described above can be found by solving a simple quadratic programming problem."
            },
            "slug": "On-a-Kernel-Based-Method-for-Pattern-Recognition,-Smola-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "On a Kernel-Based Method for Pattern Recognition, Regression, Approximation, and Operator Inversion"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A kernel-based framework for pattern recognition, regression estimation, function approximation, and multiple operator inversion is presented, adopting a regularization-theoretic framework."
            },
            "venue": {
                "fragments": [],
                "text": "Algorithmica"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 141
                            }
                        ],
                        "text": "\u2026as in Proposition 1.3.2), one can construct a mapping into a space where kacts as a dot product, ( (x) (y)) = k(x;y): (1.27)Besides (1.17), Boser, Guyon, and Vapnik (1992) and Vapnik (1995b) suggest theusage of Gaussian radial basis function kernels (Aizerman, Braverman, and\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 69
                            }
                        ],
                        "text": "(q 1 1(x);q 2 2(x); : : :): (1.26)We thus have the following result (Boser, Guyon, and Vapnik, 1992):5Proposition 1.3.3 If k is a continuous kernel of a positive integral operator (condi-tions as in Proposition 1.3.2), one can construct a mapping into a space where kacts as a dot product, ( (x)\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 102
                            }
                        ],
                        "text": "1.3), it givesrise to a number of di erent types of pattern classi ers (Vapnik and Chervonenkis,1974; Boser, Guyon, and Vapnik, 1992; Cortes and Vapnik, 1995; Vapnik, 1995b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 66
                            }
                        ],
                        "text": "The original treatments are due to Vapnik and Chervonenkis(1974), Boser, Guyon, and Vapnik (1992), Guyon, Boser, and Vapnik (1993), Cortesand Vapnik (1995), and Vapnik (1995b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 24
                            }
                        ],
                        "text": "This method was used by Boser, Guyon, and Vapnik (1992) to extend\n26 CHAPTER 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 283,
                                "start": 252
                            }
                        ],
                        "text": "\u2026it mapsinto the space of all monomials up to degree d, de ning (Vapnik, 1995b)k(x;y) = (x y+ 1)d: (1.22) 1.3.3 Feature Spaces Induced by Mercer KernelsThe question which function k does correspond to a dot product in some space F hasbeen discussed by Boser, Guyon, and Vapnik (1992); Vapnik (1995b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207165665,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2599131a4bc2fa957338732a37c744cfe3e17b24",
            "isKey": true,
            "numCitedBy": 10839,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms."
            },
            "slug": "A-training-algorithm-for-optimal-margin-classifiers-Boser-Guyon",
            "title": {
                "fragments": [],
                "text": "A training algorithm for optimal margin classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented, applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '92"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152597562"
                        ],
                        "name": "Gunnar R\u00e4tsch",
                        "slug": "Gunnar-R\u00e4tsch",
                        "structuredName": {
                            "firstName": "Gunnar",
                            "lastName": "R\u00e4tsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gunnar R\u00e4tsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2633352"
                        ],
                        "name": "J. Kohlmorgen",
                        "slug": "J.-Kohlmorgen",
                        "structuredName": {
                            "firstName": "Jens",
                            "lastName": "Kohlmorgen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kohlmorgen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5398743,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f43840dc1638a18eb6178f1060dc5f41af1c5ac7",
            "isKey": false,
            "numCitedBy": 975,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Support Vector Machines are used for time series prediction and compared to radial basis function networks. We make use of two different cost functions for Support Vectors: training with (i) an e insensitive loss and (ii) Huber's robust loss function and discuss how to choose the regularization parameters in these models. Two applications are considered: data from (a) a noisy (normal and uniform noise) Mackey Glass equation and (b) the Santa Fe competition (set D). In both cases Support Vector Machines show an excellent performance. In case (b) the Support Vector approach improves the best known result on the benchmark by a factor of 29%."
            },
            "slug": "Predicting-Time-Series-with-Support-Vector-Machines-M\u00fcller-Smola",
            "title": {
                "fragments": [],
                "text": "Predicting Time Series with Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Two different cost functions for Support Vectors are made use: training with an e insensitive loss and Huber's robust loss function and how to choose the regularization parameters in these models are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69954310"
                        ],
                        "name": "W. H\u00c3\u00a4rdle",
                        "slug": "W.-H\u00c3\u00a4rdle",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "H\u00c3\u00a4rdle",
                            "middleNames": [
                                "Karl"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. H\u00c3\u00a4rdle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 123570082,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "966c4df32a99dae7cec240e71e96ccdb5e3806c4",
            "isKey": false,
            "numCitedBy": 1588,
            "numCiting": 136,
            "paperAbstract": {
                "fragments": [],
                "text": "Applied Nonparametric Regression is the first book to bring together in one place the techniques for regression curve smoothing involving more than one variable. The computer and the development of interactive graphics programs have made curve estimation possible. This volume focuses on the applications and practical problems of two central aspects of curve smoothing: the choice of smoothing parameters and the construction of confidence bounds. HA\u00a4rdle argues that all smoothing methods are based on a local averaging mechanism and can be seen as essentially equivalent to kernel smoothing. To simplify the exposition, kernel smoothers are introduced and discussed in great detail. Building on this exposition, various other smoothing methods (among them splines and orthogonal polynomials) are presented and their merits discussed. All the methods presented can be understood on an intuitive level; however, exercises and supplemental materials are provided for those readers desiring a deeper understanding of the techniques. The methods covered in this text have numerous applications in many areas using statistical analysis. Examples are drawn from economics as well as from other disciplines including medicine and engineering."
            },
            "slug": "Applied-Nonparametric-Regression-H\u00c3\u00a4rdle",
            "title": {
                "fragments": [],
                "text": "Applied Nonparametric Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "Applied Nonparametric Regression is the first book to bring together in one place the techniques for regression curve smoothing involving more than one variable and argues that all smoothing methods are based on a local averaging mechanism and can be seen as essentially equivalent to kernel smoothing."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145833095"
                        ],
                        "name": "S. Kothari",
                        "slug": "S.-Kothari",
                        "structuredName": {
                            "firstName": "Suresh",
                            "lastName": "Kothari",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kothari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681982"
                        ],
                        "name": "H. Oh",
                        "slug": "H.-Oh",
                        "structuredName": {
                            "firstName": "Heekuck",
                            "lastName": "Oh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Oh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 15
                            }
                        ],
                        "text": "H ardle, 1990; Bishop, 1995)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 123
                            }
                        ],
                        "text": "A classical RBF systemcould also be made more discriminant by using moving centers (e.g. Poggio and Girosi,1990), or a di erent cost function, as the classi cation gure of merit (Hampshire andWaibel, 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 57
                            }
                        ],
                        "text": "This phenomenon is often referred to as over tting (e.g. Bishop, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 85
                            }
                        ],
                        "text": "For details, see LeCun,Boser, Denker, Henderson, Howard, Hubbard, and Jackel (1989); Bishop (1995); Amari, Murata,M uller, Finke, and Yang (1997).10The latter was chosen as in Sec."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 177751,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbc0a468ab103ae29717703d4aa9f682f6a2b664",
            "isKey": true,
            "numCitedBy": 15337,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-Networks-for-Pattern-Recognition-Kothari-Oh",
            "title": {
                "fragments": [],
                "text": "Neural Networks for Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Adv. Comput."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 10
                            }
                        ],
                        "text": "(From Sch olkopf, Burges, and Vapnik(1996a).)and simplicity of virtual examples approaches, while cutting down on their computa-tional cost signi cantly."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 147
                            }
                        ],
                        "text": "\u2026now in the position to beable to devote the present chapter to three techniques for incorporating task-speci cprior knowledge in SV machines (Sch olkopf, Burges, and Vapnik, 1996a; Sch olkopf,Simard, Smola, and Vapnik, 1997a).4.1 IntroductionWhen we are trying to extract regularities from data,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9756494,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "55590f229e23a8e67af7d6d36f7456a595c251d1",
            "isKey": false,
            "numCitedBy": 319,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Developed only recently, support vector learning machines achieve high generalization ability by minimizing a bound on the expected test error; however, so far there existed no way of adding knowledge about invariances of a classification problem at hand. We present a method of incorporating prior knowledge about transformation invariances by applying transformations to support vectors, the training examples most critical for determining the classification boundary."
            },
            "slug": "Incorporating-Invariances-in-Support-Vector-Sch\u00f6lkopf-Burges",
            "title": {
                "fragments": [],
                "text": "Incorporating Invariances in Support Vector Learning Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This work presents a method of incorporating prior knowledge about transformation invariances by applying transformations to support vectors, the training examples most critical for determining the classification boundary."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2889875"
                        ],
                        "name": "R. Horn",
                        "slug": "R.-Horn",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Horn",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Horn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144610701"
                        ],
                        "name": "Charles R. Johnson",
                        "slug": "Charles-R.-Johnson",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Johnson",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles R. Johnson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 133
                            }
                        ],
                        "text": "In that case,6The fact that every positive matrix is the Gram matrix of some set of vectors is well-known inlinear algebra (see e.g. Bhatia, 1997, Exercise I.5.10)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 43188859,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7c3b564bbdc8e7e3242257189ab7702d3e095115",
            "isKey": true,
            "numCitedBy": 28662,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Linear algebra and matrix theory are fundamental tools in mathematical and physical science, as well as fertile fields for research. This new edition of the acclaimed text presents results of both classic and recent matrix analyses using canonical forms as a unifying theme, and demonstrates their importance in a variety of applications. The authors have thoroughly revised, updated, and expanded on the first edition. The book opens with an extended summary of useful concepts and facts and includes numerous new topics and features, such as: - New sections on the singular value and CS decompositions - New applications of the Jordan canonical form - A new section on the Weyr canonical form - Expanded treatments of inverse problems and of block matrices - A central role for the Von Neumann trace theorem - A new appendix with a modern list of canonical forms for a pair of Hermitian matrices and for a symmetric-skew symmetric pair - Expanded index with more than 3,500 entries for easy reference - More than 1,100 problems and exercises, many with hints, to reinforce understanding and develop auxiliary themes such as finite-dimensional quantum systems, the compound and adjugate matrices, and the Loewner ellipsoid - A new appendix provides a collection of problem-solving hints."
            },
            "slug": "Matrix-analysis-Horn-Johnson",
            "title": {
                "fragments": [],
                "text": "Matrix analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This new edition of the acclaimed text presents results of both classic and recent matrix analyses using canonical forms as a unifying theme, and demonstrates their importance in a variety of applications."
            },
            "venue": {
                "fragments": [],
                "text": "Statistical Inference for Engineers and Data Scientists"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 23
                            }
                        ],
                        "text": "For a discussion, see (Simard, LeCun, and Denker,1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 263
                            }
                        ],
                        "text": "\u2026(xj Axi), witha positive de nite matrix A = B>B.1Clearly, the scheme can be iterated; however, care has to exercised, since the iteration of localinvariances would lead to global ones which are not always desirable | cf. the example of a '6'rotating into a '9' (Simard, LeCun, and Denker, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 127
                            }
                        ],
                        "text": "Thedata representation can also be changed by using a modi ed distance metric, ratherthan actually changing the patterns (e.g. Simard, LeCun, and Denker, 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 166
                            }
                        ],
                        "text": "The latter compares favourably to almost all known results on that database, and issecond only to a memory-based tangent-distance nearest neighbour classi er at 2:6%(Simard, LeCun, and Denker, 1993)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 261,
                                "start": 230
                            }
                        ],
                        "text": "Classi cation Error RateUSPS DB classical RBF RBF with SV centers full SV machineTraining 1.7% 0.0% 0.0%Test 6.7% 4.9% 4.2%is 2.5% (Bromley and S ackinger, 1991), almost matched by a memory-based Tangent-distance classi er (2.6%, Simard, LeCun, and Denker, 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11382731,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8314dda1ec43ce57ff877f8f02ed89acb68ca035",
            "isKey": true,
            "numCitedBy": 581,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Memory-based classification algorithms such as radial basis functions or K-nearest neighbors typically rely on simple distances (Euclidean, dot product...), which are not particularly meaningful on pattern vectors. More complex, better suited distance measures are often expensive and rather ad-hoc (elastic matching, deformable templates). We propose a new distance measure which (a) can be made locally invariant to any set of transformations of the input and (b) can be computed efficiently. We tested the method on large handwritten character databases provided by the Post Office and the NIST. Using invariances with respect to translation, rotation, scaling, shearing and line thickness, the method consistently outperformed all other systems tested on the same databases."
            },
            "slug": "Efficient-Pattern-Recognition-Using-a-New-Distance-Simard-LeCun",
            "title": {
                "fragments": [],
                "text": "Efficient Pattern Recognition Using a New Transformation Distance"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new distance measure which can be made locally invariant to any set of transformations of the input and can be computed efficiently is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2022386739"
                        ],
                        "name": "Peter Barlett",
                        "slug": "Peter-Barlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Barlett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Barlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740222"
                        ],
                        "name": "Wee Sun Lee",
                        "slug": "Wee-Sun-Lee",
                        "structuredName": {
                            "firstName": "Wee",
                            "lastName": "Lee",
                            "middleNames": [
                                "Sun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wee Sun Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 573509,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d19272112b50547614479a0c409fca66e3b05f7",
            "isKey": false,
            "numCitedBy": 2844,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the surprising recurring phenomena observed in experiments with boosting is that the test error of the generated classifier usually does not increase as its size becomes very large, and often is observed to decrease even after the training error reaches zero. In this paper, we show that this phenomenon is related to the distribution of margins of the training examples with respect to the generated voting classification rule, where the margin of an example is simply the difference between the number of correct votes and the maximum number of votes received by any incorrect label. We show that techniques used in the analysis of Vapnik's support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error. We also show theoretically and experimentally that boosting is especially effective at increasing the margins of the training examples. Finally, we compare our explanation to those based on the bias-variance"
            },
            "slug": "Boosting-the-margin:-A-new-explanation-for-the-of-Schapire-Freund",
            "title": {
                "fragments": [],
                "text": "Boosting the margin: A new explanation for the effectiveness of voting methods"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that techniques used in the analysis of Vapnik's support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14197727,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a4c16b4bfb2794f17f4816b621f4b97e70b7abdf",
            "isKey": false,
            "numCitedBy": 261,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper shows that if a large neural network is used for a pattern classification problem, and the learning algorithm finds a network with small weights that has small squared error on the training patterns, then the generalization performance depends on the size of the weights rather than the number of weights. More specifically, consider an l-layer feed-forward network of sigmoid units, in which the sum of the magnitudes of the weights associated with each unit is bounded by A. The misclassification probability converges to an error estimate (that is closely related to squared error on the training set) at rate O((cA)l(l+1)/2 \u221a(log n)/m) ignoring log factors, where m is the number of training patterns, n is the input dimension, and c is a constant. This may explain the generalization performance of neural networks, particularly when the number of training examples is considerably smaller than the number of weights. It also supports heuristics (such as weight decay and early stopping) that attempt to keep the weights small during training."
            },
            "slug": "For-Valid-Generalization-the-Size-of-the-Weights-is-Bartlett",
            "title": {
                "fragments": [],
                "text": "For Valid Generalization the Size of the Weights is More Important than the Size of the Network"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper shows that if a large neural network is used for a pattern classification problem, and the learning algorithm finds a network with small weights that has small squared error on the training patterns, then the generalization performance depends on the size of the weights rather than the number of weights."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 73
                            }
                        ],
                        "text": "Nevertheless, we brie ymention the case of SV regression (Vapnik, 1995b; Smola, 1996; Vapnik, Golowich,and Smola, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59853377,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6e2225c24f58a3712a0ba7ff468f5be1d67cda4c",
            "isKey": false,
            "numCitedBy": 237,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "1 Support Vector Learning Machines (SVLM) have become an emerging technique which has proven successful in many traditionally neural network dominated applications. This is also the case for Regression Estimation (RE). In particular we are able to construct spline approximations of given data independently from the number of input-dimensions regarding complexity during training and with only linear complexity for reconstruction-compared to exponential complexity in conventional methods."
            },
            "slug": "Regression-estimation-with-support-vector-learning-Smola",
            "title": {
                "fragments": [],
                "text": "Regression estimation with support vector learning machines"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "Support Vector Learning Machines are able to construct spline approximations of given data independently from the number of input-dimensions regarding complexity during training and with only linear complexity for reconstruction-compared to exponential complexity in conventional methods."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38817267"
                        ],
                        "name": "K. Sung",
                        "slug": "K.-Sung",
                        "structuredName": {
                            "firstName": "Kah",
                            "lastName": "Sung",
                            "middleNames": [
                                "Kay"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sung"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 25
                            }
                        ],
                        "text": "For further details, see(Sung, 1996; Moody and Darken, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17583351,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f3317b98195fe0be4acf7b450f015c1abca13ab9",
            "isKey": false,
            "numCitedBy": 248,
            "numCiting": 99,
            "paperAbstract": {
                "fragments": [],
                "text": "Object and pattern detection is a classical computer vision problem with many potential applications, ranging from automatic target recognition to image-based industrial inspection tasks in assembly lines. While there have been some successful object and pattern detection systems in the past, most such systems handle only specific rigid objects or patterns that can be accurately described by fixed geometric models or pictorial templates. \nThis thesis presents a learning based approach for detecting classes of objects and patterns with variable image appearance but highly predictable image boundaries. Some examples of such object and pattern classes include human faces, aerial views of structured terrain features like volcanoes, localized material defect signatures in industrial parts, certain tissue anomalies in medical images, and instances of a given digit or character, which may be written or printed in many different styles. \nThe thesis consists of two parts. In part one, we introduce our object and pattern detection approach using a concrete human face detection example. The approach first builds a distribution-based model of the target pattern class in an appropriate feature space to describe the target's variable image appearance. It then learns from examples a similarity measure for matching new patterns against the distribution-based target model. We also discuss some pertinent learning issues, including ideas on virtual example generation and example selection. The approach makes few assumptions about the target pattern class and should therefore be fairly general, as long as the target class has predictable image boundaries. We show that this is indeed the case by demonstrating the technique on two other pattern detection/recognition problems. \nBecause our object and pattern detection approach is very much learning-based, how well a system eventually performs depends heavily on the quality of training examples it receives. The second part of this thesis looks at how one can select high quality examples for function approximation learning tasks. Active learning is an area of research that investigates how a learner can intelligently select future training examples to get better approximation results with less data. We propose an active learning formulation for function approximation, and show for three specific approximation function classes, that the active example selection strategy learns its target with fewer data samples than random sampling. Finally, we simplify the original active learning formulation, and show how it leads to a tractable example selection paradigm, suitable for use in many object and pattern detection problems. (Copies available exclusively from MIT Libraries, Rm. 14-0551, Cambridge, MA 02139-4307. Ph. 617-253-5668; Fax 617-253-1690.)"
            },
            "slug": "Learning-and-example-selection-for-object-and-Sung",
            "title": {
                "fragments": [],
                "text": "Learning and example selection for object and pattern detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This thesis presents a learning based approach for detecting classes of objects and patterns with variable image appearance but highly predictable image boundaries, and proposes an active learning formulation for function approximation, and shows that the active example selection strategy learns its target with fewer data samples than random sampling."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 274
                            }
                        ],
                        "text": "The best single machine in the performance comparisons so farwas a LeNet5 convolutional neural network (0.9%); other high performance systemsinclude Tangent Distance nearest neighbour classi ers (1.1%), and LeNet4 with a lastlayer using methods of local learning (1.1%, cf. Bottou and Vapnik, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 49
                            }
                        ],
                        "text": "These feature detectors werefor instance used by Bottou and Vapnik (1992) as a preprocessing stage in their exper-iments in local learning."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7035291,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ca97e1668e305fb719845f84a05a62dfb946a5d",
            "isKey": true,
            "numCitedBy": 578,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Very rarely are training data evenly distributed in the input space. Local learning algorithms attempt to locally adjust the capacity of the training system to the properties of the training set in each area of the input space. The family of local learning algorithms contains known methods, like the k-nearest neighbors method (kNN) or the radial basis function networks (RBF), as well as new algorithms. A single analysis models some aspects of these algorithms. In particular, it suggests that neither kNN or RBF, nor nonlocal classifiers, achieve the best compromise between locality and capacity. A careful control of these parameters in a simple local learning algorithm has provided a performance breakthrough for an optical character recognition problem. Both the error rate and the rejection performance have been significantly improved."
            },
            "slug": "Local-Learning-Algorithms-Bottou-Vapnik",
            "title": {
                "fragments": [],
                "text": "Local Learning Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A single analysis suggests that neither kNN or RBF, nor nonlocal classifiers, achieve the best compromise between locality and capacity."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143615848"
                        ],
                        "name": "Rajesh P. N. Rao",
                        "slug": "Rajesh-P.-N.-Rao",
                        "structuredName": {
                            "firstName": "Rajesh",
                            "lastName": "Rao",
                            "middleNames": [
                                "P.",
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rajesh P. N. Rao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691804"
                        ],
                        "name": "D. Ballard",
                        "slug": "D.-Ballard",
                        "structuredName": {
                            "firstName": "Dana",
                            "lastName": "Ballard",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ballard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Rao and Ballard (1997) recently proposed a model in which the \\what\" and the\\where\" pathway (Mishkin, Ungerleider, and Macko, 1983) in the visual system areconceived of as estimating object identity and transformations, respectively."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14216416,
            "fieldsOfStudy": [
                "Biology",
                "Computer Science"
            ],
            "id": "472cbba98da63da22dcff0f879ed62b376328476",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Neurons in the visual cortex are known to possess localized, oriented receptive fields. It has previously been suggested that these distinctive properties may reflect an efficient image encoding strategy based on maximizing the sparseness of the distribution of output neuronal activities or alternately, extracting the independent components of natural image ensembles. Here, we show that a relatively simple neural solution to the problem of transformation-invariant visual recognition also causes localized, oriented receptive fields to be learned from natural images. These receptive fields, which code for various transformations in the image plane, allow a pair of cooperating neural networks, one estimating object identity (``what'''') and the other estimating object transformations (``where''''), to simultaneously recognize an object and estimate its pose by jointly maximizing the a posteriori probability of generating the observed visual data. We provide experimental results demonstrating the ability of these networks to factor retinal stimuli into object-centered features and object-invariant transformations. The resulting neuronal architecture suggests concrete computational roles for the neuroanatomical connections known to exist between the dorsal and ventral visual pathways."
            },
            "slug": "Localized-Receptive-Fields-May-Mediate-Recognition-Rao-Ballard",
            "title": {
                "fragments": [],
                "text": "Localized Receptive Fields May Mediate Transformation-Invariant Recognition in the Visual Cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is shown that a relatively simple neural solution to the problem of transformation-invariant visual recognition also causes localized, oriented receptive fields to be learned from natural images."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2080632378"
                        ],
                        "name": "B. Victorri",
                        "slug": "B.-Victorri",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Victorri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Victorri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 57
                            }
                        ],
                        "text": "4.2.2), on the otherhand, is comparable to the method of Simard et al. (1992) in that it is applicable for alldi erentiable local 1-parameter groups of local symmetry transformations, comprisinga fairly general class of invariances."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 229
                            }
                        ],
                        "text": "If instead ofLie groups of symmetry transformations one is dealing with discrete symmetries, as thebilateral symmetries of Vetter, Poggio, and B ultho (1994); Vetter and Poggio (1994),derivative-based methods such as the ones of Simard et al. (1992) are not applicable."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Simard et al. (1992) compare the rst two techniques and nd that for the con-sidered problem | learning a function with three plateaus where function values arelocally invariant | training on the arti cially enlarged data set is signi cantly slower,due to both correlations in the arti cial data and\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 141
                            }
                        ],
                        "text": "This is typically doneby using a modi ed error function which forces a learning machine to construct afunction with the desired invariances (Simard et al., 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 272,
                                "start": 253
                            }
                        ],
                        "text": "Train another Support Vector machine on the generated examples.1If the desired invariances are incorporated, the curves obtained by applying Lie sym-metry transformations to points on the decision surface should have tangents parallelto the latter (cf. Simard et al., 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2184474,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff32cebbdb8a436ccd8ae797647428615ae32d74",
            "isKey": true,
            "numCitedBy": 286,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "In many machine learning applications, one has access, not only to training data, but also to some high-level a priori knowledge about the desired behavior of the system. For example, it is known in advance that the output of a character recognizer should be invariant with respect to small spatial distortions of the input images (translations, rotations, scale changes, etcetera). \n \nWe have implemented a scheme that allows a network to learn the derivative of its outputs with respect to distortion operators of our choosing. This not only reduces the learning time and the amount of training data, but also provides a powerful language for specifying what generalizations we wish the network to perform."
            },
            "slug": "Tangent-Prop-A-Formalism-for-Specifying-Selected-in-Simard-Victorri",
            "title": {
                "fragments": [],
                "text": "Tangent Prop - A Formalism for Specifying Selected Invariances in an Adaptive Network"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A scheme is implemented that allows a network to learn the derivative of its outputs with respect to distortion operators of their choosing, which not only reduces the learning time and the amount of training data, but also provides a powerful language for specifying what generalizations the authors wish the network to perform."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 70
                            }
                        ],
                        "text": "The weights wi are then usually found by either error backpropagation(Rumelhart, Hinton, and Williams, 1986) or the pseudo-inverse method (Poggio andGirosi, 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 68
                            }
                        ],
                        "text": "The networks were trained by back-propagation of mean squared error(Rumelhart, Hinton, and Williams, 1986; LeCun, 1985)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 205001834,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "052b1d8ce63b07fec3de9dbb583772d860b7c769",
            "isKey": false,
            "numCitedBy": 20330,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1."
            },
            "slug": "Learning-representations-by-back-propagating-errors-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning representations by back-propagating errors"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "Back-propagation repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector, which helps to represent important features of the task domain."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050470845"
                        ],
                        "name": "H. Drucker",
                        "slug": "H.-Drucker",
                        "structuredName": {
                            "firstName": "Harris",
                            "lastName": "Drucker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Drucker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 145
                            }
                        ],
                        "text": "4.4), we also found improvements, albeit smaller ones: gen-erating Virtual Support Vectors by rotation or by the line thickness transformationof Drucker, Schapire, and Simard (1993), we constructed polynomial classi ers with3."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 234
                            }
                        ],
                        "text": "Ifthe size of a training set is multiplied by a number of desired invariances (by generatinga corresponding number of arti cial examples for each training pattern), the resultingtraining sets can get rather large (as the ones used by Drucker, Schapire, and Simard,1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 33515643,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "843ffb9898cedf899ddcdb9c4bdd10881c122429",
            "isKey": true,
            "numCitedBy": 228,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A boosting algorithm, based on the probably approximately correct (PAC) learning model is used to construct an ensemble of neural networks that significantly improves performance (compared to a single network) in optical character recognition (OCR) problems. The effect of boosting is reported on four handwritten image databases consisting of 12000 digits from segmented ZIP Codes from the United States Postal Service and the following from the National Institute of Standards and Technology: 220000 digits, 45000 upper case letters, and 45000 lower case letters. We use two performance measures: the raw error rate (no rejects) and the reject rate required to achieve a 1% error rate on the patterns not rejected. Boosting improved performance significantly, and, in some cases, dramatically."
            },
            "slug": "Boosting-Performance-in-Neural-Networks-Drucker-Schapire",
            "title": {
                "fragments": [],
                "text": "Boosting Performance in Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "The boosting algorithm is used to construct an ensemble of neural networks that significantly improves performance (compared to a single network) in optical character recognition (OCR) problems and improved performance significantly, and, in some cases, dramatically."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Pattern Recognit. Artif. Intell."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145016534"
                        ],
                        "name": "J. Moody",
                        "slug": "J.-Moody",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Moody",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Moody"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2319258"
                        ],
                        "name": "C. Darken",
                        "slug": "C.-Darken",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Darken",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Darken"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 37
                            }
                        ],
                        "text": "For further details, see(Sung, 1996; Moody and Darken, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 31251383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e7c4f513f24c3b82a1138b9f22ed87ed00cbe76",
            "isKey": false,
            "numCitedBy": 4527,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a network architecture which uses a single internal layer of locally-tuned processing units to learn both classification tasks and real-valued function approximations (Moody and Darken 1988). We consider training such networks in a completely supervised manner, but abandon this approach in favor of a more computationally efficient hybrid learning method which combines self-organized and supervised learning. Our networks learn faster than backpropagation for two reasons: the local representations ensure that only a few units respond to any given input, thus reducing computational overhead, and the hybrid learning rules are linear rather than nonlinear, thus leading to faster convergence. Unlike many existing methods for data analysis, our network architecture and learning rules are truly adaptive and are thus appropriate for real-time use."
            },
            "slug": "Fast-Learning-in-Networks-of-Locally-Tuned-Units-Moody-Darken",
            "title": {
                "fragments": [],
                "text": "Fast Learning in Networks of Locally-Tuned Processing Units"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This work proposes a network architecture which uses a single internal layer of locally-tuned processing units to learn both classification tasks and real-valued function approximations (Moody and Darken 1988)."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747836"
                        ],
                        "name": "H. B\u00fclthoff",
                        "slug": "H.-B\u00fclthoff",
                        "structuredName": {
                            "firstName": "Heinrich",
                            "lastName": "B\u00fclthoff",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. B\u00fclthoff"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 2
                            }
                        ],
                        "text": "B ultho and Edelman (1992) have shown that when recognizing unfamiliar objects,observers exhibit viewpoint e ects which suggest that they do not recover the 3-Dshape of objects, but rather build a representation based on the actual training views(cf. also Logothetis, Pauls, and Poggio, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 112
                            }
                        ],
                        "text": "Indeed, proponents of view-based object recognition theories are mainly concernedwith fast recognition tasks (B ultho and Edelman, 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 104
                            }
                        ],
                        "text": "In the above terminology, one might argue that due to their un-familiarity, the wire frame objects of B ultho and Edelman (1992) make it very hardto use the transformations which form the structure of the underlying class of views."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15899784,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb181b3bead64a4b8b08d5f0b051743b186bd040",
            "isKey": false,
            "numCitedBy": 170,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Does the human brain represent objects for recognition by storing a series of twodimensional snapshots, or are the object models, in some sense, three-dimensional analogs of the objects they represent? One way to address this question is to explore the ability of the human visual system to generalize recognition from familiar to novel views of three-dimensional objects. Three recently proposed theories of object recognition | viewpoint normalization or alignment of 3D models [Ullman, S. (1989) Cognition, 32, 193-254], linear combination of 2D views [Ullman, S. & Basri, R. (1990)], and view approximation [Poggio, T. & Edelman, S. (1990) Nature, 343, 263-266] | predict di erent patterns of generalization to novel views. We have exploited the con icting predictions to test the three theories directly, in a psychophysical experiment involving computer-generated 3D objects. Our results suggest that the human visual system is better described as recognizing these objects by 2D view interpolation than by alignment or other methods that rely on object-centered 3D models. 2 How does the human visual system represent objects for recognition? The experiments we describe address this question by testing the ability of human subjects (and of computer models instantiating particular theories of recognition) to generalize from familiar to unfamiliar views of novel objects. Since di erent theories predict di erent patterns of generalization according to the experimental conditions, this approach yields concrete evidence in favor of some of the theories, and contradicts others. Theories that rely on 3D object-centered representations The rst class of theories we have considered [1, 4, 5] represent objects by 3D models, encoded in a viewpoint-independent fashion. One such approach, recognition by alignment [1], compares the input image with the projection of a stored model after the two are brought into register. The transformation necessary to achieve this registration is computed by matching a small number of features in the image with the corresponding features in the model. The aligning transformation is computed separately for each of the models stored in the system. Recognition is declared for the model that ts the input most closely after the two are aligned, if the residual dissimilarity between them is small enough. The decision criterion for recognition in this case can be stated in the following simpli ed form: kPTX(3D) X(2D)k < (1) where T is the aligning transformation, P is a 3D ! 2D projection operator, and the norm k kmeasures the dissimilarity between the projection of the transformed 3D model X(3D) and the input image X(2D). Recognition decision is then made based on a comparison between the measured dissimilarity and a threshold . One may make a further distinction between full alignment that uses 3D models and attempts to compensate for 3D transformations of objects (such as 3 rotation in depth), and the alignment of pictorial descriptions that uses multiple views rather than a single object-centered representation. Speci cally ([1], p.228), the multiple-view version of alignment involves representation that is \\view-dependent, since a number of di erent models of the same object from di erent viewing positions will be used,\" but at the same time \\view-insensitive, since the di erences between views are partially compensated by the alignment process.\" Consequently, view-independent performance (e.g., low error rate for novel views) can be considered the central distinguishing feature of both versions of this theory. Visual systems that rely on alignment and other 3D approaches can in principle achieve near perfect recognition performance, provided that (i) the 3D models of the input objects are available, and (ii) the information needed to access the correct model is present in the image. We note that a similar behavior is predicted by those recognition theories that represent objects by 3D structural relationships between generic volumetric primitives. Theories belonging to this class (e.g., [6, 7]) tend to focus on basic-level classi cation of objects rather than on the recognition of speci c object instances,1 and will not be given further consideration in this paper. Theories that rely on 2D viewer-centered representations Two recently proposed approaches to recognition dispense with the need for storing 3D models. The rst of these, recognition by linear combination of views [2], is built on the mathematical observation that, under orthographic projection, the 1Numerous studies in cognitive science (see [8] for a review) reveal that in the hierarchical structure of object categories there exists a certain level, called basic level, which is the most salient according to a variety of criteria (such as the ease and preference of access). Taking as an example the hierarchy \\quadruped, mammal, cat, Siamese\", the basic level is that of \\cat\". Objects whose recognition implies more detailed distinctions than those required for basic-level categorization are said to belong to a subordinate level. 4 2D coordinates of an object point can be represented by a linear combination of the coordinates of the corresponding points in a small number of xed 2D views of the same object. The required number of views depends on the allowed 3D transformations of the objects and on the representation of an individual view. A polyhedral object that can undergo a general linear transformation requires three views if separate linear bases are used to represent the x and the y coordinates of a new view; two views su ce if a mixed x; y basis is used [2, 9]. The recognition criterion under one possible version of the linear combination approach [10] can be formulated schematically as kXi iX(2D) i X(2D)k < (2) where the stored views X(2D) i comprise the linear vector basis that represents an object model (i.e., spans the space of the object's views), X(2D) is the input image, and i are the coe cients estimated for the given model/image pair. A recognition system that is perfectly linear and relies exclusively on the above approach should achieve uniformly high performance on those views that fall within the space spanned by the stored set of model views, and should perform poorly on views that belong to an orthogonal space. Another approach that represents objects by sets of 2D views is view approximation by regularization networks [3, 11], which includes as a special case approximation by radial basis functions (RBFs) [12, 13]. In this approach, generalization from familiar to novel views is regarded as a problem of approximating a smooth hypersurface in the space of all possible views, with the \\height\" of the surface known only at a sparse set of points corresponding to the familiar views. The approximation can be performed by a two-stage network (see [9] for details). In the rst stage intermediate responses are formed by a collection of nonlinear \\receptive elds\" (shaped, e.g., as multidimensional Gaussians), centered at the 5 familiar views. The output of the second stage is a linear combination of the intermediate receptive eld responses. If the regularization network is trained to output the value 1 for various views of a given object, the decision criterion for recognition can be stated as jXk ckG kX(2D) X(2D) k k 1j < (3) where X(2D) is the input image, X(2D) k are the familiar or prototypical views stored in the system, ck are the linear coe cients, and the function G( ) represents the shape of the receptive eld. A recognition system based on this method is expected to perform well when the novel view is close to the stored ones (that is, when most of the features of the input image fall close to their counterparts at least in some of the stored views; cf. [14]). The performance should become progressively worse on views that are far from the familiar ones. Methods To distinguish between the theories outlined above, we have developed an experimental paradigm based on a two-alternative forced-choice (2AFC) task. Our experiments consist of two phases: training and testing. In the training phase subjects are shown a novel object (see Figure 1) de ned as the target, usually as a motion sequence of 2D views that leads to an impression of solid shape through the kinetic depth e ect. In the testing phase the subjects are presented with single static views of either the target or a distractor (one of a relatively large set of similar objects). Target test views were situated either on the equator (on the 0 75 or on the 75 360 portion of the great circle, called inter and extra conditions), or on the meridian passing through one of the training views (ortho condition) (see Figure 2). The subject's task was to press a \\yes-button\" 6 if the displayed object is the current target and a \\no-button\" otherwise, and to do it as quickly and as accurately as possible. These instructions usually resulted in mean response times around 1 sec, and in mean miss rates2 around 30%. The fast response times indicate that the subjects did not apply conscious problemsolving techniques or reason explicitly about the stimuli. In all our experiments the subjects received no feedback as to the correctness of their response. The main features of our experimental approach are as follows: We can control precisely the subject's prior exposure to the targets, by employing novel computer-generated three-dimensional objects, similar to those shown in Figure 1. We can generate an unlimited number of novel objects with controlled complexity and surface appearance. Because the stimuli are produced by computer graphics, we can conduct identical experiments with human subjects and with computational models. Results The experimental setup satis ed both requirements of the alignment theory for perfect recognition: the subjects, all of whom reported perfect perception of 3D structure from motion during training, had the opportunity to form 3D models of the stimuli, and al"
            },
            "slug": "Psychophysical-support-for-a-2D-view-interpolation-B\u00fclthoff",
            "title": {
                "fragments": [],
                "text": "Psychophysical support for a 2D view interpolation theory of object recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The results suggest that the human visual system is better described as recognizing these objects by 2D view interpolation than by alignment or other methods that rely on object-centered 3D models, as well as those recognition theories that represent objects by 3D structural relationships between generic volumetric primitives."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 601110,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "8985a9637540daa0b7b8295f8a5bbda3a3be1dea",
            "isKey": false,
            "numCitedBy": 673,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-connection-between-regularization-operators-and-Smola-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "The connection between regularization operators and support vector kernels"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194361"
                        ],
                        "name": "S. Geman",
                        "slug": "S.-Geman",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2246319"
                        ],
                        "name": "E. Bienenstock",
                        "slug": "E.-Bienenstock",
                        "structuredName": {
                            "firstName": "Elie",
                            "lastName": "Bienenstock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Bienenstock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2330895"
                        ],
                        "name": "R. Doursat",
                        "slug": "R.-Doursat",
                        "structuredName": {
                            "firstName": "Ren\u00e9",
                            "lastName": "Doursat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Doursat"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 369,
                                "start": 348
                            }
                        ],
                        "text": "We conclude this section by noting that analyses in other branches of learningtheory have led to similar insights in the trade-o between reducing the training er-ror and limiting model complexity, for instance as described by regularization theory(Tikhonov and Arsenin, 1977), Minimum Description Length (Rissanen, 1978; Kol-mogorov, 1965), or the Bias-Variance Dilemma (Geman, Bienenstock, and Doursat,1992)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 258
                            }
                        ],
                        "text": "\u2026in the trade-o between reducing the training er-ror and limiting model complexity, for instance as described by regularization theory(Tikhonov and Arsenin, 1977), Minimum Description Length (Rissanen, 1978; Kol-mogorov, 1965), or the Bias-Variance Dilemma (Geman, Bienenstock, and Doursat,1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14215320,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "a34e35dbbc6911fa7b94894dffdc0076a261b6f0",
            "isKey": false,
            "numCitedBy": 3532,
            "numCiting": 151,
            "paperAbstract": {
                "fragments": [],
                "text": "Feedforward neural networks trained by error backpropagation are examples of nonparametric regression estimators. We present a tutorial on nonparametric inference and its relation to neural networks, and we use the statistical viewpoint to highlight strengths and weaknesses of neural models. We illustrate the main points with some recognition experiments involving artificial data as well as handwritten numerals. In way of conclusion, we suggest that current-generation feedforward neural networks are largely inadequate for difficult problems in machine perception and machine learning, regardless of parallel-versus-serial hardware or other implementation issues. Furthermore, we suggest that the fundamental challenges in neural modeling are about representation rather than learning per se. This last point is supported by additional experiments with handwritten numerals."
            },
            "slug": "Neural-Networks-and-the-Bias/Variance-Dilemma-Geman-Bienenstock",
            "title": {
                "fragments": [],
                "text": "Neural Networks and the Bias/Variance Dilemma"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is suggested that current-generation feedforward neural networks are largely inadequate for difficult problems in machine perception and machine learning, regardless of parallel-versus-serial hardware or other implementation issues."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 39
                            }
                        ],
                        "text": "1 are based on Sch\u007folkopf, Burges, and Vapnik (1995), AIII Press. Section 2.5 is based on Sch\u007folkopf, Sung, Burges, Girosi, Niyogi, Poggio, and Vapnik (1996c), IEEE. Chapter 3 is based on Sch\u007folkopf, Smola, and M\u007f uller (1997b), MIT Press."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 950,
                                "start": 31
                            }
                        ],
                        "text": "For 2, this simply means that (Vapnik, 1995b) 2(x) = (x21; x22;p2 x1x2): (1.21) If x represents an image with the entries being pixel values, we can use the kernel (x y)d to work in the space spanned by products of any d pixels | provided that we are able to do our work solely in terms of dot products, without any explicit usage of a mapped pattern d(x). Using kernels of the form (1.17), we take into account higherorder statistics without the combinatorial explosion (cf. (1.13)) of time and memory complexity which goes along already with moderately high N and d. To conclude this section, note that it is possible to modify (1.17) such that it maps into the space of all monomials up to degree d, de ning (Vapnik, 1995b) k(x;y) = (x y+ 1)d: (1.22) 1.3.3 Feature Spaces Induced by Mercer Kernels The question which function k does correspond to a dot product in some space F has been discussed by Boser, Guyon, and Vapnik (1992); Vapnik (1995b). To construct a map induced by a kernel k, i."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 328,
                                "start": 39
                            }
                        ],
                        "text": "1 are based on Sch\u007folkopf, Burges, and Vapnik (1995), AIII Press. Section 2.5 is based on Sch\u007folkopf, Sung, Burges, Girosi, Niyogi, Poggio, and Vapnik (1996c), IEEE. Chapter 3 is based on Sch\u007folkopf, Smola, and M\u007f uller (1997b), MIT Press. Section 4.2.1 and gures 2.5 and 4.1 are based on Sch\u007folkopf, Burges, and Vapnik (1996a), Springer Verlag."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 80
                            }
                        ],
                        "text": "INTRODUCTION AND PRELIMINARIES the Generalized Portrait hyperplane classi er of Vapnik and Chervonenkis (1974) to nonlinear Support Vector machines (Sec."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1020,
                                "start": 80
                            }
                        ],
                        "text": "INTRODUCTION AND PRELIMINARIES the Generalized Portrait hyperplane classi er of Vapnik and Chervonenkis (1974) to nonlinear Support Vector machines (Sec. 2.1). Aizerman, Braverman, and Rozonoer (1964) call F the linearization space, and use it in the context of the potential function classi cation method to express the dot product between elements of F in terms of elements of the input space. They also consider the possibility of choosing k a priori, without being directly concerned with the corresponding mapping into F . A speci c choice of k might then correspond to a dot product between patterns mapped with a suitable . What does k look like for the case of polynomial features? We start by giving an example (Vapnik, 1995b) for N = d = 2. For the map C2 : (x1; x2) 7! (x21; x22; x1x2; x2x1); (1.15) dot products in F take the form (C2(x) C2(y)) = x21y2 1 + x22y2 2 + 2x1x2y1y2 = (x y)2; (1.16) i.e. the desired kernel k is simply the square of the dot product in input space. Boser, Guyon, and Vapnik (1992) note that the same works for arbitrary N; d 2 N: as a straightforward generalization of a result proved in the context of polynomial approximation (Poggio, 1975, Lemma 2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 42
                            }
                        ],
                        "text": "This method was used by Boser, Guyon, and Vapnik (1992) to extend"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 80
                            }
                        ],
                        "text": "INTRODUCTION AND PRELIMINARIES the Generalized Portrait hyperplane classi er of Vapnik and Chervonenkis (1974) to nonlinear Support Vector machines (Sec. 2.1). Aizerman, Braverman, and Rozonoer (1964) call F the linearization space, and use it in the context of the potential function classi cation method to express the dot product between elements of F in terms of elements of the input space."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 39
                            }
                        ],
                        "text": "1 are based on Sch\u007folkopf, Burges, and Vapnik (1995), AIII Press. Section 2.5 is based on Sch\u007folkopf, Sung, Burges, Girosi, Niyogi, Poggio, and Vapnik (1996c), IEEE."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 39
                            }
                        ],
                        "text": "1 are based on Sch\u007folkopf, Burges, and Vapnik (1995), AIII Press."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 7
                            }
                        ],
                        "text": "2.1.2 (Vapnik, 1998)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 934,
                                "start": 31
                            }
                        ],
                        "text": "For 2, this simply means that (Vapnik, 1995b) 2(x) = (x21; x22;p2 x1x2): (1.21) If x represents an image with the entries being pixel values, we can use the kernel (x y)d to work in the space spanned by products of any d pixels | provided that we are able to do our work solely in terms of dot products, without any explicit usage of a mapped pattern d(x). Using kernels of the form (1.17), we take into account higherorder statistics without the combinatorial explosion (cf. (1.13)) of time and memory complexity which goes along already with moderately high N and d. To conclude this section, note that it is possible to modify (1.17) such that it maps into the space of all monomials up to degree d, de ning (Vapnik, 1995b) k(x;y) = (x y+ 1)d: (1.22) 1.3.3 Feature Spaces Induced by Mercer Kernels The question which function k does correspond to a dot product in some space F has been discussed by Boser, Guyon, and Vapnik (1992); Vapnik (1995b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 28637672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "385197d4c02593e2823c71e4f90a0993b703620e",
            "isKey": true,
            "numCitedBy": 26320,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "slug": "Statistical-learning-theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "Statistical learning theory"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041866"
                        ],
                        "name": "L. Jackel",
                        "slug": "L.-Jackel",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Jackel",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jackel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122774836"
                        ],
                        "name": "A. Brunot",
                        "slug": "A.-Brunot",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Brunot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Brunot"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152547641"
                        ],
                        "name": "Corinna Cortes",
                        "slug": "Corinna-Cortes",
                        "structuredName": {
                            "firstName": "Corinna",
                            "lastName": "Cortes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Corinna Cortes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050470845"
                        ],
                        "name": "H. Drucker",
                        "slug": "H.-Drucker",
                        "structuredName": {
                            "firstName": "Harris",
                            "lastName": "Drucker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Drucker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145636949"
                        ],
                        "name": "Urs Muller",
                        "slug": "Urs-Muller",
                        "structuredName": {
                            "firstName": "Urs",
                            "lastName": "Muller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Urs Muller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "97914531"
                        ],
                        "name": "E. Sackinger",
                        "slug": "E.-Sackinger",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "Sackinger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Sackinger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 70
                            }
                        ],
                        "text": "Furthermore, the speed of SV wascomparatively slow to start with (cf. LeCun et al., 1995), requiring approximately 14million multiply adds for one classi cation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 71
                            }
                        ],
                        "text": "In order to become competitive with sys-tems with comparable accuracy (LeCun et al., 1995), we need approximately a factorof fty improvement in speed."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 163
                            }
                        ],
                        "text": "This database has become the standardfor performance comparisons at AT&T Bell Labs; the error rate record of 0.7% is heldby a boosted LeNet4 (Bottou et al., 1994; LeCun et al., 1995), i.e. by an ensembleof learning machines."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 95
                            }
                        ],
                        "text": "Test results on the MNIST database which are given in the literature (e.g. Bottouet al., 1994; LeCun et al., 1995) for some reason do not use the full MNIST test setof 60000 characters."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 273
                            }
                        ],
                        "text": "\u2026followed by the animal database, and by the subordinate level chair database.2.3 Digit Recognition Using Di erent KernelsHandwritten digit recognition has long served as a test bed for evaluating and bench-marking classi ers (e.g. LeCun et al., 1989; Bottou et al., 1994; LeCun et al., 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 221
                            }
                        ],
                        "text": "Another interesting performance measure is the rejection error rate, de ned as thepercentage of patterns that would have to be rejected to attain a speci ed error rate (inthe benchmark studies of Bottou et al. (1994) and LeCun et al. (1995), 0.5%)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11259076,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d50dce749321301f0104689f2dc582303a83be65",
            "isKey": true,
            "numCitedBy": 631,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "COMPARISON OF LEARNINGALGORITHMS FOR HANDWRITTEN DIGITRECOGNITIONY. LeCun, L. Jackel, L. Bottou, A. Brunot, C. Cortes,J. Denker, H. Drucker, I. Guyon, U. M \u007fuller,E. S\u007fackinger, P. Simard, and V. VapnikBell Lab oratories, Holmdel, NJ 07733, USAEmail: yann@research.att.comAbstractThis pap er compares the p erformance of several classi er algorithmson a standard database of handwritten digits. We consider not only rawaccuracy, but also rejection, training time, recognition time, and memoryrequirements.1"
            },
            "slug": "Comparison-of-learning-algorithms-for-handwritten-LeCun-Jackel",
            "title": {
                "fragments": [],
                "text": "Comparison of learning algorithms for handwritten digit recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This comparison of several learning algorithms for handwritten digits considers not only raw accuracy, but also rejection, training time, recognition time, and memory requirements."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144517651"
                        ],
                        "name": "T. Vetter",
                        "slug": "T.-Vetter",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Vetter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Vetter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747836"
                        ],
                        "name": "H. B\u00fclthoff",
                        "slug": "H.-B\u00fclthoff",
                        "structuredName": {
                            "firstName": "Heinrich",
                            "lastName": "B\u00fclthoff",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. B\u00fclthoff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3029110,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fcf4019d3cac928a63409520026bbad0636c1e80",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-importance-of-symmetry-and-virtual-views-in-Vetter-Poggio",
            "title": {
                "fragments": [],
                "text": "The importance of symmetry and virtual views in three-dimensional object recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Current Biology"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6674407,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3f600e6c6cf93e78c9e6e690443d6d22c4bf18b9",
            "isKey": false,
            "numCitedBy": 7881,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method for performing a nonlinear form of principal component analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in high-dimensional feature spaces, related to input space by some nonlinear mapfor instance, the space of all possible five-pixel products in 16 16 images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition."
            },
            "slug": "Nonlinear-Component-Analysis-as-a-Kernel-Eigenvalue-Sch\u00f6lkopf-Smola",
            "title": {
                "fragments": [],
                "text": "Nonlinear Component Analysis as a Kernel Eigenvalue Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A new method for performing a nonlinear form of principal component analysis by the use of integral operator kernel functions is proposed and experimental results on polynomial feature extraction for pattern recognition are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37274089"
                        ],
                        "name": "D. Henderson",
                        "slug": "D.-Henderson",
                        "structuredName": {
                            "firstName": "Donnie",
                            "lastName": "Henderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Henderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2799635"
                        ],
                        "name": "R. Howard",
                        "slug": "R.-Howard",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Howard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Howard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34859193"
                        ],
                        "name": "W. Hubbard",
                        "slug": "W.-Hubbard",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "Hubbard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Hubbard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041866"
                        ],
                        "name": "L. Jackel",
                        "slug": "L.-Jackel",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Jackel",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jackel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 120
                            }
                        ],
                        "text": "C.1)contains 9298 handwritten digits (7291 for training, 2007 for testing), collected frommail envelopes in Bu alo (cf. LeCun et al., 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 65
                            }
                        ],
                        "text": "This database has been used extensivelyin the literature, with a LeNet1 Convolutional Network achieving a test error rate of5."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 4
                            }
                        ],
                        "text": "0% (LeCun et al., 1989)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 83
                            }
                        ],
                        "text": "%) is competitive with convolutional 5-layer neural networks (5.0% were reportedby LeCun et al., 1989) and nonlinear Support Vector classi ers (4.0%, Table 2.4); it\n96 CHAPTER 3."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 212
                            }
                        ],
                        "text": "Other results on thisdatabase include a Euclidean distance nearest neighbour classi er (5.9%, Simard,LeCun, and Denker, 1993), a perceptron with one hidden layer, 5.9%, and a convolu-tional neural network (5.0%, LeCun et al., 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 97
                            }
                        ],
                        "text": "Even though the latter result has been obtained a number ofyears ago, it should be stressed that LeNet1 provides an architecture which contains agreat deal of prior information about the handwritten character classi cation problem."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 93
                            }
                        ],
                        "text": "They should be compared with valuesachieved on the same database with a ve-layer neural net (LeNet1, LeCun, Boser,Denker, Henderson, Howard, Hubbard, and Jackel, 1989), 5.0%, a neural net with onehidden layer, 5.9%, and the human performance, 2.5% (Bromley and S ackinger, 1991)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 250,
                                "start": 232
                            }
                        ],
                        "text": "\u2026followed by the animal database, and by the subordinate level chair database.2.3 Digit Recognition Using Di erent KernelsHandwritten digit recognition has long served as a test bed for evaluating and bench-marking classi ers (e.g. LeCun et al., 1989; Bottou et al., 1994; LeCun et al., 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 122
                            }
                        ],
                        "text": "Our system of kernel PCA feature extrac-tion plus linear support vector machine for instance performed better than LeNet1(LeCun et al., 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 41312633,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8e8f3c8d4418c8d62e306538c9c1292635e9d27",
            "isKey": true,
            "numCitedBy": 7830,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification."
            },
            "slug": "Backpropagation-Applied-to-Handwritten-Zip-Code-LeCun-Boser",
            "title": {
                "fragments": [],
                "text": "Backpropagation Applied to Handwritten Zip Code Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This paper demonstrates how constraints from the task domain can be integrated into a backpropagation network through the architecture of the network, successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743045"
                        ],
                        "name": "S. Ullman",
                        "slug": "S.-Ullman",
                        "structuredName": {
                            "firstName": "Shimon",
                            "lastName": "Ullman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ullman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 19
                            }
                        ],
                        "text": "D transformations (Ullman, 1989) is cheapin terms of storage: storing these transformations is almost for free, and storing one3-D model is reasonably cheap."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 41895215,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1844c55a8f76c09257c84e886a7bf10fc7e98e3f",
            "isKey": false,
            "numCitedBy": 654,
            "numCiting": 88,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Aligning-pictorial-descriptions:-An-approach-to-Ullman",
            "title": {
                "fragments": [],
                "text": "Aligning pictorial descriptions: An approach to object recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Cognition"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7831590,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1ad15c08556c8f8e3739703857ea01077ce738c5",
            "isKey": false,
            "numCitedBy": 2054,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method for performing a nonlinear form of Principal Component Analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in highdimensional feature spaces, related to input space by some nonlinear map; for instance the space of all possible d-pixel products in images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition."
            },
            "slug": "Kernel-Principal-Component-Analysis-Sch\u00f6lkopf-Smola",
            "title": {
                "fragments": [],
                "text": "Kernel Principal Component Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A new method for performing a nonlinear form of Principal Component Analysis by the use of integral operator kernel functions is proposed and experimental results on polynomial feature extraction for pattern recognition are presented."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5737624"
                        ],
                        "name": "E. Rosch",
                        "slug": "E.-Rosch",
                        "structuredName": {
                            "firstName": "Eleanor",
                            "lastName": "Rosch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Rosch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3050353"
                        ],
                        "name": "C. Mervis",
                        "slug": "C.-Mervis",
                        "structuredName": {
                            "firstName": "Carolyn",
                            "lastName": "Mervis",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Mervis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2926253"
                        ],
                        "name": "Wayne D. Gray",
                        "slug": "Wayne-D.-Gray",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "Gray",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wayne D. Gray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50156221"
                        ],
                        "name": "D. M. Johnson",
                        "slug": "D.-M.-Johnson",
                        "structuredName": {
                            "firstName": "D",
                            "lastName": "Johnson",
                            "middleNames": [
                                "M"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. M. Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1405696055"
                        ],
                        "name": "P. Boyes-Braem",
                        "slug": "P.-Boyes-Braem",
                        "structuredName": {
                            "firstName": "Penny",
                            "lastName": "Boyes-Braem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Boyes-Braem"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5612467,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "2c05d131576d01668ef01156ac73e52c3152384a",
            "isKey": false,
            "numCitedBy": 4981,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Basic-objects-in-natural-categories-Rosch-Mervis",
            "title": {
                "fragments": [],
                "text": "Basic objects in natural categories"
            },
            "venue": {
                "fragments": [],
                "text": "Cognitive Psychology"
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31772450"
                        ],
                        "name": "N. Logothetis",
                        "slug": "N.-Logothetis",
                        "structuredName": {
                            "firstName": "Nikos",
                            "lastName": "Logothetis",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Logothetis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144320332"
                        ],
                        "name": "J. Pauls",
                        "slug": "J.-Pauls",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Pauls",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pauls"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 256
                            }
                        ],
                        "text": "B ultho and Edelman (1992) have shown that when recognizing unfamiliar objects,observers exhibit viewpoint e ects which suggest that they do not recover the 3-Dshape of objects, but rather build a representation based on the actual training views(cf. also Logothetis, Pauls, and Poggio, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15325604,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "7d7721e2c556e02f35654428953ed83cfa8adff8",
            "isKey": false,
            "numCitedBy": 989,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Shape-representation-in-the-inferior-temporal-of-Logothetis-Pauls",
            "title": {
                "fragments": [],
                "text": "Shape representation in the inferior temporal cortex of monkeys"
            },
            "venue": {
                "fragments": [],
                "text": "Current Biology"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46515453"
                        ],
                        "name": "E. Chang",
                        "slug": "E.-Chang",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Chang",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144990248"
                        ],
                        "name": "R. Lippmann",
                        "slug": "R.-Lippmann",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Lippmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lippmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16848412,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "62dda01b95fe11a0ad0de9c14acfd7cfa5eef3c0",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "A new boundary hunting radial basis function (BH-RBF) classifier which allocates RBF centers constructively near class boundaries is described. This classifier creates complex decision boundaries only in regions where confusions occur and corresponding RBF outputs are similar. A predicted square error measure is used to determine how many centers to add and to determine when to stop adding centers. Two experiments are presented which demonstrate the advantages of the BH-RBF classifier. One uses artificial data with two classes and two input features where each class contains four clusters but only one cluster is near a decision region boundary. The other uses a large seismic database with seven classes and 14 input features. In both experiments the BH-RBF classifier provides a lower error rate with fewer centers than are required by more conventional RBF, Gaussian mixture, or MLP classifiers."
            },
            "slug": "A-Boundary-Hunting-Radial-Basis-Function-Classifier-Chang-Lippmann",
            "title": {
                "fragments": [],
                "text": "A Boundary Hunting Radial Basis Function Classifier which Allocates Centers Constructively"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A new boundary hunting radial basis function (BH-RBF) classifier which allocates RBF centers constructively near class boundaries is described, which provides a lower error rate with fewer centers than are required by more conventional RBF, Gaussian mixture, or MLP classifiers."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 150
                            }
                        ],
                        "text": "\u2026is due to the fact that the type ofregularization and the class of functions which are considered as admissible solutionsare intimately related (cf. Poggio and Girosi, 1990; Girosi, Jones, and Poggio, 1993;Smola and Sch olkopf, 1997a; Smola, Sch olkopf, and M uller, 1997): the SV algorithmis\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 29
                            }
                        ],
                        "text": "Generally, accuracies were higher for grey-scale images than they were for sil-houettes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 89
                            }
                        ],
                        "text": "A classical RBF systemcould also be made more discriminant by using moving centers (e.g. Poggio and Girosi,1990), or a di erent cost function, as the classi cation gure of merit (Hampshire andWaibel, 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 70
                            }
                        ],
                        "text": "They thought of this representationas an interpolation mechanism (cf. Poggio and Girosi, 1990), but one could of courseconceive of more sophisticated mechanisms for combining information contained inthe training views."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14892653,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "089a76dbc62a06ad30ae1925530e8733e850268e",
            "isKey": true,
            "numCitedBy": 3702,
            "numCiting": 96,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of the approximation of nonlinear mapping, (especially continuous mappings) is considered. Regularization theory and a theoretical framework for approximation (based on regularization techniques) that leads to a class of three-layer networks called regularization networks are discussed. Regularization networks are mathematically related to the radial basis functions, mainly used for strict interpolation tasks. Learning as approximation and learning as hypersurface reconstruction are discussed. Two extensions of the regularization approach are presented, along with the approach's corrections to splines, regularization, Bayes formulation, and clustering. The theory of regularization networks is generalized to a formulation that includes task-dependent clustering and dimensionality reduction. Applications of regularization networks are discussed. >"
            },
            "slug": "Networks-for-approximation-and-learning-Poggio-Girosi",
            "title": {
                "fragments": [],
                "text": "Networks for approximation and learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35191677"
                        ],
                        "name": "J. Hampshire",
                        "slug": "J.-Hampshire",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hampshire",
                            "middleNames": [
                                "B."
                            ],
                            "suffix": "II"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hampshire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1975998,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b4081a0b5d775172269854c07dadb0c07977806",
            "isKey": false,
            "numCitedBy": 162,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present single- and multispeaker recognition results for the voiced stop consonants /b, d, g/ using time-delay neural networks (TDNN), a new objective function for training these networks, and a simple arbitration scheme for improved classification accuracy. With these enhancements a median 24% reduction in the number of misclassifications made by TDNNs trained with the traditional backpropagation objective function is achieved. This redundant results in /b, d, g/ recognition rates that consistently exceed 98% for TDNNs trained with individual speakers; it yields a 98.1% recognition rate for a TDNN trained with three male speakers.<<ETX>>"
            },
            "slug": "A-novel-objective-function-for-improved-phoneme-Hampshire-Waibel",
            "title": {
                "fragments": [],
                "text": "A novel objective function for improved phoneme recognition using time delay neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The authors present single- and multispeaker recognition results for the voiced stop consonants /b, d, g/ using time-delay neural networks (TDNN), a new objective function for training these networks, and a simple arbitration scheme for improved classification accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144517651"
                        ],
                        "name": "T. Vetter",
                        "slug": "T.-Vetter",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Vetter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Vetter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 149
                            }
                        ],
                        "text": "\u2026either just 16 16 images, or images plus edge detection data), anddi erent classi ers: SV: Support Vector machine; MLP: fully connected perceptron withone hidden layer of 400 neurons; OF: oriented lter invariant feature extraction, see text;PC: quadratic polynomial classi er trained on the rst 50\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 245,
                                "start": 221
                            }
                        ],
                        "text": "Finally, in the third case, the invariance is achieved by changing the representationof the data by rst mapping them into a more suitable space; an approach pursued forinstance by Segman, Rubinstein, and Zeevi (1992), or Vetter and Poggio (1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 105
                            }
                        ],
                        "text": "Another intriguing extension of the scheme would be to use techniquesbased on image correspondence (e.g. Vetter and Poggio, 1997) to extract invariancetransformations from the training set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10047234,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "617b34332fcd1cb196f93656ee1d49561b81ebf8",
            "isKey": false,
            "numCitedBy": 471,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "The need to generate new views of a 3D object from a single real image arises in several fields, including graphics and object recognition. While the traditional approach relies on the use of 3D models, simpler techniques are applicable under restricted conditions. The approach exploits image transformations that are specific to the relevant object class, and learnable from example views of other \"prototypical\" objects of the same class. In this paper, we introduce such a technique by extending the notion of linear class proposed by the authors (1992). For linear object classes, it is shown that linear transformations can be learned exactly from a basis set of 2D prototypical views. We demonstrate the approach on artificial objects and then show preliminary evidence that the technique can effectively \"rotate\" high-resolution face images from a single 2D view."
            },
            "slug": "Linear-Object-Classes-and-Image-Synthesis-From-a-Vetter-Poggio",
            "title": {
                "fragments": [],
                "text": "Linear Object Classes and Image Synthesis From a Single Example Image"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "For linear object classes, it is shown that linear transformations can be learned exactly from a basis set of 2D prototypical views and preliminary evidence that the technique can effectively \"rotate\" high-resolution face images from a single 2D view is shown."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144517651"
                        ],
                        "name": "T. Vetter",
                        "slug": "T.-Vetter",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Vetter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Vetter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 159
                            }
                        ],
                        "text": "If instead ofLie groups of symmetry transformations one is dealing with discrete symmetries, as thebilateral symmetries of Vetter, Poggio, and B ultho (1994); Vetter and Poggio (1994),derivative-based methods such as the ones of Simard et al. (1992) are not applicable."
                    },
                    "intents": []
                }
            ],
            "corpusId": 25628199,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3c6dce15657b09846726dc1e3e442f5033503be0",
            "isKey": false,
            "numCitedBy": 89,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "According to the 1.5-views theorem (Poggio, Technical Report #9005-03, IRST, Povo, 1990; Ullman and Basri, IEEE Trans. PAMI 13, 992-1006, 1991) recognition of a specific 3D object (defined in terms of pointwise features) from a novel 2D view can be achieved from at least two 2D model views (for each object, for orthographic projection). This note considers how recognition can be achieved from a single 2D model view by exploiting prior knowledge of an object's symmetry. It is proved that, for any bilaterally symmetric 3D object, one non-accidental 2D model view is sufficient for recognition since it can be used to generate additional 'virtual' views. It is also proved that, for bilaterally symmetric objects, the correspondence of four points between two views determines the correspondence of all other points. Symmetries of higher order allow the recovery of Euclidean structure from a single 2D view."
            },
            "slug": "Symmetric-3D-objects-are-an-easy-case-for-2D-object-Vetter-Poggio",
            "title": {
                "fragments": [],
                "text": "Symmetric 3D objects are an easy case for 2D object recognition."
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is proved that, for any bilaterally symmetric 3D object, one non-accidental 2D model view is sufficient for recognition since it can be used to generate additional 'virtual' views."
            },
            "venue": {
                "fragments": [],
                "text": "Spatial vision"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144517651"
                        ],
                        "name": "T. Vetter",
                        "slug": "T.-Vetter",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Vetter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Vetter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2932365"
                        ],
                        "name": "N. Troje",
                        "slug": "N.-Troje",
                        "structuredName": {
                            "firstName": "Nikolaus",
                            "lastName": "Troje",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Troje"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 67
                            }
                        ],
                        "text": "D head model (fromthe MPI head database (Troje and B ultho , 1996; Vetter and Troje, 1997)); B: 2D imageof the rotated 3D head; C: arti cial image, generated from A using the assumption that itbelongs to a cylinder-shaped 3D object (rotation by the same angle as B)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 156
                            }
                        ],
                        "text": "This may require a trade-o : for some feature representations, theextraction process is di cult (e.g. using correspondence methods, Beymer and Poggio,1996; Vetter and Troje, 1997), whereas the computations of transformations might besimple."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14736398,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e970220d04ceaff31dbf03120213c41cd2f064c3",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Human faces differ in shape and texture. Image representations based on this separation of shape and texture information have been reported by several authors [for a review, see Science272, 1905 (1996)]. We investigate such a representation of human faces based on a separation of texture and two-dimensional shape information. Texture and shape were separated by use of pixel-by-pixel correspondence among the various images, which was established through algorithms known from optical flow computation. We demonstrate the improvement of the proposed representation over well-established pixel-based techniques in terms of coding efficiency and in terms of the ability to generalize to new images of faces. The evaluation is performed by calculating different distance measures between the original image and its reconstruction and by measuring the time that human subjects need to discriminate them."
            },
            "slug": "Separation-of-texture-and-shape-in-images-of-faces-Vetter-Troje",
            "title": {
                "fragments": [],
                "text": "Separation of texture and shape in images of faces for image coding and synthesis"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work investigates a representation of human faces based on a separation of texture and two-dimensional shape information and demonstrates the improvement of the proposed representation over well-established pixel-based techniques in terms of coding efficiency and the ability to generalize to new images of faces."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2190508"
                        ],
                        "name": "N. Grzywacz",
                        "slug": "N.-Grzywacz",
                        "structuredName": {
                            "firstName": "Norberto",
                            "lastName": "Grzywacz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Grzywacz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 124
                            }
                        ],
                        "text": "For in-stance, an RBF kernel thus corresponds to regularization with a functional containinga speci c di erential operator (Yuille and Grzywacz, 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 39137023,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "884886d3c701104083311ce57af00c9091bc1a37",
            "isKey": false,
            "numCitedBy": 148,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Tliere are a number of important phenom- ena in motion perception involving colicrcnce. Examples include motion capture and motion cooperativity. We propose a theoretical model, called the motion coherence tlieory, that gives a possible explanation for these effects (Yuille and Grzywacz, 1988a,b). In this framework, the aperture problem can also be thought of as a problem of coherence and given a similar explanation. We propose the concept of a velocity field dcfined everywhere in the image, even where there is no explicit motion information available. Through a cost function, tlie model imposes smoothness on the velocity field in a more general way than previous theories. In this paper, we provide a de- tailed theoretical analysis of the motion coherence theory. We discuss its relations with previous theories and show that some of t1ic.m arc approximations to it. A sccorid pa- per (Grzywacz, Smith, and Yuillc, 1088) provides exten- sions and cletnilcd comparisons to psychophysical plienom- cna. Tlic theory applies to both short-range and long- range motion. It places them in the same computational framework aiid provides a way to define interactions be- twcr:11 the two 1)roccsses."
            },
            "slug": "The-Motion-Coherence-Theory-Yuille-Grzywacz",
            "title": {
                "fragments": [],
                "text": "The Motion Coherence Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper proposes the concept of a velocity field everywhere in the image, even where there is no explicit motion information available, through a cost function, which imposes smoothness on the velocity field in a more general way than previous theories."
            },
            "venue": {
                "fragments": [],
                "text": "[1988 Proceedings] Second International Conference on Computer Vision"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 167309,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "20f6d89f13d8397b51f938f795e2666b4c0f33a9",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive the correspondence between regularization operators used in Regularization Networks and Hilbert Schmidt Kernels appearing in Support Vector Machines. More specifically, we prove that the Green's Functions associated with regularization operators are suitable Support Vector Kernels with equivalent regularization properties. As a by-product we show that a large number of Radial Basis Functions namely conditionally positive definite functions may be used as Support Vector kernels."
            },
            "slug": "From-Regularization-Operators-to-Support-Vector-Smola-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "From Regularization Operators to Support Vector Kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "It is proved that the Green's Functions associated with regularization operators are suitable Support Vector Kernels with equivalent regularization properties and a large number of Radial Basis Functions namely conditionally positive definite functions may be used as Support Vector kernels."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3065689"
                        ],
                        "name": "N. Matic",
                        "slug": "N.-Matic",
                        "structuredName": {
                            "firstName": "Nada",
                            "lastName": "Matic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Matic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8416014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1dd4eddb97fdf91156215c95803d688488aebef4",
            "isKey": false,
            "numCitedBy": 219,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for discovering informative patterns from data. With this method, large databases can be reduced to only a few representative data entries. Our framework encompasses also methods for cleaning databases containing corrupted data. Both on-line and off-line algorithms are proposed and experimentally checked on databases of handwritten images. The generality of the framework makes it an attractive candidate for new applications in knowledge discovery."
            },
            "slug": "Discovering-Informative-Patterns-and-Data-Cleaning-Guyon-Matic",
            "title": {
                "fragments": [],
                "text": "Discovering Informative Patterns and Data Cleaning"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A method for discovering informative patterns from data that can be reduced to only a few representative data entries and an attractive candidate for new applications in knowledge discovery is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Knowledge Discovery and Data Mining"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740300"
                        ],
                        "name": "D. Beymer",
                        "slug": "D.-Beymer",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Beymer",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Beymer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 135
                            }
                        ],
                        "text": "\u2026views were either just 16 16 images, or images plus edge detection data), anddi erent classi ers: SV: Support Vector machine; MLP: fully connected perceptron withone hidden layer of 400 neurons; OF: oriented lter invariant feature extraction, see text;PC: quadratic polynomial classi\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 132
                            }
                        ],
                        "text": "This may require a trade-o : for some feature representations, theextraction process is di cult (e.g. using correspondence methods, Beymer and Poggio,1996; Vetter and Troje, 1997), whereas the computations of transformations might besimple."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62531491,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "89121ed4d0d3db9bc192fd79f541fc299eba7d6b",
            "isKey": false,
            "numCitedBy": 304,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Computer vision researchers are developing new approaches to object recognition and detection that are based almost directly on images and avoid the use of intermediate three-dimensional models. Many of these techniques depend on a representation of images that induces a linear vector space structure and in principle requires dense feature correspondence. This image representation allows the use of learning techniques for the analysis of images (for computer vision) as well as for the synthesis of images (for computer graphics)."
            },
            "slug": "Image-Representations-for-Visual-Learning-Beymer-Poggio",
            "title": {
                "fragments": [],
                "text": "Image Representations for Visual Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Computer vision researchers are developing new approaches to object recognition and detection that are based almost directly on images and avoid the use of intermediate three-dimensional models."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143957317"
                        ],
                        "name": "R. C. Williamson",
                        "slug": "R.-C.-Williamson",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Williamson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Williamson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145004630"
                        ],
                        "name": "M. Anthony",
                        "slug": "M.-Anthony",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Anthony",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Anthony"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5280896,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "67f9e3de2fb39f051ef23b8fbed6d72de7b02900",
            "isKey": false,
            "numCitedBy": 101,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper introduces a framework for studying structural risk minimisation. The model views structural risk minimisation in a PAC context. It then considers the more general case when the hierarchy of classes is chosen in response to the data. This theoretically explains the impressive performance of the maximal margin hyperplane algorithm of Vapnik. It may also provide a general technique for exploitingserendipitous simplicity in observed data to obtain better prediction accuracy from small training sets."
            },
            "slug": "A-framework-for-structural-risk-minimisation-Shawe-Taylor-Bartlett",
            "title": {
                "fragments": [],
                "text": "A framework for structural risk minimisation"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "The paper introduces a framework for studying structural risk minimisation in a PAC context and considers the more general case when the hierarchy of classes is chosen in response to the data."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '96"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723766"
                        ],
                        "name": "H. Baird",
                        "slug": "H.-Baird",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Baird",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Baird"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 131
                            }
                        ],
                        "text": "4.1):In the rst case, the knowledge is used to generate arti cial training examples (\\vir-tual examples\", Poggio and Vetter, 1992; Baird, 1990) by transforming the trainingexamples accordingly."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61076153,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0eb7232dc0ceae32aad26c918a24e2775020d46",
            "isKey": true,
            "numCitedBy": 339,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "A lack of explicit quantitative models of imaging defects due to printing, optics, and digitization has retarded progress in some areas of document image analysis, including syntactic and structural approaches. Establishing the essential properties of such models, such as completeness (expressive power) and calibration (closeness of fit to actual image populations) remain open research problems. Work-in-progress towards a parameterized model of local imaging defects is described, together with a variety of motivating theoretical arguments and empirical evidence. A pseudo-random image generator implementing the model has been built. Applications of the generator are described, including a polyfont classifier for ASCII and a single-font classifier for a large alphabet (Tibetan U-Chen), both of which which were constructed with a minimum of manual effort. Image defect models and their associated generators permit a new kind of image database which is explicitly parameterized and indefinitely extensible, alleviating some drawbacks of existing databases."
            },
            "slug": "Document-image-defect-models-Baird",
            "title": {
                "fragments": [],
                "text": "Document image defect models"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Work-in-progress towards a parameterized model of local imaging defects is described, together with a variety of motivating theoretical arguments and empirical evidence, and a pseudo-random image generator implementing the model has been built."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35224059"
                        ],
                        "name": "S. Golowich",
                        "slug": "S.-Golowich",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Golowich",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Golowich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 19196574,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43ffa2c1a06a76e58a333f2e7d0bd498b24365ca",
            "isKey": false,
            "numCitedBy": 2603,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "The Support Vector (SV) method was recently proposed for estimating regressions, constructing multidimensional splines, and solving linear operator equations [Vapnik, 1995]. In this presentation we report results of applying the SV method to these problems."
            },
            "slug": "Support-Vector-Method-for-Function-Approximation,-Vapnik-Golowich",
            "title": {
                "fragments": [],
                "text": "Support Vector Method for Function Approximation, Regression Estimation and Signal Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This presentation reports results of applying the Support Vector method to problems of estimating regressions, constructing multidimensional splines, and solving linear operator equations."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144517651"
                        ],
                        "name": "T. Vetter",
                        "slug": "T.-Vetter",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Vetter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Vetter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 106
                            }
                        ],
                        "text": "4.1):In the rst case, the knowledge is used to generate arti cial training examples (\\vir-tual examples\", Poggio and Vetter, 1992; Baird, 1990) by transforming the trainingexamples accordingly."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3893740,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82962da5c273a9e6627a040d56c8a7973fe22440",
            "isKey": true,
            "numCitedBy": 192,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "In this note we discuss how recognition can be achieved from a single 2D model view exploiting prior knowledge of an object''s structure (e.g. symmetry). We prove that for any bilaterally symmetric 3D object one non- accidental 2D model view is sufficient for recognition. Symmetries of higher order allow the recovery of structure from one 2D view. Linear transformations can be learned exactly from a small set of examples in the case of \"linear object classes\" and used to produce new views of an object from a single view."
            },
            "slug": "Recognition-and-Structure-from-one-2D-Model-View:-Poggio-Vetter",
            "title": {
                "fragments": [],
                "text": "Recognition and Structure from one 2D Model View: Observations on Prototypes, Object Classes and Symmetries"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is proved that for any bilaterally symmetric 3D object one non- accidental 2D model view is sufficient for recognition and linear transformations can be learned exactly from a small set of examples in the case of \"linear object classes\"."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32641246"
                        ],
                        "name": "M. Nashed",
                        "slug": "M.-Nashed",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Nashed",
                            "middleNames": [
                                "Zuhair"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Nashed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 143
                            }
                        ],
                        "text": "\u2026(as it did in the case ofMercer kernels, cf. Dunford and Schwartz (1963); in fact, in the Mercer case, we evenhave trace class operators, cf. Nashed and Wahba (1974)), with a discrete spectrum,then the mapping (1.26) should no longer map into an l2 space, but into some separableHilbert\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17188204,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2989e0c4e3c4459846e4d878c0e7676d8370a536",
            "isKey": true,
            "numCitedBy": 109,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper a study of generalized inverses of linear operators in reproducing kernel Hilbert spaces (RKHS) is initiated. Explicit expressions for generalized inverses and minimal-norm solutions of linear operator equations in RKHS are obtained in several forms. The relation between the regularization operator of the equation $Af = g$ and the generalized inverse of the operator A in RKHS is demonstrated. In particular, it is shown that they are the same if the range of the operator is closed in an appropriate RKHS. Finally, properties of the regularized pseudosolutions in this setting are studied."
            },
            "slug": "Generalized-Inverses-in-Reproducing-Kernel-Spaces:-Nashed-Wahba",
            "title": {
                "fragments": [],
                "text": "Generalized Inverses in Reproducing Kernel Spaces: An Approach to Regularization of Linear Operator Equations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 15
                            }
                        ],
                        "text": "Haykin (1994); Ripley (1996) give overviews in the context of Neural Networks.1.3 Feature Space MathematicsThe present section summarizes some mathematical preliminaries which are essentialfor both Support Vector machines (Chapter 2) and nonlinear Kernel Principal Com-\n1.3."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9584248,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "877a887e7af7daebcb685e4d7b5e80f764035581",
            "isKey": true,
            "numCitedBy": 4042,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Title Type pattern recognition with neural networks in c++ PDF pattern recognition and neural networks PDF neural networks for pattern recognition advanced texts in econometrics PDF neural networks for applied sciences and engineering from fundamentals to complex pattern recognition PDF an introduction to biological and artificial neural networks for pattern recognition spie tutorial text vol tt04 tutorial texts in optical engineering PDF"
            },
            "slug": "Pattern-Recognition-and-Neural-Networks-LeCun-Bengio",
            "title": {
                "fragments": [],
                "text": "Pattern Recognition and Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785765"
                        ],
                        "name": "H. Krapp",
                        "slug": "H.-Krapp",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Krapp",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Krapp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6181236"
                        ],
                        "name": "R. Hengstenberg",
                        "slug": "R.-Hengstenberg",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Hengstenberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hengstenberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 169
                            }
                        ],
                        "text": "Of somewhat related interest are the large- eld neurons in the y's vi-sual system, coding for speci c ow elds which are generated by the y's movementin the environment (Krapp and Hengstenberg, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4360263,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "3263b41937a180c700437fc5a061373ea989c9c5",
            "isKey": false,
            "numCitedBy": 421,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "HUMANS, animals and some mobile robots use visual motion cues for object detection and navigation in structured surroundings1\u20134. Motion is commonly sensed by large arrays of small field movement detectors, each preferring motion in a particular direction5,6. Self-motion generates distinct 'optic flow fields' in the eyes that depend on the type and direction of the momentary locomotion (rotation, translation) 7. To investigate how the optic flow is processed at the neuronal level, we recorded intracellularly from identified interneurons in the third visual neuropile of the blowfly8. The distribution of local motion tuning over their huge receptive fields was mapped in detail. The global structure of the resulting 'motion response fields' is remarkably similar to optic flow fields. Thus, the organization of the receptive fields of the so-called VS neurons9,10 strongly suggests that each of these neurons specifically extracts the rotatory component of the optic flow around a particular horizontal axis. Other neurons are probably adapted to extract translatory flow components. This study shows how complex visual discrimination can be achieved by task-oriented preprocessing in single neurons."
            },
            "slug": "Estimation-of-self-motion-by-optic-flow-processing-Krapp-Hengstenberg",
            "title": {
                "fragments": [],
                "text": "Estimation of self-motion by optic flow processing in single visual interneurons"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This study shows how complex visual discrimination can be achieved by task-oriented preprocessing in single neurons by recording intracellularly from identified interneurons in the third visual neuropile of the blowfly."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12946615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b07ce649d6f6eb636872527104b0209d3edc8188",
            "isKey": false,
            "numCitedBy": 16926,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Provides a unified, comprehensive and up-to-date treatment of both statistical and descriptive methods for pattern recognition. The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "slug": "Pattern-classification-and-scene-analysis-Duda-Hart",
            "title": {
                "fragments": [],
                "text": "Pattern classification and scene analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "venue": {
                "fragments": [],
                "text": "A Wiley-Interscience publication"
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053520352"
                        ],
                        "name": "M. Kirby",
                        "slug": "M.-Kirby",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kirby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kirby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49555086"
                        ],
                        "name": "L. Sirovich",
                        "slug": "L.-Sirovich",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Sirovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Sirovich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 150
                            }
                        ],
                        "text": "\u2026this problem, however, is to use kernel functions (1.14) | we exclusively need2Note that in our derivation we could have used the known result (e.g. Kirby & Sirovich, 1990)that PCA can be carried out on the dot product matrix (xi xj)ij instead of (3.1), however, for thesake of clarity and\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 570648,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66d75a5fe9e1b6511c5135d68e9ce8c0da5a7374",
            "isKey": true,
            "numCitedBy": 2852,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of natural symmetries (mirror images) in a well-defined family of patterns (human faces) is discussed within the framework of the Karhunen-Loeve expansion. This results in an extension of the data and imposes even and odd symmetry on the eigenfunctions of the covariance matrix, without increasing the complexity of the calculation. The resulting approximation of faces projected from outside of the data set onto this optimal basis is improved on average. >"
            },
            "slug": "Application-of-the-Karhunen-Loeve-Procedure-for-the-Kirby-Sirovich",
            "title": {
                "fragments": [],
                "text": "Application of the Karhunen-Loeve Procedure for the Characterization of Human Faces"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "The use of natural symmetries (mirror images) in a well-defined family of patterns (human faces) is discussed within the framework of the Karhunen-Loeve expansion, which results in an extension of the data and imposes even and odd symmetry on the eigenfunctions of the covariance matrix."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2880906"
                        ],
                        "name": "V. Blanz",
                        "slug": "V.-Blanz",
                        "structuredName": {
                            "firstName": "Volker",
                            "lastName": "Blanz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Blanz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747836"
                        ],
                        "name": "H. B\u00fclthoff",
                        "slug": "H.-B\u00fclthoff",
                        "structuredName": {
                            "firstName": "Heinrich",
                            "lastName": "B\u00fclthoff",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. B\u00fclthoff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144517651"
                        ],
                        "name": "T. Vetter",
                        "slug": "T.-Vetter",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Vetter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Vetter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 276,
                                "start": 258
                            }
                        ],
                        "text": "The rst one used oriented lters to extract featureswhich are robust with respect to small rigid transformations of the underlying 3-Dobjects, followed by a decision stage based on comparisons with stored templates (fordetails, see Blanz, 1995; Vetter, 1994; Blanz et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 78
                            }
                        ],
                        "text": "2.11, Appendix A), di er-ent view{based recognition algorithms were compared (Blanz et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 78
                            }
                        ],
                        "text": "The training and test data was generated according to the following procedure(Blanz et al., 1996; Liter et al., 1997):Database GenerationSnapshot Sampling."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 855426,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a0f97e889f6fb4b71704f079407a5ef730ad95f",
            "isKey": false,
            "numCitedBy": 248,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Two view-based object recognition algorithms are compared: (1) a heuristic algorithm based on oriented filters, and (2) a support vector learning machine trained on low-resolution images of the objects. Classification performance is assessed using a high number of images generated by a computer graphics system under precisely controlled conditions. Training- and test-images show a set of 25 realistic three-dimensional models of chairs from viewing directions spread over the upper half of the viewing sphere. The percentage of correct identification of all 25 objects is measured."
            },
            "slug": "Comparison-of-View-Based-Object-Recognition-Using-Blanz-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "Comparison of View-Based Object Recognition Algorithms Using Realistic 3D Models"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "Two view-based object recognition algorithms are compared: a heuristic algorithm based on oriented filters, and a support vector learning machine trained on low-resolution images of the objects."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 182
                            }
                        ],
                        "text": "Classes with some internal structure can be com-pressed, hence a smaller representation is possible, which in turn makes generalizationto novel views possible (cf. Kolmogorov, 1965; Rissanen, 1978)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 192
                            }
                        ],
                        "text": "\u2026in the trade-o between reducing the training er-ror and limiting model complexity, for instance as described by regularization theory(Tikhonov and Arsenin, 1977), Minimum Description Length (Rissanen, 1978; Kol-mogorov, 1965), or the Bias-Variance Dilemma (Geman, Bienenstock, and Doursat,1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 303,
                                "start": 277
                            }
                        ],
                        "text": "We conclude this section by noting that analyses in other branches of learningtheory have led to similar insights in the trade-o between reducing the training er-ror and limiting model complexity, for instance as described by regularization theory(Tikhonov and Arsenin, 1977), Minimum Description Length (Rissanen, 1978; Kol-mogorov, 1965), or the Bias-Variance Dilemma (Geman, Bienenstock, and Doursat,1992)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 63
                            }
                        ],
                        "text": "This can be viewed in the context of Algorithmic Complexity andMinimum Description Length (Vapnik, 1995b, Chapter 5, footnote 6)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 6
                            }
                        ],
                        "text": "4% on the entry level and animal databases, respectively."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 30140639,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d382b9c11e5c6a8e173fbeb442545e3be8d3e3a5",
            "isKey": true,
            "numCitedBy": 6261,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Modeling-By-Shortest-Data-Description*-Rissanen",
            "title": {
                "fragments": [],
                "text": "Modeling By Shortest Data Description*"
            },
            "venue": {
                "fragments": [],
                "text": "Autom."
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 23
                            }
                        ],
                        "text": "To see this, we follow Wahba (1973) and recall that a RKHS is a Hilbert space offunctions f on some set C such that all evaluation functionals f 7! f(y) (y 2 C) arecontinuous."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18913893,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2d18f5188f459e42cc20c5960ecb26f9daf9687d",
            "isKey": true,
            "numCitedBy": 61,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Convergence-rates-of-certain-approximate-solutions-Wahba",
            "title": {
                "fragments": [],
                "text": "Convergence rates of certain approximate solutions to Fredholm integral equations of the first kind"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "73293614"
                        ],
                        "name": "C. Duffy",
                        "slug": "C.-Duffy",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Duffy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Duffy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143670798"
                        ],
                        "name": "R. Wurtz",
                        "slug": "R.-Wurtz",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Wurtz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Wurtz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14124318,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "c1a2b47a1c2883995a5aeddcadd87dae58c64722",
            "isKey": false,
            "numCitedBy": 895,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Neurons in the dorsomedial region of the medial superior temporal area (MSTd) have large receptive fields that include the fovea, are directionally selective for moving visual stimuli, prefer the motion of large fields to small spots, and respond to rotating and expanding patterns of motion as well as frontal parallel planar motion. These characteristics suggested that these neurons might contribute to the analysis of the large-field optic flow stimulation generated as an observer moves through the visual environment. 2. We tested the response of MSTd neurons in two awake monkeys by systematically presenting a set of translational and rotational stimuli to each neuron. These 100 X 100 degrees stimuli were the motion components from which all optic flow fields are derived. 3. In 220 single neurons we found 23% that responded primarily to one component of motion (planar, circular, or radial), 34% that responded to two components (planocircular or planoradial, but never circuloradial), and 29% that responded to all three components. 4. The number of stimulus components to which a neuron responded was unrelated to the size or eccentricity of its receptive field. 5. Triple-, double-, and single-component neurons varied widely in the strength of their responses to the preferred components. Grouping these neurons together revealed that they did not form discrete classes but rather a continuum of response selectivity. 6. This continuum was apparent in other response characteristics. Direction selectivity was weakest in triple-component neurons, strongest in single-component neurons. Significant inhibitory responses were less frequent in triple-component neurons than in single-component neurons. 7. There was some indication that the neurons of similar component classes occupied adjacent regions within MSTd, but all combinations of component and direction selectivity were occasionally found in immediate juxtaposition. 8. Experiments on a subset of neurons showed that the speed of motion, the dot density, and the number of different speed planes in the display had little influence on these responses. 9. We conclude that the selective responses of many MSTd neurons to the rotational and translational components of optic flow make these neurons reasonable candidates for contributing to the analysis of optic flow fields."
            },
            "slug": "Sensitivity-of-MST-neurons-to-optic-flow-stimuli.-A-Duffy-Wurtz",
            "title": {
                "fragments": [],
                "text": "Sensitivity of MST neurons to optic flow stimuli. I. A continuum of response selectivity to large-field stimuli."
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is concluded that the selective responses of many MSTd neurons to the rotational and translational components of optic flow make these neurons reasonable candidates for contributing to the analysis of fiber optic flow fields."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of neurophysiology"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2932365"
                        ],
                        "name": "N. Troje",
                        "slug": "N.-Troje",
                        "structuredName": {
                            "firstName": "Nikolaus",
                            "lastName": "Troje",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Troje"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747836"
                        ],
                        "name": "H. B\u00fclthoff",
                        "slug": "H.-B\u00fclthoff",
                        "structuredName": {
                            "firstName": "Heinrich",
                            "lastName": "B\u00fclthoff",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. B\u00fclthoff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12192253,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "dc43fad3c4ef79ef437ad2eec1ce22240d2011e6",
            "isKey": false,
            "numCitedBy": 100,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "How-is-bilateral-symmetry-of-human-faces-used-for-Troje-B\u00fclthoff",
            "title": {
                "fragments": [],
                "text": "How is bilateral symmetry of human faces used for recognition of novel views?"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15109515,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "51c1519a57a65351a713a3d74f8d477105df0ec3",
            "isKey": false,
            "numCitedBy": 352,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We explore methods for incorporating prior knowledge about a problem at hand in Support Vector learning machines. We show that both invariances under group transformations and prior knowledge about locality in images can be incorporated by constructing appropriate kernel functions."
            },
            "slug": "Prior-Knowledge-in-Support-Vector-Kernels-Sch\u00f6lkopf-Simard",
            "title": {
                "fragments": [],
                "text": "Prior Knowledge in Support Vector Kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "It is shown that both invariances under group transformations and prior knowledge about locality in images can be incorporated by constructing appropriate kernel functions by exploring methods for incorporating prior knowledge in Support Vector learning machines."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69924093"
                        ],
                        "name": "S. Hyakin",
                        "slug": "S.-Hyakin",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Hyakin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hyakin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Haykin (1994); Ripley (1996) give overviews in the context of Neural Networks.1.3 Feature Space MathematicsThe present section summarizes some mathematical preliminaries which are essentialfor both Support Vector machines (Chapter 2) and nonlinear Kernel Principal Com-\n1.3."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60577818,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "045310b06e8a3983a363a118cc9dcc3f292970b4",
            "isKey": true,
            "numCitedBy": 9896,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Simon Haykin Neural Networks A Comprehensive Foundation. Neural Networks A Comprehensive Foundation Simon S. Neural Networks A Comprehensive Foundation Simon S. Neural Networks A Comprehensive Foundation. Neural Networks Association for Computing Machinery. Book Review Neural Networks A Comprehensive Foundation. Neural Networks A Comprehensive Foundation Pearson. Neural networks a comprehensive foundation. Neural Networks a Comprehensive Foundation AbeBooks. Neural networks a comprehensive foundation solutions. cdn preterhuman net. Neural Networks A Comprehensive Foundation Goodreads. Neural Networks A Comprehensive Foundation Amazon it. Neural Networks A Comprehensive Foundation Amazon co uk. Neural Networks A Comprehensive Foundation 3rd Edition. Neural Networks A Comprehensive Foundation Simon. Neural Networks A Comprehensive Foundation amazon com. Neural networks a comprehensive foundation Academia edu. Neural Networks A Comprehensive Foundation Amazon. neural networks a comprehensive foundation simon haykin. Simon Haykin Neural Networks A Comprehensive Foundation. Neural Networks A comprehensive Foundation 2 ed. Simon haykin neural networks a comprehensive foundation pdf. Buy Neural Networks A Comprehensive Foundation Book. Neural networks a comprehensive foundation 2e book. Neural Networks A Comprehensive Foundation. NEURAL NETWORKS A COMPREHENSIVE FOUNDATION SIMON. Neural Networks a Comprehensive Foundation by Haykin Simon. Neural Networks A Comprehensive Foundation pdf PDF Drive. Neural Networks A Comprehensive Foundation amazon ca. Simon Haykin Neural Networks A Comprehensive Foundation. NEURAL NETWORKS A Comprehensive Foundation PDF. Neural Networks A Comprehensive Foundation pdf PDF Drive. Neural Networks A Comprehensive Foundation by Haykin. Neural Networks A Comprehensive Foundation 3rd Edition. Neural Networks A Comprehensive Foundation Simon S. Neural Networks A Comprehensive Foundation. Neural networks a comprehensive foundation Book 1994. Neural Networks A Comprehensive Foundation 2nd Edition. Neural Networks A Comprehensive Foundation S S Haykin. Neural Networks A Comprehensive Foundation International. Neural Networks A Comprehensive Foundation 2 e Pearson. Download Neural Networks A Comprehensive Foundation 2Nd. Neural Networks A comprehensive foundation Aalto"
            },
            "slug": "Neural-Networks:-A-Comprehensive-Foundation-Hyakin",
            "title": {
                "fragments": [],
                "text": "Neural Networks: A Comprehensive Foundation"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "Simon Haykin Neural Networks A Comprehensive Foundation Simon S. Haykin neural networks a comprehensive foundation pdf PDF Drive."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3092674"
                        ],
                        "name": "Joseph Segman",
                        "slug": "Joseph-Segman",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Segman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph Segman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144866196"
                        ],
                        "name": "J. Rubinstein",
                        "slug": "J.-Rubinstein",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Rubinstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rubinstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804190"
                        ],
                        "name": "Y. Zeevi",
                        "slug": "Y.-Zeevi",
                        "structuredName": {
                            "firstName": "Yehoshua",
                            "lastName": "Zeevi",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Zeevi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 180
                            }
                        ],
                        "text": "Finally, in the third case, the invariance is achieved by changing the representationof the data by rst mapping them into a more suitable space; an approach pursued forinstance by Segman, Rubinstein, and Zeevi (1992), or Vetter and Poggio (1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 39290518,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "fde0f39c995014bc27423533dc7121227e21c175",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "A method for the analysis of deformed patterns is presented and analyzed. The image is transformed into a new set of coordinates in which the deformation has a particular simple form. A number of deformations are considered. The practical implementation of the method is discussed. Similar aspects of biological vision are also considered. >"
            },
            "slug": "The-Canonical-Coordinates-Method-for-Pattern-and-Segman-Rubinstein",
            "title": {
                "fragments": [],
                "text": "The Canonical Coordinates Method for Pattern Deformation: Theoretical and Computational Considerations"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "A method for the analysis of deformed patterns is presented and analyzed where the image is transformed into a new set of coordinates in which the deformation has a particular simple form."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 36
                            }
                        ],
                        "text": "Initiated by the pioneering work of Oja (1982), a number ofunsupervised neural-network type algorithms computing principal components havebeen proposed (e.g. Sanger, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16577977,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "3e00dd12caea7c4dab1633a35d1da3cb2e76b420",
            "isKey": false,
            "numCitedBy": 2357,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "A simple linear neuron model with constrained Hebbian-type synaptic modification is analyzed and a new class of unconstrained learning rules is derived. It is shown that the model neuron tends to extract the principal component from a stationary input vector sequence."
            },
            "slug": "Simplified-neuron-model-as-a-principal-component-Oja",
            "title": {
                "fragments": [],
                "text": "Simplified neuron model as a principal component analyzer"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A simple linear neuron model with constrained Hebbian-type synaptic modification is analyzed and a new class of unconstrained learning rules is derived."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of mathematical biology"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143706693"
                        ],
                        "name": "M. Mishkin",
                        "slug": "M.-Mishkin",
                        "structuredName": {
                            "firstName": "Mortimer",
                            "lastName": "Mishkin",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mishkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830863"
                        ],
                        "name": "Leslie G. Ungerleider",
                        "slug": "Leslie-G.-Ungerleider",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Ungerleider",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Leslie G. Ungerleider"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6680922"
                        ],
                        "name": "K. Macko",
                        "slug": "K.-Macko",
                        "structuredName": {
                            "firstName": "Kathleen",
                            "lastName": "Macko",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Macko"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 93
                            }
                        ],
                        "text": "Rao and Ballard (1997) recently proposed a model in which the \\what\" and the\\where\" pathway (Mishkin, Ungerleider, and Macko, 1983) in the visual system areconceived of as estimating object identity and transformations, respectively."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15565609,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "a1430265eb509e214d2bbbb04adc8e87f3589863",
            "isKey": false,
            "numCitedBy": 2573,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Object-vision-and-spatial-vision:-two-cortical-Mishkin-Ungerleider",
            "title": {
                "fragments": [],
                "text": "Object vision and spatial vision: two cortical pathways"
            },
            "venue": {
                "fragments": [],
                "text": "Trends in Neurosciences"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2545803"
                        ],
                        "name": "M. Aizerman",
                        "slug": "M.-Aizerman",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Aizerman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Aizerman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 272,
                                "start": 234
                            }
                        ],
                        "text": "\u20261.3.2), one can construct a mapping into a space where kacts as a dot product, ( (x) (y)) = k(x;y): (1.27)Besides (1.17), Boser, Guyon, and Vapnik (1992) and Vapnik (1995b) suggest theusage of Gaussian radial basis function kernels (Aizerman, Braverman, and Rozonoer,1964) k(x;y) = exp kx yk22 2 !"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 0
                            }
                        ],
                        "text": "Aizerman, Braverman, and Rozonoer(1964) call F the linearization space, and use it in the context of the potential functionclassi cation method to express the dot product between elements of F in terms ofelements of the input space."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60493317,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3caf34c1c86633b6e80dca29e3cb2b6367a0f93",
            "isKey": true,
            "numCitedBy": 1692,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Theoretical-Foundations-of-the-Potential-Function-Aizerman",
            "title": {
                "fragments": [],
                "text": "Theoretical Foundations of the Potential Function Method in Pattern Recognition Learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2135764"
                        ],
                        "name": "A. Kolmogorov",
                        "slug": "A.-Kolmogorov",
                        "structuredName": {
                            "firstName": "Andrei",
                            "lastName": "Kolmogorov",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kolmogorov"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 164
                            }
                        ],
                        "text": "Classes with some internal structure can be com-pressed, hence a smaller representation is possible, which in turn makes generalizationto novel views possible (cf. Kolmogorov, 1965; Rissanen, 1978)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 119745517,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "83077865493592cd8ebc5c9a8b900521428491ad",
            "isKey": false,
            "numCitedBy": 2634,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Three-approaches-to-the-quantitative-definition-of-Kolmogorov",
            "title": {
                "fragments": [],
                "text": "Three approaches to the quantitative definition of information"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101655384"
                        ],
                        "name": "K. Karhunen",
                        "slug": "K.-Karhunen",
                        "structuredName": {
                            "firstName": "Kari",
                            "lastName": "Karhunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Karhunen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 162
                            }
                        ],
                        "text": "For reviews of the existing literature, see Jolli e (1986) and Diamantaras &Kung (1996); some of the classical papers are due to Pearson (1901); Hotelling (1933);Karhunen (1946)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 118738283,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8ab83ad89b2dedd642246d68ded065fec63ffa79",
            "isKey": false,
            "numCitedBy": 469,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Zur-Spektraltheorie-stochastischer-prozesse-Karhunen",
            "title": {
                "fragments": [],
                "text": "Zur Spektraltheorie stochastischer prozesse"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1946
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144246983"
                        ],
                        "name": "H. Barlow",
                        "slug": "H.-Barlow",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Barlow",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Barlow"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 113
                            }
                        ],
                        "text": "In our hope that this type of theoretical work should be ofinterest to people studying the brain, we concur with Barlow (1995):\\If arti cial neural nets, designed to imitate cognitive functions of thebrain, are truly performing tasks that are best formulated in statisticalterms, then is this not\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 142990266,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "30d9f117fc23297b1f3445d09077fa8dd28b7bbf",
            "isKey": false,
            "numCitedBy": 172,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-neuron-doctrine-in-perception.-Barlow",
            "title": {
                "fragments": [],
                "text": "The neuron doctrine in perception."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144565505"
                        ],
                        "name": "F. Klein",
                        "slug": "F.-Klein",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Klein",
                            "middleNames": [
                                "Von"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Klein"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60620433,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "2f8fe09675cbeeccf3dba6fd5dd0f95d0ee48057",
            "isKey": false,
            "numCitedBy": 383,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Programm zum eintritt in die Philosophische facultat und den Senat der K. Friedrich-Alexanders-universitat zu Erlangen."
            },
            "slug": "Vergleichende-Betrachtungen-\u00fcber-neuere-Forschungen-Klein",
            "title": {
                "fragments": [],
                "text": "Vergleichende Betrachtungen \u00fcber neuere geometrische Forschungen"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Programm zum eintritt in die Philosophische facultat und den Senat der K. Friedrich-Alexanders-universitat zu Erlangen."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1872
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1390055667"
                        ],
                        "name": "Dr. M. G. Worster",
                        "slug": "Dr.-M.-G.-Worster",
                        "structuredName": {
                            "firstName": "Dr.",
                            "lastName": "Worster",
                            "middleNames": [
                                "M.",
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dr. M. G. Worster"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 4079036,
            "fieldsOfStudy": [
                "Education",
                "Physics"
            ],
            "id": "757dd5fbc67d8f2591eb2077180af74c1797fcd1",
            "isKey": false,
            "numCitedBy": 2429,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The following people have maintained these notes."
            },
            "slug": "Methods-of-Mathematical-Physics-Worster",
            "title": {
                "fragments": [],
                "text": "Methods of Mathematical Physics"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1947
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143639508"
                        ],
                        "name": "A. Tikhonov",
                        "slug": "A.-Tikhonov",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Tikhonov",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tikhonov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102992139"
                        ],
                        "name": "Vasiliy Yakovlevich Arsenin",
                        "slug": "Vasiliy-Yakovlevich-Arsenin",
                        "structuredName": {
                            "firstName": "Vasiliy",
                            "lastName": "Arsenin",
                            "middleNames": [
                                "Yakovlevich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vasiliy Yakovlevich Arsenin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 117
                            }
                        ],
                        "text": "De ne : x 7! k(x; :); (D.32)where k is some a priori speci ed kernel, and T = P P , with a regularization operatorP (Tikhonov and Arsenin, 1977)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 89
                            }
                        ],
                        "text": "For kernel-based functionexpansions, it is shown that given a regularization operator P (Tikhonov and Arsenin,1977) mapping the functions of the learning machine into some dot product space D,the problem of minimizing the regularized riskRreg[f ] = Remp[f ] + kPfk2; (2.53)(with a regularization\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 144
                            }
                        ],
                        "text": "\u2026insights in the trade-o between reducing the training er-ror and limiting model complexity, for instance as described by regularization theory(Tikhonov and Arsenin, 1977), Minimum Description Length (Rissanen, 1978; Kol-mogorov, 1965), or the Bias-Variance Dilemma (Geman, Bienenstock, and\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 28
                            }
                        ],
                        "text": "The SV machineresults reported in the following were obtained with our default choice = 10 (cf.(2.19), Sec."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122072756,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "bc14819e745cd7af37efd09ea29773dc0065119e",
            "isKey": true,
            "numCitedBy": 7884,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Solutions-of-ill-posed-problems-Tikhonov-Arsenin",
            "title": {
                "fragments": [],
                "text": "Solutions of ill-posed problems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3267844"
                        ],
                        "name": "K. Schittkowski",
                        "slug": "K.-Schittkowski",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Schittkowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Schittkowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2907488"
                        ],
                        "name": "Christian Zillober",
                        "slug": "Christian-Zillober",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Zillober",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Zillober"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 66
                            }
                        ],
                        "text": "According to the Kuhn-Tucker theorem of optimization theory (e.g. Bertsekas,1995), at the saddle point only those Lagrange multipliers i can be nonzero whichcorrespond to constraints (2.6) which are precisely met, i.e. i [yi((zi w) + b) 1] = 0; i = 1; : : : ; `: (2.12)The patterns zi for which i >\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 280,
                                "start": 265
                            }
                        ],
                        "text": "If we leave out a pattern zi and construct the solutionfrom the remaining patterns, there are several possibilities (cf. (2.6)):2This terminology is related to corresponding terms in the theory of convex sets, relevant to convexoptimization (e.g. Luenberger, 1973; Bertsekas, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1895,
                                "start": 0
                            }
                        ],
                        "text": "Bertsekas, 1995), at the saddle point only those Lagrange multipliers i can be nonzero which correspond to constraints (2.6) which are precisely met, i.e. i [yi((zi w) + b) 1] = 0; i = 1; : : : ; `: (2.12) The patterns zi for which i > 0 are called Support Vectors.2 According to (2.12), they lie exactly at the margin.3 All remaining examples of the training set are irrelevant: their constraint (2.6) is satis ed automatically, and they do not appear in the expansion (2.11).4 This leads directly to an upper bound on the generalization ability of optimal margin hyperplanes: suppose we use the leave-one-out method to estimate the expected test error (e.g. Vapnik, 1979). If we leave out a pattern zi and construct the solution from the remaining patterns, there are several possibilities (cf. (2.6)): 2This terminology is related to corresponding terms in the theory of convex sets, relevant to convex optimization (e.g. Luenberger, 1973; Bertsekas, 1995). Given any boundary point of a convex set C, there always exists a hyperplane separating the point from the interior of the set. This is called a supporting hyperplane. SVs do lie on the boundary of the convex hulls of the two classes, thus they possess supporting hyperplanes. The SV optimal hyperplane is the hyperplane which lies in the middle of the two parallel supporting hyperplanes (of the two classes) with maximum distance. Vice versa, from the optimal hyperplane one can obtain supporting hyperplanes for all SVs of both classes by shifting it by 1=kwk in both directions. 3Note that this implies that the solution (w; b), where b is computed using the fact that yi((w zi) + b) = 1 for SVs, is in canonical form with respect to the training data. (This makes use of the reasonable assumption that the training set contains both positive and negative examples.) 4In a statistical mechanics framework, Anlauf and Biehl (1989) have put forward a similar argument for the optimal stability perceptron, also computed by contrained optimization."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 119
                            }
                        ],
                        "text": "6)): 2This terminology is related to corresponding terms in the theory of convex sets, relevant to convex optimization (e.g. Luenberger, 1973; Bertsekas, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 44060508,
            "fieldsOfStudy": [],
            "id": "d4143c46910f249bedbdc37caf88e4c292124c08",
            "isKey": true,
            "numCitedBy": 6357,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "NONLINEAR-PROGRAMMING-Schittkowski-Zillober",
            "title": {
                "fragments": [],
                "text": "NONLINEAR PROGRAMMING"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144436414"
                        ],
                        "name": "Mark S. C. Reed",
                        "slug": "Mark-S.-C.-Reed",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Reed",
                            "middleNames": [
                                "S.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark S. C. Reed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055346903"
                        ],
                        "name": "H. Simon",
                        "slug": "H.-Simon",
                        "structuredName": {
                            "firstName": "Herbert",
                            "lastName": "Simon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Simon"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 79
                            }
                        ],
                        "text": "L2(R; ); (D.24)and is a probability measure (the spectral measure of T ) (e.g. Reed and Simon,1980)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 56
                            }
                        ],
                        "text": "In that case, by the Riesz representation theorem (e.g. Reed and Simon,1980), for each y 2 C there exists a unique function of x, call it k(x;y), such thatf(y) = hf; k(:;y)i (1.30)(here, k(:;y) is the function on C obtained by xing the second argument of k toy, and h:; :i is the dot product of the\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 71
                            }
                        ],
                        "text": "The sameargument, however, also works in the case of unbounded T (e.g. Reed and Simon,1980)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 115957504,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "539f4cdfd83a6e3487c4134509c9eb687c145dc0",
            "isKey": true,
            "numCitedBy": 7648,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Method-of-Modern-Mathematical-Physics-Reed-Simon",
            "title": {
                "fragments": [],
                "text": "Method of Modern Mathematical Physics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 76
                            }
                        ],
                        "text": "The USPS database has been criticised (Burges, LeCun,private communication; Bottou et al. (1994)) as not providing the most adequateclassi er benchmark."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 207
                            }
                        ],
                        "text": "The best single machine in the performance comparisons so farwas a LeNet5 convolutional neural network (0.9%); other high performance systemsinclude Tangent Distance nearest neighbour classi ers (1.1%), and LeNet4 with a lastlayer using methods of local learning (1.1%, cf. Bottou and Vapnik, 1992)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 142
                            }
                        ],
                        "text": "This database has become the standardfor performance comparisons at AT&T Bell Labs; the error rate record of 0.7% is heldby a boosted LeNet4 (Bottou et al., 1994; LeCun et al., 1995), i.e. by an ensembleof learning machines."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 60
                            }
                        ],
                        "text": "The values gj(x) can also be used forreject decisions (e.g. Bottou et al., 1994), for instance by considering the di erencebetween the maximum and the second highest value as a measure of con dence in theclassi cation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 96
                            }
                        ],
                        "text": "PRIOR KNOWLEDGE IN SUPPORT VECTOR MACHINESmachines are known to allow for short training times (Bottou et al., 1994), the rstpoint is usually not critical."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 48
                            }
                        ],
                        "text": "In the above benchmark studies, only theboosted LeNet4 ensemble performed better (0.5%)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 100
                            }
                        ],
                        "text": "The images were rst size normalized to tinto a 20 20 pixel box, and then centered in a 28 28 image (Bottou et al., 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 271,
                                "start": 252
                            }
                        ],
                        "text": "\u2026followed by the animal database, and by the subordinate level chair database.2.3 Digit Recognition Using Di erent KernelsHandwritten digit recognition has long served as a test bed for evaluating and bench-marking classi ers (e.g. LeCun et al., 1989; Bottou et al., 1994; LeCun et al., 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 196
                            }
                        ],
                        "text": "Another interesting performance measure is the rejection error rate, de ned as thepercentage of patterns that would have to be rejected to attain a speci ed error rate (inthe benchmark studies of Bottou et al. (1994) and LeCun et al. (1995), 0.5%)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Comparison of classiier methods: a case study in handwritten digit recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 12th International Conference on Pattern Recognition and Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Ullman (1996) has put forward a multiple-view variant of his theory of \\recognition by alignment\", where objects are recognized by aligning them with stored view templates."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 311,
                                "start": 0
                            }
                        ],
                        "text": "Ullman (1996) has put forward a multiple-view variant of his theory of \\recognition by alignment\", where objects are recognized by aligning them with stored view templates. The alignment process can make use of certain transformations speci c to the object in question. The results of Troje and B\u007f ultho (1996) have shown that these transformations in some cases directly operate on 2-D views, and that they are much simpler than transformations using an underlying 3-D model: in experiments probing face recognition under varying poses, observers performed better on views which were obtained by simply applying a mirror reversal transformation to a previously seen view, rather than by rotating the head in depth to generate the true view of the other side."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Ullman (1996) has put forward a multiple-view variant of his theory of \\recognitionby alignment\", where objects are recognized by aligning them with stored view tem-plates."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 783,
                                "start": 0
                            }
                        ],
                        "text": "Ullman (1996) has put forward a multiple-view variant of his theory of \\recognition by alignment\", where objects are recognized by aligning them with stored view templates. The alignment process can make use of certain transformations speci c to the object in question. The results of Troje and B\u007f ultho (1996) have shown that these transformations in some cases directly operate on 2-D views, and that they are much simpler than transformations using an underlying 3-D model: in experiments probing face recognition under varying poses, observers performed better on views which were obtained by simply applying a mirror reversal transformation to a previously seen view, rather than by rotating the head in depth to generate the true view of the other side. Rao and Ballard (1997) recently proposed a model in which the \\what\" and the \\where\" pathway (Mishkin, Ungerleider, and Macko, 1983) in the visual system are conceived of as estimating object identity and transformations, respectively."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "High-Level Vision"
            },
            "venue": {
                "fragments": [],
                "text": "High-Level Vision"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 425,
                                "start": 306
                            }
                        ],
                        "text": "We conclude this section by noting that analyses in other branches of learning theory have led to similar insights in the trade-o between reducing the training error and limiting model complexity, for instance as described by regularization theory (Tikhonov and Arsenin, 1977), Minimum Description Length (Rissanen, 1978; Kolmogorov, 1965), or the Bias-Variance Dilemma (Geman, Bienenstock, and Doursat, 1992). Haykin (1994); Ripley (1996) give overviews in the context of Neural Networks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 159
                            }
                        ],
                        "text": "Classes with some internal structure can be compressed, hence a smaller representation is possible, which in turn makes generalization to novel views possible (cf. Kolmogorov, 1965; Rissanen, 1978)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 182
                            }
                        ],
                        "text": "Classes with some internal structure can be com-pressed, hence a smaller representation is possible, which in turn makes generalizationto novel views possible (cf. Kolmogorov, 1965; Rissanen, 1978)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 192
                            }
                        ],
                        "text": "\u2026in the trade-o between reducing the training er-ror and limiting model complexity, for instance as described by regularization theory(Tikhonov and Arsenin, 1977), Minimum Description Length (Rissanen, 1978; Kol-mogorov, 1965), or the Bias-Variance Dilemma (Geman, Bienenstock, and Doursat,1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 303,
                                "start": 277
                            }
                        ],
                        "text": "We conclude this section by noting that analyses in other branches of learningtheory have led to similar insights in the trade-o between reducing the training er-ror and limiting model complexity, for instance as described by regularization theory(Tikhonov and Arsenin, 1977), Minimum Description Length (Rissanen, 1978; Kol-mogorov, 1965), or the Bias-Variance Dilemma (Geman, Bienenstock, and Doursat,1992)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 63
                            }
                        ],
                        "text": "This can be viewed in the context of Algorithmic Complexity andMinimum Description Length (Vapnik, 1995b, Chapter 5, footnote 6)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 339,
                                "start": 305
                            }
                        ],
                        "text": "We conclude this section by noting that analyses in other branches of learning theory have led to similar insights in the trade-o between reducing the training error and limiting model complexity, for instance as described by regularization theory (Tikhonov and Arsenin, 1977), Minimum Description Length (Rissanen, 1978; Kolmogorov, 1965), or the Bias-Variance Dilemma (Geman, Bienenstock, and Doursat, 1992)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Modeling by shortest data"
            },
            "venue": {
                "fragments": [],
                "text": "description. Automatica,"
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 76
                            }
                        ],
                        "text": "The USPS database has been criticised (Burges, LeCun,private communication; Bottou et al. (1994)) as not providing the most adequateclassi er benchmark."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 207
                            }
                        ],
                        "text": "The best single machine in the performance comparisons so farwas a LeNet5 convolutional neural network (0.9%); other high performance systemsinclude Tangent Distance nearest neighbour classi ers (1.1%), and LeNet4 with a lastlayer using methods of local learning (1.1%, cf. Bottou and Vapnik, 1992)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 142
                            }
                        ],
                        "text": "This database has become the standardfor performance comparisons at AT&T Bell Labs; the error rate record of 0.7% is heldby a boosted LeNet4 (Bottou et al., 1994; LeCun et al., 1995), i.e. by an ensembleof learning machines."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 60
                            }
                        ],
                        "text": "The values gj(x) can also be used forreject decisions (e.g. Bottou et al., 1994), for instance by considering the di erencebetween the maximum and the second highest value as a measure of con dence in theclassi cation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 96
                            }
                        ],
                        "text": "PRIOR KNOWLEDGE IN SUPPORT VECTOR MACHINESmachines are known to allow for short training times (Bottou et al., 1994), the rstpoint is usually not critical."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 48
                            }
                        ],
                        "text": "In the above benchmark studies, only theboosted LeNet4 ensemble performed better (0.5%)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 100
                            }
                        ],
                        "text": "The images were rst size normalized to tinto a 20 20 pixel box, and then centered in a 28 28 image (Bottou et al., 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 271,
                                "start": 252
                            }
                        ],
                        "text": "\u2026followed by the animal database, and by the subordinate level chair database.2.3 Digit Recognition Using Di erent KernelsHandwritten digit recognition has long served as a test bed for evaluating and bench-marking classi ers (e.g. LeCun et al., 1989; Bottou et al., 1994; LeCun et al., 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 196
                            }
                        ],
                        "text": "Another interesting performance measure is the rejection error rate, de ned as thepercentage of patterns that would have to be rejected to attain a speci ed error rate (inthe benchmark studies of Bottou et al. (1994) and LeCun et al. (1995), 0.5%)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Comparison of classi er methods: a case study in handwritten digit recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 12th International Conference on Pattern Recognition and Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 97
                            }
                        ],
                        "text": "Consequently, all mathematical and statistical properties ofPCA (see for instance Jolli e, 1986; Diamantaras & Kung, 1996) carry over to kernel-based PCA, with the modi cations that they become statements about a set of points (xi); i = 1; : : : ;M , in F rather than in RN ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 273
                            }
                        ],
                        "text": "\u2026functions in the hidden layer would not su ce: already thelinear activation functions lead to the best approximation of the data (given the number of hiddennodes), so for the nonlinearities to have an e ect on the components, the architecture needs to bechanged (see e.g. Diamantaras & Kung, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 227
                            }
                        ],
                        "text": "Furthermore,in the case where we need to use a large number ` of observations, we may want towork with an algorithm for computing only the largest Eigenvalues, as for instance thepower method with de ation (for a discussion, see Diamantaras & Kung, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 115
                            }
                        ],
                        "text": "Rather than giving a fullreview of this eld here, we brie y describe just three approaches, and refer the readerto Diamantaras & Kung (1996) for more details."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 243,
                                "start": 219
                            }
                        ],
                        "text": "If we train it to reproduce the inputvalues as outputs (i.e. use it in autoassociative mode), then the hidden unit activationsform a lower-dimensional representation of the data, closely related to PCA (see forinstance Diamantaras & Kung, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Principal Component Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Principal Component Neural Networks"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 72
                            }
                        ],
                        "text": "1.3), it givesrise to a number of di erent types of pattern classi ers (Vapnik and Chervonenkis,1974; Boser, Guyon, and Vapnik, 1992; Cortes and Vapnik, 1995; Vapnik, 1995b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 35
                            }
                        ],
                        "text": "The original treatments are due to Vapnik and Chervonenkis(1974), Boser, Guyon, and Vapnik (1992), Guyon, Boser, and Vapnik (1993), Cortesand Vapnik (1995), and Vapnik (1995b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 179
                            }
                        ],
                        "text": "A sharper bound can be formulated by making a further distinction in case 2, betweenSVs that must occur in the solution, and those that can be expressed in terms of theother SVs (Vapnik and Chervonenkis, 1974)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 148
                            }
                        ],
                        "text": "\u2026Statistical Learning TheoryOut of the considerable body of theory that has been developed in statistical learningtheory by Vapnik and others (e.g. Vapnik and Chervonenkis, 1968, 1974; Vapnik, 1979,1995a,b), we brie y review a few concepts and results which are necessary in order tobe able to\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 79
                            }
                        ],
                        "text": "INTRODUCTION AND PRELIMINARIESthe Generalized Portrait hyperplane classi er of Vapnik and Chervonenkis (1974) tononlinear Support Vector machines (Sec."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theory of Pattern Recognition in Russian]. Nauka, Moscow"
            },
            "venue": {
                "fragments": [],
                "text": "Theorie der Zeichenerkennung"
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152633671"
                        ],
                        "name": "G. McLachlan",
                        "slug": "G.-McLachlan",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "McLachlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. McLachlan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72847318"
                        ],
                        "name": "J. Schurmann",
                        "slug": "J.-Schurmann",
                        "structuredName": {
                            "firstName": "Jurgen",
                            "lastName": "Schurmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schurmann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 148
                            }
                        ],
                        "text": "\u2026(2.26)) with degreed = 15 and = 100.10 In addition, we report results of Kressel (1996), who uti-lized a fully quadratic polynomial classi er (Sch urmann, 1996) trained on the rst 50databases with little redundancy it is sometimes advantageous to use batch updates with conjugategradient descent,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 126019720,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d21120ddfdc9e6b52f6207f8a177052465d22de1",
            "isKey": true,
            "numCitedBy": 210,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Pattern-Classification:-A-Unified-View-of-and-McLachlan-Schurmann",
            "title": {
                "fragments": [],
                "text": "Pattern Classification: A Unified View of Statistical and Neural Approaches."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11768867"
                        ],
                        "name": "H. Primas",
                        "slug": "H.-Primas",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Primas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Primas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 96
                            }
                        ],
                        "text": "Figure 2.5 shows how a simple binary toy problem is solved by a Support Vectormachine with a radial basis function kernel (2.27).2.1.5 SV Regression EstimationThis thesis is primarily concerned with pattern recognition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 278
                            }
                        ],
                        "text": "In mathematics, this is exempli ed in FelixKlein's Erlanger Programm (Klein, 1872) which shifts geometry away from points andlines towards transformation groups; in physics, an example is the modern de nitionof elementary particles as transformation group representations (e.g. Primas, 1983)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 124472922,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "e89802f55ec5d74a8579292410a59f85476e9ee9",
            "isKey": true,
            "numCitedBy": 459,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Chemistry,-Quantum-Mechanics-and-Reductionism-Primas",
            "title": {
                "fragments": [],
                "text": "Chemistry, Quantum Mechanics and Reductionism"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145732246"
                        ],
                        "name": "E. Polak",
                        "slug": "E.-Polak",
                        "structuredName": {
                            "firstName": "Elijah",
                            "lastName": "Polak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Polak"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 263,
                                "start": 247
                            }
                        ],
                        "text": "If we leave out a pattern zi and construct the solutionfrom the remaining patterns, there are several possibilities (cf. (2.6)):2This terminology is related to corresponding terms in the theory of convex sets, relevant to convexoptimization (e.g. Luenberger, 1973; Bertsekas, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122931192,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "95c23bb25a03d296a7eedb7c8dffe1748bb614c6",
            "isKey": true,
            "numCitedBy": 2291,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Introduction-to-linear-and-nonlinear-programming-Polak",
            "title": {
                "fragments": [],
                "text": "Introduction to linear and nonlinear programming"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 111
                            }
                        ],
                        "text": "Figure 2.5 shows how a simple binary toy problem is solved by a Support Vectormachine with a radial basis function kernel (2.27).2.1.5 SV Regression EstimationThis thesis is primarily concerned with pattern recognition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Kac and Ulam (1968) refer to this as\\[...] the immensely powerful and fruitful idea that much can be learned1The ideas put forward in the following were in uenced by discussions with people in the MPI'sobject recognition group, in particular with V. Blanz."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mathematics and Logic. Praeger, Britannica perspective"
            },
            "venue": {
                "fragments": [],
                "text": "Mathematics and Logic. Praeger, Britannica perspective"
            },
            "year": 1968
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 196
                            }
                        ],
                        "text": "\u2026edge detection:25 grey scale 11.8 9.0 7.9 7.2 6.9 6.8 6.4 6.489 grey scale 3.2 1.5 1.1 1.0 0.9 0.9 0.8 0.8100 grey scale 4.7 3.3 2.7 2.2 2.2 2.0 2.0 2.025 silhouettes 12.1 9.9 8.8 8.0 7.6 7.5 7.0 7.189 silhouettes 3.7 2.0 1.3 1.2 1.1 1.2 1.1 1.1100 silhouettes 5.4 4.0 3.2 3.1 3.0 2.9 2.7 2.6\n2.2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 169
                            }
                        ],
                        "text": "Of somewhat related interest are the large- eld neurons in the y's vi-sual system, coding for speci c ow elds which are generated by the y's movementin the environment (Krapp and Hengstenberg, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimation of self-motion by optic ow processing in single visual interneurons U. Kressel. Private communication. The quoted results are summarized on ftp://ftp.mpik-tueb"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 135
                            }
                        ],
                        "text": "The third system was a polynomial Support Vector machine (cf. (2.26)) with degreed = 15 and = 100.10 In addition, we report results of Kressel (1996), who uti-lized a fully quadratic polynomial classi er (Sch urmann, 1996) trained on the rst 50databases with little redundancy it is sometimes\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 285
                            }
                        ],
                        "text": "\u2026plus edge detection data), anddi erent classi ers: SV: Support Vector machine; MLP: fully connected perceptron withone hidden layer of 400 neurons; OF: oriented lter invariant feature extraction, see text;PC: quadratic polynomial classi er trained on the rst 50 principal components (Kressel,1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Private communication. The quoted results are"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 276
                            }
                        ],
                        "text": "\u2026cprior knowledge in SV machines (Sch olkopf, Burges, and Vapnik, 1996a; Sch olkopf,Simard, Smola, and Vapnik, 1997a).4.1 IntroductionWhen we are trying to extract regularities from data, we often have additional knowl-edge about functions that we estimate (for a review, see Abu-Mostafa, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hints. Neural Computation"
            },
            "venue": {
                "fragments": [],
                "text": "Hints. Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47652868"
                        ],
                        "name": "H. Hotelling",
                        "slug": "H.-Hotelling",
                        "structuredName": {
                            "firstName": "Harold",
                            "lastName": "Hotelling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hotelling"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 144828484,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9ebb5c0d6d54707a4d6181a693b6f755ec8a45a9",
            "isKey": false,
            "numCitedBy": 8492,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Analysis-of-a-complex-of-statistical-variables-into-Hotelling",
            "title": {
                "fragments": [],
                "text": "Analysis of a complex of statistical variables into principal components."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1933
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "79783680"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2099637865"
                        ],
                        "name": "Mozer",
                        "slug": "Mozer",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Mozer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mozer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054400760"
                        ],
                        "name": "M. Jordan",
                        "slug": "M.-Jordan",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Jordan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2214848"
                        ],
                        "name": "T. Petsche",
                        "slug": "T.-Petsche",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Petsche",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Petsche"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60518954,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "271c040ea880abc2470f72690ed89bc3d8a11a2c",
            "isKey": false,
            "numCitedBy": 213,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Improving-the-accuracy-and-speed-of-support-vector-Burges-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "Improving the accuracy and speed of support vector learning machines"
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 1997"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144517651"
                        ],
                        "name": "T. Vetter",
                        "slug": "T.-Vetter",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Vetter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Vetter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 256,
                                "start": 244
                            }
                        ],
                        "text": "The rst one used oriented lters to extract featureswhich are robust with respect to small rigid transformations of the underlying 3-Dobjects, followed by a decision stage based on comparisons with stored templates (fordetails, see Blanz, 1995; Vetter, 1994; Blanz et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60274641,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "06d322aa092c505d059780ab0d915701d7ea1b26",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-early-vision-model-for-3D-object-recognition-Vetter",
            "title": {
                "fragments": [],
                "text": "An early vision model for 3D object recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Priors, stabilizers and basis functions: From regularization to radial, tensor and additive splines. A.I. Memo No"
            },
            "venue": {
                "fragments": [],
                "text": "Priors, stabilizers and basis functions: From regularization to radial, tensor and additive splines. A.I. Memo No"
            },
            "year": 1430
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 242,
                                "start": 231
                            }
                        ],
                        "text": "The rst one used oriented lters to extract featureswhich are robust with respect to small rigid transformations of the underlying 3-Dobjects, followed by a decision stage based on comparisons with stored templates (fordetails, see Blanz, 1995; Vetter, 1994; Blanz et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 105
                            }
                        ],
                        "text": "Top, frontal and back views typically are harder to classifythan views from more generic points of view (Blanz, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bildbasierte Objekterkennung und die Bestimmung optimaler Ansichten"
            },
            "venue": {
                "fragments": [],
                "text": "Bildbasierte Objekterkennung und die Bestimmung optimaler Ansichten"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 158
                            }
                        ],
                        "text": "Initiated by the pioneering work of Oja (1982), a number ofunsupervised neural-network type algorithms computing principal components havebeen proposed (e.g. Sanger, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimal unsupervised learning in a single-layer linear feedforward network"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theory of Pattern Recognition in Russian"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Prediction with Gaussian processes. Preprint, 1997. A. Yuille and N. Grzywacz. The motion coherence theory"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the International Conference on Computer Vision"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 107
                            }
                        ],
                        "text": "The networks were trained by back-propagation of mean squared error(Rumelhart, Hinton, and Williams, 1986; LeCun, 1985)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Une proc edure d'apprentissage pour R eseau a seuil assymm etrique"
            },
            "venue": {
                "fragments": [],
                "text": "Cognitiva: A la Fronti ere de l'Intelligence Artiicielle des Sciences de la Connaissance des Neurosciences"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "From pictures to words: Making the connection"
            },
            "venue": {
                "fragments": [],
                "text": "Cognitive Psychology"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural-network and k-nearest-neighbor classiiers"
            },
            "venue": {
                "fragments": [],
                "text": "Neural-network and k-nearest-neighbor classiiers"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Aymptotic statistical theory of overtraining and cross-validation"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. on Neural Networks"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Note that some of these animals are also contained in the entry level database"
            },
            "venue": {
                "fragments": [],
                "text": "Note that some of these animals are also contained in the entry level database"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 122
                            }
                        ],
                        "text": "Other domains where researchers have recently started to investigate the use of Mercer kernels include Gaussian Processes (Williams, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 122
                            }
                        ],
                        "text": "Other domains where researchers have recently started to investigatethe use of Mercer kernels include Gaussian Processes (Williams, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Prediction with Gaussian processes"
            },
            "venue": {
                "fragments": [],
                "text": "Prediction with Gaussian processes"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 129
                            }
                        ],
                        "text": "For reviews of the existing literature, see Jolli e (1986) and Diamantaras &Kung (1996); some of the classical papers are due to Pearson (1901); Hotelling (1933);Karhunen (1946)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On lines and planes of closest t to points in space"
            },
            "venue": {
                "fragments": [],
                "text": "Philosophical Magazine"
            },
            "year": 1901
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The entry level databases contains 25 objects ((gures A.5, A.6, A.7), for which psychophysical evidence suggests that they belong to diierent entry levels in object recognition"
            },
            "venue": {
                "fragments": [],
                "text": "The entry level databases contains 25 objects ((gures A.5, A.6, A.7), for which psychophysical evidence suggests that they belong to diierent entry levels in object recognition"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Komplexe adaptive Systeme (Forum f ur Interdisziplinn are Forschung Bd. 15), pages 93 { 117. RR oll"
            },
            "venue": {
                "fragments": [],
                "text": "Komplexe adaptive Systeme (Forum f ur Interdisziplinn are Forschung Bd. 15), pages 93 { 117. RR oll"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistical Learning Theory Vapnik and A. Chervonenkis. Uniform convergence of frequencies of occurence of events to their probabilities"
            },
            "venue": {
                "fragments": [],
                "text": "Dokl. Akad. Nauk SSSR"
            },
            "year": 1968
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 147
                            }
                        ],
                        "text": "\u2026type ofregularization and the class of functions which are considered as admissible solutionsare intimately related (cf. Poggio and Girosi, 1990; Girosi, Jones, and Poggio, 1993;Smola and Sch olkopf, 1997a; Smola, Sch olkopf, and M uller, 1997): the SV algorithmis equivalent to minimizing the\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Priors, stabilizers and basis functions: From regularization to radial, tensor and additive splines. A.I. Memo No. 1430"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 148
                            }
                        ],
                        "text": "\u2026Statistical Learning TheoryOut of the considerable body of theory that has been developed in statistical learningtheory by Vapnik and others (e.g. Vapnik and Chervonenkis, 1968, 1974; Vapnik, 1979,1995a,b), we brie y review a few concepts and results which are necessary in order tobe able to\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Uniform convergence of frequencies of occurence of events to their probabilities"
            },
            "venue": {
                "fragments": [],
                "text": "Dokl. Akad. Nauk SSSR"
            },
            "year": 1968
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Comparing support vector machines with gaussian kernels to radial basis function classi ers. A.I. Memo No. 1599"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural-network and k-nearest-neighbor classi ers"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neuronale Netze: Eine Einf uhrung in die Neuroinformatik selbstorganisierender Abbildungen"
            },
            "venue": {
                "fragments": [],
                "text": "Automatica"
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 19
                            }
                        ],
                        "text": "Some authors (e.g. Ritter, Martinetz, and Schulten, 1990) havediscussed parallels between the Principal Curve algorithm and self-organizing featuremaps (Kohonen, 1982) for dimensionality reduction.4Simply using nonlinear activation functions in the hidden layer would not su ce: already thelinear\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neuronale Netze: Eine Einf uhrung in die Neuroinformatik selbstorganisierender Abbildungen"
            },
            "venue": {
                "fragments": [],
                "text": "Neuronale Netze: Eine Einf uhrung in die Neuroinformatik selbstorganisierender Abbildungen"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The animal database contains 25 diierent animals"
            },
            "venue": {
                "fragments": [],
                "text": "The animal database contains 25 diierent animals"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Psychophysical and computational experiments on the MPI object databases"
            },
            "venue": {
                "fragments": [],
                "text": "Max-Planck-Institut f ur biologische Kybernetik"
            },
            "year": 1997
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 53,
            "methodology": 37,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 113,
        "totalPages": 12
    },
    "page_url": "https://www.semanticscholar.org/paper/Support-vector-learning-Sch\u00f6lkopf/356125478f5d06b564b420755a4944254045bbbe?sort=total-citations"
}