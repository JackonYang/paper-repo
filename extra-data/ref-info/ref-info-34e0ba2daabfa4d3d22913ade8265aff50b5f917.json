{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765022"
                        ],
                        "name": "M. Enzweiler",
                        "slug": "M.-Enzweiler",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Enzweiler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Enzweiler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144854796"
                        ],
                        "name": "D. Gavrila",
                        "slug": "D.-Gavrila",
                        "structuredName": {
                            "firstName": "Dariu",
                            "lastName": "Gavrila",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gavrila"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 11
                            }
                        ],
                        "text": ", see [2], [6], [42]), a comparative study of approaches for integrating detector outputs over time has not been carried out."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Enzweiler and Gavrila [77] labeled pedestrians as belonging to one of four canonical orientations and jointly perform classification and orientation estimation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 96
                            }
                        ],
                        "text": "six additional pedestrian detection data sets, including the ETH [4], TUD-Brussels [5], Daimler [6], and INRIA [7] data sets and two variants of the Caltech data set (see Fig."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 87
                            }
                        ],
                        "text": "9 are consecutive, independently annotated frames from the Daimler detection benchmark [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 54
                            }
                        ],
                        "text": "sets and the Daimler detection benchmark (Daimler-DB) [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Enzweiler and Gavrila [6] published the Daimler detection data set and an accompanying evaluation of three detectors, performing additional experiments integrating the detectors into full systems."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 22
                            }
                        ],
                        "text": "Enzweiler and Gavrila [6] published the Daimler detection data set and an accompanying evaluation of three detectors,"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 176
                            }
                        ],
                        "text": "To increase the scope of our analysis, we benchmarked the detectors on six additional pedestrian detection data sets, including INRIA [7], TUD-Brussels [5], ETH [4], DaimlerDB [6], Caltech-Training, and Caltech-Japan."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 40
                            }
                        ],
                        "text": "some scenes are staged by actors, as in [6]) while having"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 74
                            }
                        ],
                        "text": "taken from a mobile recording setup, such as a robot or vehicle [4], [5], [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 121
                            }
                        ],
                        "text": "As mentioned, in our performance evaluation we additionally use the INRIA [7], ETH [4], TUD-Brussels [5], and Daimler-DB [6] data sets."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1192198,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5224b79368dba945a9e90506f23a1cfa91f6f404",
            "isKey": true,
            "numCitedBy": 1231,
            "numCiting": 94,
            "paperAbstract": {
                "fragments": [],
                "text": "Pedestrian detection is a rapidly evolving area in computer vision with key applications in intelligent vehicles, surveillance, and advanced robotics. The objective of this paper is to provide an overview of the current state of the art from both methodological and experimental perspectives. The first part of the paper consists of a survey. We cover the main components of a pedestrian detection system and the underlying models. The second (and larger) part of the paper contains a corresponding experimental study. We consider a diverse set of state-of-the-art systems: wavelet-based AdaBoost cascade, HOG/linSVM, NN/LRF, and combined shape-texture detection. Experiments are performed on an extensive data set captured onboard a vehicle driving through urban environment. The data set includes many thousands of training samples as well as a 27-minute test sequence involving more than 20,000 images with annotated pedestrian locations. We consider a generic evaluation setting and one specific to pedestrian detection onboard a vehicle. Results indicate a clear advantage of HOG/linSVM at higher image resolutions and lower processing speeds, and a superiority of the wavelet-based AdaBoost cascade approach at lower image resolutions and (near) real-time processing speeds. The data set (8.5 GB) is made public for benchmarking purposes."
            },
            "slug": "Monocular-Pedestrian-Detection:-Survey-and-Enzweiler-Gavrila",
            "title": {
                "fragments": [],
                "text": "Monocular Pedestrian Detection: Survey and Experiments"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An overview of the current state of the art of pedestrian detection from both methodological and experimental perspectives is provided and a clear advantage of HOG/linSVM at higher image resolutions and lower processing speeds is indicated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2340109"
                        ],
                        "name": "C. Wojek",
                        "slug": "C.-Wojek",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Wojek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Wojek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 98
                            }
                        ],
                        "text": "We evaluate 16 representative state-of-theart pedestrian detectors (previously we evaluated seven [3])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 98
                            }
                        ],
                        "text": "Overall, the evaluation protocol has changed substantially since our initial version described in [3], resulting in a more accurate and informative evaluation of the state of the art."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 57
                            }
                        ],
                        "text": "Overall, the methodology has changed substantially since [3], resulting in a more accurate and informative benchmark."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 16
                            }
                        ],
                        "text": "In earlier work [3], we introduced the Caltech Pedestrian Data Set, which includes 350,000 pedestrian bounding boxes (BB) labeled in 250,000 frames and remains the largest such data set to date."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 50
                            }
                        ],
                        "text": "Left: Strict filtering, used in our previous work [3], undercounts true positives, thus underreporting results."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 49
                            }
                        ],
                        "text": "The filtering strategy used in our previous work [3] was too stringent and resulted in underreporting of detector performance (this was also independently observed by Walk et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2451341,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3e083dc8aeb7983a5cdff146985363d38caf0886",
            "isKey": false,
            "numCitedBy": 611,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Pedestrian detection is a key problem in computer vision, with several applications including robotics, surveillance and automotive safety. Much of the progress of the past few years has been driven by the availability of challenging public datasets. To continue the rapid rate of innovation, we introduce the Caltech Pedestrian Dataset, which is two orders of magnitude larger than existing datasets. The dataset contains richly annotated video, recorded from a moving vehicle, with challenging images of low resolution and frequently occluded people. We propose improved evaluation metrics, demonstrating that commonly used per-window measures are flawed and can fail to predict performance on full images. We also benchmark several promising detection systems, providing an overview of state-of-the-art performance and a direct, unbiased comparison of existing methods. Finally, by analyzing common failure cases, we help identify future research directions for the field."
            },
            "slug": "Pedestrian-detection:-A-benchmark-Doll\u00e1r-Wojek",
            "title": {
                "fragments": [],
                "text": "Pedestrian detection: A benchmark"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The Caltech Pedestrian Dataset is introduced, which is two orders of magnitude larger than existing datasets and proposes improved evaluation metrics, demonstrating that commonly used per-window measures are flawed and can fail to predict performance on full images."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24760164"
                        ],
                        "name": "S. Walk",
                        "slug": "S.-Walk",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Walk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Walk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2029012"
                        ],
                        "name": "Nikodem Majer",
                        "slug": "Nikodem-Majer",
                        "structuredName": {
                            "firstName": "Nikodem",
                            "lastName": "Majer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nikodem Majer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144810819"
                        ],
                        "name": "K. Schindler",
                        "slug": "K.-Schindler",
                        "structuredName": {
                            "firstName": "Konrad",
                            "lastName": "Schindler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Schindler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[28] extended this framework by additionally combining local color self-similarity [MULTIFTR+CSS] and the motion features discussed above [MULTIFTR+MOTION]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206591283,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fdaf2792b841fd5eced16cef9d77cc3197cb3bf0",
            "isKey": false,
            "numCitedBy": 505,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Despite impressive progress in people detection the performance on challenging datasets like Caltech Pedestrians or TUD-Brussels is still unsatisfactory. In this work we show that motion features derived from optic flow yield substantial improvements on image sequences, if implemented correctly \u2014 even in the case of low-quality video and consequently degraded flow fields. Furthermore, we introduce a new feature, self-similarity on color channels, which consistently improves detection performance both for static images and for video sequences, across different datasets. In combination with HOG, these two features outperform the state-of-the-art by up to 20%. Finally, we report two insights concerning detector evaluations, which apply to classifier-based object detection in general. First, we show that a commonly under-estimated detail of training, the number of bootstrapping rounds, has a drastic influence on the relative (and absolute) performance of different feature/classifier combinations. Second, we discuss important intricacies of detector evaluation and show that current benchmarking protocols lack crucial details, which can distort evaluations."
            },
            "slug": "New-features-and-insights-for-pedestrian-detection-Walk-Majer",
            "title": {
                "fragments": [],
                "text": "New features and insights for pedestrian detection"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "This work shows that motion features derived from optic flow yield substantial improvements on image sequences, if implemented correctly \u2014 even in the case of low-quality video and consequently degraded flow fields, and introduces a new feature, self-similarity on color channels, which consistently improves detection performance across different datasets."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 166
                            }
                        ],
                        "text": "Moreover, numerous approaches have been proposed for speeding up detection, including speeding up the detector itself [29], [44], [46], through use of approximations [63], [89] or by using special purpose hardware such as GPUs [90] (for a review of fast detection see [63])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "05); a fast multiscale scheme is proposed in [63]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "Pedestrian Detector in the West\u201d [FPDW] [63], this"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7540163,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7fe1a8ca95b63f5c5d60f929c5822bfa7d5ac8e5",
            "isKey": false,
            "numCitedBy": 659,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "We demonstrate a multiscale pedestrian detector operating in near real time ( 6 fps on 640x480 images) with state-of-the-art detection performance. The computational bottleneck of many modern detectors is the construction of an image pyramid, typically sampled at 8-16 scales per octave, and associated feature computations at each scale. We propose a technique to avoid constructing such a finely sampled image pyramid without sacrificing performance: our key insight is that for a broad family of features, including gradient histograms, the feature responses computed at a single scale can be used to approximate feature responses at nearby scales. The approximation is accurate within an entire scale octave. This allows us to decouple the sampling of the image pyramid from the sampling of detection scales. Overall, our approximation yields a speedup of 10-100 times over competing methods with only a minor loss in detection accuracy of about 1-2% on the Caltech Pedestrian dataset across a wide range of evaluation settings. The results are confirmed on three additional datasets (INRIA, ETH, and TUD-Brussels) where our method always scores within a few percent of the state-of-the-art while being 1-2 orders of magnitude faster. The approach is general and should be widely applicable."
            },
            "slug": "The-Fastest-Pedestrian-Detector-in-the-West-Doll\u00e1r-Belongie",
            "title": {
                "fragments": [],
                "text": "The Fastest Pedestrian Detector in the West"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A technique to avoid constructing such a finely sampled image pyramid without sacrificing performance is proposed, and for a broad family of features, including gradient histograms, the feature responses computed at a single scale can be used to approximate feature responses at nearby scales."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789756"
                        ],
                        "name": "B. Leibe",
                        "slug": "B.-Leibe",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Leibe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Leibe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083209039"
                        ],
                        "name": "Edgar Seemann",
                        "slug": "Edgar-Seemann",
                        "structuredName": {
                            "firstName": "Edgar",
                            "lastName": "Seemann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edgar Seemann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 17
                            }
                        ],
                        "text": "[35] or keypoint [36], [37] based methods often fail."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[36] adapted the implicit shape model, also based on keypoints, for detecting pedestrians."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14395688,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1854005a7178b2df6afaacdcf91bc35d90616075",
            "isKey": false,
            "numCitedBy": 932,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we address the problem of detecting pedestrians in crowded real-world scenes with severe overlaps. Our basic premise is that this problem is too difficult for any type of model or feature alone. Instead, we present an algorithm that integrates evidence in multiple iterations and from different sources. The core part of our method is the combination of local and global cues via probabilistic top-down segmentation. Altogether, this approach allows examining and comparing object hypotheses with high precision down to the pixel level. Qualitative and quantitative results on a large data set confirm that our method is able to reliably detect pedestrians in crowded scenes, even when they overlap and partially occlude each other. In addition, the flexible nature of our approach allows it to operate on very small training sets."
            },
            "slug": "Pedestrian-detection-in-crowded-scenes-Leibe-Seemann",
            "title": {
                "fragments": [],
                "text": "Pedestrian detection in crowded scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Qualitative and quantitative results on a large data set confirm that the core part of the method is the combination of local and global cues via probabilistic top-down segmentation that allows examining and comparing object hypotheses with high precision down to the pixel level."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083209039"
                        ],
                        "name": "Edgar Seemann",
                        "slug": "Edgar-Seemann",
                        "structuredName": {
                            "firstName": "Edgar",
                            "lastName": "Seemann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edgar Seemann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789756"
                        ],
                        "name": "B. Leibe",
                        "slug": "B.-Leibe",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Leibe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Leibe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712041"
                        ],
                        "name": "K. Mikolajczyk",
                        "slug": "K.-Mikolajczyk",
                        "structuredName": {
                            "firstName": "Krystian",
                            "lastName": "Mikolajczyk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Mikolajczyk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "[35] or keypoint [36], [37] based methods often fail."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1664902,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "953ac43e263f5e8cec2edee378c5f0888b046ec5",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Pedestrian detection in real world scenes is a challenging problem. In recent years a variety of approaches have been proposed, and impressive results have been reported on a variety of databases. This paper systematically evaluates (1) various local shape descriptors, namely Shape Context and Local Chamfer descriptor and (2) four different interest point detectors for the detection of pedestrians. Those results are compared to the standard global Chamfer matching approach. A main result of the paper is that Shape Context trained on real edge images rather than on clean pedestrian silhouettes combined with the Hessian-Laplace detector outperforms all other tested approaches."
            },
            "slug": "An-Evaluation-of-Local-Shape-Based-Features-for-Seemann-Leibe",
            "title": {
                "fragments": [],
                "text": "An Evaluation of Local Shape-Based Features for Pedestrian Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Shape Context trained on real edge images rather than on clean pedestrian silhouettes combined with the Hessian-Laplace detector outperforms all other tested approaches for the detection of pedestrians."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765022"
                        ],
                        "name": "M. Enzweiler",
                        "slug": "M.-Enzweiler",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Enzweiler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Enzweiler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777764"
                        ],
                        "name": "Angela Eigenstetter",
                        "slug": "Angela-Eigenstetter",
                        "structuredName": {
                            "firstName": "Angela",
                            "lastName": "Eigenstetter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Angela Eigenstetter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144854796"
                        ],
                        "name": "D. Gavrila",
                        "slug": "D.-Gavrila",
                        "structuredName": {
                            "firstName": "Dariu",
                            "lastName": "Gavrila",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gavrila"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[75]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6783368,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4183d1b79d54f5638063e6c59a2a873ee2cd1bed",
            "isKey": false,
            "numCitedBy": 255,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a novel mixture-of-experts framework for pedestrian classification with partial occlusion handling. The framework involves a set of component-based expert classifiers trained on features derived from intensity, depth and motion. To handle partial occlusion, we compute expert weights that are related to the degree of visibility of the associated component. This degree of visibility is determined by examining occlusion boundaries, i.e. discontinuities in depth and motion. Occlusion-dependent component weights allow to focus the combined decision of the mixture-of-experts classifier on the unoccluded body parts. In experiments on extensive real-world data sets, with both partially occluded and non-occluded pedestrians, we obtain significant performance boosts over state-of-the-art approaches by up to a factor of four in reduction of false positives at constant detection rates. The dataset is made public for benchmarking purposes."
            },
            "slug": "Multi-cue-pedestrian-classification-with-partial-Enzweiler-Eigenstetter",
            "title": {
                "fragments": [],
                "text": "Multi-cue pedestrian classification with partial occlusion handling"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "In experiments on extensive real-world data sets, with both partially occluded and non-occluded pedestrians, significant performance boosts over state-of-the-art approaches are obtained by up to a factor of four in reduction of false positives at constant detection rates."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2570029"
                        ],
                        "name": "P. Sabzmeydani",
                        "slug": "P.-Sabzmeydani",
                        "structuredName": {
                            "firstName": "Payam",
                            "lastName": "Sabzmeydani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Sabzmeydani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10771328"
                        ],
                        "name": "Greg Mori",
                        "slug": "Greg-Mori",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Mori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg Mori"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 92
                            }
                        ],
                        "text": "Wojek and Schiele [MULTIFTR] [56] showed how a combination of Haar-like features, shapelets [33], shape context [57] and HOG features outperforms any individual feature."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 25
                            }
                        ],
                        "text": "We observed this for the SHAPELET [33] and HIKSVM [34] detectors, see also www.cs.sfu.ca/~mori/research/papers/ sabzmeydani_shapelet_cvpr07.html and http://www.cs.berkeley.edu/ ~smaji/projects/ped-detector/."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": "We observed this for the SHAPELET [33] and HIKSVM [34] detectors, see also www."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 103
                            }
                        ],
                        "text": "In nearly all cases, we used code obtained directly from the authors, the only exceptions being VJ and SHAPELET, which were reimplemented in [56]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 72
                            }
                        ],
                        "text": "In a few cases, the evaluated code differed from the published version: SHAPELET and HIKSVM have been corrected so they no longer overfit to boundary effects; we evaluate a variant of POSEINV based on boosting (which in our tests outperformed the much slower kernel SVM version); PLS\nswitched to PM NMS; and finally, the posted code for HOGLBP does not include occlusion reasoning (the improvement from occlusion reasoning was slight [59])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "Similarly, \u201cshapelets\u201d [33] are shape descriptors discriminatively learned from gradients in local patches; boosting was used to combine multiple shapelets into an overall detector [SHAPELET]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13420116,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b09d132046f3b21f98206eb514ebfcbd73f32513",
            "isKey": true,
            "numCitedBy": 356,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we address the problem of detecting pedestrians in still images. We introduce an algorithm for learning shapelet features, a set of mid-level features. These features are focused on local regions of the image and are built from low-level gradient information that discriminates between pedestrian and non-pedestrian classes. Using Ad-aBoost, these shapelet features are created as a combination of oriented gradient responses. To train the final classifier, we use AdaBoost for a second time to select a subset of our learned shapelets. By first focusing locally on smaller feature sets, our algorithm attempts to harvest more useful information than by examining all the low-level features together. We present quantitative results demonstrating the effectiveness of our algorithm. In particular, we obtain an error rate 14 percentage points lower (at 10-6 FPPW) than the previous state of the art detector of Dalal and Triggs on the INRIA dataset."
            },
            "slug": "Detecting-Pedestrians-by-Learning-Shapelet-Features-Sabzmeydani-Mori",
            "title": {
                "fragments": [],
                "text": "Detecting Pedestrians by Learning Shapelet Features"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This paper introduces an algorithm for learning shapelet features, a set of mid-level features that are built from low-level gradient information that discriminates between pedestrian and non-pedestrian classes on the INRIA dataset."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2340109"
                        ],
                        "name": "C. Wojek",
                        "slug": "C.-Wojek",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Wojek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Wojek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24760164"
                        ],
                        "name": "S. Walk",
                        "slug": "S.-Walk",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Walk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Walk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[5], who showed that certain modifications were necessary to make the motion features effective for detection."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 39
                            }
                        ],
                        "text": "Note that the ETH [4] and TUD-Brussels [5] evaluation protocols also suggested standardizing the aspect ratio, although to an arbitrarily chosen constant of 0."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 83
                            }
                        ],
                        "text": "six additional pedestrian detection data sets, including the ETH [4], TUD-Brussels [5], Daimler [6], and INRIA [7] data sets and two variants of the Caltech data set (see Fig."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 126
                            }
                        ],
                        "text": "ETH has higher density and larger scale pedestrians than the remaining data sets (we use the refined annotations published in [5])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 152
                            }
                        ],
                        "text": "To increase the scope of our analysis, we benchmarked the detectors on six additional pedestrian detection data sets, including INRIA [7], TUD-Brussels [5], ETH [4], DaimlerDB [6], Caltech-Training, and Caltech-Japan."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 153
                            }
                        ],
                        "text": "Most of the evaluated detectors were trained on the INRIA data set [7]; two were trained on TUD motion pairs (TUD-MP) (the training set for TUD-Brussels [5])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 37
                            }
                        ],
                        "text": "INRIA [7], ETH [4], and TUD-Brussels [5] pedestrian data"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 69
                            }
                        ],
                        "text": "taken from a mobile recording setup, such as a robot or vehicle [4], [5], [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 101
                            }
                        ],
                        "text": "As mentioned, in our performance evaluation we additionally use the INRIA [7], ETH [4], TUD-Brussels [5], and Daimler-DB [6] data sets."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18000078,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c8bddd67871a45bc2f0cd008648de1104b25df1",
            "isKey": false,
            "numCitedBy": 176,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Various powerful people detection methods exist. Surprisingly, most approaches rely on static image features only despite the obvious potential of motion information for people detection. This paper systematically evaluates different features and classifiers in a sliding-window framework. First, our experiments indicate that incorporating motion information improves detection performance significantly. Second, the combination of multiple and complementary feature types can also help improve performance. And third, the choice of the classifier-feature combination and several implementation details are crucial to reach best performance. In contrast to many recent papers experimental results are reported for four different datasets rather than using a single one. Three of them are taken from the literature allowing for direct comparison. The fourth dataset is newly recorded using an onboard camera driving through urban environment. Consequently this dataset is more realistic and more challenging than any currently available dataset."
            },
            "slug": "Multi-cue-onboard-pedestrian-detection-Wojek-Walk",
            "title": {
                "fragments": [],
                "text": "Multi-cue onboard pedestrian detection"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Evaluating different features and classifiers in a sliding-window framework indicates that incorporating motion information improves detection performance significantly and the combination of multiple and complementary feature types can also help improve performance."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731948"
                        ],
                        "name": "Paul A. Viola",
                        "slug": "Paul-A.-Viola",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Viola",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul A. Viola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145319478"
                        ],
                        "name": "Michael J. Jones",
                        "slug": "Michael-J.-Jones",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jones",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144626092"
                        ],
                        "name": "D. Snow",
                        "slug": "D.-Snow",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Snow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Snow"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[53] proposed computing Haar-like features on difference images, resulting in large performance gains."
                    },
                    "intents": []
                }
            ],
            "corpusId": 47726,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3702c79b8d118f8f363d685905bd285ab8e33979",
            "isKey": false,
            "numCitedBy": 1524,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a pedestrian detection system that integrates image intensity information with motion information. We use a detection style algorithm that scans a detector over two consecutive frames of a video sequence. The detector is trained (using AdaBoost) to take advantage of both motion and appearance information to detect a walking person. Past approaches have built detectors based on motion information or detectors based on appearance information, but ours is the first to combine both sources of information in a single detector. The implementation described runs at about 4 frames/second, detects pedestrians at very small scales (as small as 20 \u00d7 15 pixels), and has a very low false positive rate.Our approach builds on the detection work of Viola and Jones. Novel contributions of this paper include: (i) development of a representation of image motion which is extremely efficient, and (ii) implementation of a state of the art pedestrian detection system which operates on low resolution images under difficult conditions (such as rain and snow)."
            },
            "slug": "Detecting-Pedestrians-Using-Patterns-of-Motion-and-Viola-Jones",
            "title": {
                "fragments": [],
                "text": "Detecting Pedestrians Using Patterns of Motion and Appearance"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This pedestrian detection system is the first to combine both sources of information in a single detector, and operates on low resolution images under difficult conditions (such as rain and snow)."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49807323"
                        ],
                        "name": "Patrick Ott",
                        "slug": "Patrick-Ott",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Ott",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Patrick Ott"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056091"
                        ],
                        "name": "M. Everingham",
                        "slug": "M.-Everingham",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Everingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Everingham"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 58
                            }
                        ],
                        "text": "Color information and implicit segmentation were added in [62], with a performance improvement over pure HOG."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 93235,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3db5479118e2aa4130c4685a08e737a4857deb34",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the problem of pedestrian detection in still images. Sliding window classifiers, notably using the Histogram-of-Gradient (HOG) features proposed by Dalal and Triggs are the state-of-the-art for this task, and we base our method on this approach. We propose a novel feature extraction scheme which computes implicit \u2018soft segmentations\u2019 of image regions into foreground/background. The method yields stronger object/background edges than gray-scale gradient alone, suppresses textural and shading variations, and captures local coherence of object appearance. The main contributions of our work are: (i) incorporation of segmentation cues into object detection; (ii) integration with classifier learning cf. a post-processing filter; (iii) high computational efficiency. We report results on the INRIA person detection dataset, achieving state-of-the-art results considerably exceeding those of the original HOG detector. Preliminary results for generic object detection on the PASCAL VOC2006 dataset also show substantial improvements in accuracy."
            },
            "slug": "Implicit-color-segmentation-features-for-pedestrian-Ott-Everingham",
            "title": {
                "fragments": [],
                "text": "Implicit color segmentation features for pedestrian and object detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A novel feature extraction scheme which computes implicit \u2018soft segmentations\u2019 of image regions into foreground/background and yields stronger object/background edges than gray-scale gradient alone, suppresses textural and shading variations, and captures local coherence of object appearance."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE 12th International Conference on Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1906895"
                        ],
                        "name": "M. Andriluka",
                        "slug": "M.-Andriluka",
                        "structuredName": {
                            "firstName": "Mykhaylo",
                            "lastName": "Andriluka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Andriluka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145920814"
                        ],
                        "name": "S. Roth",
                        "slug": "S.-Roth",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206590983,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ccbc65d05e753b097a6c6b1ece25624e2ee39d5d",
            "isKey": false,
            "numCitedBy": 924,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Both detection and tracking people are challenging problems, especially in complex real world scenes that commonly involve multiple people, complicated occlusions, and cluttered or even moving backgrounds. People detectors have been shown to be able to locate pedestrians even in complex street scenes, but false positives have remained frequent. The identification of particular individuals has remained challenging as well. Tracking methods are able to find a particular individual in image sequences, but are severely challenged by real-world scenarios such as crowded street scenes. In this paper, we combine the advantages of both detection and tracking in a single framework. The approximate articulation of each person is detected in every frame based on local features that model the appearance of individual body parts. Prior knowledge on possible articulations and temporal coherency within a walking cycle are modeled using a hierarchical Gaussian process latent variable model (hGPLVM). We show how the combination of these results improves hypotheses for position and articulation of each person in several subsequent frames. We present experimental results that demonstrate how this allows to detect and track multiple people in cluttered scenes with reoccurring occlusions."
            },
            "slug": "People-tracking-by-detection-and-Andriluka-Roth",
            "title": {
                "fragments": [],
                "text": "People-tracking-by-detection and people-detection-by-tracking"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper combines the advantages of both detection and tracking in a single framework using a hierarchical Gaussian process latent variable model (hGPLVM) and presents experimental results that demonstrate how this allows to detect and track multiple people in cluttered scenes with reoccurring occlusions."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765022"
                        ],
                        "name": "M. Enzweiler",
                        "slug": "M.-Enzweiler",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Enzweiler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Enzweiler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144854796"
                        ],
                        "name": "D. Gavrila",
                        "slug": "D.-Gavrila",
                        "structuredName": {
                            "firstName": "Dariu",
                            "lastName": "Gavrila",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gavrila"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Enzweiler and Gavrila [77] labeled pedestrians as belonging to one of four canonical orientations and jointly perform classification and orientation estimation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Enzweiler and Gavrila [6] published the Daimler detection data set and an accompanying evaluation of three detectors, performing additional experiments integrating the detectors into full systems."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "Enzweiler and Gavrila [77] labeled pedestrians as belonging to one of"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Gavrila and Philomin [49], [50] employed the Hausdorff distance transform and a template hierarchy to rapidly match image edges to a set of shape templates."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16751130,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "63197bc8b23204b64c63668f4078b576d0d19b25",
            "isKey": true,
            "numCitedBy": 91,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a novel approach to single-frame pedestrian classification and orientation estimation. Unlike previous work which addressed classification and orientation separately with different models, our method involves a probabilistic framework to approach both in a unified fashion. We address both problems in terms of a set of view-related models which couple discriminative expert classifiers with sample-dependent priors, facilitating easy integration of other cues (e.g. motion, shape) in a Bayesian fashion. This mixture-of-experts formulation approximates the probability density of pedestrian orientation and scales-up to the use of multiple cameras. Experiments on large real-world data show a significant performance improvement in both pedestrian classification and orientation estimation of up to 50%, compared to state-of-the-art, using identical data and evaluation techniques."
            },
            "slug": "Integrated-pedestrian-classification-and-estimation-Enzweiler-Gavrila",
            "title": {
                "fragments": [],
                "text": "Integrated pedestrian classification and orientation estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This paper presents a novel approach to single-frame pedestrian classification and orientation estimation in terms of a set of view-related models which couple discriminative expert classifiers with sample-dependent priors, facilitating easy integration of other cues in a Bayesian fashion."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2919518"
                        ],
                        "name": "S. Munder",
                        "slug": "S.-Munder",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Munder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Munder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144854796"
                        ],
                        "name": "D. Gavrila",
                        "slug": "D.-Gavrila",
                        "structuredName": {
                            "firstName": "Dariu",
                            "lastName": "Gavrila",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gavrila"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 46
                            }
                        ],
                        "text": "Daimler classification benchmark (Daimler-CB) [21], tend to"
                    },
                    "intents": []
                }
            ],
            "corpusId": 10769792,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4ebbf3ec461f9b347937e4a5b993f12940558934",
            "isKey": false,
            "numCitedBy": 636,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Detecting people in images is key for several important application domains in computer vision. This paper presents an in-depth experimental study on pedestrian classification; multiple feature-classifier combinations are examined with respect to their ROC performance and efficiency. We investigate global versus local and adaptive versus nonadaptive features, as exemplified by PCA coefficients, Haar wavelets, and local receptive fields (LRFs). In terms of classifiers, we consider the popular support vector machines (SVMs), feedforward neural networks, and k-nearest neighbor classifier. Experiments are performed on a large data set consisting of 4,000 pedestrian and more than 25,000 nonpedestrian (labeled) images captured in outdoor urban environments. Statistically meaningful results are obtained by analyzing performance variances caused by varying training and test sets. Furthermore, we investigate how classification performance and training sample size are correlated. Sample size is adjusted by increasing the number of manually labeled training data or by employing automatic bootstrapping or cascade techniques. Our experiments show that the novel combination of SVMs with LRF features performs best. A boosted cascade of Haar wavelets can, however, reach quite competitive results, at a fraction of computational cost. The data set used in this paper is made public, establishing a benchmark for this important problem"
            },
            "slug": "An-Experimental-Study-on-Pedestrian-Classification-Munder-Gavrila",
            "title": {
                "fragments": [],
                "text": "An Experimental Study on Pedestrian Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "An in-depth experimental study on pedestrian classification; multiple feature-classifier combinations are examined with respect to their ROC performance and efficiency and show that the novel combination of SVMs with LRF features performs best."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145539040"
                        ],
                        "name": "Mohamed E. Hussein",
                        "slug": "Mohamed-E.-Hussein",
                        "structuredName": {
                            "firstName": "Mohamed",
                            "lastName": "Hussein",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohamed E. Hussein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29905643"
                        ],
                        "name": "F. Porikli",
                        "slug": "F.-Porikli",
                        "structuredName": {
                            "firstName": "Fatih",
                            "lastName": "Porikli",
                            "middleNames": [
                                "Murat"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Porikli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693428"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Davis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "A similar performance measure was used in [27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7450659,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "30a27dc1c5afa102664dee005ed232903d9a6fa0",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a framework for evaluating human detectors that considers the practical application of a detector on a full image using multisize sliding-window scanning. We produce detection error tradeoff (DET) curves relating the miss detection rate and the false-alarm rate computed by deploying the detector on cropped windows and whole images, using, in the latter, either image resize or feature resize. Plots for cascade classifiers are generated based on confidence scores instead of on variation of the number of layers. To assess a method's overall performance on a given test, we use the average log miss rate (ALMR) as an aggregate performance score. To analyze the significance of the obtained results, we conduct 10-fold cross-validation experiments. We applied our evaluation framework to two state-of-the-art cascade-based detectors on the standard INRIA person dataset and a local dataset of near-infrared images. We used our evaluation framework to study the differences between the two detectors on the two datasets with different evaluation methods. Our results show the utility of our framework. They also suggest that the descriptors used to represent features and the training window size are more important in predicting the detection performance than the nature of the imaging process, and that the choice between resizing images or features can have serious consequences."
            },
            "slug": "A-Comprehensive-Evaluation-Framework-and-a-Study-Hussein-Porikli",
            "title": {
                "fragments": [],
                "text": "A Comprehensive Evaluation Framework and a Comparative Study for Human Detectors"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A framework for evaluating human detectors that considers the practical application of a detector on a full image using multisize sliding-window scanning suggests that the descriptors used to represent features and the training window size are more important in predicting the detection performance than the nature of the imaging process."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Intelligent Transportation Systems"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24760164"
                        ],
                        "name": "S. Walk",
                        "slug": "S.-Walk",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Walk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Walk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144810819"
                        ],
                        "name": "K. Schindler",
                        "slug": "K.-Schindler",
                        "structuredName": {
                            "firstName": "Konrad",
                            "lastName": "Schindler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Schindler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 115
                            }
                        ],
                        "text": "[65] gave modest gains over linear SVMs and AdaBoost for pedestrian detection, especially when used in combination [66]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14202373,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "73764fa9bed84ad2c932dc8089ace7fa8fa7c1d3",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Pedestrian detection is an important problem in computer vision due to its importance for applications such as visual surveillance, robotics, and automotive safety. This paper pushes the state-of-the-art of pedestrian detection in two ways. First, we propose a simple yet highly effective novel feature based on binocular disparity, outperforming previously proposed stereo features. Second, we show that the combination of different classifiers often improves performance even when classifiers are based on the same feature or feature combination. These two extensions result in significantly improved performance over the state-of-the-art on two challenging datasets."
            },
            "slug": "Disparity-Statistics-for-Pedestrian-Detection:-and-Walk-Schindler",
            "title": {
                "fragments": [],
                "text": "Disparity Statistics for Pedestrian Detection: Combining Appearance, Motion and Stereo"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A simple yet highly effective novel feature based on binocular disparity is proposed, outperforming previously proposed stereo features and it is shown that the combination of different classifiers often improves performance even when classifiers are based on the same feature or feature combination."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1971349"
                        ],
                        "name": "G. Overett",
                        "slug": "G.-Overett",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Overett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Overett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47773335"
                        ],
                        "name": "L. Petersson",
                        "slug": "L.-Petersson",
                        "structuredName": {
                            "firstName": "Lars",
                            "lastName": "Petersson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Petersson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055899259"
                        ],
                        "name": "N. Brewer",
                        "slug": "N.-Brewer",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Brewer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Brewer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055526358"
                        ],
                        "name": "L. Andersson",
                        "slug": "L.-Andersson",
                        "structuredName": {
                            "firstName": "Lars",
                            "lastName": "Andersson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Andersson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50221224"
                        ],
                        "name": "N. Pettersson",
                        "slug": "N.-Pettersson",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Pettersson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Pettersson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "6k 10k\u2020 \u2013 36 36 36 3 2006 NICTA [22] mobile 18."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 73
                            }
                        ],
                        "text": "Dataset type: Older datasets, including the MIT [16], CVC [19] and NICTA [22] pedestrian datasets and the Daimler classification benchmark (Daimler-CB) [21] tend to contain cropped pedestrian windows only."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 55
                            }
                        ],
                        "text": "Older data sets, including the MIT [16], CVC [19], and NICTA [22] pedestrian data sets and the Daimler classification benchmark (Daimler-CB) [21], tend to contain cropped pedestrian windows only."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16633950,
            "fieldsOfStudy": [
                "Computer Science",
                "Environmental Science"
            ],
            "id": "f393b14d42e02a89859e52178519dbd40809ac8d",
            "isKey": false,
            "numCitedBy": 123,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a comparative analysis of different pedestrian dataset characteristics. The main goal of the research is to determine what characteristics are desirable for improved training and validation of pedestrian detectors and classifiers. The work focuses on those aspects of the dataset which affect classification success using the most common boosting methods. Dataset characteristics such as image size, aspect ratio, geometric variance and the relative scale of positive class instances (pedestrians) within the training window form an integral part of classification success. This paper will examine the effects of varying these dataset characteristics with a view to determining the recommended attributes of a high quality and challenging dataset. While the primary focus is on characteristics of the positive training dataset, some discussion of desirable attributes for the negative dataset is important and is therefore included. This paper also serves to publish our current pedestrian dataset in various forms for non-commercial use by the scientific community. We believe the published dataset to be one of the largest, most flexible, and representative datasets available for pedestrian/person detection tasks."
            },
            "slug": "A-new-pedestrian-dataset-for-supervised-learning-Overett-Petersson",
            "title": {
                "fragments": [],
                "text": "A new pedestrian dataset for supervised learning"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "This paper presents a comparative analysis of different pedestrian dataset characteristics to determine what characteristics are desirable for improved training and validation of pedestrian detectors and classifiers using the most common boosting methods."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Intelligent Vehicles Symposium"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144854796"
                        ],
                        "name": "D. Gavrila",
                        "slug": "D.-Gavrila",
                        "structuredName": {
                            "firstName": "Dariu",
                            "lastName": "Gavrila",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gavrila"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2919518"
                        ],
                        "name": "S. Munder",
                        "slug": "S.-Munder",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Munder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Munder"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 162
                            }
                        ],
                        "text": "PW evaluation is commonly used to compare classifiers (as opposed to detectors) or to evaluate systems that perform automatic region of interest (ROI) generation [30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59459,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "91335d9335e4fd06de48d769d1b79eaded4e431b",
            "isKey": false,
            "numCitedBy": 595,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a multi-cue vision system for the real-time detection and tracking of pedestrians from a moving vehicle. The detection component involves a cascade of modules, each utilizing complementary visual criteria to successively narrow down the image search space, balancing robustness and efficiency considerations. Novel is the tight integration of the consecutive modules: (sparse) stereo-based ROI generation, shape-based detection, texture-based classification and (dense) stereo-based verification. For example, shape-based detection activates a weighted combination of texture-based classifiers, each attuned to a particular body pose.Performance of individual modules and their interaction is analyzed by means of Receiver Operator Characteristics (ROCs). A sequential optimization technique allows the successive combination of individual ROCs, providing optimized system parameter settings in a systematic fashion, avoiding ad-hoc parameter tuning. Application-dependent processing constraints can be incorporated in the optimization procedure.Results from extensive field tests in difficult urban traffic conditions suggest system performance is at the leading edge."
            },
            "slug": "Multi-cue-Pedestrian-Detection-and-Tracking-from-a-Gavrila-Munder",
            "title": {
                "fragments": [],
                "text": "Multi-cue Pedestrian Detection and Tracking from a Moving Vehicle"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This paper presents a multi-cue vision system for the real-time detection and tracking of pedestrians from a moving vehicle, with results from extensive field tests in difficult urban traffic conditions suggest system performance is at the leading edge."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145347688"
                        ],
                        "name": "S. Baker",
                        "slug": "S.-Baker",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Baker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Baker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709053"
                        ],
                        "name": "Daniel Scharstein",
                        "slug": "Daniel-Scharstein",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Scharstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Scharstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153024876"
                        ],
                        "name": "J. P. Lewis",
                        "slug": "J.-P.-Lewis",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Lewis",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. P. Lewis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145920814"
                        ],
                        "name": "S. Roth",
                        "slug": "S.-Roth",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717841"
                        ],
                        "name": "R. Szeliski",
                        "slug": "R.-Szeliski",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Szeliski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Szeliski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[8] and Middlebury [9] optical flow"
                    },
                    "intents": []
                }
            ],
            "corpusId": 316800,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "804836b8ad86ef8042e3dcbd45442a52f031ee03",
            "isKey": false,
            "numCitedBy": 2377,
            "numCiting": 107,
            "paperAbstract": {
                "fragments": [],
                "text": "The quantitative evaluation of optical flow algorithms by Barron et al. (1994) led to significant advances in performance. The challenges for optical flow algorithms today go beyond the datasets and evaluation methods proposed in that paper. Instead, they center on problems associated with complex natural scenes, including nonrigid motion, real sensor noise, and motion discontinuities. We propose a new set of benchmarks and evaluation methods for the next generation of optical flow algorithms. To that end, we contribute four types of data to test different aspects of optical flow algorithms: (1)\u00a0sequences with nonrigid motion where the ground-truth flow is determined by tracking hidden fluorescent texture, (2)\u00a0realistic synthetic sequences, (3)\u00a0high frame-rate video used to study interpolation error, and (4)\u00a0modified stereo sequences of static scenes. In addition to the average angular error used by Barron et\u00a0al., we compute the absolute flow endpoint error, measures for frame interpolation error, improved statistics, and results at motion discontinuities and in textureless regions. In October 2007, we published the performance of several well-known methods on a preliminary version of our data to establish the current state of the art. We also made the data freely available on the web at http://vision.middlebury.edu/flow/. Subsequently a number of researchers have uploaded their results to our website and published papers using the data. A significant improvement in performance has already been achieved. In this paper we analyze the results obtained to date and draw a large number of conclusions from them."
            },
            "slug": "A-Database-and-Evaluation-Methodology-for-Optical-Baker-Scharstein",
            "title": {
                "fragments": [],
                "text": "A Database and Evaluation Methodology for Optical Flow"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper proposes a new set of benchmarks and evaluation methods for the next generation of optical flow algorithms and analyzes the results obtained to date to draw a large number of conclusions."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37432086"
                        ],
                        "name": "Dennis Park",
                        "slug": "Dennis-Park",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dennis Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143800213"
                        ],
                        "name": "Charless C. Fowlkes",
                        "slug": "Charless-C.-Fowlkes",
                        "structuredName": {
                            "firstName": "Charless",
                            "lastName": "Fowlkes",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charless C. Fowlkes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[84] extended this to a multiresolution model that automatically switches to parts only at sufficiently high resolutions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7481937,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "611f9faa6f3aeff3ccd674d779d52c4f9245376c",
            "isKey": false,
            "numCitedBy": 269,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Most current approaches to recognition aim to be scale-invariant. However, the cues available for recognizing a 300 pixel tall object are qualitatively different from those for recognizing a 3 pixel tall object. We argue that for sensors with finite resolution, one should instead use scale-variant, or multiresolution representations that adapt in complexity to the size of a putative detection window. We describe a multiresolution model that acts as a deformable part-based model when scoring large instances and a rigid template with scoring small instances. We also examine the interplay of resolution and context, and demonstrate that context is most helpful for detecting low-resolution instances when local models are limited in discriminative power. We demonstrate impressive results on the Caltech Pedestrian benchmark, which contains object instances at a wide range of scales. Whereas recent state-of-the-art methods demonstrate missed detection rates of 86%-37% at 1 false-positive-per-image, our multiresolution model reduces the rate to 29%."
            },
            "slug": "Multiresolution-Models-for-Object-Detection-Park-Ramanan",
            "title": {
                "fragments": [],
                "text": "Multiresolution Models for Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work describes a multiresolution model that acts as a deformable part-based model when scoring large instances and a rigid template with scoring small instances and examines the interplay of resolution and context, and demonstrates that context is most helpful for detecting low-resolution instances when local models are limited in discriminative power."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48950628"
                        ],
                        "name": "N. Dalal",
                        "slug": "N.-Dalal",
                        "structuredName": {
                            "firstName": "Navneet",
                            "lastName": "Dalal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dalal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 11
                            }
                        ],
                        "text": "estimation [55] and pairwise max (PM) suppression [71], which discards the less confident of every pair of detections that overlap sufficiently according to (1)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 117
                            }
                        ],
                        "text": "While the features were successful on a per-window basis [54], for full image detection the benefit appeared minimal [55]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 37581938,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47980c6e42f1a3381e6c5f3db7230e6a64c40218",
            "isKey": false,
            "numCitedBy": 369,
            "numCiting": 218,
            "paperAbstract": {
                "fragments": [],
                "text": "This thesis targets the detection of humans and other object classes in images and videos. Our focus is on developing robust feature extraction algorithms that encode image regions as highdimensional feature vectors that support high accuracy object/non-object decisions. To test our feature sets we adopt a relatively simple learning framework that uses linear Support Vector Machines to classify each possible image region as an object or as a non-object. The approach is data-driven and purely bottom-up using low-level appearance and motion vectors to detect objects. As a test case we focus on person detection as people are one of the most challenging object classes with many applications, for example in film and video analysis, pedestrian detection for smart cars and video surveillance. Nevertheless we do not make any strong class specific assumptions and the resulting object detection framework also gives state-of-the-art performance for many other classes including cars, motorbikes, cows and sheep. This thesis makes four main contributions. Firstly, we introduce grids of locally normalised Histograms of Oriented Gradients (HOG) as descriptors for object detection in static images. The HOG descriptors are computed over dense and overlapping grids of spatial blocks, with image gradient orientation features extracted at fixed resolution and gathered into a highdimensional feature vector. They are designed to be robust to small changes in image contour locations and directions, and significant changes in image illumination and colour, while remaining highly discriminative for overall visual form. We show that unsmoothed gradients, fine orientation voting, moderately coarse spatial binning, strong normalisation and overlapping blocks are all needed for good performance. Secondly, to detect moving humans in videos, we propose descriptors based on oriented histograms of differential optical flow. These are similar to static HOG descriptors, but instead of image gradients, they are based on local differentials of dense optical flow. They encode the noisy optical flow estimates into robust feature vectors in a manner that is robust to the overall camera motion. Several variants are proposed, some capturing motion boundaries while others encode the relative motions of adjacent image regions. Thirdly, we propose a general method based on kernel density estimation for fusing multiple overlapping detections, that takes into account the number of detections, their confidence scores and the scales of the detections. Lastly, we present work in progress on a parts based approach to person detection that first detects local body parts like heads, torso, and legs and then fuses them to create a global overall person detector."
            },
            "slug": "Finding-People-in-Images-and-Videos-Dalal",
            "title": {
                "fragments": [],
                "text": "Finding People in Images and Videos"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This thesis introduces grids of locally normalised Histograms of Oriented Gradients (HOG) as descriptors for object detection in static images and proposes descriptors based on oriented histograms of differential optical flow to detect moving humans in videos."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2433494"
                        ],
                        "name": "Andreas Ess",
                        "slug": "Andreas-Ess",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Ess",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Ess"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789756"
                        ],
                        "name": "B. Leibe",
                        "slug": "B.-Leibe",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Leibe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Leibe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144810819"
                        ],
                        "name": "K. Schindler",
                        "slug": "K.-Schindler",
                        "structuredName": {
                            "firstName": "Konrad",
                            "lastName": "Schindler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Schindler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "[30], [38], [39], [40], [41]), we refer readers to [2], [42], [43]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7616980,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0dfdb4d55430b88d01c0a6d7d6e20df0f6a932fc",
            "isKey": false,
            "numCitedBy": 291,
            "numCiting": 71,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we address the problem of multiperson tracking in busy pedestrian zones using a stereo rig mounted on a mobile platform. The complexity of the problem calls for an integrated solution that extracts as much visual information as possible and combines it through cognitive feedback cycles. We propose such an approach, which jointly estimates camera position, stereo depth, object detection, and tracking. The interplay between those components is represented by a graphical model. Since the model has to incorporate object-object interactions and temporal links to past frames, direct inference is intractable. We, therefore, propose a two-stage procedure: for each frame, we first solve a simplified version of the model (disregarding interactions and temporal continuity) to estimate the scene geometry and an overcomplete set of object detections. Conditioned on these results, we then address object interactions, tracking, and prediction in a second step. The approach is experimentally evaluated on several long and difficult video sequences from busy inner-city locations. Our results show that the proposed integration makes it possible to deliver robust tracking performance in scenes of realistic complexity."
            },
            "slug": "Robust-Multiperson-Tracking-from-a-Mobile-Platform-Ess-Leibe",
            "title": {
                "fragments": [],
                "text": "Robust Multiperson Tracking from a Mobile Platform"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This paper addresses the problem of multiperson tracking in busy pedestrian zones using a stereo rig mounted on a mobile platform with a two-stage procedure, which jointly estimates camera position, stereo depth, object detection, and tracking."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48950628"
                        ],
                        "name": "N. Dalal",
                        "slug": "N.-Dalal",
                        "structuredName": {
                            "firstName": "Navneet",
                            "lastName": "Dalal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dalal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 116
                            }
                        ],
                        "text": ", Haar wavelets [44]), color, texture (including LBP [60] and co-occurrence [85]), self-similarity [86], and motion [54] features."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[54] modeled motion statistics based on an optical flow field\u2019s internal differences, thereby compensating for uniform image motion locally."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 57
                            }
                        ],
                        "text": "While the features were successful on a per-window basis [54], for full image detection the benefit appeared minimal [55]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8729004,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44f3ac3277c2eb6e5599739eb875888c46e21d4c",
            "isKey": false,
            "numCitedBy": 1776,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Detecting humans in films and videos is a challenging problem owing to the motion of the subjects, the camera and the background and to variations in pose, appearance, clothing, illumination and background clutter. We develop a detector for standing and moving people in videos with possibly moving cameras and backgrounds, testing several different motion coding schemes and showing empirically that orientated histograms of differential optical flow give the best overall performance. These motion-based descriptors are combined with our Histogram of Oriented Gradient appearance descriptors. The resulting detector is tested on several databases including a challenging test set taken from feature films and containing wide ranges of pose, motion and background variations, including moving cameras and backgrounds. We validate our results on two challenging test sets containing more than 4400 human examples. The combined detector reduces the false alarm rate by a factor of 10 relative to the best appearance-based detector, for example giving false alarm rates of 1 per 20,000 windows tested at 8% miss rate on our Test Set 1."
            },
            "slug": "Human-Detection-Using-Oriented-Histograms-of-Flow-Dalal-Triggs",
            "title": {
                "fragments": [],
                "text": "Human Detection Using Oriented Histograms of Flow and Appearance"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A detector for standing and moving people in videos with possibly moving cameras and backgrounds is developed, testing several different motion coding schemes and showing empirically that orientated histograms of differential optical flow give the best overall performance."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51323684"
                        ],
                        "name": "David Ger\u00f3nimo",
                        "slug": "David-Ger\u00f3nimo",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Ger\u00f3nimo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Ger\u00f3nimo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782314"
                        ],
                        "name": "A. Sappa",
                        "slug": "A.-Sappa",
                        "structuredName": {
                            "firstName": "Angel",
                            "lastName": "Sappa",
                            "middleNames": [
                                "Domingo"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sappa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187725"
                        ],
                        "name": "Antonio M. L\u00f3pez",
                        "slug": "Antonio-M.-L\u00f3pez",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "L\u00f3pez",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antonio M. L\u00f3pez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786419"
                        ],
                        "name": "D. Ponsa",
                        "slug": "D.-Ponsa",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Ponsa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ponsa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 4
                            }
                        ],
                        "text": "CVC [19], and NICTA [22] pedestrian data sets and the"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 41
                            }
                        ],
                        "text": "Older data sets, including the MIT [16], CVC [19], and NICTA [22] pedestrian data sets and the Daimler classification benchmark (Daimler-CB) [21], tend to contain cropped pedestrian windows only."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6925699,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "57fe081950f21ca03b5b375ae3e84b399c015861",
            "isKey": false,
            "numCitedBy": 86,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "On-board pedestrian detection is in the frontier of the state- of-the-art since it implies processing outdoor scenarios from a mobile platform and searching for aspect-changing objects in cluttered urban environments. Most promising approaches include the development of classifiers based on feature selection and machine learning. However, they use a large number of features which compromises real-time. Thus, methods for running the classifiers in only a few image windows must be provided. In this paper we contribute in both aspects, proposing a cam- era pose estimation method for adaptive sparse image sampling, as well as a classifier for pedestrian detection based on Haar wavelets and edge orientation histograms as features and AdaBoost as learning machine. Both proposals are compared with relevant approaches in the literature, showing comparable results but reducing processing time by four for the sampling tasks and by ten for the classification one."
            },
            "slug": "Adaptive-Image-Sampling-and-Windows-Classification-Ger\u00f3nimo-Sappa",
            "title": {
                "fragments": [],
                "text": "Adaptive Image Sampling and Windows Classification for On-board Pedestrian Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A cam- era pose estimation method for adaptive sparse image sampling, as well as a classifier for pedestrian detection based on Haar wavelets and edge orientation histograms as features and AdaBoost as learning machine are proposed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2362117"
                        ],
                        "name": "M. Bajracharya",
                        "slug": "M.-Bajracharya",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Bajracharya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bajracharya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153485128"
                        ],
                        "name": "B. Moghaddam",
                        "slug": "B.-Moghaddam",
                        "structuredName": {
                            "firstName": "Baback",
                            "lastName": "Moghaddam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Moghaddam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144692242"
                        ],
                        "name": "A. Howard",
                        "slug": "A.-Howard",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Howard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Howard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7207299"
                        ],
                        "name": "Shane Brennan",
                        "slug": "Shane-Brennan",
                        "structuredName": {
                            "firstName": "Shane",
                            "lastName": "Brennan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shane Brennan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782162"
                        ],
                        "name": "L. Matthies",
                        "slug": "L.-Matthies",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Matthies",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Matthies"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "[38], [39], [40], [41]), we refer readers to [2], [42], [43]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 12109566,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df9a016950ffaaa8526e7332f0a6568ad43d054f",
            "isKey": false,
            "numCitedBy": 90,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we describe a fully integrated system for detecting, localizing, and tracking pedestrians from a moving vehicle. The system can reliably detect upright pedestrians to a range of 40 m in lightly cluttered urban environments. The system uses range data from stereo vision to segment the scene into regions of interest, from which shape features are extracted and used to classify pedestrians. The regions are tracked using shape and appearance features. Tracking is used to temporally filter classifications to improve performance and to estimate the velocity of pedestrians for use in path planning. The end-to-end system runs at 5 Hz on 1,024 \u00d7 768 imagery using a standard 2.4 GHz Intel Core 2 Quad processor, and has been integrated and tested on multiple ground vehicles and environments. We show performance on a diverse set of datasets with groundtruth in outdoor environments with varying degrees of pedestrian density and clutter. In highly cluttered urban environments, the detection rates are on a par with state-of-the-art but significantly slower systems."
            },
            "slug": "A-Fast-Stereo-based-System-for-Detecting-and-from-a-Bajracharya-Moghaddam",
            "title": {
                "fragments": [],
                "text": "A Fast Stereo-based System for Detecting and Tracking Pedestrians from a Moving Vehicle"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A fully integrated system for detecting, localizing, and tracking pedestrians from a moving vehicle that can reliably detect upright pedestrians to a range of 40 m in lightly cluttered urban environments and on a diverse set of datasets with groundtruth in outdoor environments with varying degrees of pedestrian density and clutter."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Wei Zhang",
                        "slug": "Wei-Zhang",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696991"
                        ],
                        "name": "G. Zelinsky",
                        "slug": "G.-Zelinsky",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Zelinsky",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Zelinsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145654220"
                        ],
                        "name": "D. Samaras",
                        "slug": "D.-Samaras",
                        "structuredName": {
                            "firstName": "Dimitris",
                            "lastName": "Samaras",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Samaras"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 172
                            }
                        ],
                        "text": "Moreover, numerous approaches have been proposed for speeding up detection, including speeding up the detector itself [29], [44], [46], through use of approximations [63], [89] or by using special purpose hardware such as GPUs [90] (for a review of fast detection see [63])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8356979,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "340ad17611fddb1f59a8b50114839544878229b7",
            "isKey": false,
            "numCitedBy": 134,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a multi-resolution framework inspired by human visual search for general object detection. Different resolutions are represented using a coarse-to-fine feature hierarchy. During detection, the lower resolution features are initially used to reject the majority of negative windows at relatively low cost, leaving a relatively small number of windows to be processed in higher resolutions. This enables the use of computationally more expensive higher resolution features to achieve high detection accuracy. We applied this framework on Histograms of Oriented Gradient (HOG) features for object detection. Our multi-resolution detector produced better performance for pedestrian detection than state-of-the-art methods (Dalal and Triggs, 2005), and was faster during both training and testing. Testing our method on motorbikes and cars from the VOC database revealed similar improvements in both speed and accuracy, suggesting that our approach is suitable for realtime general object detection applications."
            },
            "slug": "Real-time-Accurate-Object-Detection-using-Multiple-Zhang-Zelinsky",
            "title": {
                "fragments": [],
                "text": "Real-time Accurate Object Detection using Multiple Resolutions"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "This work proposes a multi-resolution framework inspired by human visual search for general object detection that produced better performance for pedestrian detection than state-of-the-art methods, and was faster during both training and testing."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2695106"
                        ],
                        "name": "Sibt ul Hussain",
                        "slug": "Sibt-ul-Hussain",
                        "structuredName": {
                            "firstName": "Sibt",
                            "lastName": "Hussain",
                            "middleNames": [
                                "ul"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sibt ul Hussain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 15
                            }
                        ],
                        "text": "In addition\nto HOG and LBP, [61] used local ternary patterns (variants of LBP)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 93
                            }
                        ],
                        "text": "Performance improves for all methods with increasing scale, but most for MULTIFTR+MOTION, HOGLBP, and LATSVM-V2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "In addition to HOG and LBP, [61] used local ternary patterns (variants of LBP)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10962824,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e6fa3922f146b002411a472564b1532a71ce5e1a",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a family of object detectors that provides state-of-the-art error rates on several important datasets including INRIA people and PASCAL VOC'06 and VOC'07. The method builds on a number of recent advances. It uses the Latent SVM learning framework and a rich visual feature set that incorporates Histogram of Oriented Gradient, Local Binary Pattern and Local Ternary Pattern descriptors. Partial Least Squares dimensionality reduction is included to speed the training of the basic classifier with no loss of accuracy, and to allow a two-stage quadratic classifier that further improves the results. \\iflong A simple sparsification technique can reduce the size of the feature set by around 70% with little loss of accuracy.\\fi We evaluate our methods and compare them to other recent ones on several datasets. Our basic root detectors outperform the single component part-based ones of Felzenszwalb et.al on 9 of 10 classes of VOC'06 (12% increase in Mean Average Precision) and 11 of 20 classes of VOC'07 (7% increase in MAP). On the INRIA Person dataset, they increase the Average Precision by 12% relative to Dalal \\& Triggs."
            },
            "slug": "Feature-Sets-and-Dimensionality-Reduction-for-Hussain-Triggs",
            "title": {
                "fragments": [],
                "text": "Feature Sets and Dimensionality Reduction for Visual Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A family of object detectors that provides state-of-the-art error rates on several important datasets including INRIA people and PASCAL V OC'06 and VOC'07 is described and partial Least Squares dimensionality reduction is included to speed the training of the basic classifier with no loss of accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2797063"
                        ],
                        "name": "S. Agarwal",
                        "slug": "S.-Agarwal",
                        "structuredName": {
                            "firstName": "Shivani",
                            "lastName": "Agarwal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Agarwal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144590225"
                        ],
                        "name": "D. Roth",
                        "slug": "D.-Roth",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Roth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 161
                            }
                        ],
                        "text": "Notable early approaches for unsupervised part learning, including the constellation model [79], [80] and the sparse representation approach of Agarwal and Roth [81], relied on keypoints."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 262977,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d081b80b1850df9b1e382f97a7a244890d6485e",
            "isKey": false,
            "numCitedBy": 638,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an approach for learning to detect objects in still gray images, that is based on a sparse, part-based representation of objects. A vocabulary of information-rich object parts is automatically constructed from a set of sample images of the object class of interest. Images are then represented using parts from this vocabulary, along with spatial relations observed among them. Based on this representation, a feature-efficient learning algorithm is used to learn to detect instances of the object class. The framework developed can be applied to any object with distinguishable parts in a relatively fixed spatial configuration. We report experiments on images of side views of cars. Our experiments show that the method achieves high detection accuracy on a difficult test set of real-world images, and is highly robust to partial occlusion and background variation.In addition, we discuss and offer solutions to several methodological issues that are significant for the research community to be able to evaluate object detection approaches."
            },
            "slug": "Learning-a-Sparse-Representation-for-Object-Agarwal-Roth",
            "title": {
                "fragments": [],
                "text": "Learning a Sparse Representation for Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "An approach for learning to detect objects in still gray images, that is based on a sparse, part-based representation of objects, that achieves high detection accuracy on a difficult test set of real-world images, and is highly robust to partial occlusion and background variation."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712041"
                        ],
                        "name": "K. Mikolajczyk",
                        "slug": "K.-Mikolajczyk",
                        "structuredName": {
                            "firstName": "Krystian",
                            "lastName": "Mikolajczyk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Mikolajczyk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "stage approaches have been revisited over time [17], [74],"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3870070,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9cc6155cd69023a736a7b8f8680bcd6232c840e",
            "isKey": false,
            "numCitedBy": 764,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a novel method for human detection in single images which can detect full bodies as well as close-up views in the presence of clutter and occlusion. Humans are modeled as flexible assemblies of parts, and robust part detection is the key to the approach. The parts are represented by co-occurrences of local features which captures the spatial layout of the partrsquos appearance. Feature selection and the part detectors are learnt from training images using AdaBoost. The detection algorithm is very efficient as (i) all part detectors use the same initial features, (ii) a coarse-to-fine cascade approach is used for part detection, (iii) a part assembly strategy reduces the number of spurious detections and the search space. The results outperform existing human detectors."
            },
            "slug": "Human-Detection-Based-on-a-Probabilistic-Assembly-Mikolajczyk-Schmid",
            "title": {
                "fragments": [],
                "text": "Human Detection Based on a Probabilistic Assembly of Robust Part Detectors"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A novel method for human detection in single images which can detect full bodies as well as close-up views in the presence of clutter and occlusion is described."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2433494"
                        ],
                        "name": "Andreas Ess",
                        "slug": "Andreas-Ess",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Ess",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Ess"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789756"
                        ],
                        "name": "B. Leibe",
                        "slug": "B.-Leibe",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Leibe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Leibe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 18
                            }
                        ],
                        "text": "Note that the ETH [4] and TUD-Brussels [5] evaluation protocols also suggested standardizing the aspect ratio, although to an arbitrarily chosen constant of 0."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 65
                            }
                        ],
                        "text": "six additional pedestrian detection data sets, including the ETH [4], TUD-Brussels [5], Daimler [6], and INRIA [7] data sets and two variants of the Caltech data set (see Fig."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 161
                            }
                        ],
                        "text": "To increase the scope of our analysis, we benchmarked the detectors on six additional pedestrian detection data sets, including INRIA [7], TUD-Brussels [5], ETH [4], DaimlerDB [6], Caltech-Training, and Caltech-Japan."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 15
                            }
                        ],
                        "text": "INRIA [7], ETH [4], and TUD-Brussels [5] pedestrian data"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 64
                            }
                        ],
                        "text": "taken from a mobile recording setup, such as a robot or vehicle [4], [5], [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 83
                            }
                        ],
                        "text": "As mentioned, in our performance evaluation we additionally use the INRIA [7], ETH [4], TUD-Brussels [5], and Daimler-DB [6] data sets."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16824129,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "590941c26d057f917c7f5275824d350e9aac7ba3",
            "isKey": true,
            "numCitedBy": 545,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we address the challenging problem of simultaneous pedestrian detection and ground-plane estimation from video while walking through a busy pedestrian zone. Our proposed system integrates robust stereo depth cues, ground-plane estimation, and appearance-based object detection in a principled fashion using a graphical model. Object-object occlusions lead to complex interactions in this model that make an exact solution computationally intractable. We therefore propose a novel iterative approach that first infers scene geometry using belief propagation and then resolves interactions between objects using a global optimization procedure. This approach leads to a robust solution in few iterations, while allowing object detection to benefit from geometry estimation and vice versa. We quantitatively evaluate the performance of our proposed approach on several challenging test sequences showing strolls through busy shopping streets. Comparisons to various baseline systems show that it outperforms both a system using no scene geometry and one just relying on structure-from-motion without dense stereo."
            },
            "slug": "Depth-and-Appearance-for-Mobile-Scene-Analysis-Ess-Leibe",
            "title": {
                "fragments": [],
                "text": "Depth and Appearance for Mobile Scene Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work proposes a novel iterative approach that first infers scene geometry using belief propagation and then resolves interactions between objects using a global optimization procedure, which leads to a robust solution in few iterations, while allowing object detection to benefit from geometry estimation and vice versa."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1769383"
                        ],
                        "name": "Lubomir D. Bourdev",
                        "slug": "Lubomir-D.-Bourdev",
                        "structuredName": {
                            "firstName": "Lubomir",
                            "lastName": "Bourdev",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lubomir D. Bourdev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 0
                            }
                        ],
                        "text": "Likewise, Bourdev and Malik [76] proposed to learn an exhaustive dictionary of \u201cposelets:\u201d parts clustered jointly in appearance and pose."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "Likewise, Bourdev and Malik [76] proposed to learn an exhaustive dictionary of \u201cposelets:\u201d parts clustered jointly in"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9320620,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "55b29a2505149d06d8c1d616cd30edca40cb029c",
            "isKey": false,
            "numCitedBy": 1048,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the classic problems of detection, segmentation and pose estimation of people in images with a novel definition of a part, a poselet. We postulate two criteria (1) It should be easy to find a poselet given an input image (2) it should be easy to localize the 3D configuration of the person conditioned on the detection of a poselet. To permit this we have built a new dataset, H3D, of annotations of humans in 2D photographs with 3D joint information, inferred using anthropometric constraints. This enables us to implement a data-driven search procedure for finding poselets that are tightly clustered in both 3D joint configuration space as well as 2D image appearance. The algorithm discovers poselets that correspond to frontal and profile faces, pedestrians, head and shoulder views, among others. Each poselet provides examples for training a linear SVM classifier which can then be run over the image in a multiscale scanning mode. The outputs of these poselet detectors can be thought of as an intermediate layer of nodes, on top of which one can run a second layer of classification or regression. We show how this permits detection and localization of torsos or keypoints such as left shoulder, nose, etc. Experimental results show that we obtain state of the art performance on people detection in the PASCAL VOC 2007 challenge, among other datasets. We are making publicly available both the H3D dataset as well as the poselet parameters for use by other researchers."
            },
            "slug": "Poselets:-Body-part-detectors-trained-using-3D-pose-Bourdev-Malik",
            "title": {
                "fragments": [],
                "text": "Poselets: Body part detectors trained using 3D human pose annotations"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A new dataset, H3D, is built of annotations of humans in 2D photographs with 3D joint information, inferred using anthropometric constraints, to address the classic problems of detection, segmentation and pose estimation of people in images with a novel definition of a part, a poselet."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE 12th International Conference on Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40025842"
                        ],
                        "name": "D. G\u00f3mez",
                        "slug": "D.-G\u00f3mez",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "G\u00f3mez",
                            "middleNames": [
                                "Ger\u00f3nimo"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. G\u00f3mez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187725"
                        ],
                        "name": "Antonio M. L\u00f3pez",
                        "slug": "Antonio-M.-L\u00f3pez",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "L\u00f3pez",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antonio M. L\u00f3pez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782314"
                        ],
                        "name": "A. Sappa",
                        "slug": "A.-Sappa",
                        "structuredName": {
                            "firstName": "Angel",
                            "lastName": "Sappa",
                            "middleNames": [
                                "Domingo"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sappa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143918350"
                        ],
                        "name": "T. Graf",
                        "slug": "T.-Graf",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Graf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Graf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206764452,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "451c3bea362a95e59673584b35818d426c8104f5",
            "isKey": false,
            "numCitedBy": 975,
            "numCiting": 152,
            "paperAbstract": {
                "fragments": [],
                "text": "Advanced driver assistance systems (ADASs), and particularly pedestrian protection systems (PPSs), have become an active research area aimed at improving traffic safety. The major challenge of PPSs is the development of reliable on-board pedestrian detection systems. Due to the varying appearance of pedestrians (e.g., different clothes, changing size, aspect ratio, and dynamic shape) and the unstructured environment, it is very difficult to cope with the demanded robustness of this kind of system. Two problems arising in this research area are the lack of public benchmarks and the difficulty in reproducing many of the proposed methods, which makes it difficult to compare the approaches. As a result, surveying the literature by enumerating the proposals one--after-another is not the most useful way to provide a comparative point of view. Accordingly, we present a more convenient strategy to survey the different approaches. We divide the problem of detecting pedestrians from images into different processing steps, each with attached responsibilities. Then, the different proposed methods are analyzed and classified with respect to each processing stage, favoring a comparative viewpoint. Finally, discussion of the important topics is presented, putting special emphasis on the future needs and challenges."
            },
            "slug": "Survey-of-Pedestrian-Detection-for-Advanced-Driver-G\u00f3mez-L\u00f3pez",
            "title": {
                "fragments": [],
                "text": "Survey of Pedestrian Detection for Advanced Driver Assistance Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work divides the problem of detecting pedestrians from images into different processing steps, each with attached responsibilities, and separates the different proposed methods with respect to each processing stage, favoring a comparative viewpoint."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400556488"
                        ],
                        "name": "Aharon Bar-Hillel",
                        "slug": "Aharon-Bar-Hillel",
                        "structuredName": {
                            "firstName": "Aharon",
                            "lastName": "Bar-Hillel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aharon Bar-Hillel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145176749"
                        ],
                        "name": "D. Levi",
                        "slug": "D.-Levi",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Levi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Levi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38744822"
                        ],
                        "name": "Eyal Krupka",
                        "slug": "Eyal-Krupka",
                        "structuredName": {
                            "firstName": "Eyal",
                            "lastName": "Krupka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eyal Krupka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11169951"
                        ],
                        "name": "Chen Goldberg",
                        "slug": "Chen-Goldberg",
                        "structuredName": {
                            "firstName": "Chen",
                            "lastName": "Goldberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chen Goldberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[68], who introduced a scheme for synthesizing and combining a"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15336938,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff776ffe1fd49dd13db97b75176d4fa90ead65a3",
            "isKey": false,
            "numCitedBy": 106,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new approach for learning part-based object detection through feature synthesis. Our method consists of an iterative process of feature generation and pruning. A feature generation procedure is presented in which basic part-based features are developed into a feature hierarchy using operators for part localization, part refining and part combination. Feature pruning is done using a new feature selection algorithm for linear SVM, termed Predictive Feature Selection (PFS), which is governed by weight prediction. The algorithm makes it possible to choose from O(106) features in an efficient but accurate manner. We analyze the validity and behavior of PFS and empirically demonstrate its speed and accuracy advantages over relevant competitors. We present an empirical evaluation of our method on three human detection datasets including the current de-facto benchmarks (the INRIA and Caltech pedestrian datasets) and a new challenging dataset of children images in difficult poses. The evaluation suggests that our approach is on a par with the best current methods and advances the state-of-the-art on the Caltech pedestrian training dataset."
            },
            "slug": "Part-Based-Feature-Synthesis-for-Human-Detection-Bar-Hillel-Levi",
            "title": {
                "fragments": [],
                "text": "Part-Based Feature Synthesis for Human Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "An empirical evaluation of the new approach for learning part-based object detection through feature synthesis suggests that the approach is on a par with the best current methods and advances the state-of-the-art on the Caltech pedestrian training dataset."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144471901"
                        ],
                        "name": "I. Parra",
                        "slug": "I.-Parra",
                        "structuredName": {
                            "firstName": "Ignacio",
                            "lastName": "Parra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Parra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729433"
                        ],
                        "name": "D. F. Llorca",
                        "slug": "D.-F.-Llorca",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Llorca",
                            "middleNames": [
                                "Fern\u00e1ndez"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. F. Llorca"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750559"
                        ],
                        "name": "M. Sotelo",
                        "slug": "M.-Sotelo",
                        "structuredName": {
                            "firstName": "Miguel",
                            "lastName": "Sotelo",
                            "middleNames": [
                                "\u00c1ngel"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sotelo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683950"
                        ],
                        "name": "L. M. Bergasa",
                        "slug": "L.-M.-Bergasa",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Bergasa",
                            "middleNames": [
                                "Miguel"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. M. Bergasa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685786"
                        ],
                        "name": "P. R. D. Toro",
                        "slug": "P.-R.-D.-Toro",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Toro",
                            "middleNames": [
                                "Revenga",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. R. D. Toro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31418350"
                        ],
                        "name": "J. Nuevo",
                        "slug": "J.-Nuevo",
                        "structuredName": {
                            "firstName": "Jes\u00fas",
                            "lastName": "Nuevo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nuevo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738823"
                        ],
                        "name": "M. Oca\u00f1a",
                        "slug": "M.-Oca\u00f1a",
                        "structuredName": {
                            "firstName": "Manuel",
                            "lastName": "Oca\u00f1a",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Oca\u00f1a"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2383664"
                        ],
                        "name": "M. A. Garrido",
                        "slug": "M.-A.-Garrido",
                        "structuredName": {
                            "firstName": "Miguel",
                            "lastName": "Garrido",
                            "middleNames": [
                                "\u00c1ngel",
                                "Garc\u00eda"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. A. Garrido"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[38], [39], [40], [41]), we refer readers to [2], [42], [43]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9148635,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d4953d84b893a8d6784ac4401728c6d13ae8b709",
            "isKey": false,
            "numCitedBy": 191,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a comprehensive combination of feature extraction methods for vision-based pedestrian detection in Intelligent Transportation Systems. The basic components of pedestrians are first located in the image and then combined with a support-vector-machine-based classifier. This poses the problem of pedestrian detection in real cluttered road images. Candidate pedestrians are located using a subtractive clustering attention mechanism based on stereo vision. A components-based learning approach is proposed in order to better deal with pedestrian variability, illumination conditions, partial occlusions, and rotations. Extensive comparisons have been carried out using different feature extraction methods as a key to image understanding in real traffic conditions. A database containing thousands of pedestrian samples extracted from real traffic images has been created for learning purposes at either daytime or nighttime. The results achieved to date show interesting conclusions that suggest a combination of feature extraction methods as an essential clue for enhanced detection performance"
            },
            "slug": "Combination-of-Feature-Extraction-Methods-for-SVM-Parra-Llorca",
            "title": {
                "fragments": [],
                "text": "Combination of Feature Extraction Methods for SVM Pedestrian Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A components-based learning approach is proposed in order to better deal with pedestrian variability, illumination conditions, partial occlusions, and rotations and suggest a combination of feature extraction methods as an essential clue for enhanced detection performance."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Intelligent Transportation Systems"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118775664"
                        ],
                        "name": "Xiaoyu Wang",
                        "slug": "Xiaoyu-Wang",
                        "structuredName": {
                            "firstName": "Xiaoyu",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoyu Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3244463"
                        ],
                        "name": "T. Han",
                        "slug": "T.-Han",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Han",
                            "middleNames": [
                                "X."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Han"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143653681"
                        ],
                        "name": "Shuicheng Yan",
                        "slug": "Shuicheng-Yan",
                        "structuredName": {
                            "firstName": "Shuicheng",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuicheng Yan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 438,
                                "start": 434
                            }
                        ],
                        "text": "In a few cases, the evaluated code differed from the published version: SHAPELET and HIKSVM have been corrected so they no longer overfit to boundary effects; we evaluate a variant of POSEINV based on boosting (which in our tests outperformed the much slower kernel SVM version); PLS switched to PM NMS; and finally, the posted code for HOGLBP does not include occlusion reasoning (the improvement from occlusion reasoning was slight [59])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[59] combined a texture descriptor based on local binary patterns (LBP) [60] with HOG [HOGLBP]; additionally, a linear SVM classifier was modified to perform basic occlusion reasoning."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2475434,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd9446d2b61139867662442147d81181e84ab4f2",
            "isKey": false,
            "numCitedBy": 1700,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "By combining Histograms of Oriented Gradients (HOG) and Local Binary Pattern (LBP) as the feature set, we propose a novel human detection approach capable of handling partial occlusion. Two kinds of detectors, i.e., global detector for whole scanning windows and part detectors for local regions, are learned from the training data using linear SVM. For each ambiguous scanning window, we construct an occlusion likelihood map by using the response of each block of the HOG feature to the global detector. The occlusion likelihood map is then segmented by Mean-shift approach. The segmented portion of the window with a majority of negative response is inferred as an occluded region. If partial occlusion is indicated with high likelihood in a certain scanning window, part detectors are applied on the unoccluded regions to achieve the final classification on the current scanning window. With the help of the augmented HOG-LBP feature and the global-part occlusion handling method, we achieve a detection rate of 91.3% with FPPW= 10\u22126, 94.7% with FPPW= 10\u22125, and 97.9% with FPPW= 10\u22124 on the INRIA dataset, which, to our best knowledge, is the best human detection performance on the INRIA dataset. The global-part occlusion handling method is further validated using synthesized occlusion data constructed from the INRIA and Pascal dataset."
            },
            "slug": "An-HOG-LBP-human-detector-with-partial-occlusion-Wang-Han",
            "title": {
                "fragments": [],
                "text": "An HOG-LBP human detector with partial occlusion handling"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "By combining Histograms of Oriented Gradients (HOG) and Local Binary Pattern (LBP) as the feature set, this work proposes a novel human detection approach capable of handling partial occlusion and achieves the best human detection performance on the INRIA dataset."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE 12th International Conference on Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709053"
                        ],
                        "name": "Daniel Scharstein",
                        "slug": "Daniel-Scharstein",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Scharstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Scharstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717841"
                        ],
                        "name": "R. Szeliski",
                        "slug": "R.-Szeliski",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Szeliski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Szeliski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 83
                            }
                        ],
                        "text": "data sets, the Berkeley Segmentation Data Set [10], the Middlebury Stereo Data Set [11], and the Caltech 101 [12],"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 110
                            }
                        ],
                        "text": "The Barron et al. [8] and Middlebury [9] optical flow data sets, the Berkeley Segmentation Data Set [10], the\nMiddlebury Stereo Data Set [11], and the Caltech 101 [12], Caltech 256 [13], and PASCAL [14] object recognition data sets all improved performance evaluation, added challenge, and helped drive innovation in their respective fields."
                    },
                    "intents": []
                }
            ],
            "corpusId": 195859047,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d2f78c2b2b325d72f359d4c797c9aab6a8e60942",
            "isKey": false,
            "numCitedBy": 3002,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "Stereo matching is one of the most active research areas in computer vision. While a large number of algorithms for stereo correspondence have been developed, relatively little work has been done on characterizing their performance. In this paper, we present a taxonomy of dense, two-frame stereo methods. Our taxonomy is designed to assess the different components and design decisions made in individual stereo algorithms. Using this taxonomy, we compare existing stereo methods and present experiments evaluating the performance of many different variants. In order to establish a common software platform and a collection of data sets for easy evaluation, we have designed a stand-alone, flexible C++ implementation that enables the evaluation of individual components and that can easily be extended to include new algorithms. We have also produced several new multi-frame stereo data sets with ground truth and are making both the code and data sets available on the Web. Finally, we include a comparative evaluation of a large set of today's best-performing stereo algorithms."
            },
            "slug": "A-Taxonomy-and-Evaluation-of-Dense-Two-Frame-Stereo-Scharstein-Szeliski",
            "title": {
                "fragments": [],
                "text": "A taxonomy and evaluation of dense two-frame stereo correspondence algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This paper has designed a stand-alone, flexible C++ implementation that enables the evaluation of individual components and that can easily be extended to include new algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE Workshop on Stereo and Multi-Baseline Vision (SMBV 2001)"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2340109"
                        ],
                        "name": "C. Wojek",
                        "slug": "C.-Wojek",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Wojek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Wojek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145920814"
                        ],
                        "name": "S. Roth",
                        "slug": "S.-Roth",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144810819"
                        ],
                        "name": "K. Schindler",
                        "slug": "K.-Schindler",
                        "structuredName": {
                            "firstName": "Konrad",
                            "lastName": "Schindler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Schindler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "[38], [39], [40], [41]), we refer readers to [2], [42], [43]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 117
                            }
                        ],
                        "text": "Note that full tracking may be unnecessary and methods that integrate detector outputs over a few frames may suffice [41]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1777736,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "63affa01d5b89d5d7faf89ca8155b88a3ba38759",
            "isKey": false,
            "numCitedBy": 91,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Scene understanding has (again) become a focus of computer vision research, leveraging advances in detection, context modeling, and tracking. In this paper, we present a novel probabilistic 3D scene model that encompasses multi-class object detection, object tracking, scene labeling, and 3D geometric relations. This integrated 3D model is able to represent complex interactions like inter-object occlusion, physical exclusion between objects, and geometric context. Inference allows to recover 3D scene context and perform 3D multiobject tracking from a mobile observer, for objects of multiple categories, using only monocular video as input. In particular, we show that a joint scene track-let model for the evidence collected over multiple frames substantially improves performance. The approach is evaluated for two different types of challenging on-board sequences. We first show a substantial improvement to the state-of-the-art in 3D multi-people tracking. Moreover, a similar performance gain is achieved for multi-class 3D tracking of cars and trucks on a new, challenging dataset."
            },
            "slug": "Monocular-3D-Scene-Modeling-and-Inference:-Traffic-Wojek-Roth",
            "title": {
                "fragments": [],
                "text": "Monocular 3D Scene Modeling and Inference: Understanding Multi-Object Traffic Scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A novel probabilistic 3D scene model that encompasses multi-class object detection, object tracking, scene labeling, and 3D geometric relations is presented, able to represent complex interactions like inter-object occlusion, physical exclusion between objects, and geometric context."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083209039"
                        ],
                        "name": "Edgar Seemann",
                        "slug": "Edgar-Seemann",
                        "structuredName": {
                            "firstName": "Edgar",
                            "lastName": "Seemann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edgar Seemann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739548"
                        ],
                        "name": "Mario Fritz",
                        "slug": "Mario-Fritz",
                        "structuredName": {
                            "firstName": "Mario",
                            "lastName": "Fritz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mario Fritz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 20
                            }
                        ],
                        "text": ", in crowded scenes [25], but in practice the effect is minor."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6240352,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4736c2b507a05aa3848dc049003a6b1bd62da221",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Object class detection in scenes of realistic complexity remains a challenging task in computer vision. Most recent approaches focus on a single and general model for object class detection. However, in particular in the context of image sequences, it may be advantageous to adapt the general model to a more object-instance specific model in order to detect this particular object reliably within the image sequence. In this work we present a generative object model that is capable to scale from a general object class model to a more specific object-instance model. This allows to detect class instances as well as to distinguish between individual object instances reliably. We experimentally evaluate the performance of the proposed system on both still images and image sequences."
            },
            "slug": "Towards-Robust-Pedestrian-Detection-in-Crowded-Seemann-Fritz",
            "title": {
                "fragments": [],
                "text": "Towards Robust Pedestrian Detection in Crowded Image Sequences"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work presents a generative object model that is capable to scale from a general object class model to a more specific object-instance model that allows to detect class instances as well as to distinguish between individual object instances reliably."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2124866301"
                        ],
                        "name": "A. Mohan",
                        "slug": "A.-Mohan",
                        "structuredName": {
                            "firstName": "Anuj",
                            "lastName": "Mohan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mohan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145030811"
                        ],
                        "name": "C. Papageorgiou",
                        "slug": "C.-Papageorgiou",
                        "structuredName": {
                            "firstName": "Constantine",
                            "lastName": "Papageorgiou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Papageorgiou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[73]"
                    },
                    "intents": []
                }
            ],
            "corpusId": 2559322,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "448bd4e124175ad358078a7b930ecad994c97812",
            "isKey": false,
            "numCitedBy": 1137,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a general example-based framework for detecting objects in static images by components. The technique is demonstrated by developing a system that locates people in cluttered scenes. The system is structured with four distinct example-based detectors that are trained to separately find the four components of the human body: the head, legs, left arm, and right arm. After ensuring that these components are present in the proper geometric configuration, a second example-based classifier combines the results of the component detectors to classify a pattern as either a \"person\" or a \"nonperson.\" We call this type of hierarchical architecture, in which learning occurs at multiple stages, an adaptive combination of classifiers (ACC). We present results that show that this system performs significantly better than a similar full-body person detector. This suggests that the improvement in performance is due to the component-based approach and the ACC data classification architecture. The algorithm is also more robust than the full-body person detection method in that it is capable of locating partially occluded views of people and people whose body parts have little contrast with the background."
            },
            "slug": "Example-Based-Object-Detection-in-Images-by-Mohan-Papageorgiou",
            "title": {
                "fragments": [],
                "text": "Example-Based Object Detection in Images by Components"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Results suggest that the improvement in performance is due to the component-based approach and the ACC data classification architecture, which is capable of locating partially occluded views of people and people whose body parts have little contrast with the background."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39599498"
                        ],
                        "name": "Chunhui Gu",
                        "slug": "Chunhui-Gu",
                        "structuredName": {
                            "firstName": "Chunhui",
                            "lastName": "Gu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunhui Gu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109780936"
                        ],
                        "name": "Joseph J. Lim",
                        "slug": "Joseph-J.-Lim",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Lim",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph J. Lim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778133"
                        ],
                        "name": "Pablo Arbel\u00e1ez",
                        "slug": "Pablo-Arbel\u00e1ez",
                        "structuredName": {
                            "firstName": "Pablo",
                            "lastName": "Arbel\u00e1ez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pablo Arbel\u00e1ez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652147"
                        ],
                        "name": "J. Malik",
                        "slug": "J.-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[35] or keypoint [36], [37] based methods often fail."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2100273,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "42a5da01c672b2d37e76c62f851c1f88e6b988c0",
            "isKey": false,
            "numCitedBy": 433,
            "numCiting": 112,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a unified framework for object detection, segmentation, and classification using regions. Region features are appealing in this context because: (1) they encode shape and scale information of objects naturally; (2) they are only mildly affected by background clutter. Regions have not been popular as features due to their sensitivity to segmentation errors. In this paper, we start by producing a robust bag of overlaid regions for each image using Arbeldez et al., CVPR 2009. Each region is represented by a rich set of image cues (shape, color and texture). We then learn region weights using a max-margin framework. In detection and segmentation, we apply a generalized Hough voting scheme to generate hypotheses of object locations, scales and support, followed by a verification classifier and a constrained segmenter on each hypothesis. The proposed approach significantly outperforms the state of the art on the ETHZ shape database(87.1% average detection rate compared to Ferrari et al. 's 67.2%), and achieves competitive performance on the Caltech 101 database."
            },
            "slug": "Recognition-using-regions-Gu-Lim",
            "title": {
                "fragments": [],
                "text": "Recognition using regions"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "This paper presents a unified framework for object detection, segmentation, and classification using regions using a generalized Hough voting scheme to generate hypotheses of object locations, scales and support, followed by a verification classifier and a constrained segmenter on each hypothesis."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144035504"
                        ],
                        "name": "Z. Tu",
                        "slug": "Z.-Tu",
                        "structuredName": {
                            "firstName": "Zhuowen",
                            "lastName": "Tu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Tu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 108
                            }
                        ],
                        "text": "Of all data sets, performance is best on INRIA, which contains high-resolution pedestrians, with LATSVM-V2, CHNFTRS, and FPDW achieving log-average miss rates of 20-22 percent (see Fig."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 212
                            }
                        ],
                        "text": "PM requires only a single parameter; in addition, a variant has been proposed (PM*) that allows a detection to match any subregion of another detection, resulting in improved performance (see (2) and addendum to [29])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 118
                            }
                        ],
                        "text": "Moreover, numerous approaches have been proposed for speeding up detection, including speeding up the detector itself [29], [44], [46], through use of approximations [63], [89] or by using special purpose hardware such as GPUs [90] (for a review of fast detection see [63])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 30
                            }
                        ],
                        "text": "For example, MULTIFTR+MOTION, CHNFTRS, and FPDW are all significantly better than HOG (since they are not linked)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 63
                            }
                        ],
                        "text": "MULTIFTR+MOTION slightly outperforms the other detectors, with CHNFTRS a close second."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 115
                            }
                        ],
                        "text": "Detection speed for pedestrians over 100 pixels ranges from 0:02 to 6:5 fps achieved by FPDW, a sped up version of CHNFTRS. Detecting 50 pixel pedestrians typically requires image upsampling; the slowest detectors require around 5 minutes per frame."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 57
                            }
                        ],
                        "text": "8 we plot the performance of CHNFTRS50, which is CHNFTRS [29] applied to detect pedestrians over 50 pixels."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[29] proposed an extension of [VJ] where"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 140
                            }
                        ],
                        "text": "While the slowest detector happens to also be the most accurate (MUTLIFTR+MOTION), on pedestrians over 50 pixels the two fastest detectors, CHNFTRS and FPDW, are also the second and third most accurate, respectively."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 115
                            }
                        ],
                        "text": "Performance drops significantly even under partial occlusion, leading to a log-average miss rate of 73 percent for CHNFTRS and\nMULTIFTR+MOTION."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "CHNFTRS and FPDW came in second and third with a mean rank of 3.3 and 3.8, respectively."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 22
                            }
                        ],
                        "text": "11f; MULTIFTR+MOTION, CHNFTRS, and FPDW perform best with logaverage miss rates of 51-57 percent."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 125
                            }
                        ],
                        "text": "Dolla\u0301r et al. [29] proposed an extension of [VJ] where Haar-like feature are computed over multiple channels of visual data [CHNFTRS], including LUV color channels, grayscale, gradient magnitude, and gradient magnitude quantized by orientation (implicitly computing gradient histograms), providing a simple and uniform framework for integrating multiple feature types."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "CHNFTRS, FPDW, and FEATSYNTH achieve the best relative performance, but absolute performance is quite poor with 77-78 percent log-average miss rate."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14924524,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fd375345cbd203aa9c88e1aa3c2e4e1835548b10",
            "isKey": false,
            "numCitedBy": 1233,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the performance of \u2018integral channel features\u2019 for image classification tasks, \nfocusing in particular on pedestrian detection. The general idea behind integral channel features is that multiple registered image channels are computed using linear and \nnon-linear transformations of the input image, and then features such as local sums, histograms, and Haar features and their various generalizations are efficiently computed \nusing integral images. Such features have been used in recent literature for a variety of \ntasks \u2013 indeed, variations appear to have been invented independently multiple times. \nAlthough integral channel features have proven effective, little effort has been devoted to \nanalyzing or optimizing the features themselves. In this work we present a unified view \nof the relevant work in this area and perform a detailed experimental evaluation. We \ndemonstrate that when designed properly, integral channel features not only outperform \nother features including histogram of oriented gradient (HOG), they also (1) naturally \nintegrate heterogeneous sources of information, (2) have few parameters and are insensitive to exact parameter settings, (3) allow for more accurate spatial localization during \ndetection, and (4) result in fast detectors when coupled with cascade classifiers."
            },
            "slug": "Integral-Channel-Features-Doll\u00e1r-Tu",
            "title": {
                "fragments": [],
                "text": "Integral Channel Features"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is demonstrated that when designed properly, integral channel features not only outperform other features including histogram of oriented gradient (HOG), they also result in fast detectors when coupled with cascade classifiers."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789756"
                        ],
                        "name": "B. Leibe",
                        "slug": "B.-Leibe",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Leibe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Leibe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732672"
                        ],
                        "name": "A. Leonardis",
                        "slug": "A.-Leonardis",
                        "structuredName": {
                            "firstName": "Ale\u0161",
                            "lastName": "Leonardis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Leonardis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 2
                            }
                        ],
                        "text": ", [31], [32]); such detectors cannot be evaluated using PW metrics."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14144539,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1c41c1f86b92a8c011e0324d90624d539a849b8b",
            "isKey": false,
            "numCitedBy": 1060,
            "numCiting": 95,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract\nThis paper presents a novel method for detecting and localizing objects of a visual category in cluttered real-world scenes. Our approach considers object categorization and figure-ground segmentation as two interleaved processes that closely collaborate towards a common goal. As shown in our work, the tight coupling between those two processes allows them to benefit from each other and improve the combined performance.\n\nThe core part of our approach is a highly flexible learned representation for object shape that can combine the information observed on different training examples in a probabilistic extension of the Generalized Hough Transform. The resulting approach can detect categorical objects in novel images and automatically infer a probabilistic segmentation from the recognition result. This segmentation is then in turn used to again improve recognition by allowing the system to focus its efforts on object pixels and to discard misleading influences from the background. Moreover, the information from where in the image a hypothesis draws its support is employed in an MDL based hypothesis verification stage to resolve ambiguities between overlapping hypotheses and factor out the effects of partial occlusion.\n\nAn extensive evaluation on several large data sets shows that the proposed system is applicable to a range of different object categories, including both rigid and articulated objects. In addition, its flexible representation allows it to achieve competitive object detection performance already from training sets that are between one and two orders of magnitude smaller than those used in comparable systems.\n"
            },
            "slug": "Robust-Object-Detection-with-Interleaved-and-Leibe-Leonardis",
            "title": {
                "fragments": [],
                "text": "Robust Object Detection with Interleaved Categorization and Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "A novel method for detecting and localizing objects of a visual category in cluttered real-world scenes that is applicable to a range of different object categories, including both rigid and articulated objects and able to achieve competitive object detection performance from training sets that are between one and two orders of magnitude smaller than those used in comparable systems."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2490700"
                        ],
                        "name": "Boris Babenko",
                        "slug": "Boris-Babenko",
                        "structuredName": {
                            "firstName": "Boris",
                            "lastName": "Babenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Boris Babenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144035504"
                        ],
                        "name": "Z. Tu",
                        "slug": "Z.-Tu",
                        "structuredName": {
                            "firstName": "Zhuowen",
                            "lastName": "Tu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Tu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 139
                            }
                        ],
                        "text": "Multiple instance learning (MIL) has been employed in order to automatically determine the position of parts without partlevel supervision [82], [83]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 341776,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b1a05759f570f13ebdc5ea7f7a957e41f43203d",
            "isKey": false,
            "numCitedBy": 154,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Object detection is one of the key problems in computer vision. In the last decade, discriminative learning approaches have proven effective in detecting rigid objects, achieving very low false positives rates. The field has also seen a resurgence of part-based recognition methods, with impressive results on highly articulated, diverse object categories. In this paper we propose a discriminative learning approach for detection that is inspired by part-based recognition approaches. Our method, Multiple Component Learning (mcl), automatically learns individual component classifiers and combines these into an overall classifier. Unlike previous methods, which rely on either fairly restricted part models or labeled part data, mcl learns powerful component classifiers in a weakly supervised manner, where object labels are provided but part labels are not. The basis of mcl lies in learning a set classifier; we achieve this by combining boosting with weakly supervised learning, specifically the Multiple Instance Learning framework (mil). mcl is general, and we demonstrate results on a range of data from computer audition and computer vision. In particular, mcl outperforms all existing methods on the challenging INRIA pedestrian detection dataset, and unlike methods that are not part-based, mcl is quite robust to occlusions."
            },
            "slug": "Multiple-Component-Learning-for-Object-Detection-Doll\u00e1r-Babenko",
            "title": {
                "fragments": [],
                "text": "Multiple Component Learning for Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The method, Multiple Component Learning (mcl), automatically learns individual component classifiers and combines these into an overall classifier, and unlike methods that are not part-based, mcl is quite robust to occlusions."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47655614"
                        ],
                        "name": "G. Griffin",
                        "slug": "G.-Griffin",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Griffin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Griffin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144160673"
                        ],
                        "name": "Alex Holub",
                        "slug": "Alex-Holub",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Holub",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Holub"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "Occlusions and temporal correspondences are also annotated."
                    },
                    "intents": []
                }
            ],
            "corpusId": 118828957,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a5effa909cdeafaddbbb7855037e02f8e25d632",
            "isKey": false,
            "numCitedBy": 2545,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a challenging set of 256 object categories containing a total of 30607 images. The original Caltech-101 [1] was collected by choosing a set of object categories, downloading examples from Google Images and then manually screening out all images that did not fit the category. Caltech-256 is collected in a similar manner with several improvements: a) the number of categories is more than doubled, b) the minimum number of images in any category is increased from 31 to 80, c) artifacts due to image rotation are avoided and d) a new and larger clutter category is introduced for testing background rejection. We suggest several testing paradigms to measure classification performance, then benchmark the dataset using two simple metrics as well as a state-of-the-art spatial pyramid matching [2] algorithm. Finally we use the clutter category to train an interest detector which rejects uninformative background regions."
            },
            "slug": "Caltech-256-Object-Category-Dataset-Griffin-Holub",
            "title": {
                "fragments": [],
                "text": "Caltech-256 Object Category Dataset"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A challenging set of 256 object categories containing a total of 30607 images is introduced and the clutter category is used to train an interest detector which rejects uninformative background regions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143847264"
                        ],
                        "name": "Bo Wu",
                        "slug": "Bo-Wu",
                        "structuredName": {
                            "firstName": "Bo",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bo Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144862593"
                        ],
                        "name": "R. Nevatia",
                        "slug": "R.-Nevatia",
                        "structuredName": {
                            "firstName": "Ramakant",
                            "lastName": "Nevatia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Nevatia"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 17
                            }
                        ],
                        "text": "Likewise, Wu and Nevatia [58] automatically combined HOG, edgelet, and covariance features."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 47
                            }
                        ],
                        "text": "stage approaches have been revisited over time [17], [74],"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": "Wu and Nevatia [17] utilized a large pool of short line and curve segments, called \u201cedgelet\u201d features, to represent shape locally."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 72
                            }
                        ],
                        "text": "Pedestrians can be labeled in photographs [7], [16], surveillance video [17], [24], and images"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206769463,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de1ac0baea1f907388ff5fe9fa22f25f406e2ca6",
            "isKey": true,
            "numCitedBy": 881,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a method for human detection in crowded scene from static images. An individual human is modeled as an assembly of natural body parts. We introduce edgelet features, which are a new type of silhouette oriented features. Part detectors, based on these features, are learned by a boosting method. Responses of part detectors are combined to form a joint likelihood model that includes cases of multiple, possibly inter-occluded humans. The human detection problem is formulated as maximum a posteriori (MAP) estimation. We show results on a commonly used previous dataset as well as new data sets that could not be processed by earlier methods."
            },
            "slug": "Detection-of-multiple,-partially-occluded-humans-in-Wu-Nevatia",
            "title": {
                "fragments": [],
                "text": "Detection of multiple, partially occluded humans in a single image by Bayesian combination of edgelet part detectors"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The human detection problem is formulated as maximum a posteriori (MAP) estimation, and edgelet features are introduced, which are a new type of silhouette oriented features that are learned by a boosting method."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731948"
                        ],
                        "name": "Paul A. Viola",
                        "slug": "Paul-A.-Viola",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Viola",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul A. Viola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111328101"
                        ],
                        "name": "Michael Jones",
                        "slug": "Michael-Jones",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 124
                            }
                        ],
                        "text": "Moreover, numerous approaches have been proposed for speeding up detection, including speeding up the detector itself [29], [44], [46], through use of approximations [63], [89] or by using special purpose hardware such as GPUs [90] (for a review of fast detection see [63])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": ", Haar wavelets [44]), color, texture (including LBP [60] and co-occurrence [85]), self-similarity [86], and motion [54] features."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "Support vector machines [16] and boosting [44] are the most popular choices due to their theoretical guarantees, extensibility, and good performance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "Viola and Jones [VJ] [44] built upon these ideas, introducing integral images for fast feature computation and a cascade structure for efficient detection, and utilizing AdaBoost for automatic feature selection."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2796017,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b09ec0b350f8352bce46a2f5bf7ae97c83a7b9ca",
            "isKey": true,
            "numCitedBy": 11227,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a face detection framework that is capable of processing images extremely rapidly while achieving high detection rates. There are three key contributions. The first is the introduction of a new image representation called the \u201cIntegral Image\u201d which allows the features used by our detector to be computed very quickly. The second is a simple and efficient classifier which is built using the AdaBoost learning algorithm (Freund and Schapire, 1995) to select a small number of critical visual features from a very large set of potential features. The third contribution is a method for combining classifiers in a \u201ccascade\u201d which allows background regions of the image to be quickly discarded while spending more computation on promising face-like regions. A set of experiments in the domain of face detection is presented. The system yields face detection performance comparable to the best previous systems (Sung and Poggio, 1998; Rowley et al., 1998; Schneiderman and Kanade, 2000; Roth et al., 2000). Implemented on a conventional desktop, face detection proceeds at 15 frames per second."
            },
            "slug": "Robust-Real-Time-Face-Detection-Viola-Jones",
            "title": {
                "fragments": [],
                "text": "Robust Real-Time Face Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new image representation called the \u201cIntegral Image\u201d is introduced which allows the features used by the detector to be computed very quickly and a method for combining classifiers in a \u201ccascade\u201d which allows background regions of the image to be quickly discarded while spending more computation on promising face-like regions."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145347688"
                        ],
                        "name": "S. Baker",
                        "slug": "S.-Baker",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Baker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Baker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "83165362"
                        ],
                        "name": "Iain Matthews",
                        "slug": "Iain-Matthews",
                        "structuredName": {
                            "firstName": "Iain",
                            "lastName": "Matthews",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Iain Matthews"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "for image alignment by Baker and Matthews [15]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 171
                            }
                        ],
                        "text": "In order to remove effects of the vehicle pitching and thus simplify annotation, the video was stabilized using the inverse compositional algorithm for image alignment by Baker and Matthews [15]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 186689463,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "389110a28961ebe80d8856cd204f8d8305a260ef",
            "isKey": false,
            "numCitedBy": 3039,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Since the Lucas-Kanade algorithm was proposed in 1981 image alignment has become one of the most widely used techniques in computer vision. Applications range from optical flow and tracking to layered motion, mosaic construction, and face coding. Numerous algorithms have been proposed and a wide variety of extensions have been made to the original formulation. We present an overview of image alignment, describing most of the algorithms and their extensions in a consistent framework. We concentrate on the inverse compositional algorithm, an efficient algorithm that we recently proposed. We examine which of the extensions to Lucas-Kanade can be used with the inverse compositional algorithm without any significant loss of efficiency, and which cannot. In this paper, Part 1 in a series of papers, we cover the quantity approximated, the warp update rule, and the gradient descent approximation. In future papers, we will cover the choice of the error function, how to allow linear appearance variation, and how to impose priors on the parameters."
            },
            "slug": "Lucas-Kanade-20-Years-On:-A-Unifying-Framework-Baker-Matthews",
            "title": {
                "fragments": [],
                "text": "Lucas-Kanade 20 Years On: A Unifying Framework"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An overview of image alignment is presented, describing most of the algorithms and their extensions in a consistent framework and concentrating on the inverse compositional algorithm, an efficient algorithm that was recently proposed."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9385043"
                        ],
                        "name": "W. R. Schwartz",
                        "slug": "W.-R.-Schwartz",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "Robson"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. R. Schwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2684226"
                        ],
                        "name": "Aniruddha Kembhavi",
                        "slug": "Aniruddha-Kembhavi",
                        "structuredName": {
                            "firstName": "Aniruddha",
                            "lastName": "Kembhavi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aniruddha Kembhavi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145298122"
                        ],
                        "name": "D. Harwood",
                        "slug": "D.-Harwood",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Harwood",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Harwood"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693428"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Davis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[69] represented pedestrians by edges, texture, and color and applied partial least"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5717894,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "efc0cb142588a6dd571e52b30217c4a7905f254d",
            "isKey": false,
            "numCitedBy": 548,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Significant research has been devoted to detecting people in images and videos. In this paper we describe a human detection method that augments widely used edge-based features with texture and color information, providing us with a much richer descriptor set. This augmentation results in an extremely high-dimensional feature space (more than 170,000 dimensions). In such high-dimensional spaces, classical machine learning algorithms such as SVMs are nearly intractable with respect to training. Furthermore, the number of training samples is much smaller than the dimensionality of the feature space, by at least an order of magnitude. Finally, the extraction of features from a densely sampled grid structure leads to a high degree of multicollinearity. To circumvent these data characteristics, we employ Partial Least Squares (PLS) analysis, an efficient dimensionality reduction technique, one which preserves significant discriminative information, to project the data onto a much lower dimensional subspace (20 dimensions, reduced from the original 170,000). Our human detection system, employing PLS analysis over the enriched descriptor set, is shown to outperform state-of-the-art techniques on three varied datasets including the popular INRIA pedestrian dataset, the low-resolution gray-scale DaimlerChrysler pedestrian dataset, and the ETHZ pedestrian dataset consisting of full-length videos of crowded scenes."
            },
            "slug": "Human-detection-using-partial-least-squares-Schwartz-Kembhavi",
            "title": {
                "fragments": [],
                "text": "Human detection using partial least squares analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This paper describes a human detection method that augments widely used edge-based features with texture and color information, providing us with a much richer descriptor set, and is shown to outperform state-of-the-art techniques on three varied datasets."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE 12th International Conference on Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143847264"
                        ],
                        "name": "Bo Wu",
                        "slug": "Bo-Wu",
                        "structuredName": {
                            "firstName": "Bo",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bo Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144862593"
                        ],
                        "name": "R. Nevatia",
                        "slug": "R.-Nevatia",
                        "structuredName": {
                            "firstName": "Ramakant",
                            "lastName": "Nevatia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Nevatia"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "Likewise, Wu and Nevatia [58] automatically combined HOG, edgelet, and covariance features."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 7
                            }
                        ],
                        "text": "Wu and Nevatia [17] utilized a large pool of short line and curve segments, called \u201cedgelet\u201d features, to represent shape locally."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10121747,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "26ccb9654744c86afb6ad337704a443a2bdc4909",
            "isKey": false,
            "numCitedBy": 148,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "A large variety of image features has been invented for detection of objects of a known class. We propose a framework to optimize the discrimination-efficiency tradeoff in integrating multiple, heterogeneous features for object detection. Cascade structured detectors are learned by boosting local feature based weak classifiers. Each weak classifier corresponds to a local image region, from which several different types of features are extracted. The weak classifier makes predictions by examining the features one by one; this classifier goes to the next feature only when the prediction from the already examined features is not confident enough. The order in which the features are evaluated is determined based on their computational cost normalized classification powers. We apply our approach to two object classes, pedestrians and cars. The experimental results show that our approach outperforms the state-of-the-art methods."
            },
            "slug": "Optimizing-discrimination-efficiency-tradeoff-in-Wu-Nevatia",
            "title": {
                "fragments": [],
                "text": "Optimizing discrimination-efficiency tradeoff in integrating heterogeneous local features for object detection"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A framework to optimize the discrimination-efficiency tradeoff in integrating multiple, heterogeneous features for object detection and shows that this approach outperforms the state-of-the-art methods."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056091"
                        ],
                        "name": "M. Everingham",
                        "slug": "M.-Everingham",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Everingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Everingham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 191
                            }
                        ],
                        "text": "The Barron et al. [8] and Middlebury [9] optical flow data sets, the Berkeley Segmentation Data Set [10], the\nMiddlebury Stereo Data Set [11], and the Caltech 101 [12], Caltech 256 [13], and PASCAL [14] object recognition data sets all improved performance evaluation, added challenge, and helped drive innovation in their respective fields."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 161
                            }
                        ],
                        "text": "We analyze statistical significance using the nonparametric Friedman test with a posthoc analysis; this approach was also used by Everingham et al. [14] for the PASCAL VOC challenge."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 125
                            }
                        ],
                        "text": "We perform single frame evaluation using a modified version of the scheme laid out in the PASCAL object detection challenges [14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 72
                            }
                        ],
                        "text": "A similar trend was observed in Everingham et al.\u2019s [14] analysis on the PASCAL challenge."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 115
                            }
                        ],
                        "text": "Conceptually, the log-average miss rate is similar to the average precision [26] reported for the PASCAL challenge [14] in that it represents the entire curve by a single reference value."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "\u2019s [14] analysis on the PASCAL challenge."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 91
                            }
                        ],
                        "text": "The most widely used \u201cperson\u201d data sets include subsets of the MIT LabelMe data [23] and the PASCAL VOC data sets [14]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "LATSVM-V1 was trained on Pascal [14]; LATSVM-V2 used INRIA and a later version of the latent SVM framework [72]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 158
                            }
                        ],
                        "text": "A further in-depth study by Garc\u0131\u0301a and Herrera [88] concluded that the Nemenyi posthoc test which was used by Dem sar [87] (and also in the PASCAL challenge [14]) is too conservative for n n comparisons such as in a benchmark."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 23
                            }
                        ],
                        "text": "For comparison, in the PASCAL labeling scheme [14] only the visible BB is labeled and occluded objects are marked as \u201ctruncated.\u201d"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 28
                            }
                        ],
                        "text": "Specifically, we employ the PASCAL measure, which states that their area of overlap must exceed 50 percent:\nao \u00bc: area\u00f0BBdt \\BBgt\u00de area\u00f0BBdt [BBgt\u00de > 0:5: \u00f01\u00de\nThe evaluation is insensitive to the exact threshold as long as it is below about 0.6, see Fig."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 47
                            }
                        ],
                        "text": "LabelMe data [23] and the PASCAL VOC data sets [14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 29
                            }
                        ],
                        "text": "Caltech 256 [13], and PASCAL [14] object recognition data"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 46
                            }
                        ],
                        "text": "For comparison, in the PASCAL labeling scheme [14] only the visible BB is labeled and occluded objects are marked as \u201ctruncated."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4246903,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82635fb63640ae95f90ee9bdc07832eb461ca881",
            "isKey": true,
            "numCitedBy": 11690,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension."
            },
            "slug": "The-Pascal-Visual-Object-Classes-(VOC)-Challenge-Everingham-Gool",
            "title": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes (VOC) Challenge"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The state-of-the-art in evaluated methods for both classification and detection are reviewed, whether the methods are statistically different, what they are learning from the images, and what the methods find easy or confuse."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48950628"
                        ],
                        "name": "N. Dalal",
                        "slug": "N.-Dalal",
                        "structuredName": {
                            "firstName": "Navneet",
                            "lastName": "Dalal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dalal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 40
                            }
                        ],
                        "text": "38, respectively, in the INRIA data set [7] it is just 0."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 111
                            }
                        ],
                        "text": "six additional pedestrian detection data sets, including the ETH [4], TUD-Brussels [5], Daimler [6], and INRIA [7] data sets and two variants of the Caltech data set (see Fig."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 68
                            }
                        ],
                        "text": "Nearly all modern detectors employ some form of gradient histograms [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 46
                            }
                        ],
                        "text": "Inspired by SIFT [45], Dalal and Triggs [HOG] [7] popularized histogram of oriented gradient (HOG) features for detection by showing substantial gains over intensitybased features."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 66
                            }
                        ],
                        "text": "6b, we show a heat map obtained by using BBs generated by the HOG [7] pedestrian detector with a low threshold."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 42
                            }
                        ],
                        "text": "Pedestrians can be labeled in photographs [7], [16], surveillance video [17], [24], and images"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 67
                            }
                        ],
                        "text": "Most of the evaluated detectors were trained on the INRIA data set [7]; two were trained on TUD motion pairs (TUD-MP) (the training set for TUD-Brussels [5])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "INRIA [7], ETH [4], and TUD-Brussels [5] pedestrian data"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 113
                            }
                        ],
                        "text": "The PW results are reproduced from their original publications(2) (except the VJ curve, which is reproduced from [7]); the full image results were obtained by evaluating on the same pedestrians but within their original image context."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 74
                            }
                        ],
                        "text": "As mentioned, in our performance evaluation we additionally use the INRIA [7], ETH [4], TUD-Brussels [5], and Daimler-DB [6] data sets."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206590483,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cec734d7097ab6b1e60d95228ffd64248eb89d66",
            "isKey": true,
            "numCitedBy": 29263,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds."
            },
            "slug": "Histograms-of-oriented-gradients-for-human-Dalal-Triggs",
            "title": {
                "fragments": [],
                "text": "Histograms of oriented gradients for human detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection, and the influence of each stage of the computation on performance is studied."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787591"
                        ],
                        "name": "Christoph H. Lampert",
                        "slug": "Christoph-H.-Lampert",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Lampert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christoph H. Lampert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758219"
                        ],
                        "name": "Matthew B. Blaschko",
                        "slug": "Matthew-B.-Blaschko",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Blaschko",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew B. Blaschko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6131848,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54b224478a63e33441c651175c522f3702062fc4",
            "isKey": false,
            "numCitedBy": 800,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Most successful object recognition systems rely on binary classification, deciding only if an object is present or not, but not providing information on the actual object location. To perform localization, one can take a sliding window approach, but this strongly increases the computational cost, because the classifier function has to be evaluated over a large set of candidate subwindows. In this paper, we propose a simple yet powerful branch-and-bound scheme that allows efficient maximization of a large class of classifier functions over all possible subimages. It converges to a globally optimal solution typically in sublinear time. We show how our method is applicable to different object detection and retrieval scenarios. The achieved speedup allows the use of classifiers for localization that formerly were considered too slow for this task, such as SVMs with a spatial pyramid kernel or nearest neighbor classifiers based on the chi2-distance. We demonstrate state-of-the-art performance of the resulting systems on the UIUC Cars dataset, the PASCAL VOC 2006 dataset and in the PASCAL VOC 2007 competition."
            },
            "slug": "Beyond-sliding-windows:-Object-localization-by-Lampert-Blaschko",
            "title": {
                "fragments": [],
                "text": "Beyond sliding windows: Object localization by efficient subwindow search"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A simple yet powerful branch-and-bound scheme that allows efficient maximization of a large class of classifier functions over all possible subimages and converges to a globally optimal solution typically in sublinear time is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2372810"
                        ],
                        "name": "T. Gandhi",
                        "slug": "T.-Gandhi",
                        "structuredName": {
                            "firstName": "Tarak",
                            "lastName": "Gandhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Gandhi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713989"
                        ],
                        "name": "M. Trivedi",
                        "slug": "M.-Trivedi",
                        "structuredName": {
                            "firstName": "Mohan",
                            "lastName": "Trivedi",
                            "middleNames": [
                                "Manubhai"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Trivedi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "[38], [39], [40], [41]), we refer readers to [2], [42], [43]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1029120,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "309a9f17ed711663bd6c18cb15fcba9dcd731f49",
            "isKey": false,
            "numCitedBy": 382,
            "numCiting": 112,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the recent research on the enhancement of pedestrian safety to help develop a better understanding of the nature, issues, approaches, and challenges surrounding the problem. It presents a comprehensive review of research efforts underway dealing with pedestrian safety and collision avoidance. The importance of pedestrian protection is emphasized in a global context, discussing the research programs and efforts in various countries. Pedestrian safety measures, including infrastructure enhancements and passive safety features in vehicles, are described, followed by a systematic description of active safety systems based on pedestrian detection using sensors in vehicle and infrastructure. The pedestrian detection approaches are classified according to various criteria such as the type and configuration of sensors, as well as the video cues and classifiers used in detection algorithms. It is noted that collision avoidance not only requires detection of pedestrians but also requires collision prediction using pedestrian dynamics and behavior analysis. Hence, this paper includes research dealing with probabilistic modeling of pedestrian behavior for predicting collisions between pedestrians and vehicles."
            },
            "slug": "Pedestrian-Protection-Systems:-Issues,-Survey,-and-Gandhi-Trivedi",
            "title": {
                "fragments": [],
                "text": "Pedestrian Protection Systems: Issues, Survey, and Challenges"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A comprehensive review of research efforts underway dealing with pedestrian safety and collision avoidance is presented, including research dealing with probabilistic modeling of pedestrian behavior for predicting collisions between pedestrians and vehicles."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Intelligent Transportation Systems"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143847264"
                        ],
                        "name": "Bo Wu",
                        "slug": "Bo-Wu",
                        "structuredName": {
                            "firstName": "Bo",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bo Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144862593"
                        ],
                        "name": "R. Nevatia",
                        "slug": "R.-Nevatia",
                        "structuredName": {
                            "firstName": "Ramakant",
                            "lastName": "Nevatia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Nevatia"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6687662,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44be974c7f765155980312d8bad4383a67417f89",
            "isKey": false,
            "numCitedBy": 214,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Detection of object of a known class is a fundamental problem of computer vision. The appearance of objects can change greatly due to illumination, view point, and articulation. For object classes with large intra-class variation, some divide-and-conquer strategy is necessary. Tree structured classifier models have been used for multi-view multi- pose object detection in previous work. This paper proposes a boosting based learning method, called Cluster Boosted Tree (CBT), to automatically construct tree structured object detectors. Instead of using predefined intra-class sub- categorization based on domain knowledge, we divide the sample space by unsupervised clustering based on discriminative image features selected by boosting algorithm. The sub-categorization information of the leaf nodes is sent back to refine their ancestors' classification functions. We compare our approach with previous related methods on several public data sets. The results show that our approach outperforms the state-of-the-art methods."
            },
            "slug": "Cluster-Boosted-Tree-Classifier-for-Multi-View,-Wu-Nevatia",
            "title": {
                "fragments": [],
                "text": "Cluster Boosted Tree Classifier for Multi-View, Multi-Pose Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This paper proposes a boosting based learning method, called Cluster Boosted Tree (CBT), to automatically construct tree structured object detectors, and shows that this approach outperforms the state-of-the-art methods."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2828154"
                        ],
                        "name": "Duan Tran",
                        "slug": "Duan-Tran",
                        "structuredName": {
                            "firstName": "Duan",
                            "lastName": "Tran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Duan Tran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "Joint body pose estimation and person classification can also be formulated as a structured learning problem [78]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13902758,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "35570297681daa3973498eabead361d0be961672",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Fair discriminative pedestrian finders are now available. In fact, these pedestrian finders make most errors on pedestrians in configurations that are uncommon in the training data, for example, mounting a bicycle. This is undesirable. However, the human configuration can itself be estimated discriminatively using structure learning. We demonstrate a pedestrian finder which first finds the most likely human pose in the window using a discriminative procedure trained with structure learning on a small dataset. We then present features (local histogram of oriented gradient and local PCA of gradient) based on that configuration to an SVM classifier. We show, using the INRIA Person dataset, that estimates of configuration significantly improve the accuracy of a discriminative pedestrian finder."
            },
            "slug": "Configuration-Estimates-Improve-Pedestrian-Finding-Tran-Forsyth",
            "title": {
                "fragments": [],
                "text": "Configuration Estimates Improve Pedestrian Finding"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown, using the INRIA Person dataset, that estimates of configuration significantly improve the accuracy of a discriminative pedestrian finder."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145527707"
                        ],
                        "name": "Zhe L. Lin",
                        "slug": "Zhe-L.-Lin",
                        "structuredName": {
                            "firstName": "Zhe",
                            "lastName": "Lin",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhe L. Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693428"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Davis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Lin and Davis [70] used a parttemplate tree to model a pedestrian\u2019s shape locally for the head, upper body, and legs, and extracted HOG appearance descriptors along the shape\u2019s outline [POSEINV]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 14
                            }
                        ],
                        "text": "Lin and Davis [70] used a parttemplate tree to model a pedestrian\u2019s shape locally for the"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14506749,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "40a4e30c2eca4de538d6ac39db48b2f04027ba6c",
            "isKey": false,
            "numCitedBy": 170,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a learning-based, sliding window-style approach for the problem of detecting humans in still images. Instead of traditional concatenation-style image location-based feature encoding, a global descriptor more invariant to pose variation is introduced. Specifically, we propose a principled approach to learning and classifying human/non-human image patterns by simultaneously segmenting human shapes and poses, and extracting articulation-insensitive features. The shapes and poses are segmented by an efficient, probabilistic hierarchical part-template matching algorithm, and the features are collected in the context of poses by tracing around the estimated shape boundaries. Histograms of oriented gradients are used as a source of low-level features from which our pose-invariant descriptors are computed, and kernel SVMs are adopted as the test classifiers. We evaluate our detection and segmentation approach on two public pedestrian datasets."
            },
            "slug": "A-Pose-Invariant-Descriptor-for-Human-Detection-and-Lin-Davis",
            "title": {
                "fragments": [],
                "text": "A Pose-Invariant Descriptor for Human Detection and Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work proposes a principled approach to learning and classifying human/non-human image patterns by simultaneously segmenting human shapes and poses, and extracting articulation-insensitive features."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2340109"
                        ],
                        "name": "C. Wojek",
                        "slug": "C.-Wojek",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Wojek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Wojek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 29
                            }
                        ],
                        "text": "Wojek and Schiele [MULTIFTR] [56] showed how a combination of Haar-like features, shapelets [33], shape context [57] and HOG features outperforms any individual feature."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 141
                            }
                        ],
                        "text": "In nearly all cases, we used code obtained directly from the authors, the only exceptions being VJ and SHAPELET, which were reimplemented in [56]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 39046756,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3e6fa6cf1fe2e23fdf7716f89b160333c7a93b26",
            "isKey": false,
            "numCitedBy": 266,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Over the years a number of powerful people detectors have been proposed. While it is standard to test complete detectors on publicly available datasets, it is often unclear how the different components (e.g. features and classifiers) of the respective detectors compare. Therefore, this paper contributes a systematic comparison of the most prominent and successful people detectors. Based on this evaluation we also propose a new detector that outperforms the state-of-art on the INRIA person dataset by combining multiple features."
            },
            "slug": "A-Performance-Evaluation-of-Single-and-People-Wojek-Schiele",
            "title": {
                "fragments": [],
                "text": "A Performance Evaluation of Single and Multi-feature People Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A systematic comparison of the most prominent and successful people detectors is contributed and a new detector is proposed that outperforms the state-of-art on the INRIA person dataset by combining multiple features."
            },
            "venue": {
                "fragments": [],
                "text": "DAGM-Symposium"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145030811"
                        ],
                        "name": "C. Papageorgiou",
                        "slug": "C.-Papageorgiou",
                        "structuredName": {
                            "firstName": "Constantine",
                            "lastName": "Papageorgiou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Papageorgiou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "Older data sets, including the MIT [16],"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Papageorgiou and Poggio [16] proposed one of the first sliding window detectors, applying support vector machines (SVM) to an overcomplete dictionary of multiscale Haar\n2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 61
                            }
                        ],
                        "text": "The most widely used \u201cperson\u201d data sets include subsets of the MIT LabelMe data [23] and the PASCAL VOC data sets [14]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 24
                            }
                        ],
                        "text": "Papageorgiou and Poggio [16] proposed one of the first"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 24
                            }
                        ],
                        "text": "Support vector machines [16] and boosting [44] are the most popular choices due to their theoretical guarantees, extensibility, and good performance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 31
                            }
                        ],
                        "text": "Older data sets, including the MIT [16], CVC [19], and NICTA [22] pedestrian data sets and the Daimler classification benchmark (Daimler-CB) [21], tend to contain cropped pedestrian windows only."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 47
                            }
                        ],
                        "text": "Pedestrians can be labeled in photographs [7], [16], surveillance video [17], [24], and images"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "successfully extended [16] with a two stage approach: First,"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13308232,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e6c20ed0c3f375f403ab5d750a6e9699d5c3af6a",
            "isKey": true,
            "numCitedBy": 1404,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a general, trainable system for object detection in unconstrained, cluttered scenes. The system derives much of its power from a representation that describes an object class in terms of an overcomplete dictionary of local, oriented, multiscale intensity differences between adjacent regions, efficiently computable as a Haar wavelet transform. This example-based learning approach implicitly derives a model of an object class by training a support vector machine classifier using a large set of positive and negative examples. We present results on face, people, and car detection tasks using the same architecture. In addition, we quantify how the representation affects detection performance by considering several alternate representations including pixels and principal components. We also describe a real-time application of our person detection system as part of a driver assistance system."
            },
            "slug": "A-Trainable-System-for-Object-Detection-Papageorgiou-Poggio",
            "title": {
                "fragments": [],
                "text": "A Trainable System for Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A general, trainable system for object detection in unconstrained, cluttered scenes that derives much of its power from a representation that describes an object class in terms of an overcomplete dictionary of local, oriented, multiscale intensity differences between adjacent regions, efficiently computable as a Haar wavelet transform."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144854796"
                        ],
                        "name": "D. Gavrila",
                        "slug": "D.-Gavrila",
                        "structuredName": {
                            "firstName": "Dariu",
                            "lastName": "Gavrila",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gavrila"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762260"
                        ],
                        "name": "V. Philomin",
                        "slug": "V.-Philomin",
                        "structuredName": {
                            "firstName": "Vasanth",
                            "lastName": "Philomin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Philomin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "Gavrila and Philomin [49], [50] employed the Hausdorff distance transform and a template hierarchy to rapidly match image edges to a set of shape templates."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 766556,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb490d879512b3d43b267e3ac8931c099a5a2fd3",
            "isKey": false,
            "numCitedBy": 760,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an efficient shape-based object detection method based on Distance Transforms and describes its use for real-time vision on-board vehicles. The method uses a template hierarchy to capture the variety of object shapes; efficient hierarchies can be generated offline for given shape distributions using stochastic optimization techniques (i.e. simulated annealing). Online, matching involves a simultaneous coarse-to-fine approach over the shape hierarchy and over the transformation parameters. Very large speed-up factors are typically obtained when comparing this approach with the equivalent brute-force formulation; we have measured gains of several orders of magnitudes. We present experimental results on the real-time detection of traffic signs and pedestrians from a moving vehicle. Because of the highly time sensitive nature of these vision tasks, we also discuss some hardware-specific implementations of the proposed method as far as SIMD parallelism is concerned."
            },
            "slug": "Real-time-object-detection-for-\"smart\"-vehicles-Gavrila-Philomin",
            "title": {
                "fragments": [],
                "text": "Real-time object detection for \"smart\" vehicles"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "An efficient shape-based object detection method based on Distance Transforms is presented and its use for real-time vision on-board vehicles and some hardware-specific implementations of the proposed method as far as SIMD parallelism is concerned are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Seventh IEEE International Conference on Computer Vision"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144891282"
                        ],
                        "name": "David R. Martin",
                        "slug": "David-R.-Martin",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Martin",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David R. Martin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143800213"
                        ],
                        "name": "Charless C. Fowlkes",
                        "slug": "Charless-C.-Fowlkes",
                        "structuredName": {
                            "firstName": "Charless",
                            "lastName": "Fowlkes",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charless C. Fowlkes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 46
                            }
                        ],
                        "text": "data sets, the Berkeley Segmentation Data Set [10], the Middlebury Stereo Data Set [11], and the Caltech 101 [12],"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 69
                            }
                        ],
                        "text": "The Barron et al. [8] and Middlebury [9] optical flow data sets, the Berkeley Segmentation Data Set [10], the\nMiddlebury Stereo Data Set [11], and the Caltech 101 [12], Caltech 256 [13], and PASCAL [14] object recognition data sets all improved performance evaluation, added challenge, and helped drive innovation in their respective fields."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8165754,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "33a7a59f785ef46091c30c4c85ef88c6bdabab51",
            "isKey": false,
            "numCitedBy": 2381,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of this work is to accurately detect and localize boundaries in natural scenes using local image measurements. We formulate features that respond to characteristic changes in brightness, color, and texture associated with natural boundaries. In order to combine the information from these features in an optimal way, we train a classifier using human labeled images as ground truth. The output of this classifier provides the posterior probability of a boundary at each image location and orientation. We present precision-recall curves showing that the resulting detector significantly outperforms existing approaches. Our two main results are 1) that cue combination can be performed adequately with a simple linear model and 2) that a proper, explicit treatment of texture is required to detect boundaries in natural images."
            },
            "slug": "Learning-to-detect-natural-image-boundaries-using-Martin-Fowlkes",
            "title": {
                "fragments": [],
                "text": "Learning to detect natural image boundaries using local brightness, color, and texture cues"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The two main results are that cue combination can be performed adequately with a simple linear model and that a proper, explicit treatment of texture is required to detect boundaries in natural images."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145160921"
                        ],
                        "name": "Bryan C. Russell",
                        "slug": "Bryan-C.-Russell",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Russell",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan C. Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056417995"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 13
                            }
                        ],
                        "text": "LabelMe data [23] and the PASCAL VOC data sets [14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1900911,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "092c275005ae49dc1303214f6d02d134457c7053",
            "isKey": false,
            "numCitedBy": 3076,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract\nWe seek to build a large collection of images with ground truth labels to be used for object detection and recognition research. Such data is useful for supervised learning and quantitative evaluation. To achieve this, we developed a web-based tool that allows easy image annotation and instant sharing of such annotations. Using this annotation tool, we have collected a large dataset that spans many object categories, often containing multiple instances over a wide variety of images. We quantify the contents of the dataset and compare against existing state of the art datasets used for object recognition and detection. Also, we show how to extend the dataset to automatically enhance object labels with WordNet, discover object parts, recover a depth ordering of objects in a scene, and increase the number of labels using minimal user supervision and images from the web.\n"
            },
            "slug": "LabelMe:-A-Database-and-Web-Based-Tool-for-Image-Russell-Torralba",
            "title": {
                "fragments": [],
                "text": "LabelMe: A Database and Web-Based Tool for Image Annotation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A web-based tool that allows easy image annotation and instant sharing of such annotations is developed and a large dataset that spans many object categories, often containing multiple instances over a wide variety of images is collected."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685089"
                        ],
                        "name": "Pedro F. Felzenszwalb",
                        "slug": "Pedro-F.-Felzenszwalb",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Felzenszwalb",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro F. Felzenszwalb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145689002"
                        ],
                        "name": "David A. McAllester",
                        "slug": "David-A.-McAllester",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "McAllester",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. McAllester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 107
                            }
                        ],
                        "text": "LATSVM-V1 was trained on Pascal [14]; LATSVM-V2 used INRIA and a later version of the latent SVM framework [72]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "[71], [72] proposed a discriminative part-based approach that models unknown part positions as latent variables in an SVM framework [LATSVM]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3198903,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e79272fe3d65197100eae8be9fec6469107969ae",
            "isKey": false,
            "numCitedBy": 9374,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL data sets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin-sensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI--SVM in terms of latent variables. A latent SVM is semiconvex, and the training problem becomes convex once latent information is specified for the positive examples. This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and optimizing the latent SVM objective function."
            },
            "slug": "Object-Detection-with-Discriminatively-Trained-Part-Felzenszwalb-Girshick",
            "title": {
                "fragments": [],
                "text": "Object Detection with Discriminatively Trained Part Based Models"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "An object detection system based on mixtures of multiscale deformable part models that is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges is described."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144238410"
                        ],
                        "name": "Qiang Zhu",
                        "slug": "Qiang-Zhu",
                        "structuredName": {
                            "firstName": "Qiang",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiang Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39369497"
                        ],
                        "name": "Mei-Chen Yeh",
                        "slug": "Mei-Chen-Yeh",
                        "structuredName": {
                            "firstName": "Mei-Chen",
                            "lastName": "Yeh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mei-Chen Yeh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143766349"
                        ],
                        "name": "K. Cheng",
                        "slug": "K.-Cheng",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Cheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Cheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1815078"
                        ],
                        "name": "S. Avidan",
                        "slug": "S.-Avidan",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Avidan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Avidan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 130
                            }
                        ],
                        "text": "Moreover, numerous approaches have been proposed for speeding up detection, including speeding up the detector itself [29], [44], [46], through use of approximations [63], [89] or by using special purpose hardware such as GPUs [90] (for a review of fast detection see [63])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[46] sped up HOG features by using integral histograms [47]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7800101,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "05fe01b57b3ba58dc5029c068a48567b55018ea5",
            "isKey": false,
            "numCitedBy": 1568,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We integrate the cascade-of-rejectors approach with the Histograms of Oriented Gradients (HoG) features to achieve a fast and accurate human detection system. The features used in our system are HoGs of variable-size blocks that capture salient features of humans automatically. Using AdaBoost for feature selection, we identify the appropriate set of blocks, from a large set of possible blocks. In our system, we use the integral image representation and a rejection cascade which significantly speed up the computation. For a 320 \u00d7 280 image, the system can process 5 to 30 frames per second depending on the density in which we scan the image, while maintaining an accuracy level similar to existing methods."
            },
            "slug": "Fast-Human-Detection-Using-a-Cascade-of-Histograms-Zhu-Yeh",
            "title": {
                "fragments": [],
                "text": "Fast Human Detection Using a Cascade of Histograms of Oriented Gradients"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This work integrates the cascade-of-rejectors approach with the Histograms of Oriented Gradients features to achieve a fast and accurate human detection system that can process 5 to 30 frames per second depending on the density in which the image is scanned, while maintaining an accuracy level similar to existing methods."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2490700"
                        ],
                        "name": "Boris Babenko",
                        "slug": "Boris-Babenko",
                        "structuredName": {
                            "firstName": "Boris",
                            "lastName": "Babenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Boris Babenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144035504"
                        ],
                        "name": "Z. Tu",
                        "slug": "Z.-Tu",
                        "structuredName": {
                            "firstName": "Zhuowen",
                            "lastName": "Tu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Tu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[65] proposed an approach for simultaneously separating data into coherent groups and training separate classifiers for each; Wojek et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[65] gave modest gains over linear SVMs and AdaBoost for pedestrian detection, especially when used in combination [66]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15366005,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a03cfd5c0059825c87d51f5dbf12f8a76fe9ff60",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "In object recognition in general and in face detection in par- ticular, data alignment is necessary to achieve good classification results with certain statistical learning approaches such as Viola-Jones. Data can be aligned in one of two ways: (1) by separating the data into coherent groups and training separate classifiers for each; (2) by adjusting training samples so they lie in correspondence. If done manually, both procedures are labor intensive and can significantly add to the cost of labeling. In this paper we present a unified boosting framework for simultaneous learn- ing and alignment. We present a novel boosting algorithm for Multiple Pose Learning (mpl), where the goal is to simultaneously split data into groups and train classifiers for each. We also review Multiple Instance Learning (mil), and in particular mil-boost, and describe how to use it to simultaneously train a classifier and bring data into correspondence. We show results on variations of LFW and MNIST, demonstrating the potential of these approaches."
            },
            "slug": "Simultaneous-Learning-and-Alignment:-Multi-Instance-Babenko-Tu",
            "title": {
                "fragments": [],
                "text": "Simultaneous Learning and Alignment: Multi-Instance and Multi-Pose Learning ?"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A unified boosting framework for simultaneous learn- ing and alignment is presented and a novel boosting algorithm for Multiple Pose Learning (mpl) is presented, where the goal is to simultaneously split data into groups and train classifiers for each."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144854796"
                        ],
                        "name": "D. Gavrila",
                        "slug": "D.-Gavrila",
                        "structuredName": {
                            "firstName": "Dariu",
                            "lastName": "Gavrila",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gavrila"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "Gavrila and Philomin [49], [50] employed the Hausdorff distance transform and a template hierarchy to rapidly match image edges to a set of shape templates."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1546912,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c1d6d01b2b392d99b1903334023784b60067f840",
            "isKey": false,
            "numCitedBy": 310,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a novel probabilistic approach to hierarchical, exemplar-based shape matching. No feature correspondence is needed among exemplars, just a suitable pairwise similarity measure. The approach uses a template tree to efficiently represent and match the variety of shape exemplars. The tree is generated offline by a bottom-up clustering approach using stochastic optimization. Online matching involves a simultaneous coarse-to-fine approach over the template tree and over the transformation parameters. The main contribution of this paper is a Bayesian model to estimate the a posteriori probability of the object class, after a certain match at a node of the tree. This model takes into account object scale and saliency and allows for a principled setting of the matching thresholds such that unpromising paths in the tree traversal process are eliminated early on. The proposed approach was tested in a variety of application domains. Here, results are presented on one of the more challenging domains: real-time pedestrian detection from a moving vehicle. A significant speed-up is obtained when comparing the proposed probabilistic matching approach with a manually tuned nonprobabilistic variant, both utilizing the same template tree structure."
            },
            "slug": "A-Bayesian,-Exemplar-Based-Approach-to-Hierarchical-Gavrila",
            "title": {
                "fragments": [],
                "text": "A Bayesian, Exemplar-Based Approach to Hierarchical Shape Matching"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A Bayesian model to estimate the a posteriori probability of the object class, after a certain match at a node of the tree, is presented, takes into account object scale and saliency and allows for a principled setting of the matching thresholds such that unpromising paths in the tree traversal process are eliminated early on."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144035504"
                        ],
                        "name": "Z. Tu",
                        "slug": "Z.-Tu",
                        "structuredName": {
                            "firstName": "Zhuowen",
                            "lastName": "Tu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Tu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145492205"
                        ],
                        "name": "Hai Tao",
                        "slug": "Hai-Tao",
                        "structuredName": {
                            "firstName": "Hai",
                            "lastName": "Tao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hai Tao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[67] to explore vast (possibly infinite) feature spaces using various strategies including steepest descent search prior to training a boosted classifier [FTRMINE]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8961509,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "69deed411eeff65cc0cba9e7db94ac337322089b",
            "isKey": false,
            "numCitedBy": 226,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "The efficiency and robustness of a vision system is often largely determined by the quality of the image features available to it. In data mining, one typically works with immense volumes of raw data, which demands effective algorithms to explore the data space. In analogy to data mining, the space of meaningful features for image analysis is also quite vast. Recently, the challenges associated with these problem areas have become more tractable through progress made in machine learning and concerted research effort in manual feature design by domain experts. In this paper, we propose a feature mining paradigm for image classification and examine several feature mining strategies. We also derive a principled approach for dealing with features with varying computational demands. Our goal is to alleviate the burden of manual feature design, which is a key problem in computer vision and machine learning. We include an in-depth empirical study on three typical data sets and offer theoretical explanations for the performance of various feature mining strategies. As a final confirmation of our ideas, we show results of a system, that utilizing feature mining strategies matches or outperforms the best reported results on pedestrian classification (where considerable effort has been devoted to expert feature design)."
            },
            "slug": "Feature-Mining-for-Image-Classification-Doll\u00e1r-Tu",
            "title": {
                "fragments": [],
                "text": "Feature Mining for Image Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A feature mining paradigm for image classification is proposed and several feature mining strategies are examined to alleviate the burden of manual feature design, which is a key problem in computer vision and machine learning."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685089"
                        ],
                        "name": "Pedro F. Felzenszwalb",
                        "slug": "Pedro-F.-Felzenszwalb",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Felzenszwalb",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro F. Felzenszwalb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145689002"
                        ],
                        "name": "David A. McAllester",
                        "slug": "David-A.-McAllester",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "McAllester",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. McAllester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 50
                            }
                        ],
                        "text": "estimation [55] and pairwise max (PM) suppression [71], which discards the less confident of every pair of detections that overlap sufficiently according to (1)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[71], [72] proposed a discriminative part-based approach that models unknown part positions as latent variables in an SVM framework [LATSVM]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14327585,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "860a9d55d87663ca88e74b3ca357396cd51733d0",
            "isKey": false,
            "numCitedBy": 2616,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a discriminatively trained, multiscale, deformable part model for object detection. Our system achieves a two-fold improvement in average precision over the best performance in the 2006 PASCAL person detection challenge. It also outperforms the best results in the 2007 challenge in ten out of twenty categories. The system relies heavily on deformable parts. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL challenge. Our system also relies heavily on new methods for discriminative training. We combine a margin-sensitive approach for data mining hard negative examples with a formalism we call latent SVM. A latent SVM, like a hidden CRF, leads to a non-convex training problem. However, a latent SVM is semi-convex and the training problem becomes convex once latent information is specified for the positive examples. We believe that our training methods will eventually make possible the effective use of more latent information such as hierarchical (grammar) models and models involving latent three dimensional pose."
            },
            "slug": "A-discriminatively-trained,-multiscale,-deformable-Felzenszwalb-McAllester",
            "title": {
                "fragments": [],
                "text": "A discriminatively trained, multiscale, deformable part model"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "A discriminatively trained, multiscale, deformable part model for object detection, which achieves a two-fold improvement in average precision over the best performance in the 2006 PASCAL person detection challenge and outperforms the best results in the 2007 challenge in ten out of twenty categories."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11323114"
                        ],
                        "name": "Yazhou Liu",
                        "slug": "Yazhou-Liu",
                        "structuredName": {
                            "firstName": "Yazhou",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yazhou Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145455919"
                        ],
                        "name": "S. Shan",
                        "slug": "S.-Shan",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Shan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Shan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50549852"
                        ],
                        "name": "Wenchao Zhang",
                        "slug": "Wenchao-Zhang",
                        "structuredName": {
                            "firstName": "Wenchao",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenchao Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46772547"
                        ],
                        "name": "Xilin Chen",
                        "slug": "Xilin-Chen",
                        "structuredName": {
                            "firstName": "Xilin",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xilin Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37110739"
                        ],
                        "name": "Wen Gao",
                        "slug": "Wen-Gao",
                        "structuredName": {
                            "firstName": "Wen",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wen Gao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[51] proposed \u201cgranularity-tunable\u201d features that allow for representations with levels of uncertainty ranging from edgelet to HOG type features; an extension to the spatiotemporal domain was developed in [52]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14354602,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "674216f2dca99f3c64922a06b27c5920b1a2420e",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a novel descriptor, granularity-tunable gradients partition (GGP), for human detection. The concept granularity is used to define the spatial and angular uncertainty of the line segments in the Hough space. Then this uncertainty is backprojected into the image space by orientation-space partitioning to achieve efficient implementation. By changing the granularity parameter, the level of uncertainty can be controlled quantitatively. Therefore a family of descriptors with versatile representation property can be generated. Specifically, the finely granular GGP descriptors can represent the specific geometry information of the object (the same as Edgelet); while the coarsely granular GGP descriptors can provide the statistical representation of the object (the same as histograms of oriented gradients, HOG). Moreover, the position, orientation, strength and distribution of the gradients are embedded into a unified descriptor to further improve the GGP's representation power. A cascade structured classifier is built by boosting the linear regression functions. Experimental results on INRIA dataset show that the proposed method achieves comparable results to those of the state-of-the-art methods."
            },
            "slug": "Granularity-tunable-gradients-partition-(GGP)-for-Liu-Shan",
            "title": {
                "fragments": [],
                "text": "Granularity-tunable gradients partition (GGP) descriptors for human detection"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "A novel descriptor, granularity-tunable gradients partition (GGP), for human detection using the concept granularity to define the spatial and angular uncertainty of the line segments in the Hough space is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3140335"
                        ],
                        "name": "A. Shashua",
                        "slug": "A.-Shashua",
                        "structuredName": {
                            "firstName": "Amnon",
                            "lastName": "Shashua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Shashua"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1915667"
                        ],
                        "name": "Y. Gdalyahu",
                        "slug": "Y.-Gdalyahu",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Gdalyahu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Gdalyahu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2079194"
                        ],
                        "name": "G. Hayun",
                        "slug": "G.-Hayun",
                        "structuredName": {
                            "firstName": "Gaby",
                            "lastName": "Hayun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Hayun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[48] proposed a similar representation for characterizing spatially localized parts for modeling pedestrians."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14981509,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "37a15ce03c26ec83d95bf4aaf756a41370d50353",
            "isKey": false,
            "numCitedBy": 404,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the functional and architectural breakdown of a monocular pedestrian detection system. We describe in detail our approach for single-frame classification based on a novel scheme of breaking down the class variability by repeatedly training a set of relatively simple classifiers on clusters of the training set. Single-frame classification performance results and system level performance figures for daytime conditions are presented with a discussion about the remaining gap to meet a daytime normal weather condition production system."
            },
            "slug": "Pedestrian-detection-for-driving-assistance-and-Shashua-Gdalyahu",
            "title": {
                "fragments": [],
                "text": "Pedestrian detection for driving assistance systems: single-frame classification and system level performance"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "The functional and architectural breakdown of a monocular pedestrian detection system is described and the approach for single-frame classification based on a novel scheme of breaking down the class variability by repeatedly training a set of relatively simple classifiers on clusters of the training set is described."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Intelligent Vehicles Symposium, 2004"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2177801"
                        ],
                        "name": "E. Shechtman",
                        "slug": "E.-Shechtman",
                        "structuredName": {
                            "firstName": "Eli",
                            "lastName": "Shechtman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Shechtman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144611617"
                        ],
                        "name": "M. Irani",
                        "slug": "M.-Irani",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Irani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Irani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": ", Haar wavelets [44]), color, texture (including LBP [60] and co-occurrence [85]), self-similarity [86], and motion [54] features."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2341530,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5feb23e3c01c12c797b271fa5cd2a1f2f096130f",
            "isKey": false,
            "numCitedBy": 1062,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an approach for measuring similarity between visual entities (images or videos) based on matching internal self-similarities. What is correlated across images (or across video sequences) is the internal layout of local self-similarities (up to some distortions), even though the patterns generating those local self-similarities are quite different in each of the images/videos. These internal self-similarities are efficiently captured by a compact local \"self-similarity descriptor\"', measured densely throughout the image/video, at multiple scales, while accounting for local and global geometric distortions. This gives rise to matching capabilities of complex visual data, including detection of objects in real cluttered images using only rough hand-sketches, handling textured objects with no clear boundaries, and detecting complex actions in cluttered video data with no prior learning. We compare our measure to commonly used image-based and video-based similarity measures, and demonstrate its applicability to object detection, retrieval, and action detection."
            },
            "slug": "Matching-Local-Self-Similarities-across-Images-and-Shechtman-Irani",
            "title": {
                "fragments": [],
                "text": "Matching Local Self-Similarities across Images and Videos"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "An approach for measuring similarity between visual entities (images or videos) based on matching internal self-similarities, measured densely throughout the image/video, at multiple scales, while accounting for local and global geometric distortions is presented."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710238"
                        ],
                        "name": "R. Haralick",
                        "slug": "R.-Haralick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Haralick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haralick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3144171"
                        ],
                        "name": "K. Shanmugam",
                        "slug": "K.-Shanmugam",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Shanmugam",
                            "middleNames": [
                                "Sam"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Shanmugam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686153"
                        ],
                        "name": "I. Dinstein",
                        "slug": "I.-Dinstein",
                        "structuredName": {
                            "firstName": "Its'hak",
                            "lastName": "Dinstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Dinstein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 76
                            }
                        ],
                        "text": ", Haar wavelets [44]), color, texture (including LBP [60] and co-occurrence [85]), self-similarity [86], and motion [54] features."
                    },
                    "intents": []
                }
            ],
            "corpusId": 206786900,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1fdb62555eb650662dbe2a6f3985d390861597c2",
            "isKey": false,
            "numCitedBy": 19246,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Texture is one of the important characteristics used in identifying objects or regions of interest in an image, whether the image be a photomicrograph, an aerial photograph, or a satellite image. This paper describes some easily computable textural features based on gray-tone spatial dependancies, and illustrates their application in category-identification tasks of three different kinds of image data: photomicrographs of five kinds of sandstones, 1:20 000 panchromatic aerial photographs of eight land-use categories, and Earth Resources Technology Satellite (ERTS) multispecial imagery containing seven land-use categories. We use two kinds of decision rules: one for which the decision regions are convex polyhedra (a piecewise linear decision rule), and one for which the decision regions are rectangular parallelpipeds (a min-max decision rule). In each experiment the data set was divided into two parts, a training set and a test set. Test set identification accuracy is 89 percent for the photomicrographs, 82 percent for the aerial photographic imagery, and 83 percent for the satellite imagery. These results indicate that the easily computable textural features probably have a general applicability for a wide variety of image-classification applications."
            },
            "slug": "Textural-Features-for-Image-Classification-Haralick-Shanmugam",
            "title": {
                "fragments": [],
                "text": "Textural Features for Image Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "These results indicate that the easily computable textural features based on gray-tone spatial dependancies probably have a general applicability for a wide variety of image-classification applications."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Man Cybern."
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "data sets, the Berkeley Segmentation Data Set [10], the Middlebury Stereo Data Set [11], and the Caltech 101 [12],"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6953475,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "812355cec91fa30bb50e9e992a3549af39e4f6eb",
            "isKey": false,
            "numCitedBy": 2365,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning visual models of object categories notoriously requires hundreds or thousands of training examples. We show that it is possible to learn much information about a category from just one, or a handful, of images. The key insight is that, rather than learning from scratch, one can take advantage of knowledge coming from previously learned categories, no matter how different these categories might be. We explore a Bayesian implementation of this idea. Object categories are represented by probabilistic models. Prior knowledge is represented as a probability density function on the parameters of these models. The posterior model for an object category is obtained by updating the prior in the light of one or more observations. We test a simple implementation of our algorithm on a database of 101 diverse object categories. We compare category models learned by an implementation of our Bayesian approach to models learned from by maximum likelihood (ML) and maximum a posteriori (MAP) methods. We find that on a database of more than 100 categories, the Bayesian approach produces informative models when the number of training examples is too small for other methods to operate successfully."
            },
            "slug": "One-shot-learning-of-object-categories-Fei-Fei-Fergus",
            "title": {
                "fragments": [],
                "text": "One-shot learning of object categories"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is found that on a database of more than 100 categories, the Bayesian approach produces informative models when the number of training examples is too small for other methods to operate successfully."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29905643"
                        ],
                        "name": "F. Porikli",
                        "slug": "F.-Porikli",
                        "structuredName": {
                            "firstName": "Fatih",
                            "lastName": "Porikli",
                            "middleNames": [
                                "Murat"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Porikli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 55
                            }
                        ],
                        "text": "[46] sped up HOG features by using integral histograms [47]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1122429,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fbf4d36e787e2e5c8444e1a2229b821e9cd68adf",
            "isKey": false,
            "numCitedBy": 810,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel method, which we refer as an integral histogram, to compute the histograms of all possible target regions in a Cartesian data space. Our method has three distinct advantages: 1) It is computationally superior to the conventional approach. The integral histogram method makes it possible to employ even an exhaustive search process in real-time, which was impractical before. 2) It can be extended to higher data dimensions, uniform and nonuniform bin formations, and multiple target scales without sacrificing its computational advantages. 3) It enables the description of higher level histogram features. We exploit the spatial arrangement of data points, and recursively propagate an aggregated histogram by starting from the origin and traversing through the remaining points along either a scan-line or a wave-front. At each step, we update a single bin using the values of integral histogram at the previously visited neighboring data points. After the integral histogram is propagated, histogram of any target region can be computed easily by using simple arithmetic operations."
            },
            "slug": "Integral-histogram:-a-fast-way-to-extract-in-spaces-Porikli",
            "title": {
                "fragments": [],
                "text": "Integral histogram: a fast way to extract histograms in Cartesian spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The integral histogram method makes it possible to employ even an exhaustive search process in real-time, which was impractical before, and enables the description of higher level histogram features."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2577513"
                        ],
                        "name": "Oncel Tuzel",
                        "slug": "Oncel-Tuzel",
                        "structuredName": {
                            "firstName": "Oncel",
                            "lastName": "Tuzel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oncel Tuzel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29905643"
                        ],
                        "name": "F. Porikli",
                        "slug": "F.-Porikli",
                        "structuredName": {
                            "firstName": "Fatih",
                            "lastName": "Porikli",
                            "middleNames": [
                                "Murat"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Porikli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145776090"
                        ],
                        "name": "P. Meer",
                        "slug": "P.-Meer",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Meer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Meer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[64] utilized covariance matrices computed locally over various features as object descriptors."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7817555,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "33e46a618fdb22d46951f548d6ceeb384e7f1687",
            "isKey": false,
            "numCitedBy": 938,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new algorithm to detect pedestrian in still images utilizing covariance matrices as object descriptors. Since the descriptors do not form a vector space, well known machine learning techniques are not well suited to learn the classifiers. The space of d-dimensional nonsingular covariance matrices can be represented as a connected Riemannian manifold. The main contribution of the paper is a novel approach for classifying points lying on a connected Riemannian manifold using the geometry of the space. The algorithm is tested on INRIA and DaimlerChrysler pedestrian datasets where superior detection rates are observed over the previous approaches."
            },
            "slug": "Pedestrian-Detection-via-Classification-on-Tuzel-Porikli",
            "title": {
                "fragments": [],
                "text": "Pedestrian Detection via Classification on Riemannian Manifolds"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A novel approach for classifying points lying on a connected Riemannian manifold using the geometry of the space of d-dimensional nonsingular covariance matrices as object descriptors."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11323114"
                        ],
                        "name": "Yazhou Liu",
                        "slug": "Yazhou-Liu",
                        "structuredName": {
                            "firstName": "Yazhou",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yazhou Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145455919"
                        ],
                        "name": "S. Shan",
                        "slug": "S.-Shan",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Shan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Shan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46772547"
                        ],
                        "name": "Xilin Chen",
                        "slug": "Xilin-Chen",
                        "structuredName": {
                            "firstName": "Xilin",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xilin Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3111194"
                        ],
                        "name": "J. Heikkil\u00e4",
                        "slug": "J.-Heikkil\u00e4",
                        "structuredName": {
                            "firstName": "Janne",
                            "lastName": "Heikkil\u00e4",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Heikkil\u00e4"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37110739"
                        ],
                        "name": "Wen Gao",
                        "slug": "Wen-Gao",
                        "structuredName": {
                            "firstName": "Wen",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wen Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145962204"
                        ],
                        "name": "M. Pietik\u00e4inen",
                        "slug": "M.-Pietik\u00e4inen",
                        "structuredName": {
                            "firstName": "Matti",
                            "lastName": "Pietik\u00e4inen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pietik\u00e4inen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16214804,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c1aa52ad21d0ec20102eb5402c60ac91c49612bb",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a novel descriptor for human detection in video sequence. It is referred to as spatial-temporal granularity-tunable gradients partition (STGGP), which is an extension of granularity-tunable gradients partition (GGP) from the still image domain to the spatial-temporal domain. Specifically, the moving human body is considered as a 3-dimensional entity in the spatial-temporal domain. Then in 3D Hough space, we define the generalized plane as a primitive to parse the structure of this 3D entity. The advantage of the generalized plane is that it can tolerate imperfect planes with certain level of uncertainty in rotation and translation. The robustness to the uncertainty is controlled quantitatively by the granularity parameters defined explicitly in the generalized plane. This property endows the STGGP descriptors versatile ability to represent both the deterministic structures and the statistical summarizations of the object. Moreover, the STGGP descriptor encodes much heterogeneous information such as the gradients' strength, position, and distribution, as well as their temporal motion to enrich its representation ability. We evaluate the STGGP on human detection in sequence on the public datasets and very promising results have been achieved."
            },
            "slug": "Spatial-Temporal-Granularity-Tunable-Gradients-for-Liu-Shan",
            "title": {
                "fragments": [],
                "text": "Spatial-Temporal Granularity-Tunable Gradients Partition (STGGP) Descriptors for Human Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The STGGP descriptor encodes much heterogeneous information such as the gradients' strength, position, and distribution, as well as their temporal motion to enrich its representation ability."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 97
                            }
                        ],
                        "text": "Notable early approaches for unsupervised part learning, including the constellation model [79], [80] and the sparse representation approach of Agarwal and Roth [81], relied on keypoints."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5745749,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "62837ab473124ea43cb8d7c6a4b4ee0f6f14e8c5",
            "isKey": false,
            "numCitedBy": 2487,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method to learn and recognize object class models from unlabeled and unsegmented cluttered scenes in a scale invariant manner. Objects are modeled as flexible constellations of parts. A probabilistic representation is used for all aspects of the object: shape, appearance, occlusion and relative scale. An entropy-based feature detector is used to select regions and their scale within the image. In learning the parameters of the scale-invariant object model are estimated. This is done using expectation-maximization in a maximum-likelihood setting. In recognition, this model is used in a Bayesian manner to classify images. The flexible nature of the model is demonstrated by excellent results over a range of datasets including geometrically constrained classes (e.g. faces, cars) and flexible objects (such as animals)."
            },
            "slug": "Object-class-recognition-by-unsupervised-learning-Fergus-Perona",
            "title": {
                "fragments": [],
                "text": "Object class recognition by unsupervised scale-invariant learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The flexible nature of the model is demonstrated by excellent results over a range of datasets including geometrically constrained classes (e.g. faces, cars) and flexible objects (such as animals)."
            },
            "venue": {
                "fragments": [],
                "text": "2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1815472"
                        ],
                        "name": "Anh-Tuan Nghiem",
                        "slug": "Anh-Tuan-Nghiem",
                        "structuredName": {
                            "firstName": "Anh-Tuan",
                            "lastName": "Nghiem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anh-Tuan Nghiem"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144103389"
                        ],
                        "name": "F. Br\u00e9mond",
                        "slug": "F.-Br\u00e9mond",
                        "structuredName": {
                            "firstName": "Fran\u00e7ois",
                            "lastName": "Br\u00e9mond",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Br\u00e9mond"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686585"
                        ],
                        "name": "M. Thonnat",
                        "slug": "M.-Thonnat",
                        "structuredName": {
                            "firstName": "Monique",
                            "lastName": "Thonnat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Thonnat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31987290"
                        ],
                        "name": "V. Valentin",
                        "slug": "V.-Valentin",
                        "structuredName": {
                            "firstName": "Val\u00e9ry",
                            "lastName": "Valentin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Valentin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 78
                            }
                        ],
                        "text": "Pedestrians can be labeled in photographs [7], [16], surveillance video [17], [24], and images"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8963115,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cdaced1e31d62ea1f8a62a4cc586d889ce081a38",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents the results of ETISEO, a performance evaluation project for video surveillance systems. Many other projects have already evaluated the performance of video surveillance systems, but more on an end-user point of view. ETISEO aims at studying the dependency between algorithms and the video characteristics. Firstly we describe ETISEO methodology which consists in addressing each video processing problem separately. Secondly, we present the main evaluation metrics of ETISEO as well as their benefits, limitations and conditions of use. Finally, we discuss about the contributions of ETISEO to the evaluation community."
            },
            "slug": "ETISEO,-performance-evaluation-for-video-systems-Nghiem-Br\u00e9mond",
            "title": {
                "fragments": [],
                "text": "ETISEO, performance evaluation for video surveillance systems"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "The results of ETISEO are presented, a performance evaluation project for video surveillance systems that aims at studying the dependency between algorithms and the video characteristics and the main evaluation metrics are presented."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Advanced Video and Signal Based Surveillance"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110604335"
                        ],
                        "name": "Markus Weber",
                        "slug": "Markus-Weber",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Weber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Markus Weber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678311"
                        ],
                        "name": "M. Welling",
                        "slug": "M.-Welling",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Welling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Welling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "Notable early approaches for unsupervised part learning, including the constellation model [79], [80] and the sparse representation approach of Agarwal and Roth [81], relied on keypoints."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8970876,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1cf1527807ebb16020b04d4166e7ba8d27652302",
            "isKey": false,
            "numCitedBy": 757,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method to learn object class models from unlabeled and unsegmented cluttered scenes for the purpose of visual object recognition. We focus on a particular type of model where objects are represented as flexible constellations of rigid parts (features). The variability within a class is represented by a joint probability density function (pdf) on the shape of the constellation and the output of part detectors. In a first stage, the method automatically identifies distinctive parts in the training set by applying a clustering algorithm to patterns selected by an interest operator. It then learns the statistical shape model using expectation maximization. The method achieves very good classification results on human faces and rear views of cars."
            },
            "slug": "Unsupervised-Learning-of-Models-for-Recognition-Weber-Welling",
            "title": {
                "fragments": [],
                "text": "Unsupervised Learning of Models for Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A method to learn object class models from unlabeled and unsegmented cluttered cluttered scenes for the purpose of visual object recognition that achieves very good classification results on human faces and rear views of cars."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15808204"
                        ],
                        "name": "J. Barron",
                        "slug": "J.-Barron",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Barron",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Barron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793739"
                        ],
                        "name": "David J. Fleet",
                        "slug": "David-J.-Fleet",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Fleet",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Fleet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46818737"
                        ],
                        "name": "S. Beauchemin",
                        "slug": "S.-Beauchemin",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Beauchemin",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Beauchemin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[8] and Middlebury [9] optical flow datasets, the Berkeley Segmentation Dataset [10], the Middlebury Stereo Dataset [11], and the Caltech 101 [12], Caltech 256 [13] and PASCAL [14] object recognition datasets all improved performance evaluation, added challenge, and helped drive innovation in their respective fields."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1290100,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "400f4afb2a055d95483d83c4bedab10281a8639d",
            "isKey": false,
            "numCitedBy": 4068,
            "numCiting": 125,
            "paperAbstract": {
                "fragments": [],
                "text": "While different optical flow techniques continue to appear, there has been a lack of quantitative evaluation of existing methods. For a common set of real and synthetic image sequences, we report the results of a number of regularly cited optical flow techniques, including instances of differential, matching, energy-based, and phase-based methods. Our comparisons are primarily empirical, and concentrate on the accuracy, reliability, and density of the velocity measurements; they show that performance can differ significantly among the techniques we implemented."
            },
            "slug": "Performance-of-optical-flow-techniques-Barron-Fleet",
            "title": {
                "fragments": [],
                "text": "Performance of optical flow techniques"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "These comparisons are primarily empirical, and concentrate on the accuracy, reliability, and density of the velocity measurements; they show that performance can differ significantly among the techniques the authors implemented."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2915583"
                        ],
                        "name": "E. Dickmanns",
                        "slug": "E.-Dickmanns",
                        "structuredName": {
                            "firstName": "Ernst",
                            "lastName": "Dickmanns",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Dickmanns"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": ", see [2], [6], [42]), a comparative study of approaches for integrating detector outputs over time has not been carried out."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 50
                            }
                        ],
                        "text": "[38], [39], [40], [41]), we refer readers to [2], [42], [43]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 44742529,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "1f30e6bdcd7af708f70d1957f5665843dd8e4b3e",
            "isKey": false,
            "numCitedBy": 172,
            "numCiting": 196,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction Basic Relations: Image Sequences - 'the World' Subjects and Subject Classes Application Domains, Missions and Situations Extraction of Visual Features Recursive State Estimation Beginnings of Spatio-temporal Road and Ego-state Recognition The Initialization Challenge in Dynamic Scene Understanding Recursive Estimation of Road Parameters and Ego-state while Cruising Perception of Crossroads Perception of Obstacles and Other Vehicles Sensor Requirements for Flexible Perception of Road Scenes Integrated Knowledge Representations for Dynamic Vision Mission Performance, Experimental Results Conclusions and Outlook"
            },
            "slug": "Dynamic-Vision-for-Perception-and-Control-of-Motion-Dickmanns",
            "title": {
                "fragments": [],
                "text": "Dynamic Vision for Perception and Control of Motion"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The initialization challenge in Dynamic Scene and Recursive Estimation of Road Parameters and Ego-state while Cruising Perception of Crossroads Perception of Obstacles and Other Vehicles are studied."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144522420"
                        ],
                        "name": "T. Ojala",
                        "slug": "T.-Ojala",
                        "structuredName": {
                            "firstName": "Timo",
                            "lastName": "Ojala",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ojala"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145962204"
                        ],
                        "name": "M. Pietik\u00e4inen",
                        "slug": "M.-Pietik\u00e4inen",
                        "structuredName": {
                            "firstName": "Matti",
                            "lastName": "Pietik\u00e4inen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pietik\u00e4inen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "26883091"
                        ],
                        "name": "Topi M\u00e4enp\u00e4\u00e4",
                        "slug": "Topi-M\u00e4enp\u00e4\u00e4",
                        "structuredName": {
                            "firstName": "Topi",
                            "lastName": "M\u00e4enp\u00e4\u00e4",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Topi M\u00e4enp\u00e4\u00e4"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 93
                            }
                        ],
                        "text": "Performance improves for all methods with increasing scale, but most for MULTIFTR+MOTION, HOGLBP, and LATSVM-V2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": ", Haar wavelets [44]), color, texture (including LBP [60] and co-occurrence [85]), self-similarity [86], and motion [54] features."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 343,
                                "start": 340
                            }
                        ],
                        "text": "In a few cases, the evaluated code differed from the published version: SHAPELET and HIKSVM have been corrected so they no longer overfit to boundary effects; we evaluate a variant of POSEINV based on boosting (which in our tests outperformed the much slower kernel SVM version); PLS\nswitched to PM NMS; and finally, the posted code for HOGLBP does not include occlusion reasoning (the improvement from occlusion reasoning was slight [59])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 130
                            }
                        ],
                        "text": "In addition, detectors can utilize gradients directly, as well as grayscale (e.g., Haar wavelets [44]), color, texture (including LBP [60] and co-occurrence [85]), self-similarity [86], and motion [54] features."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 79
                            }
                        ],
                        "text": "Wang et al. [59] combined a texture descriptor based on local binary patterns (LBP) [60] with HOG [HOGLBP]; additionally, a linear SVM classifier was modified to perform basic occlusion reasoning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 72
                            }
                        ],
                        "text": "[59] combined a texture descriptor based on local binary patterns (LBP) [60] with HOG [HOGLBP]; additionally, a linear SVM classifier was modified to perform basic occlusion reasoning."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 23
                            }
                        ],
                        "text": "In addition\nto HOG and LBP, [61] used local ternary patterns (variants of LBP)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14540685,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f11a7136b6b7854bd0998ef463ffa8e907c411a2",
            "isKey": true,
            "numCitedBy": 13064,
            "numCiting": 95,
            "paperAbstract": {
                "fragments": [],
                "text": "Presents a theoretically very simple, yet efficient, multiresolution approach to gray-scale and rotation invariant texture classification based on local binary patterns and nonparametric discrimination of sample and prototype distributions. The method is based on recognizing that certain local binary patterns, termed \"uniform,\" are fundamental properties of local image texture and their occurrence histogram is proven to be a very powerful texture feature. We derive a generalized gray-scale and rotation invariant operator presentation that allows for detecting the \"uniform\" patterns for any quantization of the angular space and for any spatial resolution and presents a method for combining multiple operators for multiresolution analysis. The proposed approach is very robust in terms of gray-scale variations since the operator is, by definition, invariant against any monotonic transformation of the gray scale. Another advantage is computational simplicity as the operator can be realized with a few operations in a small neighborhood and a lookup table. Experimental results demonstrate that good discrimination can be achieved with the occurrence statistics of simple rotation invariant local binary patterns."
            },
            "slug": "Multiresolution-Gray-Scale-and-Rotation-Invariant-Ojala-Pietik\u00e4inen",
            "title": {
                "fragments": [],
                "text": "Multiresolution Gray-Scale and Rotation Invariant Texture Classification with Local Binary Patterns"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A generalized gray-scale and rotation invariant operator presentation that allows for detecting the \"uniform\" patterns for any quantization of the angular space and for any spatial resolution and presents a method for combining multiple operators for multiresolution analysis."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2340109"
                        ],
                        "name": "C. Wojek",
                        "slug": "C.-Wojek",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Wojek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Wojek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1891864"
                        ],
                        "name": "Gyuri Dork\u00f3",
                        "slug": "Gyuri-Dork\u00f3",
                        "structuredName": {
                            "firstName": "Gyuri",
                            "lastName": "Dork\u00f3",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gyuri Dork\u00f3"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146216616"
                        ],
                        "name": "Andr\u00e9 Schulz",
                        "slug": "Andr\u00e9-Schulz",
                        "structuredName": {
                            "firstName": "Andr\u00e9",
                            "lastName": "Schulz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andr\u00e9 Schulz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 37286148,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a114d163aa331aaa417be65dc6ec5f898c9660f1",
            "isKey": false,
            "numCitedBy": 135,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a fast object class localization framework implemented on a data parallel architecture currently available in recent computers. Our case study, the implementation of Histograms of Oriented Gradients (HOG) descriptors, shows that just by using this recent programming model we can easily speed up an original CPU-only implementation by a factor of 34, making it unnecessary to use early rejection cascades that sacrifice classification performance, even in real-time conditions. Using recent techniques to program the Graphics Processing Unit (GPU) allow our method to scale up to the latest, as well as to future improvements of the hardware."
            },
            "slug": "Sliding-Windows-for-Rapid-Object-Class-A-Parallel-Wojek-Dork\u00f3",
            "title": {
                "fragments": [],
                "text": "Sliding-Windows for Rapid Object Class Localization: A Parallel Technique"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "A fast object class localization framework implemented on a data parallel architecture currently available in recent computers, and using recent techniques to program the Graphics Processing Unit (GPU) allow this method to scale up to the latest, as well as to future improvements of the hardware."
            },
            "venue": {
                "fragments": [],
                "text": "DAGM-Symposium"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39558258"
                        ],
                        "name": "S. Garc\u00eda",
                        "slug": "S.-Garc\u00eda",
                        "structuredName": {
                            "firstName": "Salvador",
                            "lastName": "Garc\u00eda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Garc\u00eda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699069"
                        ],
                        "name": "F. Herrera",
                        "slug": "F.-Herrera",
                        "structuredName": {
                            "firstName": "Francisco",
                            "lastName": "Herrera",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Herrera"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A further in-depth study by Garc\u0131\u0301a and Herrera [88] concluded that the Nemenyi posthoc test which was used by Dem sar [87] (and also in the PASCAL challenge [14]) is too conservative for n n comparisons such as in a benchmark."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For our analysis we use the nonparametric Friedman test along with the Shaffer posthoc test (code is available from [88])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17257979,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0f09767a303c2b97807c51996fb53dbbafedf9af",
            "isKey": false,
            "numCitedBy": 1226,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "In a recently published paper in JMLR, Dem\u02c7 sar (2006) recommends a set of non-parametric statistical tests and procedures which can be safely used for comparing the performance of classifiers over multiple data sets. After studying the paper, we realize that the paper correctly introduces the basic procedures and some of the most advanced ones when comparing a control method. However, it does not deal with some advanced topics in depth. Regarding these topics, we focus on more powerful proposals of statistical procedures for comparing n n classifiers. Moreover, we illustrate an easy way of obtaining adjusted and comparable p-values in multiple comparison procedures."
            },
            "slug": "An-Extension-on-\"Statistical-Comparisons-of-over-Garc\u00eda-Herrera",
            "title": {
                "fragments": [],
                "text": "An Extension on \"Statistical Comparisons of Classifiers over Multiple Data Sets\" for all Pairwise Comparisons"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The paper correctly introduces the basic procedures and some of the most advanced ones when comparing a control method, but it does not deal with some advanced topics in depth."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2701105"
                        ],
                        "name": "J. Dem\u0161ar",
                        "slug": "J.-Dem\u0161ar",
                        "structuredName": {
                            "firstName": "Janez",
                            "lastName": "Dem\u0161ar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dem\u0161ar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[87] found this non-parametric approach to be more robust."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "(b) Critical difference diagram [87]: the x-axis shows mean rank, blue bars link detectors for which there is insufficient evidence to declare them statistically significantly different (due to the relatively low number of performance samples and fairly high variance)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 112
                            }
                        ],
                        "text": "A further in-depth study by Garc\u0131\u0301a and Herrera [88] concludes that the Nemenyi post-hoc test which was used by [87] (and also in the PASCAL challenge [14]) is too conservative for n \u00d7 n comparisons such as in a benchmark."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[87] introduced a series of powerful statistical tests that operate on an m dataset by n algorithm performance matrix (e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7553535,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8f1408d33858a78f90f9000a34856664fc639ae4",
            "isKey": true,
            "numCitedBy": 9185,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classifiers: the Wilcoxon signed ranks test for comparison of two classifiers and the Friedman test with the corresponding post-hoc tests for comparison of more classifiers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced CD (critical difference) diagrams."
            },
            "slug": "Statistical-Comparisons-of-Classifiers-over-Data-Dem\u0161ar",
            "title": {
                "fragments": [],
                "text": "Statistical Comparisons of Classifiers over Multiple Data Sets"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A set of simple, yet safe and robust non-parametric tests for statistical comparisons of classifiers is recommended: the Wilcoxon signed ranks test for comparison of two classifiers and the Friedman test with the corresponding post-hoc tests for comparisons of more classifiers over multiple data sets."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10771328"
                        ],
                        "name": "Greg Mori",
                        "slug": "Greg-Mori",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Mori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg Mori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 112
                            }
                        ],
                        "text": "Wojek and Schiele [MULTIFTR] [56] showed how a combination of Haar-like features, shapelets [33], shape context [57] and HOG features outperforms any individual feature."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 58433,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "83c26bdfa1981b1bd8632258d4efedf3ba06910f",
            "isKey": false,
            "numCitedBy": 439,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "We demonstrate that shape contexts can be used to quickly prune a search for similar shapes. We present two algorithms for rapid shape retrieval: representative shape contexts, performing comparisons based on a small number of shape contexts, and shapemes, using vector quantization in the space of shape contexts to obtain prototypical shape pieces."
            },
            "slug": "Efficient-shape-matching-using-shape-contexts-Mori-Belongie",
            "title": {
                "fragments": [],
                "text": "Efficient shape matching using shape contexts"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "It is demonstrated that shape contexts can be used to quickly prune a search for similar shapes, and shapemes are used, using vector quantization in the space of shape contexts to obtain prototypical shape pieces."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1644050191"
                        ],
                        "name": "G. LoweDavid",
                        "slug": "G.-LoweDavid",
                        "structuredName": {
                            "firstName": "G",
                            "lastName": "LoweDavid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. LoweDavid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 17
                            }
                        ],
                        "text": "Inspired by SIFT [45], Dalal and Triggs [HOG] [7] popularized histogram of oriented gradient (HOG) features for detection by showing substantial gains over intensitybased features."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 174065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4cab9c4b571761203ed4c3a4c5a07dd615f57a91",
            "isKey": false,
            "numCitedBy": 25501,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are ..."
            },
            "slug": "Distinctive-Image-Features-from-Scale-Invariant-LoweDavid",
            "title": {
                "fragments": [],
                "text": "Distinctive Image Features from Scale-Invariant Keypoints"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144321599"
                        ],
                        "name": "M. McGill",
                        "slug": "M.-McGill",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "McGill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. McGill"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 76
                            }
                        ],
                        "text": "Conceptually, the log-average miss rate is similar to the average precision [26] reported for the PASCAL challenge [14] in that it represents the entire curve by a single reference value."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 43685115,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "49af3e80343eb80c61e727ae0c27541628c7c5e2",
            "isKey": false,
            "numCitedBy": 12605,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Some people may be laughing when looking at you reading in your spare time. Some may be admired of you. And some may want be like you who have reading hobby. What about your own feel? Have you felt right? Reading is a need and a hobby at once. This condition is the on that will make you feel that you must read. If you know are looking for the book enPDFd introduction to modern information retrieval as the choice of reading, you can find here."
            },
            "slug": "Introduction-to-Modern-Information-Retrieval-Salton-McGill",
            "title": {
                "fragments": [],
                "text": "Introduction to Modern Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Reading is a need and a hobby at once and this condition is the on that will make you feel that you must read."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[34] proposed an approximation to the histogram intersection kernel for use with SVMs [HIKSVM], allowing for substantial speedups and thus enabling a nonlinear SVM to be used in sliding-window detection."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 43
                            }
                        ],
                        "text": "We observed this for the SHAPELET [33] and HIKSVM [34] detectors, see also www.cs.sfu.ca/~mori/research/papers/ sabzmeydani_shapelet_cvpr07.html and http://www.cs.berkeley.edu/ ~smaji/projects/ped-detector/."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 50
                            }
                        ],
                        "text": "We observed this for the SHAPELET [33] and HIKSVM [34] detectors, see also www."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 179
                            }
                        ],
                        "text": "Boosted classifiers and linear SVMs are particularly well suited due to their speed; nonlinear kernels are less common, the exception being the fast histogram intersection kernel [34]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 85
                            }
                        ],
                        "text": "In a few cases, the evaluated code differed from the published version: SHAPELET and HIKSVM have been corrected so they no longer overfit to boundary effects; we evaluate a variant of POSEINV based on boosting (which in our tests outperformed the much slower kernel SVM version); PLS\nswitched to PM NMS; and finally, the posted code for HOGLBP does not include occlusion reasoning (the improvement from occlusion reasoning was slight [59])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 99
                            }
                        ],
                        "text": "Maji et al. [34] proposed an approximation to the histogram intersection kernel for use with SVMs [HIKSVM], allowing for substantial speedups and thus enabling a nonlinear SVM to be used in sliding-window detection."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Classification Using Intersection Kernel Svms Is Efficient"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2008."
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 184
                            }
                        ],
                        "text": "In a few cases, the evaluated code differed from the published version: SHAPELET and HIKSVM have been corrected so they no longer overfit to boundary effects; we evaluate a variant of POSEINV based on boosting (which in our tests outperformed the much slower kernel SVM version); PLS\nswitched to PM NMS; and finally, the posted code for HOGLBP does not include occlusion reasoning (the improvement from occlusion reasoning was slight [59])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "051 91% INRIA \u201907 POSEINV [70] 3 AdaBoost MS 96 \u223c18 ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 184
                            }
                        ],
                        "text": "Lin and Davis [70] used a parttemplate tree to model a pedestrian\u2019s shape locally for the head, upper body, and legs, and extracted HOG appearance descriptors along the shape\u2019s outline [POSEINV]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 14
                            }
                        ],
                        "text": "Lin and Davis [70] used a part-template tree to model a pedestrian\u2019s shape locally for the head, upper body and legs, and extracted HOG appearance descriptors along the shape\u2019s outline [POSEINV]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A pose-invariant descriptor for human det. and seg."
            },
            "venue": {
                "fragments": [],
                "text": "European Conf. Computer Vision,"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "Haar wavelets [44]), color, texture (including LBP [60] and co-occurrence [85]), self-similarity [86] and motion [54] features."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 93
                            }
                        ],
                        "text": "Performance improves for all methods with increasing scale, but most for MULTIFTR+MOTION, HOGLBP, and LATSVM-V2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 72
                            }
                        ],
                        "text": "[59] combined a texture descriptor based on local binary patterns (LBP) [60] with HOG [HOGLBP], additionally, a linear SVM classifier was modified to perform basic occlusion reasoning."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 343,
                                "start": 340
                            }
                        ],
                        "text": "In a few cases, the evaluated code differed from the published version: SHAPELET and HIKSVM have been corrected so they no longer overfit to boundary effects; we evaluate a variant of POSEINV based on boosting (which in our tests outperformed the much slower kernel SVM version); PLS\nswitched to PM NMS; and finally, the posted code for HOGLBP does not include occlusion reasoning (the improvement from occlusion reasoning was slight [59])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 130
                            }
                        ],
                        "text": "In addition, detectors can utilize gradients directly, as well as grayscale (e.g., Haar wavelets [44]), color, texture (including LBP [60] and co-occurrence [85]), self-similarity [86], and motion [54] features."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 79
                            }
                        ],
                        "text": "Wang et al. [59] combined a texture descriptor based on local binary patterns (LBP) [60] with HOG [HOGLBP]; additionally, a linear SVM classifier was modified to perform basic occlusion reasoning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 23
                            }
                        ],
                        "text": "In addition\nto HOG and LBP, [61] used local ternary patterns (variants of LBP)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multiresolution grayscale and rotation invariant texture classification with local binary patterns"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 24, no. 7, pp. 971\u2013987, Jul. 2002. 11, 12"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 124
                            }
                        ],
                        "text": "Moreover, numerous approaches have been proposed for speeding up detection, including speeding up the detector itself [29], [44], [46], through use of approximations [63], [89] or by using special purpose hardware such as GPUs [90] (for a review of fast detection see [63])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 14
                            }
                        ],
                        "text": "Haar wavelets [44]), color, texture (including LBP [60] and co-occurrence [85]), self-similarity [86] and motion [54] features."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 49
                            }
                        ],
                        "text": "Support vector machines (SVMs) [16] and boosting [44] are the most popular choices due to their theoretical guarantees, extensibility, and good performance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "Viola and Jones [VJ] [44] built upon these ideas, introducing integral images for fast feature computation and a cascade structure for efficient detection, and utilizing AdaBoost for automatic feature selection."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Robust real-time face det."
            },
            "venue": {
                "fragments": [],
                "text": "Intl. Journal of Computer Vision,"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7420768"
                        ],
                        "name": "M. Mckay",
                        "slug": "M.-Mckay",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Mckay",
                            "middleNames": [
                                "Pat"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mckay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2040479"
                        ],
                        "name": "F. Vaca",
                        "slug": "F.-Vaca",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Vaca",
                            "middleNames": [
                                "E"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Vaca"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 95
                            }
                        ],
                        "text": "Just in the US, nearly 5,000 of the 35,000 annual traffic crash fatalities involve pedestrians [1]; hence the considerable interest in building automated vision systems for detecting pedestrians [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 26613001,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "173f9531b167011fe2768a0fd46e160ef9c4f75c",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Pedestrian-roadway-fatalities.-Mckay-Vaca",
            "title": {
                "fragments": [],
                "text": "Pedestrian roadway fatalities."
            },
            "venue": {
                "fragments": [],
                "text": "Annals of emergency medicine"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 20
                            }
                        ],
                        "text": "CVC [19], and NICTA [22] pedestrian data sets and the"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 55
                            }
                        ],
                        "text": "Older data sets, including the MIT [16], CVC [19], and NICTA [22] pedestrian data sets and the Daimler classification benchmark (Daimler-CB) [21], tend to contain cropped pedestrian windows only."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A New Pedestrian Data Set for Supervised Learning"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE Intelligent Vehicles Symp., 2008."
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Spatial-temporal granularity-tunable gradients partition descriptors for human det"
            },
            "venue": {
                "fragments": [],
                "text": "European Conf. Computer Vision"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "Gavrila and Philomin [49], [50] employed the Hausdorff distance transform and a template hierarchy to rapidly match image edges to a set of shape templates."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Real-time object det. for smart vehicles"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Intl. Conf. Computer Vision, 1999, pp. 87\u201393. 10"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pedestrian detection in crowded scenes , \u201d in IEEE Conf"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Conf . Computer Vision and Pattern Recognition"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multiresolution models for obj. det"
            },
            "venue": {
                "fragments": [],
                "text": "European Conf. Computer Vision"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 95
                            }
                        ],
                        "text": "In earlier work [3], we introduced the Caltech Pedestrian Data Set, which includes 350,000 pedestrian bounding boxes (BB) labeled in 250,000 frames and remains the largest such data set to date."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A database and eval. methodology for optical flow"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Intl. Conf. Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[73] successfully extended [16] with a two stage approach: first head, arm and leg detectors were trained in a fully supervised manner, next the detectors\u2019 output was combined to fit a rough geometric model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Example-based object det. in images by components"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 23, no. 4, pp. 349\u2013361, Apr. 2001. 11"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multi - cue ped . classification with partial occlusion handling , \u201d in IEEE Conf"
            },
            "venue": {
                "fragments": [],
                "text": "European Conf . Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[64] utilized covariance matrices computed locally over various features as object descriptors."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ped. det. via classification on riemannian manifolds"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 30, no. 10, pp. 1713\u20131727, Oct. 2008. 11  20  SUBMISSION TO IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Beyond sliding windows : Object localization by eff . subwindow search , \u201d in IEEE Conf"
            },
            "venue": {
                "fragments": [],
                "text": "Intl . Journal of Computer Vision"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[84] extended this to a multi-resolution model that automatically switches to parts only at sufficiently high resolutions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multiresolution models for obj"
            },
            "venue": {
                "fragments": [],
                "text": "det.\u201d in European Conf. Computer Vision,"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 85
                            }
                        ],
                        "text": "Such fully supervised two stage approaches have been revisited over time [17], [74], [75]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multi-cue ped. classification with partial occlusion handling"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Conf. Computer Vision and Pattern Recognition, 2010. 11"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Feature mining for image classification , \u201d in IEEE Conf"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 145
                            }
                        ],
                        "text": "Multiple instance learning (MIL) has been employed in order to automatically determine the position of parts without partlevel supervision [82], [83]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multiple Instance Feature for Robust Part-Based Object Detection"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2009."
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 92
                            }
                        ],
                        "text": "These appear most promising for low to medium resolution settings, under which segmentation [35] or keypoint [36], [37] based methods often fail."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recog. using regions"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Conf. Computer Vision and Pattern Recognition, 2009. 10"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[48] proposed a similar representation for characterizing spatially localized parts for modeling pedestrians."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ped. det. for driving assistance systems: Single-frame classification and system level performance"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Intl. Conf. Intelligent Vehicles, 2004. 10"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[51] proposed \u2018granularity-tunable\u2019 features that allow for representations with levels of uncertainty ranging from edgelet to HOG type features; an extension to the spatio-temporal domain was developed in [52]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Granularitytunable gradients partition descriptors for human det."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Conf. Computer Vision and Pattern Recognition,"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "A similar performance measure was used in [27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A comp. eval. framework and a comparative study for human det."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Intelligent Transportation Systems,"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/publications/dlib"
            },
            "venue": {
                "fragments": [],
                "text": "For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/publications/dlib"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cluster boosted tree classifier for multiview , multi - pose object det"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Intl . Conf . Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Granularity - tunable gradients partition descriptors for human det . \u201d in"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans . Pattern Analysis and Machine Intelligence"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "[31], [32]), such detectors cannot be evaluated using PW metrics."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Beyond sliding windows: Object localization by eff. subwindow search"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Conf. Computer Vision and Pattern Recognition, 2008. 9"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adaptive image sampling and windows classification for on-board ped. det"
            },
            "venue": {
                "fragments": [],
                "text": "Intl. Conf. Computer Vision Systems"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A pose - invariant descriptor for human det . and seg . \u201d in European Conf"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 56,
            "methodology": 50,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 116,
        "totalPages": 12
    },
    "page_url": "https://www.semanticscholar.org/paper/Pedestrian-Detection:-An-Evaluation-of-the-State-of-Doll\u00e1r-Wojek/34e0ba2daabfa4d3d22913ade8265aff50b5f917?sort=total-citations"
}