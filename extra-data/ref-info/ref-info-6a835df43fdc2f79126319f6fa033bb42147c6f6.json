{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145539951"
                        ],
                        "name": "J. Pollack",
                        "slug": "J.-Pollack",
                        "structuredName": {
                            "firstName": "Jordan",
                            "lastName": "Pollack",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pollack"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "memories with fractal dimensions are further discussed elsewhere [ 36 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11012105,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d259baddcfcbcdd3471eb66444633d7c60352a8c",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "I will describe my recent results on the automatic development of fixed-width recursive distributed representations of variable-sized hierarchal data structures. One implication of this work is that certain types of AI-style data-structures can now be represented in fixed-width analog vectors. Simple inferences can be performed using the type of pattern associations that neural networks excel at Another implication arises from noting that these representations become self-similar in the limit. Once this door to chaos is opened, many interesting new questions about the representational basis of intelligence emerge, and can (and will) be discussed."
            },
            "slug": "Implications-of-Recursive-Distributed-Pollack",
            "title": {
                "fragments": [],
                "text": "Implications of Recursive Distributed Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "The recent results on the automatic development of fixed-width recursive distributed representations of variable-sized hierarchal data structures suggest that certain types of AI-style data-structures can now be represented in fixed- width analog vectors."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1516909860"
                        ],
                        "name": "D. Touretzky",
                        "slug": "D.-Touretzky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Touretzky",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Touretzky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "(2) Coarse-coding requires expensive and complex access mechanisms, such as pullout networks [25] or clause-spaces [ 20 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "in a production system [ 20 ], a primitive lisp data-structuring system called BoltzCONS"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14138392,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0cb3be87b7f4f50d62d8dbba5a2e8d78c7d90a9",
            "isKey": false,
            "numCitedBy": 204,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Pattern matching and variable binding are easily implemented in conventional computer architectures, but not necessarily in all architectures. In a distributed neural network architecture each symbol is represented by activity in many units and each unit contributes to the representation of many symbols. Manipulating symbols using this type of distributed representation is not as easy as with a local representation whore each unit denotes one symbol, but there is evidence that the distributed approach is the one chosen by nature. We describe a working implementation of a production system interpreter in a neural network using distributed representations for both symbols and rules. The research provides a detailed account of two important symbolic reasoning operations, pattern matching and variable binding, as emergent properties of collections of neuron-like elements. The success of our production system implementation goes some way towards answering a common criticism of connectionist theories: that they aren't powerful enough to do symbolic reasoning."
            },
            "slug": "Symbols-Among-the-Neurons:-Details-of-a-Inference-Touretzky-Hinton",
            "title": {
                "fragments": [],
                "text": "Symbols Among the Neurons: Details of a Connectionist Inference Architecture"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The research provides a detailed account of two important symbolic reasoning operations, pattern matching and variable binding, as emergent properties of collections of neuron-like elements in a distributed neural network."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2884373"
                        ],
                        "name": "J. Elman",
                        "slug": "J.-Elman",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Elman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Elman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 93
                            }
                        ],
                        "text": "This form of non-stationary, or \u2018\u2018Moving Target,\u2019\u2019 learning has also been explored by others [29, 30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2763403,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "668087f0ae7ce1de6e0bd0965dbb480c08103260",
            "isKey": false,
            "numCitedBy": 9863,
            "numCiting": 111,
            "paperAbstract": {
                "fragments": [],
                "text": "Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction."
            },
            "slug": "Finding-Structure-in-Time-Elman",
            "title": {
                "fragments": [],
                "text": "Finding Structure in Time"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory and suggests a method for representing lexical categories and the type/token distinction is developed."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308302"
                        ],
                        "name": "D. Ackley",
                        "slug": "D.-Ackley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Ackley",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ackley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 63
                            }
                        ],
                        "text": "This looks suspiciously like a network for the Encoder Problem [27]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12174018,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657",
            "isKey": false,
            "numCitedBy": 3396,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Learning-Algorithm-for-Boltzmann-Machines-Ackley-Hinton",
            "title": {
                "fragments": [],
                "text": "A Learning Algorithm for Boltzmann Machines"
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 50027191,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3106e66537a0c8f53278e553bcb38f0b0992ec0e",
            "isKey": false,
            "numCitedBy": 1243,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Given a \u0302 network of simple computing elements and some entities to be represented, the most straightforward scheme is to use one computing element for each entity. This is called a local representation. It is easy to understand and easy to implement because the structure of the physical network mirrors the structure of the knowledge it contains. This report describes a different type of representation that is less familiar and harder to think about than local representations. Each entity is represented by a pattern of activity distributed over many computing elements, and each computing element is involved in representing many different entities. The strength of this more complicated kind of representation does not lie in its notational convenience or its ease of implementation in a conventional computer, but rather in the efficiency with which it makes use of the processing abilities of networks of simple, neuron-like computing elements. Every representational scheme has its good and bad points. Distributed representations are no exception. Some desirable properties like content-addressable memory and automatic generalization arise very naturally from the use of patterns of activity as representations. Other properties, like the ability to temporarily store a large set of arbitrary associations, are much harder to achieve. The best psychological evidence for distributed representations is the degree to which their strengths and weaknesses match those of the human mind. ^This research was supported by a grant from the System Development Foundation. I thank Jim Anderson, Dave Ackley Dana Ballard, Francis Crick, Scott Fahlman, Jerry Feldman, Christopher Longuet-Higgins, Don Norman, Terry Sejnowski, and Tim Shallice for helpful discussions. Jay McClelland and Dave Rumelhart helped me refine and rewrite many of the ideas presented here A substantially revised version of this report will appear as a chapter by Hinton, McClelland and Rumelhart in Parallel Distributed Processing: Explorations in the micro-structure of cognition, edited by McClelland and Rumelhart)"
            },
            "slug": "Distributed-Representations-Hinton-McClelland",
            "title": {
                "fragments": [],
                "text": "Distributed Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This report describes a different type of representation that is less familiar and harder to think about than local representations, which makes use of the processing abilities of networks of simple, neuron-like computing elements."
            },
            "venue": {
                "fragments": [],
                "text": "The Philosophy of Artificial Intelligence"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681269"
                        ],
                        "name": "M. Fanty",
                        "slug": "M.-Fanty",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Fanty",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Fanty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 165
                            }
                        ],
                        "text": "The limitation shows in the fact that pure connectionism has generated somewhat unsatisfying systems in this domain; for example, parsers for fixed length sentences [1-4], without embedded structures [5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62386308,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0e0efb073163b35bae48b95c540240a000a18ca2",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : This paper presents a simple algorithm which converts any context-free grammar (without epsilon productions) into a connectionist network which parses strings (of arbitrary but fixed maximum length) in the language defined by that grammar. The network is fast and deterministic. Some modifications of the network are also explored, including parsing near misses, disambiguating and learning new productions dynamically. (Author)"
            },
            "slug": "Context-free-parsing-with-connectionist-networks-Fanty",
            "title": {
                "fragments": [],
                "text": "Context-free parsing with connectionist networks"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A simple algorithm is presented which converts any context-free grammar (without epsilon productions) into a connectionist network which parses strings in the language defined by that grammar, which is fast and deterministic."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145903504"
                        ],
                        "name": "R. Rosenfeld",
                        "slug": "R.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rosenfeld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1516909860"
                        ],
                        "name": "D. Touretzky",
                        "slug": "D.-Touretzky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Touretzky",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Touretzky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 20
                            }
                        ],
                        "text": "R. Rosenfeld and D. Touretzky, Four capacity models for coarse-coded symbol memories, Complex Systems 2, (1988), 463-484."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 6
                            }
                        ],
                        "text": "D. S. Touretzky, Representing and transforming recursive objects in a neural network, or \u2018\u2018trees do grow on Boltzmann machines\u2019\u2019, Proceedings of the 1986 Institute of Electrical and Electronics Engineers International Conference on Systems, Man, and Cybernetics, Atlanta, GA, 1986."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 127
                            }
                        ],
                        "text": "J. B. Pollack, Implications of Recursive Distributed Representations, in Advances in Neural Information Processing Systems, D. Touretzky (ed.)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "3 Rosenfeld and Touretzky [26] provide a nice analysis of coarse-coded symbol memories."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 6
                            }
                        ],
                        "text": "D. S. Touretzky, BoltzCONS: Reconciling connectionism with the recursive nature of stacks and trees, Proceedings of the 8th Annual Conference of the Cognitive Science Society, Amherst, MA, 1986, 522-530."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 303,
                                "start": 294
                            }
                        ],
                        "text": "So a system, for example, which represented the spellings of words as sets of letter-pairs would not be able to represent the word yoyo, and even if the breadth were increased to three, the system would still not be able to represent words with duplicate triples such as banana.2\nAlthough both Touretzky\u2019s and Rumelhart & McClelland\u2019s coarse-coded represen-\ntations were fairly successful for their circumscribed tasks, there remain some problems:\n(1) A large amount of human effort was involved in the design, compression and tuning\nof these representations, and it is often not clear how to translate that effort across domains."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 62
                            }
                        ],
                        "text": "Comments from B. Chandrasekaran, G. Hinton, J. McClelland, D. Touretzky, T. VanGelder, and many, many others helped to improve this presentation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 6
                            }
                        ],
                        "text": "D. S. Touretzky and G. E. Hinton, Symbols among the neurons: details of a connectionist inference architecture, Proceedings of the Ninth International Joint Conference on Artificial Intelligence, Los Angeles, CA, 1985."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 13
                            }
                        ],
                        "text": "For example, Touretzky has developed a coarse-coded memory system and used it\nin a production system [20], a primitive lisp data-structuring system called BoltzCONS [21], and a combination of the two for simple tree manipulations [22]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8177088,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "114b2df7ba63669c2f901d4e3c298d9360c4ae7d",
            "isKey": true,
            "numCitedBy": 10,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Coarse-coded symbol memories have appeared in several neutral network symbol processing models. In order to determine how these models would scale, one must first have some understanding of the mathematics of coarse-coded representations. We define the general structure of coarse-coded symbol memories, and discuss their strengths and weaknesses. Memory schemes can be characterized by their memory size, symbol-set size and capacity. We derive mathematical relationships between these parameters for various memory schemes, using both analysis and numerical methods. Finally, we compare the predicted capacity of one of the schemes with actual measurements of the coarse-coded working memory of distributed connectionist production system (DCPS), Touretzky and Hinton's distributed connectionist production system. (jg)"
            },
            "slug": "Four-Capacity-Models-for-Coarse-Coded-Symbol-Rosenfeld-Touretzky",
            "title": {
                "fragments": [],
                "text": "Four Capacity Models for Coarse-Coded Symbol Memories"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The general structure of coarse-coded symbol memories is defined, and their strengths and weaknesses are discussed, and mathematical relationships between these parameters for various memory schemes are derived using both analysis and numerical methods."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788375"
                        ],
                        "name": "D. Waltz",
                        "slug": "D.-Waltz",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Waltz",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Waltz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145539951"
                        ],
                        "name": "J. Pollack",
                        "slug": "J.-Pollack",
                        "structuredName": {
                            "firstName": "Jordan",
                            "lastName": "Pollack",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pollack"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 41
                            }
                        ],
                        "text": "1 Hybrid (connectionist-symbolic) models [6-9] have the potential for more powerful representations, but do not insist on the neural plausibility constraints which create the limitations in the first place."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17836106,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c96fe25817c5fca96719cfa56cdaeeb2d17c93d7",
            "isKey": false,
            "numCitedBy": 518,
            "numCiting": 86,
            "paperAbstract": {
                "fragments": [],
                "text": "This is a description of research in developing a natural language processing system with modular knowledge sources but strongly interactive processing. The system offers insights into a variety of linguistic phenomena and allows easy testing of a variety of hypotheses. Language interpretation takes place on a activation network which is dynamically created from input, recent context, and long-term knowledge. Initially ambiguous and unstable, the network settles on a single interpretation, using a parallel, analog relaxation process. We also describe a parallel model for the representation of context and of the priming of concepts. Examples illustrating contextual influence on meaning interpretation and \u201csemantic garden path\u201d sentence processing, among other issues, are included."
            },
            "slug": "Massively-Parallel-Parsing:-A-Strongly-Interactive-Waltz-Pollack",
            "title": {
                "fragments": [],
                "text": "Massively Parallel Parsing: A Strongly Interactive Model of Natural Language Interpretation"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This work describes a parallel model for the representation of context and of the priming of concepts in a natural language processing system with modular knowledge sources but strongly interactive processing."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2546518"
                        ],
                        "name": "D. Plaut",
                        "slug": "D.-Plaut",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Plaut",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Plaut"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802785"
                        ],
                        "name": "S. Nowlan",
                        "slug": "S.-Nowlan",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Nowlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nowlan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15150815,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4a42b2104ca8ff891ae77c40a915d4c94c8f8428",
            "isKey": false,
            "numCitedBy": 390,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Rumelhart, Hinton and Williams (Rumelhart 86) describe a learning procedure for layered networks of deterministic, neuron-like units. This paper describes further research on the learning procedure. We start by describing the units, the way they are connected, the learning procedure, and the extension to iterative nets. We then give an example in which a network learns a set of filters that enable it to discriminate formant-like patterns in the presence of noise. The speed of learning is strongly dependent on the shape of the surface formed by the error measure in weight space . We give examples of the shape of the error surface for a typical task and illustrate how an acceleration method speeds up descent in weight space. The main drawback of the learning procedure is the way it scales as the size of the task and the network increases. We give some preliminary results on scaling and show how the magnitude of the optimal weight changes depends on the fan-in of the units. Additional results illustrate the effects on learning speed of the amount of interaction between the weights. A variation of the learning procedure that back-propagates desired state information rather than error gradients is developed and compared with the standard procedure. Finally, we discuss the relationship between our iterative networks and the analog networks described by Hopefield and Tank (Hopfield 85). The learning procedure can discover appropriate weights in their kind of network, as well as determine an optimal schedule for varying the nonlinearity of the units during a search."
            },
            "slug": "Experiments-on-Learning-by-Back-Propagation.-Plaut-Nowlan",
            "title": {
                "fragments": [],
                "text": "Experiments on Learning by Back Propagation."
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The learning procedure can discover appropriate weights in their kind of network, as well as determine an optimal schedule for varying the nonlinearity of the units during a search."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3140752"
                        ],
                        "name": "Charles R. Rosenberg",
                        "slug": "Charles-R.-Rosenberg",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Rosenberg",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles R. Rosenberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 87
                            }
                        ],
                        "text": "One way to overcome this length limitation is by \"sliding\" the input across the buffer [15, 16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12926318,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de996c32045df6f7b404dda2a753b6a9becf3c08",
            "isKey": false,
            "numCitedBy": 1885,
            "numCiting": 229,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes NETtalk, a class of massively-parallel network systems that learn to convert English text to speech. The memory representations for pronunciations are learned by practice and are shared among many processing units. The performance of NETtalk has some similarities with observed human performance. (i) The learning follows a power law. (ii) The more words the network learns, the better it is at generalizing and correctly pronouncing new words, (iii) The performance of the network degrades very slowly as connections in the network are damaged: no single link or processing unit is essential. (iv) Relearning after damage is much faster than learning during the original training. (v) Distributed or spaced practice is more effective for long-term retention than massed practice. Network models can be constructed that have the same performance and learning characteristics on a particular task, but differ completely at the levels of synaptic strengths and single-unit responses. However, hierarchical clustering techniques applied to NETtalk reveal that these different networks have similar internal representations of letter-to-sound correspondences within groups of processing units. This suggests that invariant internal representations may be found in assemblies of neurons intermediate in size between highly localized and completely distributed representations."
            },
            "slug": "Parallel-Networks-that-Learn-to-Pronounce-English-Sejnowski-Rosenberg",
            "title": {
                "fragments": [],
                "text": "Parallel Networks that Learn to Pronounce English Text"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "H hierarchical clustering techniques applied to NETtalk reveal that these different networks have similar internal representations of letter-to-sound correspondences within groups of processing units, which suggests that invariant internal representations may be found in assemblies of neurons intermediate in size between highly localized and completely distributed representations."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145903504"
                        ],
                        "name": "R. Rosenfeld",
                        "slug": "R.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rosenfeld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1516909860"
                        ],
                        "name": "D. Touretzky",
                        "slug": "D.-Touretzky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Touretzky",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Touretzky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16032366,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "33b5978384dd535b7fc18563e1ca9f8f77067658",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Coarse-coded symbol memories have appeare d in several neural net work symbol processing models. T hey are st atic memories that use overlap ping codes to store mu ltiple items simultaneo usly. In order to determine how t hese models would scale, one must first have some understanding of the mathematics of coarse-coded represe ntations . The general struct ure of coarse-coded symbo l memories is defined, and their strengths and weaknesses are discussed . Memory schemes can be characterized by their memory size, symbol-set size, and capacity. We derive mathematical relationships between these par ameters for various memory schemes, using both analysis and numerical method s. We find a simple linear relat ionship between the resources allocated to the syste m and the capacity t hey yield . The predicted capacity of one of the schemes is compar ed wit h actual measur ements of the coar secoded working memory of DCP S, Touret zky and Hinto n's dist ributed connectionist product ion system. Finally we provide a heurist ic algorithm for generating receptive fields which is efficient and pro duces good results in practice."
            },
            "slug": "Coarse-Coded-Symbol-Memories-and-Their-Properties-Rosenfeld-Touretzky",
            "title": {
                "fragments": [],
                "text": "Coarse-Coded Symbol Memories and Their Properties"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The predicted capacity of one of the schemes is compared with the actual measur ements of the coar secoded working memory of DCP S, Touret zky and Hinto n's dist ributed connectionist product ion system, and a simple linear relat ionship between the resources allocated to the syste m and the capacity is found."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1925215"
                        ],
                        "name": "W. Lehnert",
                        "slug": "W.-Lehnert",
                        "structuredName": {
                            "firstName": "Wendy",
                            "lastName": "Lehnert",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Lehnert"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 41
                            }
                        ],
                        "text": "1 Hybrid (connectionist-symbolic) models [6-9] have the potential for more powerful representations, but do not insist on the neural plausibility constraints which create the limitations in the first place."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 35858430,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "59ab7e538b055f4b8396edb74eee83c3319d6fe4",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent experiments indicate that a case-based approach to the problem of word pronunciation is effective as the basis for a system that learns to pronounce English words. More generally, the approach taken here illustrates how a case-based reasoner can access a large knowledge base containing hundreds of potentially relevant cases and consolidate these multiple knowledge sources using numerical relaxation over a structured network. In response to a test item, a search space is first generated and structured as a lateral inhibition network. Then a spreading activation algorithm is applied to this search space using activation levels derived from the case base. In this paper we describe the general design of our model and report preliminary test results based on a training vocabulary of 750 words. Our approach combines traditional heuristic methods for memory organization with connectionist-inspired techniques for network manipulation in an effort to exploit the best of both information-processing methodologies."
            },
            "slug": "Case-based-Problem-Solving-with-a-Large-Knowledge-Lehnert",
            "title": {
                "fragments": [],
                "text": "Case-based Problem Solving with a Large Knowledge Base of Learned Cases"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "The general design of the model is described and preliminary test results based on a training vocabulary of 750 words are reported, which show how a case-based reasoner can access a large knowledge base containing hundreds of potentially relevant cases and consolidate these multiple knowledge sources using numerical relaxation over a structured network."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145539951"
                        ],
                        "name": "J. Pollack",
                        "slug": "J.-Pollack",
                        "structuredName": {
                            "firstName": "Jordan",
                            "lastName": "Pollack",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pollack"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "1 Hybrid (connectionist-symbolic) models [ 6-9 ] have the potential for more powerful representations, but do not insist on the neural plausibility constraints which create the limitations in the first place."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 19580865,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9d650199b3d24cee292b04a6cc8062080462b9a",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : The knowledge needed to process natural language comes from many sources. While the knowledge itself may be broken up modularly, into knowledge of syntax, semantics, etc., the actual processing should be completely integrated. This form of processing is not easily amenable to the type of processing done by serial von Neumann computers. This work in progress is an investigation of the use of a highly parallel, spreading activation and lateral inhibition network as a mechanism for integrated natural language processing. (Author)"
            },
            "slug": "Natural-Language-Processing-Using-Spreading-and-Pollack",
            "title": {
                "fragments": [],
                "text": "Natural Language Processing Using Spreading Activation and Lateral Inhibition."
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "This work in progress is an investigation of the use of a highly parallel, spreading activation and lateral inhibition network as a mechanism for integrated natural language processing."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 54
                            }
                        ],
                        "text": "2 To point out this \"Banana Problem\" with Rumelhart & McClelland\u2019s actual representation, which was phonological rather than orthographic, Pinker and Prince [24] discovered words with enough internal duplication in the Oykangand language."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 63
                            }
                        ],
                        "text": "Similarly, in their past-tense model, Rumelhart and McClelland [23] developed an implicitly sequential representation, where a set of well-formed overlapping triples could be interpreted as a sequence."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 331,
                                "start": 321
                            }
                        ],
                        "text": "So a system, for example, which represented the spellings of words as sets of letter-pairs would not be able to represent the word yoyo, and even if the breadth were increased to three, the system would still not be able to represent words with duplicate triples such as banana.2\nAlthough both Touretzky\u2019s and Rumelhart & McClelland\u2019s coarse-coded represen-\ntations were fairly successful for their circumscribed tasks, there remain some problems:\n(1) A large amount of human effort was involved in the design, compression and tuning\nof these representations, and it is often not clear how to translate that effort across domains."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 38
                            }
                        ],
                        "text": "Similarly, in their past-tense model, Rumelhart and McClelland [23] developed an\nimplicitly sequential representation, where a set of well-formed overlapping triples could be interpreted as a sequence."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 47
                            }
                        ],
                        "text": "Comments from B. Chandrasekaran, G. Hinton, J. McClelland, D. Touretzky, T. VanGelder, and many, many others helped to improve this presentation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 163
                            }
                        ],
                        "text": "R. Williams, The Logic of Activation Functions, in Parallel Distributed Processing: Experiments in the Microstructure of Cognition, vol. 1, D. E. Rumelhart, J. L. McClelland and the PDP research Group (ed.)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 220
                            }
                        ],
                        "text": "D. E. Rumelhart, G. Hinton and R. Williams, Learning Internal Representations through Error Propagation, in Parallel Distributed Processing: Experiments in the Microstructure of Cognition, vol. 1, D. E. Rumelhart, J. L. McClelland and the PDP research Group (ed.)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 6
                            }
                        ],
                        "text": "J. L. McClelland and D. E. Rumelhart, An interactive activation model of the effect of context in perception: Part 1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 39
                            }
                        ],
                        "text": "Both early connectionist work, such as McClelland & Rumelhart\u2019s word recognition model [13], as well as more modern efforts [4, 14] use this approach, which is not able to represent or process sequences longer than a predetermined bound."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 26
                            }
                        ],
                        "text": "D. E. Rumelhart and J. L. McClelland, On Learning the Past Tenses of English Verbs, in Parallel Distributed Processing: Experiments in the Microstructure of Cognition, vol. 2, J. L. McClelland, D. E. Rumelhart and the PDP research Group (ed.)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 3
                            }
                        ],
                        "text": "J. McClelland and A. Kawamoto, Mechanisms of Sentence Processing: Assigning Roles to Constituents, in Parallel Distributed Processing: Experiments in the Microstructure of Cognition, vol. 2, J. L. McClelland, D. E. Rumelhart and the PDP research Group (ed.)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 56499839,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "4fa569625b5ab35e955a8d5be11a4aa9f59ca424",
            "isKey": true,
            "numCitedBy": 1468,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : This paper presents an alternative to the standard rule based account of a child's acquisition of the past tense in English. Children are typically said to pass through a three-phase acquisition process in which they first learn past tense by rote, then learn the past tense rule and over regularize, and then finally learn the exceptions to the rule. We show that the acquisition data can be accounted for in more detail by dispensing with the assumption that the child learns rules and substituting in its place a simple homogeneous learning procedure. We show how rule-like behavior can emerge from the interactions among a network of units encoding the root form to past tense mapping. A large computer simulation of the learning process demonstrates the operating principles of our alternative account, shows how details of the acquisition process not captured by the rule account emerge, and makes predictions about other details of the acquisition process not yet observed. Keywords: Learning; networks; Language; Verbs; Perceptions; Morphology."
            },
            "slug": "On-learning-the-past-tenses-of-English-verbs:-rules-Rumelhart-McClelland",
            "title": {
                "fragments": [],
                "text": "On learning the past-tenses of English verbs: implicit rules or parallel distributed processing"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown how rule-like behavior can emerge from the interactions among a network of units encoding the root form to past tense mapping, and how details of the acquisition process not captured by the rule account emerge."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 100
                            }
                        ],
                        "text": "To solve the problem of feature superposition, one might use full-size constituent\nbuffers, such as Agent, Action, and Object [5]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 126
                            }
                        ],
                        "text": "To solve the problem of feature superposition, one might use full-size constituent buffers, such as Agent, Action, and Object [5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 200
                            }
                        ],
                        "text": "The limitation shows in the fact that pure connectionism has generated somewhat unsatisfying systems in this domain; for example, parsers for fixed length sentences [1-4], without embedded structures [5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7675858,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "9928cac725ebe6db7b974bbd65738d33dc95332d",
            "isKey": false,
            "numCitedBy": 377,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : How do we assign nouns correctly to their underlying case roles in English, and how do we select an appropriate verb frame to assign these nouns to? How do we know whether a noun phrase is a modifier of a preceding noun phrase or an argument of the verb? How do we select the correct meaning of each noun in the sentence, and how do we allow content to modulate its meaning? How do we know how to handle these new nouns and verbs? In this article we describe a simulation model that addresses these questions from a perspective quite different from the conventional perspective found in Computational Linguistics. Words are treated as patterns of activation, and knowledge about them is stored in distributed form, in the connections in a large network of simple neutron-like processing units. The model exhibits considerable facility in dealing with the problems of frame selection, role assignment, disambiguation, etc, and suggests a natural way to resolve unappealing aspects of the idea that there is a fixed set of individuated case roles. So far, our simulation model can only process one clause sentences. Possible extensions to multi-clause sentences are described."
            },
            "slug": "Mechanisms-of-Sentence-Processing:-Assigning-Roles-McClelland-Rumelhart",
            "title": {
                "fragments": [],
                "text": "Mechanisms of Sentence Processing: Assigning Roles to Constituents of Sentences"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A simulation model is described that exhibits considerable facility in dealing with the problems of frame selection, role assignment, disambiguation, etc, and suggests a natural way to resolve unappealing aspects of the idea that there is a fixed set of individuated case roles."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144473519"
                        ],
                        "name": "M. Mozer",
                        "slug": "M.-Mozer",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Mozer",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mozer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 93
                            }
                        ],
                        "text": "(2) Coarse-coding requires expensive and complex access mechanisms, such as pullout networks [25] or clause-spaces [20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60663543,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14ad467038282291669442d962ae69deb64dd696",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Massively parallel, distributed models of computation offer a new approach to the representation and manipulation of knowledge. This paper reports on an application of parallel models to the area of information retrieval. The retrieval system described makes dynamic use of the internal structure of a database to infer relationships among items in the database. Using these relationships, the system can help overcome incompleteness and imprecision in requests for information, as well as in the database itself. (Author)"
            },
            "slug": "Inductive-Information-Retrieval-Using-Parallel-Mozer",
            "title": {
                "fragments": [],
                "text": "Inductive Information Retrieval Using Parallel Distributed Computation."
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The retrieval system described makes dynamic use of the internal structure of a database to infer relationships among items in the database, which can help overcome incompleteness and imprecision in requests for information, as well as in thedatabase itself."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38663378"
                        ],
                        "name": "J. Fodor",
                        "slug": "J.-Fodor",
                        "structuredName": {
                            "firstName": "Jerry",
                            "lastName": "Fodor",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Fodor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194015"
                        ],
                        "name": "Z. Pylyshyn",
                        "slug": "Z.-Pylyshyn",
                        "structuredName": {
                            "firstName": "Zenon",
                            "lastName": "Pylyshyn",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Pylyshyn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 29043627,
            "fieldsOfStudy": [
                "Philosophy",
                "Psychology"
            ],
            "id": "56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7",
            "isKey": false,
            "numCitedBy": 3540,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Connectionism-and-cognitive-architecture:-A-Fodor-Pylyshyn",
            "title": {
                "fragments": [],
                "text": "Connectionism and cognitive architecture: A critical analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Cognition"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "94653581"
                        ],
                        "name": "J. H. Holland",
                        "slug": "J.-H.-Holland",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Holland",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H. Holland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2009767"
                        ],
                        "name": "K. Holyoak",
                        "slug": "K.-Holyoak",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Holyoak",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Holyoak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2518186"
                        ],
                        "name": "R. Nisbett",
                        "slug": "R.-Nisbett",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Nisbett",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Nisbett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756123"
                        ],
                        "name": "P. Thagard",
                        "slug": "P.-Thagard",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Thagard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Thagard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743808"
                        ],
                        "name": "S. Smoliar",
                        "slug": "S.-Smoliar",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Smoliar",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Smoliar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1097618,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "fd8ce955dc0c570b66305dfbc65e4ed5f37658d0",
            "isKey": false,
            "numCitedBy": 2244,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Two psychologists, a computer scientist, and a philosopher have collaborated to present a framework for understanding processes of inductive reasoning and learning in organisms and machines. Theirs is the first major effort to bring the ideas of several disciplines to bear on a subject that has been a topic of investigation since the time of Socrates. The result is an integrated account that treats problem solving and induction in terms of rule-based mental models. Induction is included in the Computational Models of Cognition and Perception Series. A Bradford Book."
            },
            "slug": "Induction:-Processes-of-Inference,-Learning,-and-Holland-Holyoak",
            "title": {
                "fragments": [],
                "text": "Induction: Processes of Inference, Learning, and Discovery"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Induction is the first major effort to bring the ideas of several disciplines to bear on a subject that has been a topic of investigation since the time of Socrates and is included in the Computational Models of Cognition and Perception Series."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Expert"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143956638"
                        ],
                        "name": "R. Allen",
                        "slug": "R.-Allen",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Allen",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Allen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 124
                            }
                        ],
                        "text": "Both early connectionist work, such as McClelland & Rumelhart\u2019s word recognition model [13], as well as more modern efforts [4, 14] use this approach, which is not able to represent or process sequences longer than a predetermined bound."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59702970,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "70623eb30f076d6de599c4dff13b04bd62e29e0c",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent developments in neural algorithms provide a new approach to natural language processing. Two sets of brief studies show how networks may be developed for processing simple demonstratives and analogies. Two longer studies consider pronoun reference and natural language translation. Taken together, the studies provide additional support for the applicability of these algorithms to natural language processing."
            },
            "slug": "Several-Studies-on-Natural-Language-\u00b7and-Allen",
            "title": {
                "fragments": [],
                "text": "Several Studies on Natural Language \u00b7and Back-Propagation"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "Two sets of brief studies show how networks may be developed for processing simple demonstratives and analogies and provide additional support for the applicability of neural algorithms to natural language processing."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2020074"
                        ],
                        "name": "A. Lapedes",
                        "slug": "A.-Lapedes",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Lapedes",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lapedes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2542113"
                        ],
                        "name": "R. Farber",
                        "slug": "R.-Farber",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Farber",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Farber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18474528,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2d7de94252e5040a38ebaaf535841d3500791c79",
            "isKey": false,
            "numCitedBy": 373,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "There is presently great interest in the abilities of neural networks to mimic \"qualitative reasoning\" by manipulating neural incodings of symbols. Less work has been performed on using neural networks to process floating point numbers and it is sometimes stated that neural networks are somehow inherently inaccurate and therefore best suited for \"fuzzy\" qualitative reasoning. Nevertheless, the potential speed of massively parallel operations make neural net \"number crunching\" an interesting topic to explore. In this paper we discuss some of our work in which we demonstrate that for certain applications neural networks can achieve significantly higher numerical accuracy than more conventional techniques. In particular, prediction of future values of a chaotic time series can be performed with exceptionally high accuracy. We analyze how a neural net is able to do this, and in the process show that a large class of functions from Rn \u2192 Rm may be accurately approximated by a backpropagation neural net with just two \"hidden\" layers. The network uses this functional approximation to perform either interpolation (signal processing applications) or extrapolation (symbol processing applications). Neural nets therefore use quite familiar methods to perform their tasks. The geometrical viewpoint advocated here seems to be a useful approach to analyzing neural network operation and relates neural networks to well studied topics in functional approximation."
            },
            "slug": "How-Neural-Nets-Work-Lapedes-Farber",
            "title": {
                "fragments": [],
                "text": "How Neural Nets Work"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper demonstrates that for certain applications neural networks can achieve significantly higher numerical accuracy than more conventional techniques, and shows that prediction of future values of a chaotic time series can be performed with exceptionally high accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764952"
                        ],
                        "name": "K. Hornik",
                        "slug": "K.-Hornik",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Hornik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hornik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2964655"
                        ],
                        "name": "M. Stinchcombe",
                        "slug": "M.-Stinchcombe",
                        "structuredName": {
                            "firstName": "Maxwell",
                            "lastName": "Stinchcombe",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stinchcombe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2757547,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "f22f6972e66bdd2e769fa64b0df0a13063c0c101",
            "isKey": false,
            "numCitedBy": 17353,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multilayer-feedforward-networks-are-universal-Hornik-Stinchcombe",
            "title": {
                "fragments": [],
                "text": "Multilayer feedforward networks are universal approximators"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14632613"
                        ],
                        "name": "S. Krantz",
                        "slug": "S.-Krantz",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Krantz",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Krantz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 201
                            }
                        ],
                        "text": "In the limit, especially if there are any dense \"patches\" of 2-space which need to be covered, it can no longer be a 1-dimensional curve, but must become a space-filling curve with a fractal dimension [35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 189885729,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "3e7595d0eec4bb914753b69e18d15a51e1c7adad",
            "isKey": false,
            "numCitedBy": 1766,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Editor's note: The following articles by Steven G. Krantz and Benoit B. Mandelbrot have an unusual history. In the fall of 1988, Krantz asked the Bulletin of the American Mathematical Society Book Reviews editor, Edgar Lee Stout, whether he could review the books The Science of Fractal Images (edited by Heinz-Otto Peitgen and Dietmar Saupe) and The Beauty of Fractals (by Heinz-Otto Peitgen and Peter Richter) for the Bulletin. Subject to editorial approval, Stout agreed. Krantz submitted the review in mid-November. The editor requested a few changes, they were made, and the piece was accepted. Krantz received the galley proofs in mid-January of 1989. Meanwhile, Krantz circulated copies of the review to a number of people, including Mandelbrot, who took strong exception to the review and wrote a rebuttal. Stout encouraged Krantz to withdraw his review from the Bulletin and to publish it in a forum that accepted rebuttals. Krantz refused to withdraw his review, but he suggested that the Bulletin publish Mandelbrot's rebuttal along with the review. However, the policy of the American Mathematical Society (AMS) prohibits responses in the Bulletin to reviews. Stout then asked Krantz to make a number of revisions to soften the review. Krantz made the requested changes. After further thought, Stout decided that even the revised review (printed here) was not appropriate for the Bulletin, and he retracted his acceptance of the review. Krantz appealed the matter to the Council for the American Mathematical Society, which decided to support Stout\" s editorial prerogative. The AMS Council suggested that Krantz's review and Mandelbrot's response be published in the Notices of the AMS. Krantz felt that the Bulletin should not reject a previously accepted review. Because Krantz was dissatisfied with his treatment by the AMS, he did not agree to have his review printed in the Notices of the AMS. The Mathematical Intelligencer, which welcomes controversy, is happy to publish Krantz' s review and Mandelbrot' s response. A recent cocktail party conversation at my university was concerned with the quest ion of whether academics are more eccentric or more depressive than the average functioning adult. At one point a clinical psychologist joined in and asserted that the matter had been studied in detail and the answer is \"no.\" In fact, no profession seems to have more eccentric and depressive people than any other. The only exceptions, he went on to say, are mathematicians and oboe players. Apparently the property that mathematicians and oboe players have in common is that both do something that is quite difficult and which few others ap-"
            },
            "slug": "Fractal-geometry-Krantz",
            "title": {
                "fragments": [],
                "text": "Fractal geometry"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "The first is due to Hinton [42], who showed that, when properly constrained, a connectionist network can develop semantically interpretable representations on its hidden units."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 20
                            }
                        ],
                        "text": "The first is due\nto Hinton [42], who showed that, when properly constrained, a connectionist network can develop semantically interpretable representations on its hidden units."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 33
                            }
                        ],
                        "text": "D. C. Plaut, S. Nowlan and G. E. Hinton, Experiments on learning by back-propagation, CMU-CS86-126, Computer Science Dept., Carnegie Mellon University, Pittsburgh, 1986."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 3
                            }
                        ],
                        "text": "G. Hinton, Representing Part-Whole hierarchies in connectionist networks, Proceedings of the Tenth Annual Conference of the Cognitive Science Society, Montreal, 1988, 48-54."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 127
                            }
                        ],
                        "text": "Distributed Representations have been the focus of much research (including the\nwork reported herein) since the circulation of Hinton\u2019s 1984 report [17] discussing the properties of representations in which \"each entity is represented by a pattern of activity distributed over many computing elements, and each computed element is involved in representing many different entities.\""
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 87
                            }
                        ],
                        "text": "A solution to the feature-buffer dichotomy problem was anticipated and sketched out by Hinton [19], and involved having a \"reduced description\" for NURSE RIDING ELEPHANT which would fit into the constituent buffers along with patterns for JOHN and SAW."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 36
                            }
                        ],
                        "text": "Comments from B. Chandrasekaran, G. Hinton, J. McClelland, D. Touretzky, T. VanGelder, and many, many others helped to improve this presentation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 20
                            }
                        ],
                        "text": "D. E. Rumelhart, G. Hinton and R. Williams, Learning Internal Representations through Error Propagation, in Parallel Distributed Processing: Experiments in the Microstructure of Cognition, vol. 1, D. E. Rumelhart, J. L. McClelland and the PDP research Group (ed.)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 6
                            }
                        ],
                        "text": "G. E. Hinton, Learning Distributed Representations of Concepts, Proceedings of the Eighth Annual Conference of the Cognitive Science Society, Amherst, MA, 1986, 1-12."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 6
                            }
                        ],
                        "text": "G. E. Hinton, Distributed Representations, CMU-CS-84-157, Carnegie-Mellon University, Computer Science Department, Pittsburgh, PA, 1984."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 20
                            }
                        ],
                        "text": "D. H. Ackley, G. E. Hinton and T. J. Sejnowski, A learning algorithm for Boltzmann Machines, Cognitive Science 9, (1985), 147-169."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 149
                            }
                        ],
                        "text": "The systematic patterns developed by RAAM are a very new kind of representation,\na recursive, distributed representation, which seems to instantiate Hinton\u2019s notion of the \"reduced description\" mentioned earlier [19]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 26
                            }
                        ],
                        "text": "D. S. Touretzky and G. E. Hinton, Symbols among the neurons: details of a connectionist inference architecture, Proceedings of the Ninth International Joint Conference on Artificial Intelligence, Los Angeles, CA, 1985."
                    },
                    "intents": []
                }
            ],
            "corpusId": 53796860,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4ade4934db522fe6d634ff6f48887da46eedb4d1",
            "isKey": true,
            "numCitedBy": 902,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-distributed-representations-of-concepts.-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning distributed representations of concepts."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 20
                            }
                        ],
                        "text": "The first is due\nto Hinton [42], who showed that, when properly constrained, a connectionist network can develop semantically interpretable representations on its hidden units."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 33
                            }
                        ],
                        "text": "D. C. Plaut, S. Nowlan and G. E. Hinton, Experiments on learning by back-propagation, CMU-CS86-126, Computer Science Dept., Carnegie Mellon University, Pittsburgh, 1986."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 212
                            }
                        ],
                        "text": "The systematic patterns developed by RAAM are a very new kind of representation, a recursive, distributed representation, which seems to instantiate Hinton\u2019s notion of the \"reduced description\" mentioned earlier [19]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 3
                            }
                        ],
                        "text": "G. Hinton, Representing Part-Whole hierarchies in connectionist networks, Proceedings of the Tenth Annual Conference of the Cognitive Science Society, Montreal, 1988, 48-54."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 96
                            }
                        ],
                        "text": "\" A solution to the feature-buffer dichotomy problem was anticipated and sketched out by Hinton [19], and involved having a \"reduced description\" for NURSE RIDING ELEPHANT which would fit into the constituent buffers along with patterns for JOHN and SAW."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 127
                            }
                        ],
                        "text": "Distributed Representations have been the focus of much research (including the\nwork reported herein) since the circulation of Hinton\u2019s 1984 report [17] discussing the properties of representations in which \"each entity is represented by a pattern of activity distributed over many computing elements, and each computed element is involved in representing many different entities.\""
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 87
                            }
                        ],
                        "text": "A solution to the feature-buffer dichotomy problem was anticipated and sketched out by Hinton [19], and involved having a \"reduced description\" for NURSE RIDING ELEPHANT which would fit into the constituent buffers along with patterns for JOHN and SAW."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 36
                            }
                        ],
                        "text": "Comments from B. Chandrasekaran, G. Hinton, J. McClelland, D. Touretzky, T. VanGelder, and many, many others helped to improve this presentation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 20
                            }
                        ],
                        "text": "D. E. Rumelhart, G. Hinton and R. Williams, Learning Internal Representations through Error Propagation, in Parallel Distributed Processing: Experiments in the Microstructure of Cognition, vol. 1, D. E. Rumelhart, J. L. McClelland and the PDP research Group (ed.)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 6
                            }
                        ],
                        "text": "G. E. Hinton, Learning Distributed Representations of Concepts, Proceedings of the Eighth Annual Conference of the Cognitive Science Society, Amherst, MA, 1986, 1-12."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 6
                            }
                        ],
                        "text": "G. E. Hinton, Distributed Representations, CMU-CS-84-157, Carnegie-Mellon University, Computer Science Department, Pittsburgh, PA, 1984."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 20
                            }
                        ],
                        "text": "D. H. Ackley, G. E. Hinton and T. J. Sejnowski, A learning algorithm for Boltzmann Machines, Cognitive Science 9, (1985), 147-169."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 149
                            }
                        ],
                        "text": "The systematic patterns developed by RAAM are a very new kind of representation,\na recursive, distributed representation, which seems to instantiate Hinton\u2019s notion of the \"reduced description\" mentioned earlier [19]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 26
                            }
                        ],
                        "text": "D. S. Touretzky and G. E. Hinton, Symbols among the neurons: details of a connectionist inference architecture, Proceedings of the Ninth International Joint Conference on Artificial Intelligence, Los Angeles, CA, 1985."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Representing Part-Whole hierarchies in connectionist networks"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Tenth Annual Conference of the Cognitive Science Society,"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 87
                            }
                        ],
                        "text": "Both early connectionist work, such as McClelland & Rumelhart\u2019s word recognition model [13], as well as more modern efforts [4, 14] use this approach, which is not able to represent or process sequences longer than a predetermined bound."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 144594708,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "e6af68db1e6a291cbf6afb265f5fc2e82423b71b",
            "isKey": false,
            "numCitedBy": 4172,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-interactive-activation-model-of-context-effects-McClelland-Rumelhart",
            "title": {
                "fragments": [],
                "text": "An interactive activation model of context effects in letter perception: I. An account of basic findings."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62186699,
            "fieldsOfStudy": [],
            "id": "3302a19539ccfa8ed3a8361ace8947ddbba1acf5",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An introduction to computing with neural nets"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37684949"
                        ],
                        "name": "R. Williams",
                        "slug": "R.-Williams",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Williams",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Williams"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Sigma-Pi [ 41 ]. Basically, a cascaded network consists of two subnetworks: The function"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59904899,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "635002028da357f80e6e39dc04356990c3eaaf6d",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-logic-of-activation-functions-Williams",
            "title": {
                "fragments": [],
                "text": "The logic of activation functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2693903"
                        ],
                        "name": "S. Pinker",
                        "slug": "S.-Pinker",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Pinker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Pinker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48435467"
                        ],
                        "name": "Alan S. Prince",
                        "slug": "Alan-S.-Prince",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Prince",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alan S. Prince"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 17
                            }
                        ],
                        "text": "S. Pinker and A. Prince, On Language and Connectionism: Analysis of a parallel distributed processing model of language inquisition., Cognition 28, (1988), 73-193."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 157
                            }
                        ],
                        "text": "2 To point out this \"Banana Problem\" with Rumelhart & McClelland\u2019s actual representation, which was phonological rather than orthographic, Pinker and Prince [24] discovered words with enough internal duplication in the Oykangand language."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59701592,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5abcbd535d69cc7ec94ce184ad096ac3cc68e848",
            "isKey": false,
            "numCitedBy": 116,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-language-and-connectionism-Pinker-Prince",
            "title": {
                "fragments": [],
                "text": "On language and connectionism"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744679"
                        ],
                        "name": "B. Selman",
                        "slug": "B.-Selman",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Selman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Selman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 165
                            }
                        ],
                        "text": "The limitation shows in the fact that pure connectionism has generated somewhat unsatisfying systems in this domain; for example, parsers for fixed length sentences [1-4], without embedded structures [5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59667609,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d401b99e4cb7e972e55ab3742bb32567049c89c",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Rule-Based-Processing-in-a-Connectionist-System-for-Selman",
            "title": {
                "fragments": [],
                "text": "Rule-Based Processing in a Connectionist System for Natural Language Understanding"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47927070"
                        ],
                        "name": "A. H. Kawamoto",
                        "slug": "A.-H.-Kawamoto",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Kawamoto",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. H. Kawamoto"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 105
                            }
                        ],
                        "text": "A good example of a connectionist model using such a representation is Kawamoto\u2019s work on lexical access [18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 57884962,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3377f033d24f8cce555b6b21385caaa1ffe3c28a",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Dynamic-processes-in-the-(re)solution-of-lexical-Kawamoto",
            "title": {
                "fragments": [],
                "text": "Dynamic processes in the (re)solution of lexical ambiguity"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 142
                            }
                        ],
                        "text": "This induction, of global success arising from only local improvements, is similar to the Bucket Brigade principle used in classifier systems [32]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 236503748,
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and P"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 234,
                                "start": 230
                            }
                        ],
                        "text": "For example, Touretzky has developed a coarse-coded memory system and used it in a production system [20], a primitive lisp data-structuring system called BoltzCONS [21], and a combination of the two for simple tree manipulations [22]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Representing and transforming recursive objects in a neural network, or \u2018\u2018trees do grow on Boltzmann machines\u2019"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1986 Institute of Electrical and Electronics Engineers International Conference on Systems,"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A context - free connectionist parser which is not connectionist , but then it is not really context - free either ."
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning Internal Representations through Error Propagation, in Parallel Distributed Processing: Experiments in the Microstructure of Cognition"
            },
            "venue": {
                "fragments": [],
                "text": "Learning Internal Representations through Error Propagation, in Parallel Distributed Processing: Experiments in the Microstructure of Cognition"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 148
                            }
                        ],
                        "text": "Distributed Representations have been the focus of much research (including the work reported herein) since the circulation of Hinton\u2019s 1984 report [17] discussing the properties of representations in which \"each entity is represented by a pattern of activity distributed over many computing elements, and each computed element is involved in representing many different entities."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 79
                            }
                        ],
                        "text": "Instead, avant-garde connectionist representations were based on coarse-coding [17], which allows multiple semi-independent representational elements to be simultaneously present, by superposition, in a feature vector."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Distributed Representations, CMU-CS-84-157, Carnegie-Mellon University"
            },
            "venue": {
                "fragments": [],
                "text": "Computer Science Department,"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 100
                            }
                        ],
                        "text": "A feasibility study of this concept has been performed as well, using a sequential cascaded network [40], a higher-order network with a more restricted topology than Sigma-Pi [41]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cascaded Back Propagation on Dynamic Connectionist Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Ninth Conference of the Cognitive Science Society,"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 41
                            }
                        ],
                        "text": "1 Hybrid (connectionist-symbolic) models [6-9] have the potential for more powerful representations, but do not insist on the neural plausibility constraints which create the limitations in the first place."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Parallel Natural Language Processing Architecture with Distributed Control"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Ninth Annual Conference of the Cognitive Science Society,"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 17
                            }
                        ],
                        "text": "D. S. Touretzky, BoltzCONS: Reconciling connectionism with the recursive nature of stacks and trees, Proceedings of the 8th Annual Conference of the Cognitive Science Society, Amherst, MA, 1986, 522-530."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 155
                            }
                        ],
                        "text": "For example, Touretzky has developed a coarse-coded memory system and used it\nin a production system [20], a primitive lisp data-structuring system called BoltzCONS [21], and a combination of the two for simple tree manipulations [22]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 165
                            }
                        ],
                        "text": "For example, Touretzky has developed a coarse-coded memory system and used it in a production system [20], a primitive lisp data-structuring system called BoltzCONS [21], and a combination of the two for simple tree manipulations [22]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Reconciling connectionism with the recursive nature of stacks and trees"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 8th Annual Conference of the Cognitive Science Society,"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An introduction to computing with neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "An introduction to computing with neural networks"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 165
                            }
                        ],
                        "text": "The limitation shows in the fact that pure connectionism has generated somewhat unsatisfying systems in this domain; for example, parsers for fixed length sentences [1-4], without embedded structures [5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Connectionist Parsing"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Seventh Annual Conference of the Cognitive Science Society,"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 41
                            }
                        ],
                        "text": "1 Hybrid (connectionist-symbolic) models [6-9] have the potential for more powerful representations, but do not insist on the neural plausibility constraints which create the limitations in the first place."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and D"
            },
            "venue": {
                "fragments": [],
                "text": "L. Waltz, Natural Language Processing Using Spreading Activation and Lateral Inhibition, Proceedings of the Fourth Annual Cognitive Science Conference, Ann Arbor, MI,"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "PARSNIP: A connectionist network that learns natural language grammar from exposure to natural language sentences"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Ninth Conference of the Cognitive Science Society"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 141
                            }
                        ],
                        "text": "4 I also assume that the reader is, by now, familiar with this standard, as well as with the backpropagation technique for adjusting weights [12], and will not attempt a re-presentation of the mathematics."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 62
                            }
                        ],
                        "text": "Recursive Auto-Associative Memory (RAAM) uses backpropagation [12] on a non-stationary environment to devise patterns which stand for all of the internal nodes of fixed-valence trees."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[12] demonstrated only a 8-3-8 network, but other successful uses include a 64-16-64 network [28] and a 270-45-270 network [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning Internal Representations through Error Propagation, in Parallel Distributed Processing: Experiments in the Microstructure of Cognition, vol"
            },
            "venue": {
                "fragments": [],
                "text": "1, D. E. Rumelhart, J. L. McClelland and the PDP research Group (ed.), MIT Press, Cambridge,"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 237
                            }
                        ],
                        "text": "What about a system which would need to follow long chains of such implications? There has recently been some work showing that under certain conditions, feed-forward networks with hidden layers can compute arbitrary non-linear mappings [37-39]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and R"
            },
            "venue": {
                "fragments": [],
                "text": "M. Farber, How Neural Nets Work, LAUR-88-418, Los Alamos,"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A connectionist network that learns natural language grammar from exposure to natural language sentences"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and G"
            },
            "venue": {
                "fragments": [],
                "text": "E. Hinton, Experiments on learning by back-propagation, CMU-CS86-126, Computer Science Dept., Carnegie Mellon University, Pittsburgh,"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 175
                            }
                        ],
                        "text": "A feasibility study of this concept has been performed as well, using a sequential cascaded network [40], a higher-order network with a more restricted topology than Sigma-Pi [41]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 166
                            }
                        ],
                        "text": "A feasibility study of this concept has been performed as well, using a sequential\ncascaded network [40], a higher-order network with a more restricted topology than Sigma-Pi [41]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Logic of Activation Functions, in Parallel Distributed Processing: Experiments in the Microstructure of Cognition, vol"
            },
            "venue": {
                "fragments": [],
                "text": "1, D. E. Rumelhart, J. L. McClelland and the PDP research Group (ed.), MIT Press, Cambridge,"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 87
                            }
                        ],
                        "text": "One way to overcome this length limitation is by \"sliding\" the input across the buffer [15, 16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and E"
            },
            "venue": {
                "fragments": [],
                "text": "Santos, A context-free connectionist parser which is not connectionist, but then it is not really context-free either., in Advances in Connectionist & Neural Computation Theory, J. Barnden and J. Pollack (ed.), Ablex, Norwood, NJ,"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 115
                            }
                        ],
                        "text": "(2) Coarse-coding requires expensive and complex access mechanisms, such as pullout networks [25] or clause-spaces [20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 101
                            }
                        ],
                        "text": "For example, Touretzky has developed a coarse-coded memory system and used it in a production system [20], a primitive lisp data-structuring system called BoltzCONS [21], and a combination of the two for simple tree manipulations [22]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and G"
            },
            "venue": {
                "fragments": [],
                "text": "E. Hinton, Symbols among the neurons: details of a connectionist inference architecture, Proceedings of the Ninth International Joint Conference on Artificial Intelligence, Los Angeles, CA,"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning Internal Representations from GrayScales Images : An Example of Extensional Programming"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Seventh Annual Conference of the Cognitive Science Society , Seattle"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On Learning the Past Tenses of English Verbs, in Parallel Distributed Processing: Experiments in the Microstructure of Cognition"
            },
            "venue": {
                "fragments": [],
                "text": "On Learning the Past Tenses of English Verbs, in Parallel Distributed Processing: Experiments in the Microstructure of Cognition"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "BoltzCONS: Reconciling connectionism with the recursive nature of stacks and trees"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 8th Annual Conference of the Cognitive Science Society"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Distributed Representations, CMU-CS-84-157"
            },
            "venue": {
                "fragments": [],
                "text": "Distributed Representations, CMU-CS-84-157"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 111
                            }
                        ],
                        "text": "The notions of associative and reconstructive memories with fractal dimensions are further discussed elsewhere [36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Implications of Recursive Distributed Representations, in Advances in Neural Information Processing Systems, D"
            },
            "venue": {
                "fragments": [],
                "text": "Touretzky (ed.), Morgan Kaufman, Los Gatos, CA,"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 93
                            }
                        ],
                        "text": "[12] demonstrated only a 8-3-8 network, but other successful uses include a 64-16-64 network [28] and a 270-45-270 network [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and D"
            },
            "venue": {
                "fragments": [],
                "text": "Zipser, Learning Internal Representations from Gray-Scales Images: An Example of Extensional Programming., Proceedings of the Seventh Annual Conference of the Cognitive Science Society, Seattle,"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 93
                            }
                        ],
                        "text": "This form of non-stationary, or \u2018\u2018Moving Target,\u2019\u2019 learning has also been explored by others [29, 30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Forming Global Representations with Back Propagation, Proceedings of the Institute of Electrical and Electronics Engineers Second Annual International  32  J"
            },
            "venue": {
                "fragments": [],
                "text": "B. Pollack Conference on Neural Networks, San Diego,"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning Internal Representations from Gray-Scales Images: An Example of Extensional Programming"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Seventh Annual Conference of the Cognitive Science Society"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "Saund [34] has investigated (non-recursive) auto-association as a method of dimensionality reduction, and asserted that, in order to work, the map must be constrained to form a small dimensional parametric surface in the larger dimensional space."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dimensionality Reduction and Constraint in Later Vision"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Ninth Annual Conference of the Cognitive Science Society,"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 237
                            }
                        ],
                        "text": "What about a system which would need to follow long chains of such implications? There has recently been some work showing that under certain conditions, feed-forward networks with hidden layers can compute arbitrary non-linear mappings [37-39]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An introduction to computing with neural networks, Institute of Electrical and Electronics Engineers"
            },
            "venue": {
                "fragments": [],
                "text": "ASSP Magazine April,"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mechanisms of Sentence Processing: Assigning Roles to Constituents, in Parallel Distributed Processing: Experiments in the Microstructure of Cognition"
            },
            "venue": {
                "fragments": [],
                "text": "Mechanisms of Sentence Processing: Assigning Roles to Constituents, in Parallel Distributed Processing: Experiments in the Microstructure of Cognition"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pollack , Cascaded Back Propagation on Dynamic Connectionist Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Ninth Conference of the Cognitive Science Society , Seattle"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Forming Global Representations with Back Propagation"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Institute of Electrical and Electronics Engineers Second Annual International J. B. Pollack Conference on Neural Networks"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning Internal Representations through Error Propagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 32,
            "methodology": 11
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 63,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/Recursive-Distributed-Representations-Pollack/6a835df43fdc2f79126319f6fa033bb42147c6f6?sort=total-citations"
}