{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2032184078"
                        ],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10328909,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "424561d8585ff8ebce7d5d07de8dbf7aae5e7270",
            "isKey": false,
            "numCitedBy": 32559,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available"
            },
            "slug": "Faster-R-CNN:-Towards-Real-Time-Object-Detection-Ren-He",
            "title": {
                "fragments": [],
                "text": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work introduces a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals and further merge RPN and Fast R-CNN into a single network by sharing their convolutionAL features."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47033130"
                        ],
                        "name": "Lichao Huang",
                        "slug": "Lichao-Huang",
                        "structuredName": {
                            "firstName": "Lichao",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lichao Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2143686789"
                        ],
                        "name": "Yi Yang",
                        "slug": "Yi-Yang",
                        "structuredName": {
                            "firstName": "Yi",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30639994"
                        ],
                        "name": "Yafeng Deng",
                        "slug": "Yafeng-Deng",
                        "structuredName": {
                            "firstName": "Yafeng",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yafeng Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2119041121"
                        ],
                        "name": "Yinan Yu",
                        "slug": "Yinan-Yu",
                        "structuredName": {
                            "firstName": "Yinan",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yinan Yu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14710847,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aeb86c216e32b39c8716fe5a832aa7b135164a87",
            "isKey": false,
            "numCitedBy": 308,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "How can a single fully convolutional neural network (FCN) perform on object detection? We introduce DenseBox, a unified end-to-end FCN framework that directly predicts bounding boxes and object class confidences through all locations and scales of an image. Our contribution is two-fold. First, we show that a single FCN, if designed and optimized carefully, can detect multiple different objects extremely accurately and efficiently. Second, we show that when incorporating with landmark localization during multi-task learning, DenseBox further improves object detection accuray. We present experimental results on public benchmark datasets including MALF face detection and KITTI car detection, that indicate our DenseBox is the state-of-the-art system for detecting challenging objects such as faces and cars."
            },
            "slug": "DenseBox:-Unifying-Landmark-Localization-with-End-Huang-Yang",
            "title": {
                "fragments": [],
                "text": "DenseBox: Unifying Landmark Localization with End to End Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "DenseBox is introduced, a unified end-to-end FCN framework that directly predicts bounding boxes and object class confidences through all locations and scales of an image and shows that when incorporating with landmark localization during multi-task learning, DenseBox further improves object detection accuray."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7408951"
                        ],
                        "name": "Jeff Donahue",
                        "slug": "Jeff-Donahue",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Donahue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Donahue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652147"
                        ],
                        "name": "J. Malik",
                        "slug": "J.-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 58
                            }
                        ],
                        "text": "Currently, most of the CNN-based object detection methods [2] [4] [8] could be summarized as a three-step pipeline: firstly, region proposals are extracted as object candidates from a given image."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 315,
                                "start": 312
                            }
                        ],
                        "text": "While the deep convolutional neural networks (CNNs) has witnessed major breakthroughs in visual object recognition [3] [11] [13], the CNN-based object detectors have also achieved the state-of-the-arts results on a wide range of applications, such as face detection [8] [5], pedestrian detection [9] [4] and etc [2] [1] [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 215827080,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f4df08d9072fc2ac181b7fced6a245315ce05c8",
            "isKey": false,
            "numCitedBy": 17087,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn."
            },
            "slug": "Rich-Feature-Hierarchies-for-Accurate-Object-and-Girshick-Donahue",
            "title": {
                "fragments": [],
                "text": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This paper proposes a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3131569"
                        ],
                        "name": "Haoxiang Li",
                        "slug": "Haoxiang-Li",
                        "structuredName": {
                            "firstName": "Haoxiang",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haoxiang Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145527707"
                        ],
                        "name": "Zhe L. Lin",
                        "slug": "Zhe-L.-Lin",
                        "structuredName": {
                            "firstName": "Zhe",
                            "lastName": "Lin",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhe L. Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720987"
                        ],
                        "name": "Xiaohui Shen",
                        "slug": "Xiaohui-Shen",
                        "structuredName": {
                            "firstName": "Xiaohui",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaohui Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145561604"
                        ],
                        "name": "Jonathan Brandt",
                        "slug": "Jonathan-Brandt",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Brandt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Brandt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144988571"
                        ],
                        "name": "G. Hua",
                        "slug": "G.-Hua",
                        "structuredName": {
                            "firstName": "Gang",
                            "lastName": "Hua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Hua"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 66
                            }
                        ],
                        "text": "Currently, most of the CNN-based object detection methods [2] [4] [8] could be summarized as a three-step pipeline: firstly, region proposals are extracted as object candidates from a given image."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 269,
                                "start": 266
                            }
                        ],
                        "text": "While the deep convolutional neural networks (CNNs) has witnessed major breakthroughs in visual object recognition [3] [11] [13], the CNN-based object detectors have also achieved the state-of-the-arts results on a wide range of applications, such as face detection [8] [5], pedestrian detection [9] [4] and etc [2] [1] [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 124
                            }
                        ],
                        "text": "The popular region proposal methods include Selective Search [12], EdgeBoxes [15], or the early stages of cascade detectors [8]; secondly, the extracted proposals are fed into a deep CNN for recognition and categorization; finally, the bounding box regression technique is employed to refine the coarse proposals into more accurate object bounds."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14252649,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e61eb4a5c6fe6c0fdb36cfb84d460ee1524099f",
            "isKey": false,
            "numCitedBy": 1046,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "In real-world face detection, large visual variations, such as those due to pose, expression, and lighting, demand an advanced discriminative model to accurately differentiate faces from the backgrounds. Consequently, effective models for the problem tend to be computationally prohibitive. To address these two conflicting challenges, we propose a cascade architecture built on convolutional neural networks (CNNs) with very powerful discriminative capability, while maintaining high performance. The proposed CNN cascade operates at multiple resolutions, quickly rejects the background regions in the fast low resolution stages, and carefully evaluates a small number of challenging candidates in the last high resolution stage. To improve localization effectiveness, and reduce the number of candidates at later stages, we introduce a CNN-based calibration stage after each of the detection stages in the cascade. The output of each calibration stage is used to adjust the detection window position for input to the subsequent stage. The proposed method runs at 14 FPS on a single CPU core for VGA-resolution images and 100 FPS using a GPU, and achieves state-of-the-art detection performance on two public face detection benchmarks."
            },
            "slug": "A-convolutional-neural-network-cascade-for-face-Li-Lin",
            "title": {
                "fragments": [],
                "text": "A convolutional neural network cascade for face detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work proposes a cascade architecture built on convolutional neural networks (CNNs) with very powerful discriminative capability, while maintaining high performance, and introduces a CNN-based calibration stage after each of the detection stages in the cascade."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2750647"
                        ],
                        "name": "Zequn Jie",
                        "slug": "Zequn-Jie",
                        "structuredName": {
                            "firstName": "Zequn",
                            "lastName": "Jie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zequn Jie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40250403"
                        ],
                        "name": "Xiaodan Liang",
                        "slug": "Xiaodan-Liang",
                        "structuredName": {
                            "firstName": "Xiaodan",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodan Liang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33221685"
                        ],
                        "name": "Jiashi Feng",
                        "slug": "Jiashi-Feng",
                        "structuredName": {
                            "firstName": "Jiashi",
                            "lastName": "Feng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiashi Feng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145069283"
                        ],
                        "name": "W. Lu",
                        "slug": "W.-Lu",
                        "structuredName": {
                            "firstName": "Wen",
                            "lastName": "Lu",
                            "middleNames": [
                                "Feng"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064431634"
                        ],
                        "name": "Francis Eng Hock Tay",
                        "slug": "Francis-Eng-Hock-Tay",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Tay",
                            "middleNames": [
                                "Eng",
                                "Hock"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Francis Eng Hock Tay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143653681"
                        ],
                        "name": "Shuicheng Yan",
                        "slug": "Shuicheng-Yan",
                        "structuredName": {
                            "firstName": "Shuicheng",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuicheng Yan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 7
                            }
                        ],
                        "text": "In [5] [7], `2 loss is also employed to regress the object bounding box via CNNs, which could be defined as:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12065717,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0ea10588f150867599e024a154abd8c2b35bb975",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Object proposal is essential for current state-of-the-art object detection pipelines. However, the existing proposal methods generally fail in producing results with satisfying localization accuracy. The case is even worse for small objects, which, however, are quite common in practice. In this paper, we propose a novel scale-aware pixelwise object proposal network (SPOP-net) to tackle the challenges. The SPOP-net can generate proposals with high recall rate and average best overlap, even for small objects. In particular, in order to improve the localization accuracy, a fully convolutional network is employed which predicts locations of object proposals for each pixel. The produced ensemble of pixelwise object proposals enhances the chance of hitting the object significantly without incurring heavy extra computational cost. To solve the challenge of localizing objects at small scale, two localization networks, which are specialized for localizing objects with different scales are introduced, following the divide-and-conquer philosophy. Location outputs of these two networks are then adaptively combined to generate the final proposals by a large-/small-size weighting network. Extensive evaluations on PASCAL VOC 2007 and COCO 2014 show the SPOP network is superior over the state-of-the-art models. The high-quality proposals from SPOP-net also significantly improve the mean average precision of object detection with Fast-Regions with CNN features framework. Finally, the SPOP-net (trained on PASCAL VOC) shows great generalization performance when testing it on ILSVRC 2013 validation set."
            },
            "slug": "Scale-Aware-Pixelwise-Object-Proposal-Networks-Jie-Liang",
            "title": {
                "fragments": [],
                "text": "Scale-Aware Pixelwise Object Proposal Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel scale-aware pixelwise object proposal network (SPOP-net) to tackle the challenges of localizing objects at small scale and significantly improve the mean average precision of object detection with Fast-Regions with CNN features framework."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34838386"
                        ],
                        "name": "K. Simonyan",
                        "slug": "K.-Simonyan",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Simonyan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Simonyan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 84
                            }
                        ],
                        "text": "As illustrated in Figure 2, the architecture of UnitBox is derived from VGG16 model [11], in which we remove the fully connected layers and add two branches of fully convolutional layers to predict the pixel-wise bounding boxes and classification scores, respectively."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 119
                            }
                        ],
                        "text": "While the deep convolutional neural networks (CNNs) has witnessed major breakthroughs in visual object recognition [3] [11] [13], the CNN-based object detectors have also achieved the state-of-the-arts results on a wide range of applications, such as face detection [8] [5], pedestrian detection [9] [4] and etc [2] [1] [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14124313,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb42cf88027de515750f230b23b1a057dc782108",
            "isKey": false,
            "numCitedBy": 62216,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision."
            },
            "slug": "Very-Deep-Convolutional-Networks-for-Large-Scale-Simonyan-Zisserman",
            "title": {
                "fragments": [],
                "text": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This work investigates the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting using an architecture with very small convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2969311"
                        ],
                        "name": "Zhangyang Wang",
                        "slug": "Zhangyang-Wang",
                        "structuredName": {
                            "firstName": "Zhangyang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhangyang Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3307026"
                        ],
                        "name": "Shiyu Chang",
                        "slug": "Shiyu-Chang",
                        "structuredName": {
                            "firstName": "Shiyu",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shiyu Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2680237"
                        ],
                        "name": "Yingzhen Yang",
                        "slug": "Yingzhen-Yang",
                        "structuredName": {
                            "firstName": "Yingzhen",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yingzhen Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771885"
                        ],
                        "name": "Ding Liu",
                        "slug": "Ding-Liu",
                        "structuredName": {
                            "firstName": "Ding",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ding Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652752"
                        ],
                        "name": "Thomas S. Huang",
                        "slug": "Thomas-S.-Huang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Huang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas S. Huang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9688096,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "288d2704205d9ca68660b9f3a8fda17e18329c13",
            "isKey": false,
            "numCitedBy": 187,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Visual recognition research often assumes a sufficient resolution of the region of interest (ROI). That is usually violated in practice, inspiring us to explore the Very Low Resolution Recognition (VLRR) problem. Typically, the ROI in a VLRR problem can be smaller than 16 16 pixels, and is challenging to be recognized even by human experts. We attempt to solve the VLRR problem using deep learning methods. Taking advantage of techniques primarily in super resolution, domain adaptation and robust regression, we formulate a dedicated deep learning method and demonstrate how these techniques are incorporated step by step. Any extra complexity, when introduced, is fully justified by both analysis and simulation results. The resulting Robust Partially Coupled Networks achieves feature enhancement and recognition simultaneously. It allows for both the flexibility to combat the LR-HR domain mismatch, and the robustness to outliers. Finally, the effectiveness of the proposed models is evaluated on three different VLRR tasks, including face identification, digit recognition and font recognition, all of which obtain very impressive performances."
            },
            "slug": "Studying-Very-Low-Resolution-Recognition-Using-Deep-Wang-Chang",
            "title": {
                "fragments": [],
                "text": "Studying Very Low Resolution Recognition Using Deep Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work forms a dedicated deep learning method that allows for both the flexibility to combat the LR-HR domain mismatch, and the robustness to outliers, and achieves feature enhancement and recognition simultaneously."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40250403"
                        ],
                        "name": "Xiaodan Liang",
                        "slug": "Xiaodan-Liang",
                        "structuredName": {
                            "firstName": "Xiaodan",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodan Liang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3493398"
                        ],
                        "name": "Shengmei Shen",
                        "slug": "Shengmei-Shen",
                        "structuredName": {
                            "firstName": "Shengmei",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shengmei Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39001620"
                        ],
                        "name": "Tingfa Xu",
                        "slug": "Tingfa-Xu",
                        "structuredName": {
                            "firstName": "Tingfa",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tingfa Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33221685"
                        ],
                        "name": "Jiashi Feng",
                        "slug": "Jiashi-Feng",
                        "structuredName": {
                            "firstName": "Jiashi",
                            "lastName": "Feng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiashi Feng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143653681"
                        ],
                        "name": "Shuicheng Yan",
                        "slug": "Shuicheng-Yan",
                        "structuredName": {
                            "firstName": "Shuicheng",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuicheng Yan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 299,
                                "start": 296
                            }
                        ],
                        "text": "While the deep convolutional neural networks (CNNs) has witnessed major breakthroughs in visual object recognition [3] [11] [13], the CNN-based object detectors have also achieved the state-of-the-arts results on a wide range of applications, such as face detection [8] [5], pedestrian detection [9] [4] and etc [2] [1] [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3894476,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d65d315fb12b71a06662498b3b92033763fee23c",
            "isKey": false,
            "numCitedBy": 543,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we consider the problem of pedestrian detection in natural scenes. Intuitively, instances of pedestrians with different spatial scales may exhibit dramatically different features. Thus, large variance in instance scales, which results in undesirable large intracategory variance in features, may severely hurt the performance of modern object instance detection methods. We argue that this issue can be substantially alleviated by the divide-and-conquer philosophy. Taking pedestrian detection as an example, we illustrate how we can leverage this philosophy to develop a Scale-Aware Fast R-CNN (SAF R-CNN) framework. The model introduces multiple built-in subnetworks which detect pedestrians with scales from disjoint ranges. Outputs from all of the subnetworks are then adaptively combined to generate the final detection results that are shown to be robust to large variance in instance scales, via a gate function defined over the sizes of object proposals. Extensive evaluations on several challenging pedestrian detection datasets well demonstrate the effectiveness of the proposed SAF R-CNN. Particularly, our method achieves state-of-the-art performance on Caltech [P. Dollar, C. Wojek, B. Schiele, and P. Perona, \u201cPedestrian detection: An evaluation of the state of the art,\u201d IEEE Trans. Pattern Anal. Mach. Intell., vol. 34, no. 4, pp. 743\u2013761, Apr. 2012], and obtains competitive results on INRIA [N. Dalal and B. Triggs, \u201cHistograms of oriented gradients for human detection,\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2005, pp. 886\u2013893], ETH [A. Ess, B. Leibe, and L. V. Gool, \u201cDepth and appearance for mobile scene analysis,\u201d in Proc. Int. Conf. Comput. Vis., 2007, pp. 1\u20138], and KITTI [A. Geiger, P. Lenz, and R. Urtasun, \u201cAre we ready for autonomous driving? The KITTI vision benchmark suite,\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2012, pp. 3354\u20133361]."
            },
            "slug": "Scale-Aware-Fast-R-CNN-for-Pedestrian-Detection-Liang-Shen",
            "title": {
                "fragments": [],
                "text": "Scale-Aware Fast R-CNN for Pedestrian Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper argues that the issue of large variance in instance scales, which results in undesirable large intracategory variance in features, may severely hurt the performance of modern object instance detection methods, can be substantially alleviated by the divide-and-conquer philosophy."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Multimedia"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "92887925"
                        ],
                        "name": "Shuo Yang",
                        "slug": "Shuo-Yang",
                        "structuredName": {
                            "firstName": "Shuo",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuo Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47571885"
                        ],
                        "name": "Ping Luo",
                        "slug": "Ping-Luo",
                        "structuredName": {
                            "firstName": "Ping",
                            "lastName": "Luo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ping Luo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717179"
                        ],
                        "name": "Chen Change Loy",
                        "slug": "Chen-Change-Loy",
                        "structuredName": {
                            "firstName": "Chen",
                            "lastName": "Loy",
                            "middleNames": [
                                "Change"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chen Change Loy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50295995"
                        ],
                        "name": "Xiaoou Tang",
                        "slug": "Xiaoou-Tang",
                        "structuredName": {
                            "firstName": "Xiaoou",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoou Tang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 141
                            }
                        ],
                        "text": "The weights of UnitBox are initialized from a VGG-16 model pre-trained on ImageNet, and then fine-tuned on the public face dataset WiderFace [14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12090268,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "52d7eb0fbc3522434c13cc247549f74bb9609c5d",
            "isKey": false,
            "numCitedBy": 935,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Face detection is one of the most studied topics in the computer vision community. Much of the progresses have been made by the availability of face detection benchmark datasets. We show that there is a gap between current face detection performance and the real world requirements. To facilitate future face detection research, we introduce the WIDER FACE dataset1, which is 10 times larger than existing datasets. The dataset contains rich annotations, including occlusions, poses, event categories, and face bounding boxes. Faces in the proposed dataset are extremely challenging due to large variations in scale, pose and occlusion, as shown in Fig. 1. Furthermore, we show that WIDER FACE dataset is an effective training source for face detection. We benchmark several representative detection systems, providing an overview of state-of-the-art performance and propose a solution to deal with large scale variation. Finally, we discuss common failure cases that worth to be further investigated."
            },
            "slug": "WIDER-FACE:-A-Face-Detection-Benchmark-Yang-Luo",
            "title": {
                "fragments": [],
                "text": "WIDER FACE: A Face Detection Benchmark"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "There is a gap between current face detection performance and the real world requirements, and the WIDER FACE dataset, which is 10 times larger than existing datasets is introduced, which contains rich annotations, including occlusions, poses, event categories, and face bounding boxes."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771551"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 115
                            }
                        ],
                        "text": "While the deep convolutional neural networks (CNNs) has witnessed major breakthroughs in visual object recognition [3] [11] [13], the CNN-based object detectors have also achieved the state-of-the-arts results on a wide range of applications, such as face detection [8] [5], pedestrian detection [9] [4] and etc [2] [1] [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206594692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "isKey": false,
            "numCitedBy": 95302,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."
            },
            "slug": "Deep-Residual-Learning-for-Image-Recognition-He-Zhang",
            "title": {
                "fragments": [],
                "text": "Deep Residual Learning for Image Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This work presents a residual learning framework to ease the training of networks that are substantially deeper than those used previously, and provides comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2536361"
                        ],
                        "name": "J. Hosang",
                        "slug": "J.-Hosang",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Hosang",
                            "middleNames": [
                                "Hendrik"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hosang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187309"
                        ],
                        "name": "Mohamed Omran",
                        "slug": "Mohamed-Omran",
                        "structuredName": {
                            "firstName": "Mohamed",
                            "lastName": "Omran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohamed Omran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798000"
                        ],
                        "name": "Rodrigo Benenson",
                        "slug": "Rodrigo-Benenson",
                        "structuredName": {
                            "firstName": "Rodrigo",
                            "lastName": "Benenson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rodrigo Benenson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 62
                            }
                        ],
                        "text": "Currently, most of the CNN-based object detection methods [2] [4] [8] could be summarized as a three-step pipeline: firstly, region proposals are extracted as object candidates from a given image."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 303,
                                "start": 300
                            }
                        ],
                        "text": "While the deep convolutional neural networks (CNNs) has witnessed major breakthroughs in visual object recognition [3] [11] [13], the CNN-based object detectors have also achieved the state-of-the-arts results on a wide range of applications, such as face detection [8] [5], pedestrian detection [9] [4] and etc [2] [1] [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11581211,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3054aa4f655c279cc79223970f80b11c67231b0b",
            "isKey": false,
            "numCitedBy": 321,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we study the use of convolutional neural networks (convnets) for the task of pedestrian detection. Despite their recent diverse successes, convnets historically underperform compared to other pedestrian detectors. We deliberately omit explicitly modelling the problem into the network (e.g. parts or occlusion modelling) and show that we can reach competitive performance without bells and whistles. In a wide range of experiments we analyse small and big convnets, their architectural choices, parameters, and the influence of different training data, including pretraining on surrogate tasks. We present the best convnet detectors on the Caltech and KITTI dataset. On Caltech our convnets reach top performance both for the Caltech1x and Caltech10x training setup. Using additional data at training time our strongest convnet model is competitive even to detectors that use additional data (optical flow) at test time."
            },
            "slug": "Taking-a-deeper-look-at-pedestrians-Hosang-Omran",
            "title": {
                "fragments": [],
                "text": "Taking a deeper look at pedestrians"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper analyses small and big convnets, their architectural choices, parameters, and the influence of different training data, including pretraining on surrogate tasks, and presents the best convnet detectors on the Caltech and KITTI dataset."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699161"
                        ],
                        "name": "C. L. Zitnick",
                        "slug": "C.-L.-Zitnick",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Zitnick",
                            "middleNames": [
                                "Lawrence"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Zitnick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 124
                            }
                        ],
                        "text": "On the other hand, a majority of those methods are typically based on image over-segmentation [12] or dense sliding windows [15], which are computationally expensive and have hamper their deployments in the real-time detection systems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "The popular region proposal methods include Selective Search [12], EdgeBoxes [15], or the early stages of cascade detectors [8]; secondly, the extracted proposals are fed into a deep CNN for recognition and categorization; finally, the bounding box regression technique is employed to refine the coarse proposals into more accurate object bounds."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5984060,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b183947ee15718b45546eda6b01e179b9a95421f",
            "isKey": false,
            "numCitedBy": 2411,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of object proposals is an effective recent approach for increasing the computational efficiency of object detection. We propose a novel method for generating object bounding box proposals using edges. Edges provide a sparse yet informative representation of an image. Our main observation is that the number of contours that are wholly contained in a bounding box is indicative of the likelihood of the box containing an object. We propose a simple box objectness score that measures the number of edges that exist in the box minus those that are members of contours that overlap the box\u2019s boundary. Using efficient data structures, millions of candidate boxes can be evaluated in a fraction of a second, returning a ranked set of a few thousand top-scoring proposals. Using standard metrics, we show results that are significantly more accurate than the current state-of-the-art while being faster to compute. In particular, given just 1000 proposals we achieve over 96% object recall at overlap threshold of 0.5 and over 75% recall at the more challenging overlap of 0.7. Our approach runs in 0.25 seconds and we additionally demonstrate a near real-time variant with only minor loss in accuracy."
            },
            "slug": "Edge-Boxes:-Locating-Object-Proposals-from-Edges-Zitnick-Doll\u00e1r",
            "title": {
                "fragments": [],
                "text": "Edge Boxes: Locating Object Proposals from Edges"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel method for generating object bounding box proposals using edges is proposed, showing results that are significantly more accurate than the current state-of-the-art while being faster to compute."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1882784"
                        ],
                        "name": "Vasileios Belagiannis",
                        "slug": "Vasileios-Belagiannis",
                        "structuredName": {
                            "firstName": "Vasileios",
                            "lastName": "Belagiannis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vasileios Belagiannis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48631088"
                        ],
                        "name": "Xinchao Wang",
                        "slug": "Xinchao-Wang",
                        "structuredName": {
                            "firstName": "Xinchao",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinchao Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166314"
                        ],
                        "name": "Horesh Ben Shitrit",
                        "slug": "Horesh-Ben-Shitrit",
                        "structuredName": {
                            "firstName": "Horesh",
                            "lastName": "Shitrit",
                            "middleNames": [
                                "Ben"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Horesh Ben Shitrit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113434990"
                        ],
                        "name": "K. Hashimoto",
                        "slug": "K.-Hashimoto",
                        "structuredName": {
                            "firstName": "Kiyoshi",
                            "lastName": "Hashimoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hashimoto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40487171"
                        ],
                        "name": "R. Stauder",
                        "slug": "R.-Stauder",
                        "structuredName": {
                            "firstName": "Ralf",
                            "lastName": "Stauder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Stauder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716469"
                        ],
                        "name": "Y. Aoki",
                        "slug": "Y.-Aoki",
                        "structuredName": {
                            "firstName": "Yoshimitsu",
                            "lastName": "Aoki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Aoki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2464991"
                        ],
                        "name": "M. Kranzfelder",
                        "slug": "M.-Kranzfelder",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kranzfelder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kranzfelder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144523132"
                        ],
                        "name": "A. Schneider",
                        "slug": "A.-Schneider",
                        "structuredName": {
                            "firstName": "Armin",
                            "lastName": "Schneider",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Schneider"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717736"
                        ],
                        "name": "P. Fua",
                        "slug": "P.-Fua",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Fua",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Fua"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46505857"
                        ],
                        "name": "Slobodan Ilic",
                        "slug": "Slobodan-Ilic",
                        "structuredName": {
                            "firstName": "Slobodan",
                            "lastName": "Ilic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Slobodan Ilic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49993325"
                        ],
                        "name": "H. Feu\u00dfner",
                        "slug": "H.-Feu\u00dfner",
                        "structuredName": {
                            "firstName": "Hubertus",
                            "lastName": "Feu\u00dfner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Feu\u00dfner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145587210"
                        ],
                        "name": "Nassir Navab",
                        "slug": "Nassir-Navab",
                        "structuredName": {
                            "firstName": "Nassir",
                            "lastName": "Navab",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nassir Navab"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 319,
                                "start": 316
                            }
                        ],
                        "text": "While the deep convolutional neural networks (CNNs) has witnessed major breakthroughs in visual object recognition [3] [11] [13], the CNN-based object detectors have also achieved the state-of-the-arts results on a wide range of applications, such as face detection [8] [5], pedestrian detection [9] [4] and etc [2] [1] [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 27145244,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ad15e64c93676b57ebcef4c2c4183d41b246f8ae",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "Multiple human pose estimation is an important yet challenging problem. In an operating room (OR) environment, the 3D body poses of surgeons and medical staff can provide important clues for surgical workflow analysis. For that purpose, we propose an algorithm for localizing and recovering body poses of multiple human in an OR environment under a multi-camera setup. Our model builds on 3D Pictorial Structures and 2D body part localization across all camera views, using convolutional neural networks (ConvNets). To evaluate our algorithm, we introduce a dataset captured in a real OR environment. Our dataset is unique, challenging and publicly available with annotated ground truths. Our proposed algorithm yields to promising pose estimation results on this dataset."
            },
            "slug": "Parsing-human-skeletons-in-an-operating-room-Belagiannis-Wang",
            "title": {
                "fragments": [],
                "text": "Parsing human skeletons in an operating room"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An algorithm for localizing and recovering body poses of multiple human in an OR environment under a multi-camera setup using convolutional neural networks (ConvNets) is proposed and yields to promising pose estimation results."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Vision and Applications"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1823362"
                        ],
                        "name": "J. Uijlings",
                        "slug": "J.-Uijlings",
                        "structuredName": {
                            "firstName": "Jasper",
                            "lastName": "Uijlings",
                            "middleNames": [
                                "R.",
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Uijlings"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756979"
                        ],
                        "name": "K. V. D. Sande",
                        "slug": "K.-V.-D.-Sande",
                        "structuredName": {
                            "firstName": "Koen",
                            "lastName": "Sande",
                            "middleNames": [
                                "E.",
                                "A.",
                                "van",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. V. D. Sande"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695527"
                        ],
                        "name": "T. Gevers",
                        "slug": "T.-Gevers",
                        "structuredName": {
                            "firstName": "Theo",
                            "lastName": "Gevers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Gevers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144638781"
                        ],
                        "name": "A. Smeulders",
                        "slug": "A.-Smeulders",
                        "structuredName": {
                            "firstName": "Arnold",
                            "lastName": "Smeulders",
                            "middleNames": [
                                "W.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Smeulders"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 94
                            }
                        ],
                        "text": "On the other hand, a majority of those methods are typically based on image over-segmentation [12] or dense sliding windows [15], which are computationally expensive and have hamper their deployments in the real-time detection systems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 61
                            }
                        ],
                        "text": "The popular region proposal methods include Selective Search [12], EdgeBoxes [15], or the early stages of cascade detectors [8]; secondly, the extracted proposals are fed into a deep CNN for recognition and categorization; finally, the bounding box regression technique is employed to refine the coarse proposals into more accurate object bounds."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 216077384,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "38b6540ddd5beebffd05047c78183f7575559fb2",
            "isKey": false,
            "numCitedBy": 4752,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99\u00a0% recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software: http://disi.unitn.it/~uijlings/SelectiveSearch.html)."
            },
            "slug": "Selective-Search-for-Object-Recognition-Uijlings-Sande",
            "title": {
                "fragments": [],
                "text": "Selective Search for Object Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "This paper introduces selective search which combines the strength of both an exhaustive search and segmentation, and shows that its selective search enables the use of the powerful Bag-of-Words model for recognition."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2246870"
                        ],
                        "name": "Vidit Jain",
                        "slug": "Vidit-Jain",
                        "structuredName": {
                            "firstName": "Vidit",
                            "lastName": "Jain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vidit Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389846455"
                        ],
                        "name": "E. Learned-Miller",
                        "slug": "E.-Learned-Miller",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Learned-Miller",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Learned-Miller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 118
                            }
                        ],
                        "text": "To demonstrate the effectiveness of the proposed method, we compare the UnitBox with the state-of-the-arts methods on FDDB."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 153
                            }
                        ],
                        "text": "In this section, we apply the proposed IoU loss as well as the UnitBox on face detection task, and report our experimental results on the FDDB benchmark [6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 82
                            }
                        ],
                        "text": "We apply UnitBox on face detection task, and achieve the best performance on FDDB [6] among all published methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 76
                            }
                        ],
                        "text": "As a result, the proposed UnitBox has achieved the best detection result on FDDB among all published methods."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8299268,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ed33eba8143d2f8940dc034ee1cc9e80a65c3560",
            "isKey": true,
            "numCitedBy": 851,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Despite the maturity of face detection research, it remains difficult to compare different algorithms for face detection. This is partly due to the lack of common evaluation schemes. Also, existing data sets for evaluating face detection algorithms do not capture some aspects of face appearances that are manifested in real-world scenarios. In this work, we address both of these issues. We present a new data set of face images with more faces and more accurate annotations for face regions than in previous data sets. We also propose two rigorous and precise methods for evaluating the performance of face detection algorithms. We report results of several standard algorithms on the new benchmark."
            },
            "slug": "FDDB:-A-benchmark-for-face-detection-in-settings-Jain-Learned-Miller",
            "title": {
                "fragments": [],
                "text": "FDDB: A benchmark for face detection in unconstrained settings"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A new data set of face images with more faces and more accurate annotations for face regions than in previous data sets is presented and two rigorous and precise methods for evaluating the performance of face detection algorithms are proposed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 11
                            }
                        ],
                        "text": "2 we can see that, given two pixels, one falls in a larger bounding box while the other falls in a smaller one, the former will have a larger effect on the penalty than the latter, since the `2 loss is unnormalized."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Scale-aware Pixel-wise Object Proposal Networks. ArXiv e-prints"
            },
            "venue": {
                "fragments": [],
                "text": "Scale-aware Pixel-wise Object Proposal Networks. ArXiv e-prints"
            },
            "year": 2016
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 10,
            "methodology": 7
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 16,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/UnitBox:-An-Advanced-Object-Detection-Network-Yu-Jiang/22264e60f1dfbc7d0b52549d1de560993dd96e46?sort=total-citations"
}