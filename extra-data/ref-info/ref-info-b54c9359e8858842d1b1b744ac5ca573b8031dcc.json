{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6336008,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b03f43c5620bfc8993ea25dee20ce52a203ebcf7",
            "isKey": false,
            "numCitedBy": 91,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a framework for designing incremental learning algorithms derived from generalized entropy functionals. Our approach is based on the use of Bregman divergences together with the associated class of additive models constructed using the Legendre transform. A particular one-parameter family of Bregman divergences is shown to yield a family of loss functions that includes the log-likelihood criterion of logistic regression as a special case, and that closely approximates the exponential loss criterion used in the AdaBoost algorithms of Schapire et a/., as the natural parameter of the family varies. We also show how the quadratic approximation of the gain in Bregman divergence results in a weighted least-squares criterion. This leads to a family of incremental learning algorithms that builds upon and extends the recent interpretation of boosting in terms of additive models proposed by Friedman, Hastie, and Tibshirani."
            },
            "slug": "Additive-models,-boosting,-and-inference-for-Lafferty",
            "title": {
                "fragments": [],
                "text": "Additive models, boosting, and inference for generalized divergences"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A framework for designing incremental learning algorithms derived from generalized entropy functionals based on the use of Bregman divergences together with the associated class of additive models constructed using the Legendre transform is presented."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '99"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2714577"
                        ],
                        "name": "S. D. Pietra",
                        "slug": "S.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pietra",
                            "middleNames": [
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944066"
                        ],
                        "name": "V. D. Pietra",
                        "slug": "V.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Pietra",
                            "middleNames": [
                                "J.",
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10730308,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "25a1797148c7decdacce96ccd1c8419a51f1c8c1",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : In this paper, the authors formulate and prove a convex duality theorem for minimizing a general class of Bregman distances subject to linear constraints. The duality result is then used to derive iterative algorithms for solving the associated optimization problem. Their presentation is motivated by the recent work of Collins, Schapire, and Singer (2001), who showed how certain boosting algorithms and maximum likelihood logistic regression can be unified within the framework of Bregman distances. In particular, specific instances of the results given here are used by Collins et al. (2001) to show the convergence of a family of iterative algorithms for minimizing the exponential or logistic loss. Following an introduction, Section 2 recalls the standard definitions from convex analysis that will be required, and presents the technical assumptions made on the class of Bregman distances that the authors work with. They also introduce some new terminology, using the terms Legendre-Bregman conjugate and Legendre-Bregman projection to extend the classical notion of the Legendre conjugate and transform to Bregman distances. Section 3 contains the statement and proof of the duality theorem that connects the primal problem with its dual, showing that the solution is characterized in geometrical terms by a Pythagorean equality. Section 4 defines the notion of an auxiliary function, which is used to construct iterative algorithms for solving constrained optimization problems. This section shows how convexity can be used to derive an auxiliary function for Bregman distances based on separable functions. The last section summarizes the main results of the paper."
            },
            "slug": "Duality-and-Auxiliary-Functions-for-Bregman-Pietra-Pietra",
            "title": {
                "fragments": [],
                "text": "Duality and Auxiliary Functions for Bregman Distances"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Given this assumption, it has been noted by Breiman (1997a, 1997b) and various later authors (Friedman et al., 2000; Mason et al., 1999; R\u00e4tsch, Onoda, & M\u00fcller, 2001; Schapire & Singer, 1999) that the choice of both hj and are done in such a way as to cause the greatest decrease in the exponential loss induced by , given that only a single component of is to be updated."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9913392,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f4493eff2531536a7aeb3fc11d62c30a8f487f6",
            "isKey": false,
            "numCitedBy": 4829,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Boosting is one of the most important recent developments in classification methodology. Boosting works by sequentially applying a classification algorithm to reweighted versions of the training data and then taking a weighted majority vote of the sequence of classifiers thus produced. For many classification algorithms, this simple strategy results in dramatic improvements in performance. We show that this seemingly mysterious phenomenon can be understood in terms of well-known statistical principles, namely additive modeling and maximum likelihood. For the two-class problem, boosting can be viewed as an approximation to additive modeling on the logistic scale using maximum Bernoulli likelihood as a criterion. We develop more direct approximations and show that they exhibit nearly identical results to boosting. Direct multiclass generalizations based on multinomial likelihood are derived that exhibit performance comparable to other recently proposed multiclass generalizations of boosting in most situations, and far superior in some. We suggest a minor modification to boosting that can reduce computation, often by factors of 10 to 50. Finally, we apply these insights to produce an alternative formulation of boosting decision trees. This approach, based on best-first truncated tree induction, often leads to better performance, and can provide interpretable descriptions of the aggregate decision rule. It is also much faster computationally, making it more suitable to large-scale data mining applications."
            },
            "slug": "Special-Invited-Paper-Additive-logistic-regression:-Friedman",
            "title": {
                "fragments": [],
                "text": "Special Invited Paper-Additive logistic regression: A statistical view of boosting"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work shows that this seemingly mysterious phenomenon of boosting can be understood in terms of well-known statistical principles, namely additive modeling and maximum likelihood, and develops more direct approximations and shows that they exhibit nearly identical results to boosting."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2329907,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9a9309e056272ff2076f447df8dbc536f46fc466",
            "isKey": false,
            "numCitedBy": 1918,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe several improvements to Freund and Schapire's AdaBoost boosting algorithm, particularly in a setting in which hypotheses may assign confidences to each of their predictions. We give a simplified analysis of AdaBoost in this setting, and we show how this analysis can be used to find improved parameter settings as well as a refined criterion for training weak hypotheses. We give a specific method for assigning confidences to the predictions of decision trees, a method closely related to one used by Quinlan. This method also suggests a technique for growing decision trees which turns out to be identical to one proposed by Kearns and Mansour. We focus next on how to apply the new boosting algorithms to multiclass classification problems, particularly to the multi-label case in which each example may belong to more than one class. We give two boosting methods for this problem, plus a third method based on output coding. One of these leads to a new method for handling the single-label case which is simpler but as effective as techniques suggested by Freund and Schapire. Finally, we give some experimental results comparing a few of the algorithms discussed in this paper."
            },
            "slug": "Improved-Boosting-Algorithms-Using-Confidence-rated-Schapire-Singer",
            "title": {
                "fragments": [],
                "text": "Improved Boosting Algorithms Using Confidence-rated Predictions"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "Several improvements to Freund and Schapire's AdaBoost boosting algorithm are described, particularly in a setting in which hypotheses may assign confidences to each of their predictions."
            },
            "venue": {
                "fragments": [],
                "text": "COLT' 98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6644398,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ccf5208521cb8c35f50ee8873df89294b8ed7292",
            "isKey": false,
            "numCitedBy": 13124,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone?Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in Rn. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line."
            },
            "slug": "A-decision-theoretic-generalization-of-on-line-and-Freund-Schapire",
            "title": {
                "fragments": [],
                "text": "A decision-theoretic generalization of on-line learning and an application to boosting"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "The model studied can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting, and it is shown that the multiplicative weight-update Littlestone?Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems."
            },
            "venue": {
                "fragments": [],
                "text": "EuroCOLT"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2714577"
                        ],
                        "name": "S. D. Pietra",
                        "slug": "S.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pietra",
                            "middleNames": [
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944066"
                        ],
                        "name": "V. D. Pietra",
                        "slug": "V.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Pietra",
                            "middleNames": [
                                "J.",
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Pietra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1936665,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ac1ca26102c1c928c86395b85105f1069f1a6f35",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "| We present a class of statistical learning algorithms formulated in terms of minimizing Bregman distances, a family of generalized entropy measures associated with convex functions. The inductive learning scheme is akin to growing a decision tree, with the Bregman distance lling the role of the impurity function in tree-based classi ers. Our approach is based on two components. In the feature selection step, each linear constraint in a pool of candidate features is evaluated by the reduction in Bregman distance that would result from adding it to the model. In the constraint satisfaction step, all of the parameters are adjusted to minimize the Bregman distance subject to the chosen constraints. We introduce a new iterative estimation algorithm for carrying out both the feature selection and constraint satisfaction steps, and outline a proof of the convergence of these algorithms."
            },
            "slug": "Statistical-Learning-Algorithms-Based-on-Bregman-Lafferty-Pietra",
            "title": {
                "fragments": [],
                "text": "Statistical Learning Algorithms Based on Bregman Distances"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A new iterative estimation algorithm is introduced for carrying out both the feature selection and constraint satisfaction steps of the inductive learning scheme, and a proof of the convergence of these algorithms is outlined."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700597"
                        ],
                        "name": "Jyrki Kivinen",
                        "slug": "Jyrki-Kivinen",
                        "structuredName": {
                            "firstName": "Jyrki",
                            "lastName": "Kivinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jyrki Kivinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This is a new result: Although Kivinen and Warmuth [19] and Mason et al."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Our work builds heavily on that of Kivinen and Warmuth [19] who, along with Lafferty, were the first to make a connection between AdaBoost and information geometry."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13289060,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c64f76165e66bdce90544b8edff39997fa55c6f3",
            "isKey": false,
            "numCitedBy": 134,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the AdaBoost procedure for boosting weak learners. In AdaBoost, a key step is choosing a new distribution on the training examples based on the old distribution and the mistakes made by the present weak hypothesis. We show how AdaBoost\u2019s choice of the new distribution can be seen as an approximate solution to the following problem: Find a new distribution that is closest to the old distribution subject to the constraint that the new distribution is orthogonal to the vector of mistakes of the current weak hypothesis. The distance (or divergence) between distributions is measured by the relative entropy. Alternatively, we could say that AdaBoost approximately projects the distribution vector onto a hyperplane defined by the mistake vector. We show that this new view of AdaBoost as an entropy projection is dual to the usual view of AdaBoost as minimizing the normalization factors of the updated distributions."
            },
            "slug": "Boosting-as-entropy-projection-Kivinen-Warmuth",
            "title": {
                "fragments": [],
                "text": "Boosting as entropy projection"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "It is shown how AdaBoost\u2019s choice of the new distribution can be seen as an approximate solution to the following problem: Find a new distribution that is closest to the old distribution subject to the constraint that thenew distribution is orthogonal to the vector of mistakes of the current weak hypothesis."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '99"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152597562"
                        ],
                        "name": "Gunnar R\u00e4tsch",
                        "slug": "Gunnar-R\u00e4tsch",
                        "structuredName": {
                            "firstName": "Gunnar",
                            "lastName": "R\u00e4tsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gunnar R\u00e4tsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35205384"
                        ],
                        "name": "T. Onoda",
                        "slug": "T.-Onoda",
                        "structuredName": {
                            "firstName": "Takashi",
                            "lastName": "Onoda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Onoda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3144723,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "03e8d6373b63bb15e11d3092477c55c74c063b72",
            "isKey": false,
            "numCitedBy": 1258,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently ensemble methods like ADABOOST have been applied successfully in many problems, while seemingly defying the problems of overfitting.ADABOOST rarely overfits in the low noise regime, however, we show that it clearly does so for higher noise levels. Central to the understanding of this fact is the margin distribution. ADABOOST can be viewed as a constraint gradient descent in an error function with respect to the margin. We find that ADABOOST asymptotically achieves a hard margin distribution, i.e. the algorithm concentrates its resources on a few hard-to-learn patterns that are interestingly very similar to Support Vectors. A hard margin is clearly a sub-optimal strategy in the noisy case, and regularization, in our case a \u201cmistrust\u201d in the data, must be introduced in the algorithm to alleviate the distortions that single difficult patterns (e.g. outliers) can cause to the margin distribution. We propose several regularization methods and generalizations of the original ADABOOST algorithm to achieve a soft margin. In particular we suggest (1) regularized ADABOOSTREG where the gradient decent is done directly with respect to the soft margin and (2) regularized linear and quadratic programming (LP/QP-) ADABOOST, where the soft margin is attained by introducing slack variables.Extensive simulations demonstrate that the proposed regularized ADABOOST-type algorithms are useful and yield competitive results for noisy data."
            },
            "slug": "Soft-Margins-for-AdaBoost-R\u00e4tsch-Onoda",
            "title": {
                "fragments": [],
                "text": "Soft Margins for AdaBoost"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is found that ADABOOST asymptotically achieves a hard margin distribution, i.e. the algorithm concentrates its resources on a few hard-to-learn patterns that are interestingly very similar to Support Vectors."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700597"
                        ],
                        "name": "Jyrki Kivinen",
                        "slug": "Jyrki-Kivinen",
                        "structuredName": {
                            "firstName": "Jyrki",
                            "lastName": "Kivinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jyrki Kivinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6130401,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "98eed3f082351c4821d1edb315846207a8fefbe9",
            "isKey": false,
            "numCitedBy": 911,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider two algorithm for on-line prediction based on a linear model. The algorithms are the well-known Gradient Descent (GD) algorithm and a new algorithm, which we call EG(+/-). They both maintain a weight vector using simple updates. For the GD algorithm, the update is based on subtracting the gradient of the squared error made on a prediction. The EG(+/-) algorithm uses the components of the gradient in the exponents of factors that are used in updating the weight vector multiplicatively. We present worst-case loss bounds for EG(+/-) and compare them to previously known bounds for the GD algorithm. The bounds suggest that the losses of the algorithms are in general incomparable, but EG(+/-) has a much smaller loss if only a few components of the input are relevant for the predictions. We have performed experiments, which show that our worst-case upper bounds are quite tight already on simple artificial data."
            },
            "slug": "Exponentiated-Gradient-Versus-Gradient-Descent-for-Kivinen-Warmuth",
            "title": {
                "fragments": [],
                "text": "Exponentiated Gradient Versus Gradient Descent for Linear Predictors"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The bounds suggest that the losses of the algorithms are in general incomparable, but EG(+/-) has a much smaller loss if only a few components of the input are relevant for the predictions, which is quite tight already on simple artificial data."
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Comput."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14849468,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "65b7b1a0d61fd012f10cfce642d4aa4dec9a5829",
            "isKey": false,
            "numCitedBy": 223,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work has shown that adaptively reweighting the training set, growing a classifier using the new weights, and combining the classifiers constructed to date can significantly decrease generalization error. Procedures of this type were called arcing by Breiman[1996]. The first successful arcing procedure was introduced by Freund and Schapire[1995,1996] and called Adaboost. In an effort to explain why Adaboost works, Schapire et.al. [1997] derived a bound on the generalization error of a convex combination of classifiers in terms of the margin. We introduce a function called the edge, which differs from the margin only if there are more than two classes. A framework for understanding arcing algorithms is defined. In this framework, we see that the arcing algorithms currently in the literature are optimization algorithms which minimize some function of the edge. A relation is derived between the optimal reduction in the maximum value of the edge and the PAC concept of weak learner. Two algorithms are described which achieve the optimal reduction. Tests on both synthetic and real data cast doubt on the Schapire et.al. explanation."
            },
            "slug": "Arcing-the-edge-Breiman",
            "title": {
                "fragments": [],
                "text": "Arcing the edge"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A framework for understanding arcing algorithms is defined and a relation is derived between the optimal reduction in the maximum value of the edge and the PAC concept of weak learner."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700597"
                        ],
                        "name": "Jyrki Kivinen",
                        "slug": "Jyrki-Kivinen",
                        "structuredName": {
                            "firstName": "Jyrki",
                            "lastName": "Kivinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jyrki Kivinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A variety of work in the online learning literature, such as the work by Littlestone, Long, and Warmuth (1995) and the work by  Kivinen and Warmuth (1997, 2001)  on exponentiated gradient methods, also use Bregman divergences, and techniques that are related to the auxiliary function method."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 470902,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1554931d9ce2f18e5515b24601c3f18ebe76c80d",
            "isKey": false,
            "numCitedBy": 138,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We study on-line generalized linear regression with multidimensional outputs, i.e., neural networks with multiple output nodes but no hidden nodes. We allow at the final layer transfer functions such as the softmax function that need to consider the linear activations to all the output neurons. The weight vectors used to produce the linear activations are represented indirectly by maintaining separate parameter vectors. We get the weight vector by applying a particular parameterization function to the parameter vector. Updating the parameter vectors upon seeing new examples is done additively, as in the usual gradient descent update. However, by using a nonlinear parameterization function between the parameter vectors and the weight vectors, we can make the resulting update of the weight vector quite different from a true gradient descent update. To analyse such updates, we define a notion of a matching loss function and apply it both to the transfer function and to the parameterization function. The loss function that matches the transfer function is used to measure the goodness of the predictions of the algorithm. The loss function that matches the parameterization function can be used both as a measure of divergence between models in motivating the update rule of the algorithm and as a measure of progress in analyzing its relative performance compared to an arbitrary fixed model. As a result, we have a unified treatment that generalizes earlier results for the gradient descent and exponentiated gradient algorithms to multidimensional outputs, including multiclass logistic regression."
            },
            "slug": "Relative-Loss-Bounds-for-Multidimensional-Problems-Kivinen-Warmuth",
            "title": {
                "fragments": [],
                "text": "Relative Loss Bounds for Multidimensional Regression Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A unified treatment of on-line generalized linear regression with multidimensional outputs, i.e., neural networks with multiple output nodes but no hidden nodes, is studied, which generalizes earlier results for the gradient descent and exponentiated gradient algorithms to multiddimensional outputs, including multiclass logistic regression."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700597"
                        ],
                        "name": "Jyrki Kivinen",
                        "slug": "Jyrki-Kivinen",
                        "structuredName": {
                            "firstName": "Jyrki",
                            "lastName": "Kivinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jyrki Kivinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15718458,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4dc175e8f6e7ca5c40ffd6fb9c6b92323bf7daf2",
            "isKey": false,
            "numCitedBy": 281,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider two algorithms for on-line prediction based on a linear model. The algorithms are the well-known Gradient Descent (GD) algorithm and a new algorithm, which we call EG *. They both maintain a weight vector using simple updates. For the GD algorithm, the weight vector is updated by subtracting from it the gradient of the squared error made on a prediction multiplied by a parameter called the learning rate. The EG* uses the components of the gradient in the exponents of factors that are used in updating the weight vector multiplicatively. We present worst-case on-line loss bounds for EG* and compare them to previously known bounds for the GD algorithm. The bounds suggest that although the on-line losses of the algorithms are in general incomparable, EG * has a much smaller loss if only few of the input variables are relevant for the predictions. Experiments show that the worst-case upper bounds are quite tight already on simple artificial data. Our main methodological idea is using a distance function between weight vectors both in motivating the algorithms and as a potential function in an amortized analysis that leads to worst-case loss bounds. Using squared Euclidean distance leads to the GD algorithm, and using the relative entropy leads to the EG* algorithm."
            },
            "slug": "Additive-versus-exponentiated-gradient-updates-for-Kivinen-Warmuth",
            "title": {
                "fragments": [],
                "text": "Additive versus exponentiated gradient updates for linear prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The main methodological idea is using a distance function between weight vectors both in motivating the algorithms and as a potential function in an amortized analysis that leads to worst-case loss bounds."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '95"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143857271"
                        ],
                        "name": "Nigel P. Duffy",
                        "slug": "Nigel-P.-Duffy",
                        "structuredName": {
                            "firstName": "Nigel",
                            "lastName": "Duffy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nigel P. Duffy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1772099"
                        ],
                        "name": "D. Helmbold",
                        "slug": "D.-Helmbold",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Helmbold",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Helmbold"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12926428,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c6cd4446aa0398a5d2ed1b2f74bdadfbcb6629cc",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent interpretations of the Adaboost algorithm view it as performing a gradient descent on a potential function. Simply changing the potential function allows one to create new algorithms related to AdaBoost. However, these new algorithms are generally not known to have the formal boosting property. This paper examines the question of which potential functions lead to new algorithms that are boosters. The two main results are general sets of conditions on the potential; one set implies that the resulting algorithm is a booster, while the other implies that the algorithm is not. These conditions are applied to previously studied potential functions, such as those used by LogitBoost and Doom II."
            },
            "slug": "Potential-Boosters-Duffy-Helmbold",
            "title": {
                "fragments": [],
                "text": "Potential Boosters?"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "The question of which potential functions lead to new algorithms that are boosters is examined, and two main results are general sets of conditions on the potential that imply that the resulting algorithm is a booster, while the other implies that the algorithm is not."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2714577"
                        ],
                        "name": "S. D. Pietra",
                        "slug": "S.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pietra",
                            "middleNames": [
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944066"
                        ],
                        "name": "V. D. Pietra",
                        "slug": "V.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Pietra",
                            "middleNames": [
                                "J.",
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 982,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b951b9f78b98a186ba259027996a48e4189d37e5",
            "isKey": false,
            "numCitedBy": 1305,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a technique for constructing random fields from a set of training samples. The learning paradigm builds increasingly complex fields by allowing potential functions, or features, that are supported by increasingly large subgraphs. Each feature has a weight that is trained by minimizing the Kullback-Leibler divergence between the model and the empirical distribution of the training data. A greedy algorithm determines how features are incrementally added to the field and an iterative scaling algorithm is used to estimate the optimal values of the weights. The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated. Relations to other learning approaches, including decision trees, are given. As a demonstration of the method, we describe its application to the problem of automatic word classification in natural language processing."
            },
            "slug": "Inducing-Features-of-Random-Fields-Pietra-Pietra",
            "title": {
                "fragments": [],
                "text": "Inducing Features of Random Fields"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14488820,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a92684c164b0c46020a371ae5116df74bb37a412",
            "isKey": false,
            "numCitedBy": 551,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The theory behind the success of adaptive reweighting and combining algorithms (arcing) such as Adaboost (Freund & Schapire, 1996a, 1997) and others in reducing generalization error has not been well understood. By formulating prediction as a game where one player makes a selection from instances in the training set and the other a convex linear combination of predictors from a finite set, existing arcing algorithms are shown to be algorithms for finding good game strategies. The minimax theorem is an essential ingredient of the convergence proofs. An arcing algorithm is described that converges to the optimal strategy. A bound on the generalization error for the combined predictors in terms of their maximum error is proven that is sharper than bounds to date. Schapire, Freund, Bartlett, and Lee (1997) offered an explanation of why Adaboost works in terms of its ability to produce generally high margins. The empirical comparison of Adaboost to the optimal arcing algorithm shows that their explanation is not complete."
            },
            "slug": "Prediction-Games-and-Arcing-Algorithms-Breiman",
            "title": {
                "fragments": [],
                "text": "Prediction Games and Arcing Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "The theory behind the success of adaptive reweighting and combining algorithms (arcing) such as Adaboost and others in reducing generalization error has not been well understood, and an explanation of whyAdaboost works in terms of its ability to produce generally high margins is offered."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2227181"
                        ],
                        "name": "K. H\u00f6ffgen",
                        "slug": "K.-H\u00f6ffgen",
                        "structuredName": {
                            "firstName": "Klaus-Uwe",
                            "lastName": "H\u00f6ffgen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. H\u00f6ffgen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723095"
                        ],
                        "name": "H. Simon",
                        "slug": "H.-Simon",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Simon",
                            "middleNames": [
                                "Ulrich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Simon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1823251"
                        ],
                        "name": "K. S. V. Horn",
                        "slug": "K.-S.-V.-Horn",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Horn",
                            "middleNames": [
                                "S.",
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. S. V. Horn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 160
                            }
                        ],
                        "text": "Although minimization of the number of classification errors may be a worthwhile goal, in its most general form, the problem is intractable (see, for instance, [15])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6707032,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fef36189265d90252106cdfd64e0a8d9d5c4c58d",
            "isKey": false,
            "numCitedBy": 184,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the problem of learning concepts by presenting labeled and randomly chosen training\u2013examples to single neurons. It is well-known that linear halfspaces are learnable by the method of linear programming. The corresponding (Mc-Culloch-Pitts) neurons are therefore efficiently trainable to learn an unknown halfspace from examples. We want to analyze how fast the learning performance degrades when the representational power of the neuron is overstrained, i.e., if more complex concepts than just halfspaces are allowed. We show that a neuron cannot efficently find its probably almost optimal adjustment (unless RP = NP). If the weights and the threshold of the neuron have a fixed constant bound on their coding length, the situation is even worse: There is in general no polynomial time training method which bounds the resulting prediction error of the neuron by k.opt for a fixed constant k (unless RP = NP). Other variants of learning more complex concepts than halfspaces by single neurons are also investigated. We show that neither heuristical learning nor learning by sigmoidal neurons with a constant reject-rate is efficiently possible (unless RP = NP)."
            },
            "slug": "Robust-trainability-of-single-neurons-H\u00f6ffgen-Simon",
            "title": {
                "fragments": [],
                "text": "Robust trainability of single neurons"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that a neuron cannot efficently find its probably almost optimal adjustment (unless RP = NP) and neither heuristical learning nor learning by sigmoidal neurons with a constant reject-rate is efficiently possible."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '92"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756798"
                        ],
                        "name": "I. Csisz\u00e1r",
                        "slug": "I.-Csisz\u00e1r",
                        "structuredName": {
                            "firstName": "Imre",
                            "lastName": "Csisz\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Csisz\u00e1r"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 32
                            }
                        ],
                        "text": "The result appears to be due to Csisza\u0301r [6, 7] and Topsoe [21]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 40
                            }
                        ],
                        "text": "The result appears to be due to Csisz\u00e1r [8, 9] and Topsoe [26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 9
                            }
                        ],
                        "text": "See also Csisza\u0301r\u2019s survey article [8]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 18053591,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "72b2aeeb76dbff312321ccbcc58e85009e0b57ae",
            "isKey": false,
            "numCitedBy": 1450,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.. Institute of Mathematical Statistics is collaborating with JSTOR to digitize, preserve and extend access to The Annals of Probability. Some geometric properties of PD's are established, Kullback's I-divergence playing the role of squared Euclidean distance. The minimum discrimination information problem is viewed as that of projecting a PD onto a convex set of PD's and useful existence theorems for and characterizations of the minimizing PD are arrived at. A natural generalization of known iterative algorithms converging to the minimizing PD in special situations is given; even for those special cases, our convergence proof is more generally valid than those previously published. As corollaries of independent interest, generalizations of known results on the existence of PD's or nonnegative matrices of a certain form are obtained. The Lagrange multiplier technique is not used."
            },
            "slug": "$I$-Divergence-Geometry-of-Probability-and-Problems-Csisz\u00e1r",
            "title": {
                "fragments": [],
                "text": "$I$-Divergence Geometry of Probability Distributions and Minimization Problems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153717533"
                        ],
                        "name": "Carlos Domingo",
                        "slug": "Carlos-Domingo",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Domingo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlos Domingo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145581400"
                        ],
                        "name": "Osamu Watanabe",
                        "slug": "Osamu-Watanabe",
                        "structuredName": {
                            "firstName": "Osamu",
                            "lastName": "Watanabe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Osamu Watanabe"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 30
                            }
                        ],
                        "text": "[11] Carlos Domingo and Osamu Watanabe."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 57
                            }
                        ],
                        "text": "As pointed out by Watanabe [27] and Domingo and Watanabe [13], this is not possible with AdaBoost since i ts weights may become extremely large."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 11
                            }
                        ],
                        "text": "[22] Osamu Watanabe."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 36
                            }
                        ],
                        "text": "As pointed out by Watanabe [22] and Domingo and Watanabe [11], this is not possible with AdaBoost since its weights may become extremely large."
                    },
                    "intents": []
                }
            ],
            "corpusId": 13357023,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "45d1b3db184157aa46a84cc1aa85f9bcd6db70c1",
            "isKey": true,
            "numCitedBy": 38,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a experimental evaluation of a boosting based learning system and show that can be run efficiently over a large dataset. The system uses as base learner decision stumps, single atribute decision trees with only two terminal nodes. To select the best decision stump at each iteration we use an adaptive sampling method. As a boosting algorithm, we use a modification of AdaBoost that is suitable to be combined with a base learner that does not use all the dataset. We provide experimental evidence that our method is as accurate as the equivalent algorithm that uses all the dataset but much faster."
            },
            "slug": "Scaling-Up-a-Boosting-Based-Learner-via-Adaptive-Domingo-Watanabe",
            "title": {
                "fragments": [],
                "text": "Scaling Up a Boosting-Based Learner via Adaptive Sampling"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This paper presents an experimental evaluation of a boosting based learning system that can be run efficiently over a large dataset and provides experimental evidence that the method is as accurate as the equivalent algorithm that uses all the dataset but much faster."
            },
            "venue": {
                "fragments": [],
                "text": "PAKDD"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2586148"
                        ],
                        "name": "L. Mason",
                        "slug": "L.-Mason",
                        "structuredName": {
                            "firstName": "Llew",
                            "lastName": "Mason",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Mason"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3772657,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "29b7eebc893acd2c2596de227333480e7a118af8",
            "isKey": false,
            "numCitedBy": 819,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The application of boosting procedures to decision tree algorithms has been shown to produce very accurate classi ers. These classiers are in the form of a majority vote over a number of decision trees. Unfortunately, these classi ers are often large, complex and di\u00c6cult to interpret. This paper describes a new type of classi cation rule, the alternating decision tree, which is a generalization of decision trees, voted decision trees and voted decision stumps. At the same time classi ers of this type are relatively easy to interpret. We present a learning algorithm for alternating decision trees that is based on boosting. Experimental results show it is competitive with boosted decision tree algorithms such as C5.0, and generates rules that are usually smaller in size and thus easier to interpret. In addition these rules yield a natural measure of classi cation con dence which can be used to improve the accuracy at the cost of abstaining from predicting examples that are hard to classify."
            },
            "slug": "The-Alternating-Decision-Tree-Learning-Algorithm-Freund-Mason",
            "title": {
                "fragments": [],
                "text": "The Alternating Decision Tree Learning Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A new type of classi cation rule, the alternating decision tree, which is a generalization of decision trees, voted decision trees and voted decision stumps and generates rules that are usually smaller in size and thus easier to interpret."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767325"
                        ],
                        "name": "Y. Censor",
                        "slug": "Y.-Censor",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Censor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Censor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144855064"
                        ],
                        "name": "A. Lent",
                        "slug": "A.-Lent",
                        "structuredName": {
                            "firstName": "Arnold",
                            "lastName": "Lent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lent"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 119559562,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "f2f68eb6b1389d019bf05df8d7cb3e48c7896974",
            "isKey": false,
            "numCitedBy": 443,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The iterative primal-dual method of Bregman for solving linearly constrained convex programming problems, which utilizes nonorthogonal projections onto hyperplanes, is represented in a compact form, and a complete proof of convergence is given for an almost cyclic control of the method. Based on this, a new algorithm for solving interval convex programming problems, i.e., problems of the form minf(x), subject to \u03b3\u2264Ax\u2264\u03b4, is proposed. For a certain family of functionsf(x), which includes the norm \u2225x\u2225 and thex logx entropy function, convergence is proved. The present row-action method is particularly suitable for handling problems in which the matrixA is large (or huge) and sparse."
            },
            "slug": "An-iterative-row-action-method-for-interval-convex-Censor-Lent",
            "title": {
                "fragments": [],
                "text": "An iterative row-action method for interval convex programming"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The iterative primal-dual method of Bregman for solving linearly constrained convex programming problems, which utilizes nonorthogonal projections onto hyperplanes, is represented in a compact form, and a complete proof of convergence is given for an almost cyclic control of the method."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50056360"
                        ],
                        "name": "William W. Cohen",
                        "slug": "William-W.-Cohen",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Cohen",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William W. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "we could easily convert any system such as SLIPPER [7], BoosTexter [25] or alternating trees [15] to use logistic loss."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 195625660,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9d06638df32f8feefb95ef5a4769adbb1ae6297d",
            "isKey": false,
            "numCitedBy": 392,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe SLIPPER, a new rule learner that generates rulesets by repeatedly boosting a simple, greedy, rule-builder. Like the rulesets built by other rule learners, the ensemble of rules created by SLIPPER is compact and comprehensible. This is made possible by imposing appropriate constraints on the rule-builder, and by use of a recently-proposed generalization of Adaboost called confidence-rated boosting. In spite of its relative simplicity, SLIPPER is highly scalable, and an effective learner. Experimentally, SLIPPER scales no worse than O(n log n), where n is the number of examples, and on a set of 32 benchmark problems, SLIPPER achieves lower error rates than RIPPER 20 times, and lower error rates than C4.5rules 22 times."
            },
            "slug": "A-simple,-fast,-and-effective-rule-learner-Cohen-Singer",
            "title": {
                "fragments": [],
                "text": "A simple, fast, and effective rule learner"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "SLIPPER, a new rule learner that generates rulesets by repeatedly boosting a simple, greedy, rule-builder, is described, and like the rulesets built by other rule learners, the ensemble of rules created by SLIPPER is compact and comprehensible."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI 1999"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145581400"
                        ],
                        "name": "Osamu Watanabe",
                        "slug": "Osamu-Watanabe",
                        "structuredName": {
                            "firstName": "Osamu",
                            "lastName": "Watanabe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Osamu Watanabe"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 28172530,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "854cbfe3a955258f7bcbd0ea5a43ae10bba8e724",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Machine learning has been one of the important subjects of AI that is motivated by many real world applications. In theoretical computer science, researchers also have introduced mathematical frameworks for investigating machine learning, and in these frameworks, many interesting results have been obtained. Now we are proceeding to a new stage to study how to apply these fruitful theoretical results to real problems. We point out in this paper that \"adaptivity\" is one of the important issues when we consider applications of learning techniques, and we propose one learning algorithm with this feature."
            },
            "slug": "From-Computational-Learning-Theory-to-Discovery-Watanabe",
            "title": {
                "fragments": [],
                "text": "From Computational Learning Theory to Discovery Science"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is pointed out in this paper that \"adaptivity\" is one of the important issues when the authors consider applications of learning techniques, and a learning algorithm with this feature is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "ICALP"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2961287"
                        ],
                        "name": "N. Littlestone",
                        "slug": "N.-Littlestone",
                        "structuredName": {
                            "firstName": "Nick",
                            "lastName": "Littlestone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Littlestone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144007105"
                        ],
                        "name": "Philip M. Long",
                        "slug": "Philip-M.-Long",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Long",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip M. Long"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In related work,  Littlestone, Long, and Warmuth (1995)  describe algorithms where convergence properties are analyzed through a method that is similar to the auxiliary function techniques."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "A variety of work in the online learning literature, such as the work by  Littlestone, Long, and Warmuth (1995)  and the work by Kivinen and Warmuth (1997, 2001) on exponentiated gradient methods, also use Bregman divergences, and techniques that are related to the auxiliary function method."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13012680,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "ed6f52a5a6ec4f30eecc35035078b5d673748528",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an algorithm for the on-line learning of linear functions which is optimal to within a constant factor with respect to bounds on the sum of squared errors for a worst case sequence of trials. The bounds are logarithmic in the number of variables. Furthermore, the algorithm is shown to be optimally robust with respect to noise in the data (again to within a constant factor)."
            },
            "slug": "On-line-learning-of-linear-functions-Littlestone-Long",
            "title": {
                "fragments": [],
                "text": "On-line learning of linear functions"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "An algorithm for the on-line learning of linear functions which is optimal to within a constant factor with respect to bounds on the sum of squared errors for a worst case sequence of trials is presented."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '91"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9300024"
                        ],
                        "name": "F. Tops\u00f8e",
                        "slug": "F.-Tops\u00f8e",
                        "structuredName": {
                            "firstName": "Flemming",
                            "lastName": "Tops\u00f8e",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Tops\u00f8e"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18236183,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5eea000d6ddb8f37c487d323b5ffe5570be5cbc4",
            "isKey": false,
            "numCitedBy": 162,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Let (I, </) be a measurable space, referred to as the state space. The state space is discrete if I is countable and J consists of all subsets of I. By 77 we denote the set of countable decompositions of I in measurable sets, directed in the usual way by refinement. 770 denotes the subset of 17 consisting of finite decompositions. By a distribution we mean a probability measure on (/, J). The set of all distributions is denoted by M. For p.e M and n e II, \\x | n denotes the restriction of \\x to the ff-algebra generated by %. For a point function <p on I, <<p, /i> denotes the expectation of q> w.r.t. /.i. For a set function (p on \u00ab/, <<p, /*>\u201e denotes expectation of (p w.r.t. H | n, i.e. (<p,H->K = Z<l>(A)n(A)."
            },
            "slug": "Information-theoretical-optimization-techniques-Tops\u00f8e",
            "title": {
                "fragments": [],
                "text": "Information-theoretical optimization techniques"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "Let (I, </) be a measurable space, referred to as the state space, and 77 denotes the set of countable decompositions of I in measurable sets, directed in the usual way by refinement."
            },
            "venue": {
                "fragments": [],
                "text": "Kybernetika"
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767325"
                        ],
                        "name": "Y. Censor",
                        "slug": "Y.-Censor",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Censor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Censor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732625"
                        ],
                        "name": "S. Zenios",
                        "slug": "S.-Zenios",
                        "structuredName": {
                            "firstName": "Stavros",
                            "lastName": "Zenios",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Zenios"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 68
                            }
                        ],
                        "text": "SeealsoCsis\u017aar\u2019s survey article[10] aswell asCensorandZenios\u2019 s book[5]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 115480745,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5d044b2ece3579ad1d6452f725524694a078b4cc",
            "isKey": false,
            "numCitedBy": 393,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Foreword Preface Glossary of Symbols 1. Introduction Part I Theory 2. Generalized Distances and Generalized Projections 3. Proximal Minimization with D-Functions Part II Algorithms 4. Penalty Methods, Barrier Methods and Augmented Lagrangians 5. Iterative Methods for Convex Feasibility Problems 6. Iterative Algorithms for Linearly Constrained Optimization Problems 7. Model Decomposition Algorithms 8. Decompositions in Interior Point Algorithms Part III Applications 9. Matrix Estimation Problems 10. Image Reconsturction from Projections 11. The Inverse Problem in Radiation Therapy Treatment Planning 12. Multicommodity Network Flow Problems 13. Planning Under Uncertainty 14. Decompositions for Parallel Computing 15. Numerical Investigations"
            },
            "slug": "Parallel-Optimization:-Theory,-Algorithms,-and-Censor-Zenios",
            "title": {
                "fragments": [],
                "text": "Parallel Optimization: Theory, Algorithms, and Applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756798"
                        ],
                        "name": "I. Csisz\u00e1r",
                        "slug": "I.-Csisz\u00e1r",
                        "structuredName": {
                            "firstName": "Imre",
                            "lastName": "Csisz\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Csisz\u00e1r"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": "See also Csisz\u00e1r\u2019s survey article [10] a s well as Censor and Zenios\u2019s book [5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 118126338,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "84e66daa4cda27a4c6dee96f779c261a14515fb1",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This is a mathematically oriented survey about the method of maximum entropy or minimum I-divergence, with a critical treatment of its various justifications and relation to Bayesian statistics. Information theoretic ideas are given substantial attention, including \u201cinformation geometry\u201d. The axiomatic approach is considered as the best justification of maxent, as well as of alternate methods of minimizing some Bregman distance or f-divergence other than I-divergence. The possible interpretation of such alternate methods within the original maxent paradigm is also considered."
            },
            "slug": "Maxent,-Mathematics,-and-Information-Theory-Csisz\u00e1r",
            "title": {
                "fragments": [],
                "text": "Maxent, Mathematics, and Information Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This is a mathematically oriented survey about the method of maximum entropy or minimum I-divergence, with a critical treatment of its various justifications and relation to Bayesian statistics."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "we could easily convert any system such as SLIPPER [7], BoosT exter [25] or alternating trees [15] to use logistic loss."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2185716,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e7a07c3aaef303850e5a1fcc81bb44f6d2db6696",
            "isKey": false,
            "numCitedBy": 2272,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "This work focuses on algorithms which learn from examples to perform multiclass text and speech categorization tasks. Our approach is based on a new and improved family of boosting algorithms. We describe in detail an implementation, called BoosTexter, of the new boosting algorithms for text categorization tasks. We present results comparing the performance of BoosTexter and a number of other text-categorization algorithms on a variety of tasks. We conclude by describing the application of our system to automatic call-type identification from unconstrained spoken customer responses."
            },
            "slug": "BoosTexter:-A-Boosting-based-System-for-Text-Schapire-Singer",
            "title": {
                "fragments": [],
                "text": "BoosTexter: A Boosting-based System for Text Categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work describes in detail an implementation, called BoosTexter, of the new boosting algorithms for text categorization tasks, and presents results comparing the performance of Boos Texter and a number of other text-categorization algorithms on a variety of tasks."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388387856"
                        ],
                        "name": "N. Cesa-Bianchi",
                        "slug": "N.-Cesa-Bianchi",
                        "structuredName": {
                            "firstName": "Nicol\u00f2",
                            "lastName": "Cesa-Bianchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cesa-Bianchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46486898"
                        ],
                        "name": "A. Krogh",
                        "slug": "A.-Krogh",
                        "structuredName": {
                            "firstName": "Anders",
                            "lastName": "Krogh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krogh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 47
                            }
                        ],
                        "text": "Our work builds heavily on that of Kivinen and Warmuth [16] who, along with Lafferty, were the first to make a connection between AdaBoost and information geometry."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 43
                            }
                        ],
                        "text": "This is a new result: Although Kivinen and Warmuth [16] and Mason et al. [19] have given convergence proofs for AdaBoost, their proofs depend on assumptions about the given minimization problem which may not hold in all cases."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 88
                            }
                        ],
                        "text": "Another sequential-update algorithm for a different but related problem was proposed by Cesa-Bianchi, Krogh and Warmuth [5]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 12
                            }
                        ],
                        "text": "Kivinen and Warmuth also described updates for general Bregman distances including, as one of their examples, the Bregman distance that we use to capture logistic regression."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 39
                            }
                        ],
                        "text": "ACKNOWLEDGMENTS\nMany thanks to Manfred Warmuth for first teaching us about Bregman distances and for many comments on an earlier draft."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 121
                            }
                        ],
                        "text": "Another sequential-update a lgorithm for a different but related problem was proposed by Cesa-Bianchi, Krogh and Warmuth [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 34
                            }
                        ],
                        "text": "[16] Jyrki Kivinen and Manfred K. Warmuth."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14750074,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "07cd971696d1bdc5381b4c6416fbe7635706dcfe",
            "isKey": true,
            "numCitedBy": 11,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "An approximate steepest descent strategy is described, converging in families of regular exponential densities to maximum likelihood estimates of density functions. These density estimates are also obtained by an application of the principle of minimum relative entropy subject to empirical constraints. We prove tight bounds on the increase of the log-likelihood at each iteration of our strategy for families of exponential densities whose log-densities are spanned by a set of bounded basis functions. >"
            },
            "slug": "Bounds-on-approximate-steepest-descent-for-in-Cesa-Bianchi-Krogh",
            "title": {
                "fragments": [],
                "text": "Bounds on approximate steepest descent for likelihood maximization in exponential families"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "An approximate steepest descent strategy is described, converging in families of regular exponential densities to maximum likelihood estimates of density functions, obtained by an application of the principle of minimum relative entropy subject to empirical constraints."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682548"
                        ],
                        "name": "J. Buhmann",
                        "slug": "J.-Buhmann",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Buhmann",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Buhmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15479156,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3a58c3eafcc642ffa2e571e069e53f20bb1d1150",
            "isKey": false,
            "numCitedBy": 538,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "Partitioning a data set and extracting hidden structure from the data arises in different application areas of pattern recognition, speech and image processing. Pairwise data clustering is a combinatorial optimization method for data grouping which extracts hidden structure from proximity data. We describe a deterministic annealing approach to pairwise clustering which shares the robustness properties of maximum entropy inference. The resulting Gibbs probability distributions are estimated by mean-field approximation. A new structure-preserving algorithm to cluster dissimilarity data and to simultaneously embed these data in a Euclidian vector space is discussed which can be used for dimensionality reduction and data visualization. The suggested embedding algorithm which outperforms conventional approaches has been implemented to analyze dissimilarity data from protein analysis and from linguistics. The algorithm for pairwise data clustering is used to segment textured images."
            },
            "slug": "Pairwise-Data-Clustering-by-Deterministic-Annealing-Hofmann-Buhmann",
            "title": {
                "fragments": [],
                "text": "Pairwise Data Clustering by Deterministic Annealing"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A deterministic annealing approach to pairwise clustering is described which shares the robustness properties of maximum entropy inference and the resulting Gibbs probability distributions are estimated by mean-field approximation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710580"
                        ],
                        "name": "A. Berger",
                        "slug": "A.-Berger",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Berger",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2714577"
                        ],
                        "name": "S. D. Pietra",
                        "slug": "S.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pietra",
                            "middleNames": [
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944066"
                        ],
                        "name": "V. D. Pietra",
                        "slug": "V.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Pietra",
                            "middleNames": [
                                "J.",
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Pietra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1085832,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb486e03369a64de2d5b0df86ec0a7b55d3907db",
            "isKey": false,
            "numCitedBy": 3452,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing."
            },
            "slug": "A-Maximum-Entropy-Approach-to-Natural-Language-Berger-Pietra",
            "title": {
                "fragments": [],
                "text": "A Maximum Entropy Approach to Natural Language Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A maximum-likelihood approach for automatically constructing maximum entropy models is presented and how to implement this approach efficiently is described, using as examples several problems in natural language processing."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35180576"
                        ],
                        "name": "L. Bregman",
                        "slug": "L.-Bregman",
                        "structuredName": {
                            "firstName": "Lev",
                            "lastName": "Bregman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bregman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 869,
                                "start": 172
                            }
                        ],
                        "text": "Our work is based directly on the general setting of Lafferty , Della Pietra and Della Pietra (1997) in which one attempts to solve optimization problems based on g eneral Bregman distances. They gave a method for deriving and analyzing parallel-update algorit hms in this setting through the use of auxiliary functions. All of our algorithms and convergence proofs are based on this method. Our work builds on several previous papers which have compar ed boosting approaches to logistic regression. Friedman, Hastie and Tibshirani (2000) first no ted the similarity between the boosting and logistic regression loss functions, and derived the sequen tial-update algorithm LogitBoost for the logistic loss. However, unlike our algorithm, theirs requires that t he weak learner solve least-squares problems rather than classification problems. Duffy and Helmbold (1999) gave conditions under which a loss function gives a boosting algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2836,
                                "start": 147
                            }
                        ],
                        "text": "We give a unified account of boosting and logistic regression in which we show that both learning problems can be cast in terms of optimization of Bregman distances. In our framework, the two problems become very similar, the only real difference being in the choice of Breg man distance: unnormalized relative entropy for boosting, and binary relative entropy for logistic regress ion. The similarity of the two problems in our framework allows us to design and analyze algorithms for both simultaneously. We are now able to borrow methods fr om the maximum-entropy literature for logistic regression and apply them to the exponential loss u sed by AdaBoost, especially convergence-proof techniques. Conversely, we can now easily adapt boosting me thods to the problem of minimizing the logistic loss used in logistic regression. The result is a family of ne w algorithms for both problems together with convergence proofs for the new algorithms as well as AdaBoos t. For both AdaBoost and logistic regression, we attempt to cho ose the parameters or weights associated with a given family of functions calledfeaturesor, in the boosting literature, weak hypotheses . AdaBoost works by sequentially updating these parameters one by one. That is, on each of a series of iterations, a single feature (weak hypothesis) is chosen and the paramete r associated with that single feature is adjusted. In contrast, methods for logistic regression, most notably iterative scaling (Darroch & Ratcliff, 1972; Della Pietra, Della Pietra, & Lafferty, 1997), update all pa rameters in parallel on each iteration. Our first new algorithm is a method for optimizing the exponen tial loss using parallel updates. It seems plausible that a parallel-update method will often converg e faster than a sequential-update method, provided that the number of features is not so large as to make parallel updates infeasible. A few experiments described at the end of this paper suggest that this is the cas e. Our second algorithm is a parallel-update method for the log istic loss. Although parallel-update algorithms are well known for this function, the updates tha t we derive are new. Because of the unified treatment we give to the exponential and logistic loss funct io s, we are able to present and prove the convergence of the algorithms for these two losses simultan eously. The same is true for the other algorithms presented in this paper as well. We next describe and analyze sequential-update algorithms for the two loss functions. For exponential loss, this algorithm is equivalent to the AdaBoost algorith m of Freund and Schapire (1997). By viewing the algorithm in our framework, we are able to prove that AdaB oost correctly converges to the minimum of the exponential loss function. This is a new result: Altho ugh Kivinen and Warmuth (1999) and Mason et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3282,
                                "start": 147
                            }
                        ],
                        "text": "We give a unified account of boosting and logistic regression in which we show that both learning problems can be cast in terms of optimization of Bregman distances. In our framework, the two problems become very similar, the only real difference being in the choice of Breg man distance: unnormalized relative entropy for boosting, and binary relative entropy for logistic regress ion. The similarity of the two problems in our framework allows us to design and analyze algorithms for both simultaneously. We are now able to borrow methods fr om the maximum-entropy literature for logistic regression and apply them to the exponential loss u sed by AdaBoost, especially convergence-proof techniques. Conversely, we can now easily adapt boosting me thods to the problem of minimizing the logistic loss used in logistic regression. The result is a family of ne w algorithms for both problems together with convergence proofs for the new algorithms as well as AdaBoos t. For both AdaBoost and logistic regression, we attempt to cho ose the parameters or weights associated with a given family of functions calledfeaturesor, in the boosting literature, weak hypotheses . AdaBoost works by sequentially updating these parameters one by one. That is, on each of a series of iterations, a single feature (weak hypothesis) is chosen and the paramete r associated with that single feature is adjusted. In contrast, methods for logistic regression, most notably iterative scaling (Darroch & Ratcliff, 1972; Della Pietra, Della Pietra, & Lafferty, 1997), update all pa rameters in parallel on each iteration. Our first new algorithm is a method for optimizing the exponen tial loss using parallel updates. It seems plausible that a parallel-update method will often converg e faster than a sequential-update method, provided that the number of features is not so large as to make parallel updates infeasible. A few experiments described at the end of this paper suggest that this is the cas e. Our second algorithm is a parallel-update method for the log istic loss. Although parallel-update algorithms are well known for this function, the updates tha t we derive are new. Because of the unified treatment we give to the exponential and logistic loss funct io s, we are able to present and prove the convergence of the algorithms for these two losses simultan eously. The same is true for the other algorithms presented in this paper as well. We next describe and analyze sequential-update algorithms for the two loss functions. For exponential loss, this algorithm is equivalent to the AdaBoost algorith m of Freund and Schapire (1997). By viewing the algorithm in our framework, we are able to prove that AdaB oost correctly converges to the minimum of the exponential loss function. This is a new result: Altho ugh Kivinen and Warmuth (1999) and Mason et al. (1999) have given convergence proofs for AdaBoost , their proofs depend on assumptions about the given minimization problem which may not hold in all case s. Our proof holds in general without such assumptions. Our unified view leads directly to a sequential-update algor ithm for logistic regression that is only a minor modification of AdaBoost and which is very similar to the algo rithm proposed by Duffy and Helmbold (1999). Like AdaBoost, this algorithm can be used in conjunction wit h any classification algorithm, usually called the weak learning algorithm, that can accept a distribution over examples and return a weak hypothesis with low error rate with respect to the distribution."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2628,
                                "start": 147
                            }
                        ],
                        "text": "We give a unified account of boosting and logistic regression in which we show that both learning problems can be cast in terms of optimization of Bregman distances. In our framework, the two problems become very similar, the only real difference being in the choice of Breg man distance: unnormalized relative entropy for boosting, and binary relative entropy for logistic regress ion. The similarity of the two problems in our framework allows us to design and analyze algorithms for both simultaneously. We are now able to borrow methods fr om the maximum-entropy literature for logistic regression and apply them to the exponential loss u sed by AdaBoost, especially convergence-proof techniques. Conversely, we can now easily adapt boosting me thods to the problem of minimizing the logistic loss used in logistic regression. The result is a family of ne w algorithms for both problems together with convergence proofs for the new algorithms as well as AdaBoos t. For both AdaBoost and logistic regression, we attempt to cho ose the parameters or weights associated with a given family of functions calledfeaturesor, in the boosting literature, weak hypotheses . AdaBoost works by sequentially updating these parameters one by one. That is, on each of a series of iterations, a single feature (weak hypothesis) is chosen and the paramete r associated with that single feature is adjusted. In contrast, methods for logistic regression, most notably iterative scaling (Darroch & Ratcliff, 1972; Della Pietra, Della Pietra, & Lafferty, 1997), update all pa rameters in parallel on each iteration. Our first new algorithm is a method for optimizing the exponen tial loss using parallel updates. It seems plausible that a parallel-update method will often converg e faster than a sequential-update method, provided that the number of features is not so large as to make parallel updates infeasible. A few experiments described at the end of this paper suggest that this is the cas e. Our second algorithm is a parallel-update method for the log istic loss. Although parallel-update algorithms are well known for this function, the updates tha t we derive are new. Because of the unified treatment we give to the exponential and logistic loss funct io s, we are able to present and prove the convergence of the algorithms for these two losses simultan eously. The same is true for the other algorithms presented in this paper as well. We next describe and analyze sequential-update algorithms for the two loss functions. For exponential loss, this algorithm is equivalent to the AdaBoost algorith m of Freund and Schapire (1997). By viewing the algorithm in our framework, we are able to prove that AdaB oost correctly converges to the minimum of the exponential loss function."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2860,
                                "start": 147
                            }
                        ],
                        "text": "We give a unified account of boosting and logistic regression in which we show that both learning problems can be cast in terms of optimization of Bregman distances. In our framework, the two problems become very similar, the only real difference being in the choice of Breg man distance: unnormalized relative entropy for boosting, and binary relative entropy for logistic regress ion. The similarity of the two problems in our framework allows us to design and analyze algorithms for both simultaneously. We are now able to borrow methods fr om the maximum-entropy literature for logistic regression and apply them to the exponential loss u sed by AdaBoost, especially convergence-proof techniques. Conversely, we can now easily adapt boosting me thods to the problem of minimizing the logistic loss used in logistic regression. The result is a family of ne w algorithms for both problems together with convergence proofs for the new algorithms as well as AdaBoos t. For both AdaBoost and logistic regression, we attempt to cho ose the parameters or weights associated with a given family of functions calledfeaturesor, in the boosting literature, weak hypotheses . AdaBoost works by sequentially updating these parameters one by one. That is, on each of a series of iterations, a single feature (weak hypothesis) is chosen and the paramete r associated with that single feature is adjusted. In contrast, methods for logistic regression, most notably iterative scaling (Darroch & Ratcliff, 1972; Della Pietra, Della Pietra, & Lafferty, 1997), update all pa rameters in parallel on each iteration. Our first new algorithm is a method for optimizing the exponen tial loss using parallel updates. It seems plausible that a parallel-update method will often converg e faster than a sequential-update method, provided that the number of features is not so large as to make parallel updates infeasible. A few experiments described at the end of this paper suggest that this is the cas e. Our second algorithm is a parallel-update method for the log istic loss. Although parallel-update algorithms are well known for this function, the updates tha t we derive are new. Because of the unified treatment we give to the exponential and logistic loss funct io s, we are able to present and prove the convergence of the algorithms for these two losses simultan eously. The same is true for the other algorithms presented in this paper as well. We next describe and analyze sequential-update algorithms for the two loss functions. For exponential loss, this algorithm is equivalent to the AdaBoost algorith m of Freund and Schapire (1997). By viewing the algorithm in our framework, we are able to prove that AdaB oost correctly converges to the minimum of the exponential loss function. This is a new result: Altho ugh Kivinen and Warmuth (1999) and Mason et al. (1999) have given convergence proofs for AdaBoost , their proofs depend on assumptions about the given minimization problem which may not hold in all case s."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4105,
                                "start": 147
                            }
                        ],
                        "text": "We give a unified account of boosting and logistic regression in which we show that both learning problems can be cast in terms of optimization of Bregman distances. In our framework, the two problems become very similar, the only real difference being in the choice of Breg man distance: unnormalized relative entropy for boosting, and binary relative entropy for logistic regress ion. The similarity of the two problems in our framework allows us to design and analyze algorithms for both simultaneously. We are now able to borrow methods fr om the maximum-entropy literature for logistic regression and apply them to the exponential loss u sed by AdaBoost, especially convergence-proof techniques. Conversely, we can now easily adapt boosting me thods to the problem of minimizing the logistic loss used in logistic regression. The result is a family of ne w algorithms for both problems together with convergence proofs for the new algorithms as well as AdaBoos t. For both AdaBoost and logistic regression, we attempt to cho ose the parameters or weights associated with a given family of functions calledfeaturesor, in the boosting literature, weak hypotheses . AdaBoost works by sequentially updating these parameters one by one. That is, on each of a series of iterations, a single feature (weak hypothesis) is chosen and the paramete r associated with that single feature is adjusted. In contrast, methods for logistic regression, most notably iterative scaling (Darroch & Ratcliff, 1972; Della Pietra, Della Pietra, & Lafferty, 1997), update all pa rameters in parallel on each iteration. Our first new algorithm is a method for optimizing the exponen tial loss using parallel updates. It seems plausible that a parallel-update method will often converg e faster than a sequential-update method, provided that the number of features is not so large as to make parallel updates infeasible. A few experiments described at the end of this paper suggest that this is the cas e. Our second algorithm is a parallel-update method for the log istic loss. Although parallel-update algorithms are well known for this function, the updates tha t we derive are new. Because of the unified treatment we give to the exponential and logistic loss funct io s, we are able to present and prove the convergence of the algorithms for these two losses simultan eously. The same is true for the other algorithms presented in this paper as well. We next describe and analyze sequential-update algorithms for the two loss functions. For exponential loss, this algorithm is equivalent to the AdaBoost algorith m of Freund and Schapire (1997). By viewing the algorithm in our framework, we are able to prove that AdaB oost correctly converges to the minimum of the exponential loss function. This is a new result: Altho ugh Kivinen and Warmuth (1999) and Mason et al. (1999) have given convergence proofs for AdaBoost , their proofs depend on assumptions about the given minimization problem which may not hold in all case s. Our proof holds in general without such assumptions. Our unified view leads directly to a sequential-update algor ithm for logistic regression that is only a minor modification of AdaBoost and which is very similar to the algo rithm proposed by Duffy and Helmbold (1999). Like AdaBoost, this algorithm can be used in conjunction wit h any classification algorithm, usually called the weak learning algorithm, that can accept a distribution over examples and return a weak hypothesis with low error rate with respect to the distribution. However, th is new algorithm provably minimizes the logistic loss rather than the arguably less natural exponential loss used by AdaBoost. A potentially important advantage of the new algorithm for l gistic regression is that the weights that it places on examples are bounded in [0;1]. This suggests that it may be possible to use the new algorith m in a setting in which the boosting algorithm selects example s to present to the weak learning algorithm by filtering a stream of examples (such as a very large dataset ). As pointed out by Watanabe (1999) and Domingo and Watanabe (2000), this is not possible with AdaBo ost since its weights may become extremely large."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 539,
                                "start": 172
                            }
                        ],
                        "text": "Our work is based directly on the general setting of Lafferty , Della Pietra and Della Pietra (1997) in which one attempts to solve optimization problems based on g eneral Bregman distances. They gave a method for deriving and analyzing parallel-update algorit hms in this setting through the use of auxiliary functions. All of our algorithms and convergence proofs are based on this method. Our work builds on several previous papers which have compar ed boosting approaches to logistic regression. Friedman, Hastie and Tibshirani (2000) first no ted the similarity between the boosting and logistic regression loss functions, and derived the sequen tial-update algorithm LogitBoost for the logistic loss."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 166
                            }
                        ],
                        "text": "The information-geometric view that we take also shows that some of the algorithms we study, including AdaBoost, fit into a family of algorithms described in 1967 by Bregman (1967), and elaborated upon by Censor and Lent (1981), for satisfying a set of constraints."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 121309410,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "44a6b76e5cbc61330663d0a9f393caf91a3a1be8",
            "isKey": true,
            "numCitedBy": 2440,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-relaxation-method-of-finding-the-common-point-Bregman",
            "title": {
                "fragments": [],
                "text": "The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2586148"
                        ],
                        "name": "L. Mason",
                        "slug": "L.-Mason",
                        "structuredName": {
                            "firstName": "Llew",
                            "lastName": "Mason",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Mason"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47392513"
                        ],
                        "name": "Jonathan Baxter",
                        "slug": "Jonathan-Baxter",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Baxter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Baxter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40073871"
                        ],
                        "name": "Marcus Frean",
                        "slug": "Marcus-Frean",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Frean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcus Frean"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 63
                            }
                        ],
                        "text": "It has been noted by Breiman [3, 4] and variou s later authors [17, 22, 23, 24] that both of these steps are done in such a way as to (approximately) cau se the greatest decrease in the exponential loss."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "\u2019s \u201cAnyBoost\u201d family of algorithms [22]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[22] have given convergence proofs for AdaBoost, their proofs de pend on assumptions about the given minimization problem which may not hold in all cases."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60744708,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f5e23d650853dc7f3dbe4370d4ace6be55f931ae",
            "isKey": true,
            "numCitedBy": 324,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Introduction, Optimizing Cost Functions of the Margin, A Gradient Descent View of Voting Methods, Theoretically Motivated Cost Functions, Convergence Results, Experiments, Conclusions, Acknowledgments"
            },
            "slug": "Functional-Gradient-Techniques-for-Combining-Mason-Baxter",
            "title": {
                "fragments": [],
                "text": "Functional Gradient Techniques for Combining Hypotheses"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "This chapter contains sections titled: Introduction, Optimizing Cost Functions of the Margin, A Gradient Descent View of Voting Methods, Theoretically Motivated Cost Functions, Convergence Results, Experiments, Conclusions, and Acknowledgments."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756798"
                        ],
                        "name": "I. Csisz\u00e1r",
                        "slug": "I.-Csisz\u00e1r",
                        "structuredName": {
                            "firstName": "Imre",
                            "lastName": "Csisz\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Csisz\u00e1r"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 119485712,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "84d129f24907c735ca4e4f5f7568db1243ddc00d",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of minimizing a functional over a convex set of non-negative functions is considered, when the functional to be minimized is an f-entropy, or f-divergence resp. Bregman distance from a given function."
            },
            "slug": "Generalized-projections-for-non-negative-functions-Csisz\u00e1r",
            "title": {
                "fragments": [],
                "text": "Generalized projections for non-negative functions"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The problem of minimizing a functional over a convex set of non-negative functions is considered, when the functional to be minimized is an f-entropy, or f-divergence resp."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1995 IEEE International Symposium on Information Theory"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388387856"
                        ],
                        "name": "N. Cesa-Bianchi",
                        "slug": "N.-Cesa-Bianchi",
                        "structuredName": {
                            "firstName": "Nicol\u00f2",
                            "lastName": "Cesa-Bianchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cesa-Bianchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32150954"
                        ],
                        "name": "S. Goldman",
                        "slug": "S.-Goldman",
                        "structuredName": {
                            "firstName": "Sally",
                            "lastName": "Goldman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Goldman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 49319948,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "f71c68f0c7a94851f06de396dff2e2588ad44768",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Proceedings of the Annual Workshops on Computational Learning Theory, available for 1988-2000."
            },
            "slug": "Proceedings-of-the-Thirteenth-Annual-Conference-on-Cesa-Bianchi-Goldman",
            "title": {
                "fragments": [],
                "text": "Proceedings of the Thirteenth Annual Conference on Computational Learning Theory (COLT 2000), June 28 - July 1, 2000, Palo Alto, California, USA"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Proceedings of the Annual Workshops on Computational Learning Theory, available for 1988-2000."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 302,
                                "start": 23
                            }
                        ],
                        "text": "We take Theorem 1 from Lafferty, Della Pietra and Della Pietr a (1997). We do not give the full details of the conditions that F must satisfy for this theorem to hold since these go beyond th e scope of the present paper. Instead, we refer the reader to Della Pietra, Della Pi etra and Lafferty (2001) for a precise formulation of these conditions and a complete proof."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 483,
                                "start": 23
                            }
                        ],
                        "text": "We take Theorem 1 from Lafferty, Della Pietra and Della Pietr a (1997). We do not give the full details of the conditions that F must satisfy for this theorem to hold since these go beyond th e scope of the present paper. Instead, we refer the reader to Della Pietra, Della Pi etra and Lafferty (2001) for a precise formulation of these conditions and a complete proof. A proof for the case of (normalized) relative entropy is given by Della Pietra, Della Pietra and Lafferty (1997). Moreover, t heir proof requires very minor modifications for all of the cases considered in the present paper."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 23
                            }
                        ],
                        "text": "We take Theorem 1 from Lafferty, Della Pietra and Della Pietr a (1997). We do not give the full details of the conditions that F must satisfy for this theorem to hold since these go beyond th e scope of the present paper."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 656,
                                "start": 23
                            }
                        ],
                        "text": "We take Theorem 1 from Lafferty, Della Pietra and Della Pietr a (1997). We do not give the full details of the conditions that F must satisfy for this theorem to hold since these go beyond th e scope of the present paper. Instead, we refer the reader to Della Pietra, Della Pi etra and Lafferty (2001) for a precise formulation of these conditions and a complete proof. A proof for the case of (normalized) relative entropy is given by Della Pietra, Della Pietra and Lafferty (1997). Moreover, t heir proof requires very minor modifications for all of the cases considered in the present paper. Closely rel ated results are given by Censor and Lent (1981) and Csisz\u00e1r (1991, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Boosting as entropy pro"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 31
                            }
                        ],
                        "text": "This is a new result: Although Kivinen and Warmuth (1999) and Mason et al. (1999) have given convergence proofs for AdaBoost, their proofs depend on assumptions about the given minimization problem which may not hold in all cases."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "Although most of this paper considers only the binary case in which there are just two possible labels associated with each example, it turns out that the multiclass case requires no additional work."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 52
                            }
                        ],
                        "text": "In fact, this algorithm is the same as one given by Duffy and Helmbold (1999) except for the choice of \u03b1t ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 22
                            }
                        ],
                        "text": "This is a new result: Although Kivinen and Warmuth [16] and Mason et al. [19] have given convergence proofs for AdaBoost, their proofs depend on assumptions about the given minimization problem which may not hold in all cases."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "Although minimization of the number of classification errors may be a worthwhile goal, in its most general form, the problem is intractable (see, for instance, [15])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "Although parallel-update algorithms are well known for this function, the updates that we derive are new, and preliminary experiments indicate that these new updates may also be much faster."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 31
                            }
                        ],
                        "text": "This is a new result: Although Kivinen and Warmuth (1999) and Mason et al."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Potential boosters? In Advances in neural information processing systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 39
                            }
                        ],
                        "text": "Generalizedandimprovediterative scaling[11, 12] arepopularparallel-updatemethodsfor minimizing this loss."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 129
                            }
                        ],
                        "text": "AdaBoostworksby sequentially updatingtheseparametersoneby one,whereasmethodsfor logistic regression,mostnotablyiterative scaling [11, 12], updateall parametersin parallelon eachiteration."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 95
                            }
                        ],
                        "text": "In this section,we describethegeneralizediterative scaling(GIS) procedureof DarrochandRatcliff [11] for comparisonto our algorithms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 90
                            }
                        ],
                        "text": "Forcomparison, wealsodescribethegeneralizediterativescalingalgorithmof DarrochandRatcliff [11]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 65
                            }
                        ],
                        "text": "Parallel-updatemethodsfor LogLoss arewell known (see,for example,[11, 12])."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalizediterative scalingfor log-linearmodels.TheAnnalsof"
            },
            "venue": {
                "fragments": [],
                "text": "Mathematical Statistics,"
            },
            "year": 1972
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 107
                            }
                        ],
                        "text": "Variants of our sequential-update algorithms fit into the general family of \u201carcing\u201d algorithms presented by Breiman [4, 3], as well as Mason et al.\u2019s \u201cAnyBoost\u201d family of algorithms [19]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 117
                            }
                        ],
                        "text": "Variants of our sequential-update algorithms fit into the general family of \u201carcing\u201d algorithms presented by Breiman [4, 3], as well as Mason et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 21
                            }
                        ],
                        "text": "It has been noted by Breiman [3, 4] and various later authors that both of these\nsteps are done in such a way as to (approximately) cause the greatest decrease in the exponential loss."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 29
                            }
                        ],
                        "text": "It has been noted by Breiman [3, 4] and various later authors that both of these"
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Prediction games and arcing classifiers"
            },
            "venue": {
                "fragments": [],
                "text": "Te chnical Report 504, Statistics Department,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 68
                            }
                        ],
                        "text": "Parallelupdate methods for LogLossare well known (see, for example, [9, 10])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 140
                            }
                        ],
                        "text": "AdaBoost works by sequentially updating these parameters one by one, whereas methods for logistic regression,most notably iterative scaling [9, 10], are iterative but update all parameters in parallel on each iteration."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 118
                            }
                        ],
                        "text": "9 A COMPARISON TO ITERATIVE\nSCALING\nIn this section, we describe the generalized iterative scaling (GIS) procedure of Darroch and Ratcliff [9] for comparison to our algorithms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 100
                            }
                        ],
                        "text": "For comparison,we also describe the generalized iterative scaling algorithm of Darroch and Ratcliff [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 103
                            }
                        ],
                        "text": "In this section, we describe the generalized iterative scaling (GIS) procedure of Darroch and Ratcliff [9] for comparison to our algorithms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 43
                            }
                        ],
                        "text": "Generalized and improved iterative scaling [9, 10] are popular parallel-update methods for minimizing this loss."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalized iterative sca  ling for log-linear models.The Annals of Mathematical Statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 152
                            }
                        ],
                        "text": "As mentioned above, this algorithm is essentially equivale nt to AdaBoost, specifically, the version of AdaBoost first presented by Freund and Schapire [16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 46
                            }
                        ],
                        "text": "For instance, the boosting algorithm AdaBoost [16, 24] is based on the exponen tial loss m Xi=1 exp ( yif (xi)): (2)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 101
                            }
                        ],
                        "text": "For exponential loss, this algorithm is equivalent to the AdaBoost algorith m of Freund and Schapire [16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 369,
                                "start": 350
                            }
                        ],
                        "text": "1) Let 1 = 0 For t = 1; 2; : : : :\nq\nt\n= (M\nt ) q0\nj\nt = arg max j\nm\nX\ni=1\nq\nt;i\nM\nij\nr\nt\n=\nm\nX\ni=1\nq\nt;i\nM\nij\nt\nZ\nt\n=\nm\nX\ni=1\nq\nt;i\nt\n= 1 2 ln Z t + r t Z\nt\nr\nt\nt;j\n=\nt if j = j t\n0 else Update parameters:\nt+1 = t + t\nAs mentioned above, this algorithm is essentially equivalent to AdaBoost, specifically, the version of AdaBoost first presented by Freund and Schapire [13]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 80
                            }
                        ],
                        "text": "For exponential loss, this algorithm is equivalent to the AdaBoost algorithm of Freund and Schapire [13]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A decision-theoret ic generalization of on-line learning and an application to boosting.Journal of Computer and System Sciences"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 139
                            }
                        ],
                        "text": "9 A COMPARISON TO ITERATIVE SCALING In this section, we describe the generalized iterative scaling (GIS) procedure of Darroch and Ratcliff [9] for comparison to our algorithms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 69
                            }
                        ],
                        "text": "Parallelupdate methods for LogLoss are well known (see, for example, [9, 10])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 140
                            }
                        ],
                        "text": "AdaBoost works by sequentially updating these parameters one by one, whereas methods for logistic regression,most notably iterative scaling [9, 10], are iterative but update all parameters in parallel on each iteration."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 118
                            }
                        ],
                        "text": "9 A COMPARISON TO ITERATIVE\nSCALING\nIn this section, we describe the generalized iterative scaling (GIS) procedure of Darroch and Ratcliff [9] for comparison to our algorithms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 100
                            }
                        ],
                        "text": "For comparison,we also describe the generalized iterative scaling algorithm of Darroch and Ratcliff [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 76
                            }
                        ],
                        "text": "i=1 ln 1+ exp y i f (x i ) : (3) Generalized and improved iterative scaling [9, 10] are popular parallel-update methods for minimizing this loss."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalized iterative sca ling for log-linear models.The"
            },
            "venue": {
                "fragments": [],
                "text": "Annals of Mathematical Statistics,"
            },
            "year": 1972
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 13
                            }
                        ],
                        "text": "However, the Bregman distance that they used differed slightly from the one that we have chosen (normalized relative entropy rather than unnormalized relative entropy) so that AdaBoost\u2019s fit in this model was not quite complete; in particular, their convergence proof depended on assumptions that do not hold in general."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 57
                            }
                        ],
                        "text": "They showed that the updateused Morespecifically, Bregman[2] describesoptimizationmethodsbasedonBregmandistanceswhereoneconstraintis satisfiedat eachiteration,for example,amethodwheretheconstraintwhichmakesthemostimpactontheobjectivefunctionis greedilychosen at eachiteration."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 69
                            }
                        ],
                        "text": "There is a natural optimization problem that can be asociated with a Bregman distance, namely, to find the vector p 2 \u2206 that is closest to a given vectorq0 2 \u2206 subject to a set of linear constraints."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 158
                            }
                        ],
                        "text": "The information-geometric view that we take also shows that the algorithms we study, including AdaBoost, fit into a family of algorithms described in 1967 by Bregman [2] for satisfying a set of constraints."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 168
                            }
                        ],
                        "text": "Our work is based directly on the general setting of Lafferty, Della Pietra and Della Pietra [18] in which one attempts to solve optimization problems based on general Bregman distances."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 41
                            }
                        ],
                        "text": "Statistical learning algorithms based on Bregman distances."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 109
                            }
                        ],
                        "text": "In our framework, the two problems become extremely similar, the only real difference being in the choice of Bregman distance: unnormalized relative entropy for boosting, and binary relative entropy for logistic regression."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 10
                            }
                        ],
                        "text": "[2] L. M. Bregman."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 162
                            }
                        ],
                        "text": "1 INTRODUCTION\nWe give a unified account of boosting and logistic regression in which we show that both learning problems can be cast in terms of optimization of Bregman distances."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 139
                            }
                        ],
                        "text": "The only important difference is in the choice of the functionF , namely,\nF (p) =\nm\nX\ni=1\np\ni ln p i + (1 p i ) ln(1 p i ) :\nThe resulting Bregman distance is\nD B p k q =\nm\nX\ni=1\np\ni\nln\np\ni\nq\ni\n+ (1 p i ) ln\n1 p i\n1 q i\n:\nTrivially,\nD B 0 k q =\nm\nX\ni=1\nln(1 q i ): (14)\nFor this choice ofF , it can be verified using calculus that\n(v q)\ni\n=\nq\ni\ne\nv\ni\n1 q i + q i e v i\n(15)\nso that\nQ =\n8\n<\n:\nq 2 [0; 1]m q\ni\n=\n0\n@\nn\nX\nj=1\nj\ny\ni\nh\nj\n(x\ni\n)\n1\nA\n; 2 R\nn\n9\n=\n;\n:\nwhere (x) = (1+ ex) 1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 140
                            }
                        ],
                        "text": "Lafferty [17] went further in studying the relationship\nbetween logistic regression and the exponential loss through the use of a family of Bregman distances."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 55
                            }
                        ],
                        "text": "Kivinen and Warmuth also described updates for general Bregman distances including, as one of their examples, the Bregman distance that we use to capture logistic regression."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 11
                            }
                        ],
                        "text": "The use of Bregman distances that we describe has important differences leading to a natural treatment of the exponential loss and a new view of logistic regression."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 192
                            }
                        ],
                        "text": "The convex functionF that we use for this case is\nF (p) =\nm\nX\ni=1\n2\n4\nX\n` 6=y\ni\np\ni;` ln p i;` + (1 p\u0304 i ) ln(1 p\u0304 i )\n3\n5\nwhich is defined over the space\n\u2206 = n p 2 R B\n+\nj 8i : p\u0304 i\n1 o :\nThe resulting Bregman distance is\nB\nF\np k q\n=\nm\nX\ni=1\n2\n4\nX\n` 6=y\np\ni;`\nln\np\ni;`\nq\ni;`\n+ (1 p\u0304 i ) ln\n1 p\u0304 i\n1 q\u0304 i\n3\n5\n:\nClearly,\nB\nF\n0 k q =\nm\nX\ni=1\nln(1 q\u0304 i ):\nIt can be shown that\n(v q)\n(i;`)\n=\nq\ni;`\ne\nv\ni;`\n1 q\u0304 i +\nP\n` 6=y\ni\nq\ni;`\ne\nv\ni;`\n:\nAssumption 1 can be verified by noting that\nB\nF\n0 k v q ) B\nF\n0 k q\n=\nm\nX\ni=1\nln\n1 q\u0304 i\n1 (v q) i\n!"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 75
                            }
                        ],
                        "text": "ACKNOWLEDGMENTS\nMany thanks to Manfred Warmuth for first teaching us about Bregman distances and for many comments on an earlier draft."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 90
                            }
                        ],
                        "text": "3 BREGMAN-DISTANCE OPTIMIZATION\nIn this section, we give background on optimization using Bregman distances."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 49
                            }
                        ],
                        "text": "Section 3 gives background on optimization using Bregman distances, and Section 4 then describes how boosting and logistic regression can be cast within this framework."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 161
                            }
                        ],
                        "text": "Theinformation-geometric view thatwe take alsoshows thatsomeof thealgorithmswe study, includingAdaBoost,fit into a family of algorithmsdescribedin 1967by Bregman[2] for satisfyingasetof constraints."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 4
                            }
                        ],
                        "text": "The Bregman distance associated withF is defined for p;q 2 \u2206 to be\nB\nF\np k q\n:\n= F (p) F (q) rF (q) (p q):\nFor instance, when\nF (p) =\nm\nX\ni=1\np\ni ln p i ; (4)\nB\nF\nis the (unnormalized) relative entropy\nD U p k q =\nm\nX\ni=1\np\ni\nln\np\ni\nq\ni\n+ q\ni\np\ni\n:\nIt can be shown that, in general, every Bregman distance is nonnegative and is equal to zero if and only if its two arguments are equal."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The relaxationmethodof finding the commonpoint of convex setsandits applicationto the solutionof problemsin convex programming.U.S.S.R"
            },
            "venue": {
                "fragments": [],
                "text": "ComputationalMathematicsandMathematicalPhysics,"
            },
            "year": 1967
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 43
                            }
                        ],
                        "text": "Generalized and improved iterative scaling [11, 12] are pop ular parallel-update methods for minimizing this loss."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 70
                            }
                        ],
                        "text": "Parallel-update methods for LogLoss are well known (see, for example, [11, 12])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 101
                            }
                        ],
                        "text": "For comparison, we also describe the generalized iterative scaling algorithm of Darroch and Ratcliff [11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 104
                            }
                        ],
                        "text": "In this section, we describe the generalized iterative scal ing (GIS) procedure of Darroch and Ratcliff [11] for comparison to our algorithms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 118
                            }
                        ],
                        "text": "9 A COMPARISON TO ITERATIVE\nSCALING\nIn this section, we describe the generalized iterative scaling (GIS) procedure of Darroch and Ratcliff [9] for comparison to our algorithms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 79
                            }
                        ],
                        "text": "For comparison,we also describe the generalized iterative scaling algorithm of Darroch and Ratcliff [9]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 142
                            }
                        ],
                        "text": "AdaBoost works by sequentially updating these parameters one by one, whereas methods for lo gistic regression, most notably iterative scaling [11, 12], update all parameters in parallel on each itera tion."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalized iterative sc aling for log-linear models.The"
            },
            "venue": {
                "fragments": [],
                "text": "Annals of Mathematical Statistics,"
            },
            "year": 1972
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 102
                            }
                        ],
                        "text": "Proof sketch: As noted above, a complete and general proof is given by Della Pietra, Della Pietra and Lafferty (2001). However, the proof given by Della Pietra, D ella Pietra and Lafferty (1997) for normalized relative entropy can be modified very easily for all of the cas es of interest in the present paper."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 76
                            }
                        ],
                        "text": "The improved i t rative scaling algorithm of Della Pietra, Della Pietra and Lafferty (1997) also requires only these mi lder assumptions but is more complicated to implement, requiring a numerical search (such as Newton-Ra phson) for each feature on each iteration."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 97
                            }
                        ],
                        "text": "To prove convergence, we use the auxiliary-function techni que of Della Pietra, Della Pietra and Lafferty (1997). Very roughly, the idea of the proof is to derive a nonnegative lower bound called an auxiliary function on how much the loss decreases on each iteration."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Additive models, boosting and inferen"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 82
                            }
                        ],
                        "text": "Ourwork is baseddirectlyon thegeneralsettingof Lafferty, DellaPietraandDellaPietra[21] in which oneattemptsto solve optimizationproblemsbasedon generalBregmandistances."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 73
                            }
                        ],
                        "text": "Wetake thestatement of thistheoremfromLafferty, DellaPietraandDellaPietra[21]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "Theparticularset-upthatwefollow is takenprimarily from Lafferty, DellaPietraandDella Pietra[21]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "StephenDella Pietra,andVincentDella Pietra.Statisticallearningalgorithmsbasedon Bregmandistances.In"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedingsof theCanadianWorkshopon InformationTheory,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 100
                            }
                        ],
                        "text": "For exponential loss, this algorithm is equivalent to the AdaBoost algorithm of Freund and Schapire [13]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 369,
                                "start": 350
                            }
                        ],
                        "text": "1) Let 1 = 0 For t = 1; 2; : : : :\nq\nt\n= (M\nt ) q0\nj\nt = arg max j\nm\nX\ni=1\nq\nt;i\nM\nij\nr\nt\n=\nm\nX\ni=1\nq\nt;i\nM\nij\nt\nZ\nt\n=\nm\nX\ni=1\nq\nt;i\nt\n= 1 2 ln Z t + r t Z\nt\nr\nt\nt;j\n=\nt if j = j t\n0 else Update parameters:\nt+1 = t + t\nAs mentioned above, this algorithm is essentially equivalent to AdaBoost, specifically, the version of AdaBoost first presented by Freund and Schapire [13]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 151
                            }
                        ],
                        "text": "As mentioned above, this algorithm is essentially equivalent to AdaBoost, specifically, the version of AdaBoost first presented by Freund and Schapire [13]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 46
                            }
                        ],
                        "text": "For instance, the boosting algorithm AdaBoost [13, 20] is based on the exponential loss"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A decision-theoret ic g neralization of on-line learning and an application to boost ing"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Computer and System Sciences,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 42
                            }
                        ],
                        "text": "For instance,the boostingalgorithmAdaBoost[16, 24] is basedon theexponentialloss % B ( @ I J K ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 141
                            }
                        ],
                        "text": "As mentionedabove, this algorithmis essentiallyequivalent to AdaBoost,specifically, the versionof AdaBoostfirst presentedby FreundandSchapire[16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "For exponential loss,this algorithmis equivalentto theAdaBoostalgorithmof FreundandSchapire[16]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Schapire.A decision-theoreticgeneralizationof on-linelearningandanapplication to boosting.Journalof ComputerandSystemSciences"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 44
                            }
                        ],
                        "text": "The improved iterative scaling algorithm of Della Pietra, Della Pietra and Lafferty [10] also requires only these milder assumptions but is much more complicated to implement, requiring a numerical search (such as NewtonRaphson) for each feature on each iteration."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 64
                            }
                        ],
                        "text": "To prove convergence,we use the auxiliary-function technique of Della Pietra, Della Pietra and Lafferty [10]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 63
                            }
                        ],
                        "text": "Our work is based directly on the general setting of Lafferty, Della Pietra and Della Pietra [18] in which one attempts to solve optimization problems based on general Bregman distances."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 53
                            }
                        ],
                        "text": "We take the statement of this theorem from Lafferty, Della Pietra and Della Pietra [18]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 31
                            }
                        ],
                        "text": "[18] John D. Lafferty, Stephen Della Pietra, and Vincent Della Pietra."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 13
                            }
                        ],
                        "text": "[10] Stephen Della Pietra, Vincent Della Pietra, and John Lafferty."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 66
                            }
                        ],
                        "text": "A proof for the case of (normalized) relative entropy is given by Della Pietra, Della Pietra and Lafferty [10]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 71
                            }
                        ],
                        "text": "The particular set-up that we follow is taken primarily from Lafferty, Della Pietra and Della Pietra [18]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 88
                            }
                        ],
                        "text": "We largely follow the description of GIS given by Berger, Della Pietra and Della Pietra [1] for the multiclass case."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 42
                            }
                        ],
                        "text": "References\n[1] Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A maximum entropy approach to natural language"
            },
            "venue": {
                "fragments": [],
                "text": "processing.Computational Linguistics,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 54
                            }
                        ],
                        "text": "It hasbeennotedby Breiman[3, 4] andvariouslaterauthors[17, 22, 23, 24] thatboth of thesestepsaredonein sucha way asto (approximately)causethegreatestdecreasein theexponential loss."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": "\u2019s \u201cAnyBoost\u201d family of algorithms[22]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[22] havegivenconvergenceproofsfor AdaBoost,theirproofsdependonassumptionsaboutthegivenminimization problemwhichmaynothold in all cases."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "PeterBartlett,andMarcusFrean.Functionalgradienttechniquesfor combining hypotheses"
            },
            "venue": {
                "fragments": [],
                "text": "Advancesin LargeMargin Classifiers"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 44
                            }
                        ],
                        "text": "The improved iterative scaling algorithm of Della Pietra, Della Pietra and Lafferty [10] also requires only these milder assumptions but is much more complicated to implement, requiring a numerical search (such as NewtonRaphson) for each feature on each iteration."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 64
                            }
                        ],
                        "text": "To prove convergence,we use the auxiliary-function technique of Della Pietra, Della Pietra and Lafferty [10]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 63
                            }
                        ],
                        "text": "Our work is based directly on the general setting of Lafferty, Della Pietra and Della Pietra [18] in which one attempts to solve optimization problems based on general Bregman distances."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 53
                            }
                        ],
                        "text": "We take the statement of this theorem from Lafferty, Della Pietra and Della Pietra [18]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 31
                            }
                        ],
                        "text": "[18] John D. Lafferty, Stephen Della Pietra, and Vincent Della Pietra."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 13
                            }
                        ],
                        "text": "[10] Stephen Della Pietra, Vincent Della Pietra, and John Lafferty."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 66
                            }
                        ],
                        "text": "A proof for the case of (normalized) relative entropy is given by Della Pietra, Della Pietra and Lafferty [10]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 71
                            }
                        ],
                        "text": "The particular set-up that we follow is taken primarily from Lafferty, Della Pietra and Della Pietra [18]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 88
                            }
                        ],
                        "text": "We largely follow the description of GIS given by Berger, Della Pietra and Della Pietra [1] for the multiclass case."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 42
                            }
                        ],
                        "text": "References\n[1] Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A maximum entropy approach to natural language processing.Computational Linguistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 55
                            }
                        ],
                        "text": "Our work builds heavily on that of Kivinen and Warmuth [16] who, along with Lafferty, were the first to make a connection between AdaBoost and information geometry."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "This is a new result: Although Kivinen and Warmuth [16] and Mason et al."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 22
                            }
                        ],
                        "text": "This is a new result: Although Kivinen and Warmuth [16] and Mason et al. [19] have given convergence proofs for AdaBoost, their proofs depend on assumptions about the given minimization problem which may not hold in all cases."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 112
                            }
                        ],
                        "text": "Another sequential-update algorithm for a different but related problem was proposed by Cesa-Bianchi, Krogh and Warmuth [5]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Kivinen and Warmuth also described updates for general Bregman distances including, as one of their examples, the Bregman distance that we use to capture logistic regression."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 39
                            }
                        ],
                        "text": "ACKNOWLEDGMENTS\nMany thanks to Manfred Warmuth for first teaching us about Bregman distances and for many comments on an earlier draft."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 34
                            }
                        ],
                        "text": "[16] Jyrki Kivinen and Manfred K. Warmuth."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Boosting as entro py projection"
            },
            "venue": {
                "fragments": [],
                "text": "InProceedings of the Twelfth Annual Conference on Computational Learning Theory,"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756798"
                        ],
                        "name": "I. Csisz\u00e1r",
                        "slug": "I.-Csisz\u00e1r",
                        "structuredName": {
                            "firstName": "Imre",
                            "lastName": "Csisz\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Csisz\u00e1r"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 121942145,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9e1d0d2a04f98a79d857951f9e38cb879bb32416",
            "isKey": false,
            "numCitedBy": 789,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Why-least-squares-and-maximum-entropy-An-axiomatic-Csisz\u00e1r",
            "title": {
                "fragments": [],
                "text": "Why least squares and maximum entropy? An axiomatic approach to inference for linear inverse problems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756798"
                        ],
                        "name": "I. Csisz\u00e1r",
                        "slug": "I.-Csisz\u00e1r",
                        "structuredName": {
                            "firstName": "Imre",
                            "lastName": "Csisz\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Csisz\u00e1r"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 32
                            }
                        ],
                        "text": "The result appears to be due to Csisza\u0301r [6, 7] and Topsoe [21]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 40
                            }
                        ],
                        "text": "The result appears to be due to Csisz\u00e1r [8, 9] and Topsoe [26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 9
                            }
                        ],
                        "text": "See also Csisza\u0301r\u2019s survey article [8]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 123328758,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a205db456e235809fafdb99bd2b1c7b81302c1fa",
            "isKey": false,
            "numCitedBy": 346,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Sanov-Property,-Generalized-$I$-Projection-and-a-Csisz\u00e1r",
            "title": {
                "fragments": [],
                "text": "Sanov Property, Generalized $I$-Projection and a Conditional Limit Theorem"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49223598"
                        ],
                        "name": "J. Darroch",
                        "slug": "J.-Darroch",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Darroch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Darroch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12360582"
                        ],
                        "name": "D. Ratcliff",
                        "slug": "D.-Ratcliff",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Ratcliff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ratcliff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 120862597,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "37c931cbaa9217b829596dd196520a838562a109",
            "isKey": false,
            "numCitedBy": 1329,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Generalized-Iterative-Scaling-for-Log-Linear-Models-Darroch-Ratcliff",
            "title": {
                "fragments": [],
                "text": "Generalized Iterative Scaling for Log-Linear Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145835461"
                        ],
                        "name": "Rajeev Sharma",
                        "slug": "Rajeev-Sharma",
                        "structuredName": {
                            "firstName": "Rajeev",
                            "lastName": "Sharma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rajeev Sharma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3222903"
                        ],
                        "name": "T. Leen",
                        "slug": "T.-Leen",
                        "structuredName": {
                            "firstName": "Todd",
                            "lastName": "Leen",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Leen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800603"
                        ],
                        "name": "M. Pavel",
                        "slug": "M.-Pavel",
                        "structuredName": {
                            "firstName": "Misha",
                            "lastName": "Pavel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pavel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 56885085,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "12a82f8ec267c3973af51caabd09b48bff3809ca",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Advances-in-Neural-Information-Processing-Systems-Sharma-Leen",
            "title": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems 11"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Warmuth . Boosting as entropy projection"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Twelfth Annual Conference on Computational Learning Theory"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Additivelogistic regression : A statistical view of boosting"
            },
            "venue": {
                "fragments": [],
                "text": "The Annals of Statistics"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Received October"
            },
            "venue": {
                "fragments": [],
                "text": "Received October"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Additive models , boostingand inferencefor generalizeddivergences"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedingsof the TwelfthAnnualConferenceonComputationalLearningTheory , pages"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Scaling up a boosting-ba"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "BoosTexter: A boosting"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Scaling up a boosting-ba"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "\u2019s \u201cAnyBoost\u201d family of algorithms [19]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[19] have given convergence proofs for AdaBoost, their proofs depend on assumptions about the given minimization problem which may not hold in all cases."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Functional gradient techniques for combining hypot heses. InAdvances in Large Margin Classifiers"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 33
                            }
                        ],
                        "text": "Theresultappearsto beduetoCsis\u017aar[8, 9] andTopsoe[26]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sanov property, generalizedI-projection anda conditionallimit theorem"
            },
            "venue": {
                "fragments": [],
                "text": "Annalsof Probability,"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 83
                            }
                        ],
                        "text": "We largely follow thedescriptionof GIS given by Berger, Della Pietra andDellaPietra[1] for themulticlasscase."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "StephenA. Della Pietra,andVincentJ.Della Pietra.A maximumentropy approachto natural languageprocessing.ComputationalLinguistics, 22(1):39\u201371,1996"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 65
                            }
                        ],
                        "text": "Closely rel ated results are given by Censor and Lent (1981) and Csisz\u00e1r (1991, 1995). See also Censor and Zenios\u2019s book (1997). Theorem 1 Let ~ p, q 0 , M, , F , B F , P andQ be as above."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalized projections for non-nega"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 51
                            }
                        ],
                        "text": "The result appears to be due to Csisza\u0301r [6, 7] and Topsoe [21]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 58
                            }
                        ],
                        "text": "The result appears to be due to Csisz\u00e1r [8, 9] and Topsoe [26]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 8
                            }
                        ],
                        "text": "[21] F. Topsoe."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information theoretical optimization tech  niques.Kybernetika"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 45
                            }
                        ],
                        "text": "we couldeasilyconvert any systemsuchasSLIPPER[7], BoosTexter [25] or alternatingtrees[15] to use logistic loss."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "CohenandYoramSinger. A simple,fast,andeffective rule learner"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedingsof theSixteenth NationalConferenceonArtificial Intelligence,"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "ditive logistic regression : a statistical view of boosting H\u00f6ffgen and Hans - U . Simon . Robust trainability of single neurons"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 53
                            }
                        ],
                        "text": "Our work is based directly on the general setting of Lafferty , Della Pietra and Della Pietra (1997) in which one attempts to solve optimization problems based on g eneral Bregman distances."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 0
                            }
                        ],
                        "text": "Kivinen and Warmuth (1999) show that the normalized distrib u ion converges in the case that q ? 6= 0."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Relative loss bounds fo"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 61
                            }
                        ],
                        "text": "we couldeasilyconvert any systemsuchasSLIPPER[7], BoosTexter [25] or alternatingtrees[15] to use logistic loss."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "BoosTexter: A boosting-basedsystemfor text categorization"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning,"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Scaling up a boosting-ba"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u1e26offgen and Hans - U . Simon . Robust trainability of single neurons"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Warmuth . Boosting as entropy projection"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Twelfth Annual Conference on Computational Learning Theory"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "H\u00f6ffgen and"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Schapire . A decisiontheoretic generalization of on - line learning and an application to boosting"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Computer and System Sciences"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "SeealsoCsis\u017aar\u2019s survey article[10] aswell asCensorandZenios\u2019 s book[5]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maxent,mathematics,  andinformationtheory"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedingsof theFifteenthInternationalWorkshop on MaximumEntropyandBayesianMethods,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 162
                            }
                        ],
                        "text": "Although minimization of the number of classific ation errors may be a worthwhile goal, in its most general form, the proble m is intractable (see, for instance, [18])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Robust trainabil ity of single neurons"
            },
            "venue": {
                "fragments": [],
                "text": "InProceedings of the Fifth Annual ACM Workshop on Computational Learning Theory  ,"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Additive logistic regression : a statistical view of boost"
            },
            "venue": {
                "fragments": [],
                "text": "The Annals of Statistics , to appear"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 94
                            }
                        ],
                        "text": "we could easily convert any system such as SLIPPER [7], BoosT exter [25] or alternating trees [15] to use logistic loss."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The alternating decision tr  ee learning algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "InMachine Learning: Proceedings of the Sixteenth International Conference"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Schapire and Yoram Singer . BoosTexter : A boostingbased system for text categorization"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Schapire and Yoram Singer . Improved boosting algorithms using confidencerated predictions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Simon . Robusttrainabilityof singleneurons"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedingsof theFifth Annual ACM Workshopon ComputationalLearningTheory"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 35
                            }
                        ],
                        "text": "Our work builds heavily on that of Kivinen and Warmuth (1999) who, along with Lafferty, were the first to make a connection between AdaBoost and information geome try."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Robust trainabilit"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 111
                            }
                        ],
                        "text": "Anothersequential-update algorithmfor a differentbut relatedproblemwas proposedby Cesa-Bianchi, KroghandWarmuth[6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Boundson approximatesteepest  descentfor likelihoodmaximizationin exponentialfamilies"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactionson InformationTheory,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information theoretical optimization techniques. Kybernetika"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Schapire . A decisiontheoretic g neralization of on - line learning and an application to boosting"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Computer and System Sciences"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Schapire . A decisiontheoreticgeneralizationof onlinelearningandanapplication to boosting"
            },
            "venue": {
                "fragments": [],
                "text": "Journalof ComputerandSystemSciences"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A simple, fast, and effecti"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Warmuth . Boundson approximatesteepestdescentfor likelihoodmaximizationin exponentialfamilies"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactionson InformationTheory"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 51
                            }
                        ],
                        "text": "we could easily convert any system such as SLIPPER [7], BoosT exter [25] or alternating trees [15] to use logistic loss."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A simple, fast, and eff  ective rule learner"
            },
            "venue": {
                "fragments": [],
                "text": "InProceedings of the Sixteenth National Conference on Artificial Intelligence"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 33
                            }
                        ],
                        "text": "Theresultappearsto beduetoCsis\u017aar[8, 9] andTopsoe[26]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "I-divergencegeometryof probabilitydistributionsandminimizationproblems.TheAnnalsof Probability, 3(1):146\u2013158"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "I - divergencegeometryof probabilitydistributionsandminimizationproblems"
            },
            "venue": {
                "fragments": [],
                "text": "TheAnnalsof Probability"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 277,
                                "start": 119
                            }
                        ],
                        "text": "9 A comparison to iterative scaling In this section, we describe the generalized iterative scal ing (GIS) procedure of Darroch and Ratcliff (1972) for comparison to our algorithms. We largely follow the desc ription of GIS given by Berger, Della Pietra and Della Pietra (1996) for the multiclass case."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 119
                            }
                        ],
                        "text": "9 A comparison to iterative scaling In this section, we describe the generalized iterative scal ing (GIS) procedure of Darroch and Ratcliff (1972) for comparison to our algorithms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalized iterativ"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Schapire . A decision - theoretic g n - eralization of on - line learning and an application to boosting"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Computer and System Sciences"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Berger , Stephen A . Della Pietra , and Vincent J . Della Pietra . A maximum entropy approach to natural language processing"
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improved boosting algo"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 35
                            }
                        ],
                        "text": "Our work builds heavily on that of Kivinen and Warmuth (1999) who, along with Lafferty, were the first to make a connection between AdaBoost and information geome try."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Robust trainability o"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Additive logistic regression: a statistical view of boosting. The Annals of Statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fromcomputationalearningtheoryto discovery science"
            },
            "venue": {
                "fragments": [],
                "text": "Topsoe . Informationtheoreticaloptimizationtechniques . Kybernetika"
            },
            "year": 1979
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Schapire and Yoram Singer . Improved boostingalgorithms using confidencerated predictions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Prediction games and arcing classifier"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The alternating decision tre"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 25,
            "methodology": 26,
            "result": 4
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 103,
        "totalPages": 11
    },
    "page_url": "https://www.semanticscholar.org/paper/Logistic-Regression,-AdaBoost-and-Bregman-Distances-Collins-Schapire/b54c9359e8858842d1b1b744ac5ca573b8031dcc?sort=total-citations"
}