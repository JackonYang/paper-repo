{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733922"
                        ],
                        "name": "A. Hanson",
                        "slug": "A.-Hanson",
                        "structuredName": {
                            "firstName": "Allen",
                            "lastName": "Hanson",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hanson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389846455"
                        ],
                        "name": "E. Learned-Miller",
                        "slug": "E.-Learned-Miller",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Learned-Miller",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Learned-Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2862313"
                        ],
                        "name": "J. Weinman",
                        "slug": "J.-Weinman",
                        "structuredName": {
                            "firstName": "Jerod",
                            "lastName": "Weinman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weinman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 703895,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "40b2c3e558563404ac0ce1ba2b11c8642651d8f6",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 144,
            "paperAbstract": {
                "fragments": [],
                "text": "Although an automated reader for the blind first appeared nearly two-hundred years ago, computers can currently \"read\" document text about as well as a seven-year-old. Scene text recognition brings many new challenges. A central limitation of current approaches is a feed-forward, bottom-up, pipelined architecture that isolates the many tasks and information involved in reading. The result is a system that commits errors from which it cannot recover and has components that lack access to relevant information. \nWe propose a system for scene text reading that in its design, training, and operation is more integrated. First, we present a simple contextual model for text detection that is ignorant of any recognition. Through the use of special features and data context, this model performs well on the detection task, but limitations remain due to the lack of interpretation. We then introduce a recognition model that integrates several information sources, including font consistency and a lexicon, and compare it to approaches using pipelined architectures with similar information. Next we examine a more unified detection and recognition framework where features are selected based on the joint task of detection and recognition, rather than each task individually. This approach yields better results with fewer features. Finally, we demonstrate a model that incorporates segmentation and recognition at both the character and word levels. Text with difficult layouts and low resolution are more accurately recognized by this integrated approach. By more tightly coupling several aspects of detection and recognition, we hope to establish a new unified way of approaching the problem that will lead to improved performance. We would like computers to become accomplished grammar-school level readers."
            },
            "slug": "Unified-detection-and-recognition-for-reading-text-Hanson-Learned-Miller",
            "title": {
                "fragments": [],
                "text": "Unified detection and recognition for reading text in scene images"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work presents a simple contextual model for text detection that is ignorant of any recognition, and introduces a recognition model that integrates several information sources, including font consistency and a lexicon, and compares it to approaches using pipelined architectures with similar information."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46772671"
                        ],
                        "name": "Xiangrong Chen",
                        "slug": "Xiangrong-Chen",
                        "structuredName": {
                            "firstName": "Xiangrong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiangrong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 61234963,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "37ba7b9a823e8a400046bd149b7756adf5d698da",
            "isKey": false,
            "numCitedBy": 512,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper gives an algorithm for detecting and reading text in natural images. The algorithm is intended for use by blind and visually impaired subjects walking through city scenes. We first obtain a dataset of city images taken by blind and normally sighted subjects. From this dataset, we manually label and extract the text regions. Next we perform statistical analysis of the text regions to determine which image features are reliable indicators of text and have low entropy (i.e. feature response is similar for all text images). We obtain weak classifiers by using joint probabilities for feature responses on and off text. These weak classifiers are used as input to an AdaBoost machine learning algorithm to train a strong classifier. In practice, we trained a cascade with 4 strong classifiers containing 79 features. An adaptive binarization and extension algorithm is applied to those regions selected by the cascade classifier. Commercial OCR software is used to read the text or reject it as a non-text region. The overall algorithm has a success rate of over 90% (evaluated by complete detection and reading of the text) on the test set and the unread text is typically small and distant from the viewer."
            },
            "slug": "Detecting-and-reading-text-in-natural-scenes-Chen-Yuille",
            "title": {
                "fragments": [],
                "text": "Detecting and reading text in natural scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The overall algorithm has a success rate of over 90% (evaluated by complete detection and reading of the text) on the test set and the unread text is typically small and distant from the viewer."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2004"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145815031"
                        ],
                        "name": "S. Lucas",
                        "slug": "S.-Lucas",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Lucas",
                            "middleNames": [
                                "M.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lucas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87531536"
                        ],
                        "name": "A. Panaretos",
                        "slug": "A.-Panaretos",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Panaretos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Panaretos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073684197"
                        ],
                        "name": "Luis Sosa",
                        "slug": "Luis-Sosa",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Sosa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luis Sosa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052189571"
                        ],
                        "name": "Anthony Tang",
                        "slug": "Anthony-Tang",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anthony Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108862960"
                        ],
                        "name": "Shirley Wong",
                        "slug": "Shirley-Wong",
                        "structuredName": {
                            "firstName": "Shirley",
                            "lastName": "Wong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shirley Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114080648"
                        ],
                        "name": "Robert Young",
                        "slug": "Robert-Young",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Young",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Young"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[1]\u2010[4]). ICDAR 1 has organised two competitions (2003 and 2005) for the robust detection of wild text based on a standard set of labelled images. The results are summarised in [ 11 ], [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6379469,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce39eb5cc1049a1060a499d6b6e94c8b2ec11da1",
            "isKey": false,
            "numCitedBy": 591,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the robust reading competitions forICDAR 2003. With the rapid growth in research over thelast few years on recognizing text in natural scenes, thereis an urgent need to establish some common benchmarkdatasets, and gain a clear understanding of the current stateof the art. We use the term robust reading to refer to text imagesthat are beyond the capabilities of current commercialOCR packages. We chose to break down the robust readingproblem into three sub-problems, and run competitionsfor each stage, and also a competition for the best overallsystem. The sub-problems we chose were text locating,character recognition and word recognition.By breaking down the problem in this way, we hope togain a better understanding of the state of the art in eachof the sub-problems. Furthermore, our methodology involvesstoring detailed results of applying each algorithm toeach image in the data sets, allowing researchers to study indepth the strengths and weaknesses of each algorithm. Thetext locating contest was the only one to have any entries.We report the results of this contest, and show cases wherethe leading algorithms succeed and fail."
            },
            "slug": "ICDAR-2003-robust-reading-competitions-Lucas-Panaretos",
            "title": {
                "fragments": [],
                "text": "ICDAR 2003 robust reading competitions"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The robust reading problem was broken down into three sub-problems, and competitions for each stage, and also a competition for the best overall system, which was the only one to have any entries."
            },
            "venue": {
                "fragments": [],
                "text": "Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403625247"
                        ],
                        "name": "N. Ben-Haim",
                        "slug": "N.-Ben-Haim",
                        "structuredName": {
                            "firstName": "Nadav",
                            "lastName": "Ben-Haim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ben-Haim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 32149926,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "70ed10a19ad4d933f7b48918e9c11549075e36f1",
            "isKey": false,
            "numCitedBy": 1,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This thesis addresses the problem of reading image text, which we define here as a digital image of machine printed text. Images of license plates, signs, and scanned documents fall into this category, whereas images of handwriting do not. Automatically reading image text is a very well researched problem, which falls into the broader category of Optical Character Recognition (OCR). Virtually all work in this domain begins by segmenting characters from the image and proceeds with a classification stage to identify each character. This conventional approach is not best suited for task specific recognition such as reading license plates, scanned documents, or freeway signs, which can often be blurry and poor quality. In this thesis, we apply a boosting framework to the character recognition problem, which allows us to avoid character segmentation altogether. This approach allows us to read blurry, poor quality images that are difficult to segment. When there is a constrained domain, there is generally a large amount of training images available. Our approach benefits from this since it is entirely based on machine learning. We perform experiments on hand labeled datasets of low resolution license plate images and demonstrate highly encouraging results. In addition, we show that if enough domain knowledge is available, we can avoid the arduous task of hand-labeling examples by automatically synthesizing training data"
            },
            "slug": "Task-specific-image-text-recognition-Ben-Haim",
            "title": {
                "fragments": [],
                "text": "Task specific image text recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This thesis applies a boosting framework to the character recognition problem, which allows to avoid character segmentation altogether and allows to read blurry, poor quality images that are difficult to segment."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728108"
                        ],
                        "name": "M. Mirmehdi",
                        "slug": "M.-Mirmehdi",
                        "structuredName": {
                            "firstName": "Majid",
                            "lastName": "Mirmehdi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mirmehdi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069641275"
                        ],
                        "name": "Paul Clark",
                        "slug": "Paul-Clark",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067690641"
                        ],
                        "name": "J. Lam",
                        "slug": "J.-Lam",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Lam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lam"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 24278217,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "334c9efc1bc28b5e14f9d13757dd8a600c711563",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Document recognition is a lively research area with much effort concentrated on optical character recognition. Less attention is paid to locating and extracting text from the general (non-desktop, non-scanner) environment. Such contact-free extraction of text from a general scene has applications in the context of wearable computing, robotic vision, point and click document capture, or as an aid for visually handicapped people. Here, a novel automatic text reading system is introduced using an active camera focused on text regions already located in the scene (using our recent work). Initially, a located region of text is analysed to determine the optimal zoom that would foveate onto it. Then a number of images are captured over the text region to construct a high-resolution mosaic composite of the whole region. This magnified image of the text is suitable for reading by humans or for recognition by OCR, or even for text-to speech synthesis. Although we employed a low resolution camera, we still obtained very good results."
            },
            "slug": "A-non-contact-method-of-capturing-low-resolution-Mirmehdi-Clark",
            "title": {
                "fragments": [],
                "text": "A non-contact method of capturing low-resolution text for OCR"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A novel automatic text reading system is introduced using an active camera focused on text regions already located in the scene (using this recent work), and a number of images are captured over the text region to construct a high-resolution mosaic composite of the whole region."
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Analysis & Applications"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32524746"
                        ],
                        "name": "J. Maye",
                        "slug": "J.-Maye",
                        "structuredName": {
                            "firstName": "J\u00e9r\u00f4me",
                            "lastName": "Maye",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Maye"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684369"
                        ],
                        "name": "Luciano Spinello",
                        "slug": "Luciano-Spinello",
                        "structuredName": {
                            "firstName": "Luciano",
                            "lastName": "Spinello",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luciano Spinello"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2750689"
                        ],
                        "name": "Rudolph Triebel",
                        "slug": "Rudolph-Triebel",
                        "structuredName": {
                            "firstName": "Rudolph",
                            "lastName": "Triebel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rudolph Triebel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720483"
                        ],
                        "name": "R. Siegwart",
                        "slug": "R.-Siegwart",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Siegwart",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Siegwart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7320889,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc43e35c1140fd57de9244eb1ed3cf382902952f",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Most large-scale public environments provide direction signs to facilitate the orientation for humans and to find their way to a goal location in the environment. Thus, for a robot operating in the same environment, it would be beneficial to interpret such signs correctly for a safe and efficient navigation. In this work, we propose a novel approach to infer the meaning of direction signs and to use that for navigation, i.e., to find a mapping of a detected sign to a motion direction. Our method uses a hierarchical extension of the Implicit Shape Model framework called HISM that does not require any hand-labeled training data to detect the signs. On the lower level of this two-stage hierarchy, ISM is applied to image descriptors as in the standard approach. On the higher level, ISM operates on subparts of signs called tokens, using weights learned from data. The interpretation of the signs is inferred by associating navigation data to direction instructions. We conducted experiments from image data acquired in an airport terminal, aiming towards the implementation of a robotic guide, with promising results."
            },
            "slug": "Inferring-the-semantics-of-direction-signs-in-Maye-Spinello",
            "title": {
                "fragments": [],
                "text": "Inferring the semantics of direction signs in public places"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work proposes a novel approach to infer the meaning of direction signs and to use that for navigation, i.e., to find a mapping of a detected sign to a motion direction."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE International Conference on Robotics and Automation"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7855312"
                        ],
                        "name": "S. Escalera",
                        "slug": "S.-Escalera",
                        "structuredName": {
                            "firstName": "Sergio",
                            "lastName": "Escalera",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Escalera"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46176857"
                        ],
                        "name": "Xavier Bar\u00f3",
                        "slug": "Xavier-Bar\u00f3",
                        "structuredName": {
                            "firstName": "Xavier",
                            "lastName": "Bar\u00f3",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xavier Bar\u00f3"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793079"
                        ],
                        "name": "Jordi Vitri\u00e0",
                        "slug": "Jordi-Vitri\u00e0",
                        "structuredName": {
                            "firstName": "Jordi",
                            "lastName": "Vitri\u00e0",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jordi Vitri\u00e0"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143601910"
                        ],
                        "name": "P. Radeva",
                        "slug": "P.-Radeva",
                        "structuredName": {
                            "firstName": "Petia",
                            "lastName": "Radeva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Radeva"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18308881,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "594fd09c92f0623e24557853244527e18c2f951f",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Text detection in urban scenes is a hard task due to the high variability of text appearance: different text fonts, changes in the point of view, or partial occlusion are just a few problems. Text detection can be specially suited for georeferencing business, navigation, tourist assistance, or to help visual impaired people. In this paper, we propose a general methodology to deal with the problem of text detection in outdoor scenes. The method is based on learning spatial information of gradient based features and Census Transform images using a cascade of classifiers. The method is applied in the context of Mobile Mapping systems, where a mobile vehicle captures urban image sequences. Moreover, a cover data set is presented and tested with the new methodology. The results show high accuracy when detecting multi-linear text regions with high variability of appearance, at same time that it preserves a low false alarm rate compared to classical approaches."
            },
            "slug": "Text-Detection-in-Urban-Scenes-Escalera-Bar\u00f3",
            "title": {
                "fragments": [],
                "text": "Text Detection in Urban Scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The results show high accuracy when detecting multi-linear text regions with high variability of appearance, at same time that it preserves a low false alarm rate compared to classical approaches."
            },
            "venue": {
                "fragments": [],
                "text": "CCIA"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20063251"
                        ],
                        "name": "Marc Lalonde",
                        "slug": "Marc-Lalonde",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Lalonde",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc Lalonde"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145468862"
                        ],
                        "name": "L. Gagnon",
                        "slug": "L.-Gagnon",
                        "structuredName": {
                            "firstName": "Langis",
                            "lastName": "Gagnon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gagnon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15786232,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3a3df3ccda1f362c21636c65ef8e3c029cbabcf3",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a method for spotting key-text in videos, based on a cascade of classifiers trained with Adaboost. The video is first reduced to a set of key-frames. Each key-frame is then analyzed for its text content. Text spotting is performed by scanning the image with a variable-size window (to account for scale) within which simple features (mean/variance of grayscale values and x/y derivatives) are extracted in various sub-areas. Training builds classifiers using the most discriminant spatial combinations of features for text detection. The text-spotting module outputs a decision map of the size of the input key-frame showing regions of interest that may contain text suitable for recognition by an OCR system. Performance is measured against a dataset of 147 key-frames extracted from 22 documentary films of the National Film Board (NFB) of Canada. A detection rate of 97% is obtained with relatively few false alarms."
            },
            "slug": "Key-text-spotting-in-documentary-videos-using-Lalonde-Gagnon",
            "title": {
                "fragments": [],
                "text": "Key-text spotting in documentary videos using Adaboost"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "This paper presents a method for spotting key-text in videos, based on a cascade of classifiers trained with Adaboost, which results in a detection rate of 97% and relatively few false alarms."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46772671"
                        ],
                        "name": "Xiangrong Chen",
                        "slug": "Xiangrong-Chen",
                        "structuredName": {
                            "firstName": "Xiangrong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiangrong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10392481,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db9bbd21e91789ceec68bd6950940c873d3264fe",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Real-time object detection is essential for many computer vision applications. Many rapid detection algorithms are based on using cascades of tests. But existing design criteria for cascades either ignore the time complexity of the tests or make over-simplified assumptions about them. This paper gives a criterion for designing a time-efficient cascade that explicitly takes into account the time complexity of tests (as evaluated by computer run time) including the time for pre-processing. We design a greedy algorithm to minimize this criterion (noting that the full problem is NP-complete). Finally, we illustrate our method on the task of text detection in city scenes. This gives a text detection algorithm that runs at 0.025 seconds per 320\u00d7240 image, which is equivalent to 40 frames per second. This is a speed up factor of 2.5 compared to our previous text detector. It gives a realtime system which can be used for applications to help the blind and visually impaired."
            },
            "slug": "A-Time-Efficient-Cascade-for-Real-Time-Object-With-Chen-Yuille",
            "title": {
                "fragments": [],
                "text": "A Time-Efficient Cascade for Real-Time Object Detection: With applications for the visually impaired"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A criterion for designing a time-efficient cascade that explicitly takes into account the time complexity of tests (as evaluated by computer run time) including the time for pre-processing is given."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05) - Workshops"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109365947"
                        ],
                        "name": "Xiaoqing Liu",
                        "slug": "Xiaoqing-Liu",
                        "structuredName": {
                            "firstName": "Xiaoqing",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoqing Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804589"
                        ],
                        "name": "J. Samarabandu",
                        "slug": "J.-Samarabandu",
                        "structuredName": {
                            "firstName": "Jagath",
                            "lastName": "Samarabandu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Samarabandu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14683566,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5203d9bf7039e93fef46295d2ae1b0262afc166",
            "isKey": false,
            "numCitedBy": 109,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Scene text is an important feature to be extracted, especially in vision-based mobile robot navigation as many potential landmarks such as nameplates and information signs contain text. This paper proposes an edge-based text region extraction algorithm, which is robust with respect to font sizes, styles, color/intensity, orientations, effects of illumination, reflections, shadows, perspective distortion, and the complexity of image backgrounds. Performance of the proposed algorithm is compared against a number of widely used text localization algorithms and the results show that this method can quickly and effectively localize and extract text regions from real scenes and can be used in mobile robot navigation under an indoor environment to detect text based landmarks."
            },
            "slug": "An-edge-based-text-region-extraction-algorithm-for-Liu-Samarabandu",
            "title": {
                "fragments": [],
                "text": "An edge-based text region extraction algorithm for indoor mobile robot navigation"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper proposes an edge-based text region extraction algorithm, which is robust with respect to font sizes, styles, color/intensity, orientations, effects of illumination, reflections, shadows, perspective distortion, and the complexity of image backgrounds."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE International Conference Mechatronics and Automation, 2005"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3331461"
                        ],
                        "name": "S. Hanif",
                        "slug": "S.-Hanif",
                        "structuredName": {
                            "firstName": "Shehzad",
                            "lastName": "Hanif",
                            "middleNames": [
                                "Muhammad"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hanif"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2554802"
                        ],
                        "name": "L. Prevost",
                        "slug": "L.-Prevost",
                        "structuredName": {
                            "firstName": "Lionel",
                            "lastName": "Prevost",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Prevost"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29439090"
                        ],
                        "name": "Pablo Negri",
                        "slug": "Pablo-Negri",
                        "structuredName": {
                            "firstName": "Pablo",
                            "lastName": "Negri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pablo Negri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 15480487,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1dc8be7a356184357178a03fdf03a71e710f52b3",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a text detection and localization method. Our detection technique is based on a cascade of boosted ensemble and localizer uses standard image processing techniques. We propose a small set of features (39 in total) capable of detecting various type of text in grey level natural scene images. Two weak learners, linear discriminant function and log likelihood-ratio test under gaussian assumption, are evaluated. Single features and combination of features are used to form weak classifiers. The proposed scheme is evaluated on ICDAR 2003 robust reading and text locating database. The results are encouraging and the detector can process an images of 640 times 480 pixels in less than 2 seconds."
            },
            "slug": "A-cascade-detector-for-text-detection-in-natural-Hanif-Prevost",
            "title": {
                "fragments": [],
                "text": "A cascade detector for text detection in natural scene images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A small set of features capable of detecting various type of text in grey level natural scene images is proposed, and the detector can process an images of 640 times 480 pixels in less than 2 seconds."
            },
            "venue": {
                "fragments": [],
                "text": "2008 19th International Conference on Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3126798"
                        ],
                        "name": "B. Epshtein",
                        "slug": "B.-Epshtein",
                        "structuredName": {
                            "firstName": "Boris",
                            "lastName": "Epshtein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Epshtein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20592981"
                        ],
                        "name": "E. Ofek",
                        "slug": "E.-Ofek",
                        "structuredName": {
                            "firstName": "Eyal",
                            "lastName": "Ofek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Ofek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743988"
                        ],
                        "name": "Y. Wexler",
                        "slug": "Y.-Wexler",
                        "structuredName": {
                            "firstName": "Yonatan",
                            "lastName": "Wexler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Wexler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 10
                            }
                        ],
                        "text": "The contributions of this work consist of a robotic system which exploits a valuable but thus far unused navigational and informational resource using vision and optical character\n978-1-4244-6676-4/10/$25.00 \u00a92010 IEEE 3181\nrecognition (OCR)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8890220,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "39c4ae83b5c92e0fa55de1ec7e5cf12589c408db",
            "isKey": true,
            "numCitedBy": 1470,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel image operator that seeks to find the value of stroke width for each image pixel, and demonstrate its use on the task of text detection in natural images. The suggested operator is local and data dependent, which makes it fast and robust enough to eliminate the need for multi-scale computation or scanning windows. Extensive testing shows that the suggested scheme outperforms the latest published algorithms. Its simplicity allows the algorithm to detect texts in many fonts and languages."
            },
            "slug": "Detecting-text-in-natural-scenes-with-stroke-width-Epshtein-Ofek",
            "title": {
                "fragments": [],
                "text": "Detecting text in natural scenes with stroke width transform"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A novel image operator is presented that seeks to find the value of stroke width for each image pixel, and its use on the task of text detection in natural images is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145815031"
                        ],
                        "name": "S. Lucas",
                        "slug": "S.-Lucas",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Lucas",
                            "middleNames": [
                                "M.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lucas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1842569,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bbf50fe5622253f401e892ed943a18033e18b7b9",
            "isKey": false,
            "numCitedBy": 318,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the results of the ICDAR 2005 competition for locating text in camera captured scenes. For this we used the same data as the ICDAR 2003 competition, which has been kept private until now. This allows a direct comparison with the 2003 entries. The main result is that the leading 2005 entry has improved significantly on the leading 2003 entry, with an increase in average f-score from 0.5 to 0.62, where the f-score is the same adapted information retrieval measure used for the 2003 competition. The paper also discusses the Web-based deployment and evaluation of text locating systems, and one of the leading entries has now been deployed in this way. This mode of usage could lead to more complete and more immediate knowledge of the strengths and weaknesses of each newly developed system."
            },
            "slug": "ICDAR-2005-text-locating-competition-results-Lucas",
            "title": {
                "fragments": [],
                "text": "ICDAR 2005 text locating competition results"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The main result is that the leading 2005 entry has improved significantly on the leading 2003 entry, with an increase in average f- score from 0.5 to 0.62, where the f-score is the same adapted information retrieval measure used for the 2003 competition."
            },
            "venue": {
                "fragments": [],
                "text": "Eighth International Conference on Document Analysis and Recognition (ICDAR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121267347"
                        ],
                        "name": "K. Jung",
                        "slug": "K.-Jung",
                        "structuredName": {
                            "firstName": "Keechul",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144602022"
                        ],
                        "name": "K. Kim",
                        "slug": "K.-Kim",
                        "structuredName": {
                            "firstName": "Kwang",
                            "lastName": "Kim",
                            "middleNames": [
                                "In"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "The contributions of this work consist of a robotic system which exploits a valuable but thus far unused navigational and informational resource using vision and optical character\n978-1-4244-6676-4/10/$25.00 \u00a92010 IEEE 3181\nrecognition (OCR)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5999466,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cedf72be1fe814ef2ee9d65633dc3226f80f0785",
            "isKey": true,
            "numCitedBy": 936,
            "numCiting": 100,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Text-information-extraction-in-images-and-video:-a-Jung-Kim",
            "title": {
                "fragments": [],
                "text": "Text information extraction in images and video: a survey"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6644398,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ccf5208521cb8c35f50ee8873df89294b8ed7292",
            "isKey": false,
            "numCitedBy": 13116,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone?Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in Rn. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line."
            },
            "slug": "A-decision-theoretic-generalization-of-on-line-and-Freund-Schapire",
            "title": {
                "fragments": [],
                "text": "A decision-theoretic generalization of on-line learning and an application to boosting"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "The model studied can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting, and it is shown that the multiplicative weight-update Littlestone?Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems."
            },
            "venue": {
                "fragments": [],
                "text": "EuroCOLT"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731948"
                        ],
                        "name": "Paul A. Viola",
                        "slug": "Paul-A.-Viola",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Viola",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul A. Viola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111328101"
                        ],
                        "name": "Michael Jones",
                        "slug": "Michael-Jones",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2796017,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b09ec0b350f8352bce46a2f5bf7ae97c83a7b9ca",
            "isKey": false,
            "numCitedBy": 11222,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a face detection framework that is capable of processing images extremely rapidly while achieving high detection rates. There are three key contributions. The first is the introduction of a new image representation called the \u201cIntegral Image\u201d which allows the features used by our detector to be computed very quickly. The second is a simple and efficient classifier which is built using the AdaBoost learning algorithm (Freund and Schapire, 1995) to select a small number of critical visual features from a very large set of potential features. The third contribution is a method for combining classifiers in a \u201ccascade\u201d which allows background regions of the image to be quickly discarded while spending more computation on promising face-like regions. A set of experiments in the domain of face detection is presented. The system yields face detection performance comparable to the best previous systems (Sung and Poggio, 1998; Rowley et al., 1998; Schneiderman and Kanade, 2000; Roth et al., 2000). Implemented on a conventional desktop, face detection proceeds at 15 frames per second."
            },
            "slug": "Robust-Real-Time-Face-Detection-Viola-Jones",
            "title": {
                "fragments": [],
                "text": "Robust Real-Time Face Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new image representation called the \u201cIntegral Image\u201d is introduced which allows the features used by the detector to be computed very quickly and a method for combining classifiers in a \u201ccascade\u201d which allows background regions of the image to be quickly discarded while spending more computation on promising face-like regions."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9913392,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f4493eff2531536a7aeb3fc11d62c30a8f487f6",
            "isKey": false,
            "numCitedBy": 4825,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Boosting is one of the most important recent developments in classification methodology. Boosting works by sequentially applying a classification algorithm to reweighted versions of the training data and then taking a weighted majority vote of the sequence of classifiers thus produced. For many classification algorithms, this simple strategy results in dramatic improvements in performance. We show that this seemingly mysterious phenomenon can be understood in terms of well-known statistical principles, namely additive modeling and maximum likelihood. For the two-class problem, boosting can be viewed as an approximation to additive modeling on the logistic scale using maximum Bernoulli likelihood as a criterion. We develop more direct approximations and show that they exhibit nearly identical results to boosting. Direct multiclass generalizations based on multinomial likelihood are derived that exhibit performance comparable to other recently proposed multiclass generalizations of boosting in most situations, and far superior in some. We suggest a minor modification to boosting that can reduce computation, often by factors of 10 to 50. Finally, we apply these insights to produce an alternative formulation of boosting decision trees. This approach, based on best-first truncated tree induction, often leads to better performance, and can provide interpretable descriptions of the aggregate decision rule. It is also much faster computationally, making it more suitable to large-scale data mining applications."
            },
            "slug": "Special-Invited-Paper-Additive-logistic-regression:-Friedman",
            "title": {
                "fragments": [],
                "text": "Special Invited Paper-Additive logistic regression: A statistical view of boosting"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work shows that this seemingly mysterious phenomenon of boosting can be understood in terms of well-known statistical principles, namely additive modeling and maximum likelihood, and develops more direct approximations and shows that they exhibit nearly identical results to boosting."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403828372"
                        ],
                        "name": "R. Ramos-Garijo",
                        "slug": "R.-Ramos-Garijo",
                        "structuredName": {
                            "firstName": "Rafael",
                            "lastName": "Ramos-Garijo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ramos-Garijo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145000362"
                        ],
                        "name": "M. Prats",
                        "slug": "M.-Prats",
                        "structuredName": {
                            "firstName": "Mario",
                            "lastName": "Prats",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Prats"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27421519"
                        ],
                        "name": "P. Sanz",
                        "slug": "P.-Sanz",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Sanz",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Sanz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144424962"
                        ],
                        "name": "A. P. Pobil",
                        "slug": "A.-P.-Pobil",
                        "structuredName": {
                            "firstName": "Angel",
                            "lastName": "Pobil",
                            "middleNames": [
                                "Pascual",
                                "del"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. P. Pobil"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14575666,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "433504fcd292237fdd2e066f3c2ba09811f1f814",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents work in progress towards a complete system working to assist users in a library. With this aim, the system must be capable to looking for a specific book in a shelf, asked by any user, and whether it is found, deliver it as soon as possible to the user. To get its objectives the system integrates automatic object recognition, visually guided grasping, and force feedback, among other advanced capabilities. Implementation details about the main modules developed presently are shown. Finally, after success in preliminary results obtained in our campus, we are encouraged to fallow working in this way to obtain the complete prototype."
            },
            "slug": "An-autonomous-assistant-robot-for-book-manipulation-Ramos-Garijo-Prats",
            "title": {
                "fragments": [],
                "text": "An autonomous assistant robot for book manipulation in a library"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This paper presents work in progress towards a complete system working to assist users in a library that integrates automatic object recognition, visually guided grasping, and force feedback, among other advanced capabilities."
            },
            "venue": {
                "fragments": [],
                "text": "SMC'03 Conference Proceedings. 2003 IEEE International Conference on Systems, Man and Cybernetics. Conference Theme - System Security and Assurance (Cat. No.03CH37483)"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "80238870"
                        ],
                        "name": "G. Engel",
                        "slug": "G.-Engel",
                        "structuredName": {
                            "firstName": "Gunter",
                            "lastName": "Engel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Engel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1827360"
                        ],
                        "name": "D. Greve",
                        "slug": "D.-Greve",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Greve",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Greve"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46229373"
                        ],
                        "name": "J. Lubin",
                        "slug": "J.-Lubin",
                        "structuredName": {
                            "firstName": "J.M.",
                            "lastName": "Lubin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lubin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1772264"
                        ],
                        "name": "E. Schwartz",
                        "slug": "E.-Schwartz",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Schwartz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 46942066,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cb390b262b9dd76a7fcc386389689a9732897d59",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, the design and implementation of a miniature visually guided robot vehicle are described. The design principle underlying this vehicle, which is called CORTEX-II, is the space-variant visual architecture of visual cortex. This allows a wide visual field and high resolution to be maintained in a small and relatively inexpensive system. With a total weight of under twenty kg and size of about .5 meters, CORTEX-II incorporates a three degree of freedom active vision system (pan, tilt and zoom) mounted on a 1 degree of freedom \"neck\", 160 Mflops (loosely coupled array of four TI320C40 parallel DSP's) and 48 megabytes of memory. Topics discussed here are modification of a stock RC vehicle platform, integration of the parallel DSP platform, active vision system, battery and power systems, communications, and a programming environment for development of the real-time parallel tasks that is transparently portable between simulation and robot run time contexts. Potential applications for systems of this type include traffic monitoring, autonomous driving research, security and surveillance, space-exploration, smart weapons systems, and other application domains in which small, low-cost, high performance visually guided robots are required."
            },
            "slug": "Space-variant-active-vision-and-visually-guided-and-Engel-Greve",
            "title": {
                "fragments": [],
                "text": "Space-variant active vision and visually guided robotics: design and construction of a high-performance miniature vehicle"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Modification of a stock RC vehicle platform, integration of the parallel DSP platform, active vision system, battery and power systems, communications, and a programming environment for development of the real-time parallel tasks that is transparently portable between simulation and robot run time contexts are described."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 12th IAPR International Conference on Pattern Recognition, Vol. 3 - Conference C: Signal Processing (Cat. No.94CH3440-5)"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068881536"
                        ],
                        "name": "David Meyer",
                        "slug": "David-Meyer",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Meyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Meyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765887"
                        ],
                        "name": "D. Kriegman",
                        "slug": "D.-Kriegman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Kriegman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kriegman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10016074,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7bf0a1aa1d0228a51d24c0c3a83eceb937a6ae25",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "1.1 (a) A Dutch license plate and (b) a California license plate. Most cars in our datasets have plates of the form shown in (b), but at a much lower resolution. ."
            },
            "slug": "Video-based-Car-Surveillance:-License-Plate,-Make,-Belongie-Meyer",
            "title": {
                "fragments": [],
                "text": "Video-based Car Surveillance: License Plate, Make, and Model Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "Most cars in the authors' datasets have plates of the form shown in (b), but at a much lower resolution, than the Dutch and California license plates, which are used in this study."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157708855"
                        ],
                        "name": "R. Smith",
                        "slug": "R.-Smith",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Smith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7038773,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "89d9aae7e0c8b6edd56d0d79b277c07b7ab66fda",
            "isKey": false,
            "numCitedBy": 1508,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The Tesseract OCR engine, as was the HP Research Prototype in the UNLV Fourth Annual Test of OCR Accuracy, is described in a comprehensive overview. Emphasis is placed on aspects that are novel or at least unusual in an OCR engine, including in particular the line finding, features/classification methods, and the adaptive classifier."
            },
            "slug": "An-Overview-of-the-Tesseract-OCR-Engine-Smith",
            "title": {
                "fragments": [],
                "text": "An Overview of the Tesseract OCR Engine"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The Tesseract OCR engine, as was the HP Research Prototype in the UNLV Fourth Annual Test of OCR Accuracy, is described in a comprehensive overview."
            },
            "venue": {
                "fragments": [],
                "text": "Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2274030"
                        ],
                        "name": "Andrea Carbone",
                        "slug": "Andrea-Carbone",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Carbone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrea Carbone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48973977"
                        ],
                        "name": "Alberto Finzi",
                        "slug": "Alberto-Finzi",
                        "structuredName": {
                            "firstName": "Alberto",
                            "lastName": "Finzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alberto Finzi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064084"
                        ],
                        "name": "Andrea Orlandini",
                        "slug": "Andrea-Orlandini",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Orlandini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrea Orlandini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695016"
                        ],
                        "name": "F. Pirri",
                        "slug": "F.-Pirri",
                        "structuredName": {
                            "firstName": "F.",
                            "lastName": "Pirri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Pirri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35097069"
                        ],
                        "name": "G. Ugazio",
                        "slug": "G.-Ugazio",
                        "structuredName": {
                            "firstName": "Giorgio",
                            "lastName": "Ugazio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Ugazio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6927247,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cca7a462fd18f35938dddffa37738d4c626f2136",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we describe a model-based approach to the executive control of a rescue rover. We show how this control architecture naturally supports human-robot interaction in the diverse activities needed in rescue and search. We illustrate the approach by considering human-robot interaction in the domain of the RoboCup rescue competition. We discuss the implementation and tests done both during RoboCup contests and in the laboratory, to show performances according to different working modalities such as fully operated, supervised, fully autonomous."
            },
            "slug": "Augmenting-situation-awareness-via-model-based-in-Carbone-Finzi",
            "title": {
                "fragments": [],
                "text": "Augmenting situation awareness via model-based control in rescue robots"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "This work describes a model-based approach to the executive control of a rescue rover and shows how this control architecture naturally supports human-robot interaction in the diverse activities needed in rescue and search."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE/RSJ International Conference on Intelligent Robots and Systems"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46443338"
                        ],
                        "name": "Jeremy Clear",
                        "slug": "Jeremy-Clear",
                        "structuredName": {
                            "firstName": "Jeremy",
                            "lastName": "Clear",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeremy Clear"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We use word frequencies obtained from the British National Corpus [24], a collection of approximately 100\u00d7 10 words encompassing ca."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59825318,
            "fieldsOfStudy": [
                "Sociology"
            ],
            "id": "b806606a08cf97bc48adcde9abd34732c37c2774",
            "isKey": false,
            "numCitedBy": 326,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-British-national-corpus-Clear",
            "title": {
                "fragments": [],
                "text": "The British national corpus"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 113
                            }
                        ],
                        "text": "Other non-document OCR applications include detecting text in television streams [13], licence plate recognition [14]\u2013[16], and assistive devices for the visually impaired [17], [18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Two-stage licence plate detection using gentle adaboost"
            },
            "venue": {
                "fragments": [],
                "text": "First Asian Conf. on Intelligent Information and Database Systems, 2009."
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The British national corpus, \" in The digital word: textbased computing in the humanities"
            },
            "venue": {
                "fragments": [],
                "text": "The British national corpus, \" in The digital word: textbased computing in the humanities"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Two-stage licence plate detection using gentle adaboost"
            },
            "venue": {
                "fragments": [],
                "text": "First Asian Conf. on Intelligent Information and Database Systems"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "ICDAR 2003 robust reading competitions Proceedings of the Seventh Intl. Conf. on Document Analysis and Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "ICDAR 2003 robust reading competitions Proceedings of the Seventh Intl. Conf. on Document Analysis and Recognition"
            },
            "year": 2003
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 4,
            "methodology": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 27,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Using-text-spotting-to-query-the-world-Posner-Corke/10eb6f459b6f02ae49e392d1834f56e8156732c5?sort=total-citations"
}