{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115196978"
                        ],
                        "name": "Huiping Li",
                        "slug": "Huiping-Li",
                        "structuredName": {
                            "firstName": "Huiping",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huiping Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3272081"
                        ],
                        "name": "O. Kia",
                        "slug": "O.-Kia",
                        "structuredName": {
                            "firstName": "Omid",
                            "lastName": "Kia",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Kia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 146
                            }
                        ],
                        "text": "In our previous work we have developed algorithms to detect and track text in video sequences [7] and enhance textual image on single video frame [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "In [9] we describe a text enhancement scheme on single frames."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 34
                            }
                        ],
                        "text": "First, as we previously addressed [9], video frames are typically limited in spatial resolution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206406471,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9612a282060e74a6fe64e34cfc0643f13f6672e1",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "One difficulty with using text from digital video for indexing and retrieval is that video images are often in low resolution and poor quality, and as a result, the text can not be recognized adequately by most commercial OCR software. Text image enhancement is necessary to achieve reasonable OCR accuracy. Our enhancement consists of two main procedures, resolution enhancement based on Shannon interpolation and text separation from complex image background. Experiments show our enhancement approach improves OCR accuracy considerably."
            },
            "slug": "Text-enhancement-in-digital-video-Li-Kia",
            "title": {
                "fragments": [],
                "text": "Text enhancement in digital video"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Experiments show the enhancement approach improves OCR accuracy considerably, and consists of two main procedures, resolution enhancement based on Shannon interpolation and text separation from complex image background."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115196978"
                        ],
                        "name": "Huiping Li",
                        "slug": "Huiping-Li",
                        "structuredName": {
                            "firstName": "Huiping",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huiping Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 40
                            }
                        ],
                        "text": "Use text extraction scheme addressed in [6] to detect text."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 43
                            }
                        ],
                        "text": "This work has been published previously in [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12368399,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bc8b866cc58e82e6413367c8d770ef681e5abe66",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Scene and graphic text can provide important supplemental index information in video sequences. In this paper we address the problem automatically identifying text regions in digital video key frames. The text contained in video frames is typically very noisy because it is aliased and/or digitized at a much lower resolution than typical document images, making identification, extraction and recognition difficult. The proposed method is based on the use of a hybrid wavelet/neural network segmenter on a series of overlapping small windows to classify regions which contain text. To detect text over a wide range of font sizes, the method is applied to a pyramid of images and the regions identified at each level are integrated."
            },
            "slug": "Automatic-identification-of-text-in-digital-video-Li-Doermann",
            "title": {
                "fragments": [],
                "text": "Automatic identification of text in digital video key frames"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The proposed method is based on the use of a hybrid wavelet/neural network segmenter on a series of overlapping small windows to classify regions which contain text to detect text over a wide range of font sizes."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115196978"
                        ],
                        "name": "Huiping Li",
                        "slug": "Huiping-Li",
                        "structuredName": {
                            "firstName": "Huiping",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huiping Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3272081"
                        ],
                        "name": "O. Kia",
                        "slug": "O.-Kia",
                        "structuredName": {
                            "firstName": "Omid",
                            "lastName": "Kia",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Kia"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 94
                            }
                        ],
                        "text": "In our previous work we have developed algorithms to detect and track text in video sequences [7] and enhance textual image on single video frame [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 19
                            }
                        ],
                        "text": "As we point out in [7], pure translational model can only track the text with simple motion."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15485643,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f8f5c282dc11937d29183b955dc3e4fbb677571b",
            "isKey": false,
            "numCitedBy": 652,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "Text that appears in a scene or is graphically added to video can provide an important supplemental source of index information as well as clues for decoding the video's structure and for classification. In this work, we present algorithms for detecting and tracking text in digital video. Our system implements a scale-space feature extractor that feeds an artificial neural processor to detect text blocks. Our text tracking scheme consists of two modules: a sum of squared difference (SSD)-based module to find the initial position and a contour-based module to refine the position. Experiments conducted with a variety of video sources show that our scheme can detect and track text robustly."
            },
            "slug": "Automatic-text-detection-and-tracking-in-digital-Li-Doermann",
            "title": {
                "fragments": [],
                "text": "Automatic text detection and tracking in digital video"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work presents algorithms for detecting and tracking text in digital video that implements a scale-space feature extractor that feeds an artificial neural processor to detect text blocks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739319"
                        ],
                        "name": "R. Lienhart",
                        "slug": "R.-Lienhart",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Lienhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lienhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083891040"
                        ],
                        "name": "F. Stuber",
                        "slug": "F.-Stuber",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Stuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Stuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 110
                            }
                        ],
                        "text": "Lienhart describes a text recognition system in digital video and achieves a recognition result of nearly 80% [11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 78
                            }
                        ],
                        "text": "Although some authors develop their own OCR software for the recognition task [16, 11], We use commercial OCR software to evaluate our algorithm since the result from commercial OCR software should be more objective."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 117
                            }
                        ],
                        "text": "Although some work has been done on character recognition in scene images [15], WWW images [16, 17] and video frames [11], the literature on textual image enhancement in scenic image and digital video is extremely sparse."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14147742,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "778a307aa0cf8b2ed273b9089cb9aa8210f49f24",
            "isKey": false,
            "numCitedBy": 205,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We have developed algorithms for automatic character segmentation in motion pictures which extract automatically and reliably the text in pre-title sequences, credit titles, and closing sequences with title and credits. The algorithms we propose make use of typical characteristics of text in videos in order to enhance segmentation and, consequently, recognition performance. As a result, we get segmented characters from video pictures. These can be parsed by any OCR software. The recognition results of multiple instances of the same character throughout subsequent frames are combined to enhance recognition result and to compute the final output. We have tested our segmentation algorithms in a series of experiments with video clips recorded from television and achieved good segmentation results."
            },
            "slug": "Automatic-text-recognition-in-digital-videos-Lienhart-Stuber",
            "title": {
                "fragments": [],
                "text": "Automatic text recognition in digital videos"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "Algorithms for automatic character segmentation in motion pictures which extract automatically and reliably the text in pre-title sequences, credit titles, and closing sequences with title and credits are developed."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2907696"
                        ],
                        "name": "J. Shim",
                        "slug": "J.-Shim",
                        "structuredName": {
                            "firstName": "Jae",
                            "lastName": "Shim",
                            "middleNames": [
                                "Chang"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145253787"
                        ],
                        "name": "C. Dorai",
                        "slug": "C.-Dorai",
                        "structuredName": {
                            "firstName": "Chitra",
                            "lastName": "Dorai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Dorai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70029967"
                        ],
                        "name": "R. Bolle",
                        "slug": "R.-Bolle",
                        "structuredName": {
                            "firstName": "Ruud",
                            "lastName": "Bolle",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bolle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12062439,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1eb854ce0539b6fd18dbba942f52a80a735f5c8e",
            "isKey": false,
            "numCitedBy": 142,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Efficient content-based retrieval of image and video databases is an important application due to rapid proliferation of digital video data on the Internet and corporate intranets. Text either embedded or superimposed within video frames is very useful for describing the contents of the frames, as it enables both keyword and free-text based search, automatic video logging, and video cataloging. We have developed a scheme for automatically extracting text from digital images and videos for content annotation and retrieval. We present our approach to robust text extraction from video frames, which can handle complex image backgrounds, deal with different font sizes, font styles, and font appearances such as normal and inverse video. Our algorithm results in segmented characters that can be directly processed by an OCR system to produce ASCII text. Results from our experiments with over 5000 frames obtained from twelve MPEG video streams demonstrate the good performance of our system in terms of text identification accuracy and computational efficiency."
            },
            "slug": "Automatic-text-extraction-from-video-for-annotation-Shim-Dorai",
            "title": {
                "fragments": [],
                "text": "Automatic text extraction from video for content-based annotation and retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work has developed a scheme for automatically extracting text from digital images and videos for content annotation and retrieval that results in segmented characters that can be directly processed by an OCR system to produce ASCII text."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50385983"
                        ],
                        "name": "P. Stubberud",
                        "slug": "P.-Stubberud",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Stubberud",
                            "middleNames": [
                                "Allen"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Stubberud"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2499975"
                        ],
                        "name": "J. Kanai",
                        "slug": "J.-Kanai",
                        "structuredName": {
                            "firstName": "Junichi",
                            "lastName": "Kanai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kanai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2220739"
                        ],
                        "name": "V. Kalluri",
                        "slug": "V.-Kalluri",
                        "structuredName": {
                            "firstName": "Venugopal",
                            "lastName": "Kalluri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Kalluri"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "Stubberud [13] presented an adaptive technique that restores touching or broken character images to improve performance of OCR system."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 23545227,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "85d8e03772f7f071d826c123bce950ced36d4ae7",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "To improve the performance of an optical character recognition (OCR) system, an adaptive technique that restores touching or broken character images is proposed. By using the output from an OCR system and a distorted text image, this technique trains an adaptive restoration filter and then applies the filter to the distorted text image that the OCR system could not recognize. To demonstrate the performance of this technique, two synthesized images containing only touching characters and two synthesized images containing only broken characters were processed. The results show that this technique can improve both pixel and character accuracy of text images containing touching or broken characters."
            },
            "slug": "Adaptive-image-restoration-of-text-images-that-or-Stubberud-Kanai",
            "title": {
                "fragments": [],
                "text": "Adaptive image restoration of text images that contain touching or broken characters"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This technique trains an adaptive restoration filter and applies the filter to the distorted text image that the OCR system could not recognize, showing that this technique can improve both pixel and character accuracy of text images containing touching or broken characters."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd International Conference on Document Analysis and Recognition"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739319"
                        ],
                        "name": "R. Lienhart",
                        "slug": "R.-Lienhart",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Lienhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lienhart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8416045,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0dd1def5778f24c2c5a5f1c114846326e8f86123",
            "isKey": false,
            "numCitedBy": 143,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Efficient indexing and retrieval of digital video is an important aspect of video databases. One powerful index for retrieval is the text appearing in them. It enables content- based browsing. We present our methods for automatic segmentation and recognition of text in digital videos. The algorithms we propose make use of typical characteristics of text in videos in order to enable and enhance segmentation and recognition performance. Especially the inter-frame dependencies of the characters provide new possibilities for their refinement. Then, a straightforward indexing and retrieval scheme is introduced. It is used in the experiments to demonstrate that the proposed text segmentation and text recognition algorithms are suitable for indexing and retrieval of relevant video scenes in and from a video data base. Our experimental results are very encouraging and suggest that these algorithms can be used in video retrieval applications as well as to recognize higher semantics in video."
            },
            "slug": "Automatic-text-recognition-for-video-indexing-Lienhart",
            "title": {
                "fragments": [],
                "text": "Automatic text recognition for video indexing"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The proposed text segmentation and text recognition algorithms are suitable for indexing and retrieval of relevant video scenes in and from a video data base and suggest that they can be used in video retrieval applications as well as to recognize higher semantics in video."
            },
            "venue": {
                "fragments": [],
                "text": "MULTIMEDIA '96"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48764203"
                        ],
                        "name": "Victor Wu",
                        "slug": "Victor-Wu",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Victor Wu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 140
                            }
                        ],
                        "text": "Wu and Manmatha describes a text extraction and recognition system and achieves 84% correct OCR rate based only on \u201cOCRable\u201d text in images [15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 74
                            }
                        ],
                        "text": "Although some work has been done on character recognition in scene images [15], WWW images [16, 17] and video frames [11], the literature on textual image enhancement in scenic image and digital video is extremely sparse."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 53908609,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "87794fa9895507e6469231135d1c9d44a0d7944c",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "A four-step system which automatically detects and extracts text in images is presented. First, a texture segmentation scheme is used to focus attention on regions where text may occur. Second , strokes are extracted from the segmented text regions, and then processed to form rectangular boxes surrounding the corresponding text strings. Multi-scale processing is used to account for signiicant font size variations. Third, text is extracted by cleaning up the background and binarizing the detected text strings. Finally , better text bounding boxes are generated by using the binarized text as strokes. Text is then cleaned and binarized from these new boxes, and can then be passed through a commercial OCR engine for recognition. The system is stable, robust, and works well on images (with or without structured layouts) from a wide variety of sources, including digitized video frames, photographs, newspapers, advertisements in magazines/newspapers, stock cer-tiicates, and personal checks. Any opinions, ndings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reeect those of the sponsors."
            },
            "slug": "Automatic-Text-Detection-and-Recognition-Wu",
            "title": {
                "fragments": [],
                "text": "Automatic Text Detection and Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "A four-step system which automatically detects and extracts text in images is presented, which is stable, robust, and works well on images from a wide variety of sources, including digitized video frames, photographs, newspapers, advertisements in magazines/newspapers, stock cer-tiicates, and personal checks."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2461436"
                        ],
                        "name": "Jiangying Zhou",
                        "slug": "Jiangying-Zhou",
                        "structuredName": {
                            "firstName": "Jiangying",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiangying Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1828940"
                        ],
                        "name": "D. Lopresti",
                        "slug": "D.-Lopresti",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Lopresti",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lopresti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3198175"
                        ],
                        "name": "T. Tasdizen",
                        "slug": "T.-Tasdizen",
                        "structuredName": {
                            "firstName": "Tolga",
                            "lastName": "Tasdizen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Tasdizen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 90
                            }
                        ],
                        "text": "Zhou and Lopresti describes their work on text extraction and recognition from WWW images [16, 17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 91
                            }
                        ],
                        "text": "Although some work has been done on character recognition in scene images [15], WWW images [16, 17] and video frames [11], the literature on textual image enhancement in scenic image and digital video is extremely sparse."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10184381,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0bad076c5f89ac43026a2b5fca648d488f54f45e",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we consider the problem of locating and extracting text from WWW images. A previous algorithm based on color clustering and connected components analysis works well as long as the color of each character is relatively uniform and the typography is fairly simple. It breaks down quickly, however, when these assumptions are violated. In this paper, we describe more robust techniques for dealing with this challenging problem. We present an improved color clustering algorithm that measures similarity based on both RGB and spatial proximity. Layout analysis is also incorporated to handle more complex typography. THese changes significantly enhance the performance of our text detection procedure."
            },
            "slug": "Finding-text-in-color-images-Zhou-Lopresti",
            "title": {
                "fragments": [],
                "text": "Finding text in color images"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An improved color clustering algorithm that measures similarity based on both RGB and spatial proximity is presented, and layout analysis is also incorporated to handle more complex typography."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2461436"
                        ],
                        "name": "Jiangying Zhou",
                        "slug": "Jiangying-Zhou",
                        "structuredName": {
                            "firstName": "Jiangying",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiangying Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1828940"
                        ],
                        "name": "D. Lopresti",
                        "slug": "D.-Lopresti",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Lopresti",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lopresti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113455824"
                        ],
                        "name": "Zhibin Lei",
                        "slug": "Zhibin-Lei",
                        "structuredName": {
                            "firstName": "Zhibin",
                            "lastName": "Lei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhibin Lei"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 29236195,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5f719c98abe9e3da61cb39ca1b836aeb09a4624",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A significant amount of text now present in World Wide Web documents is embedded in image data, and a large portion of it does not appear elsewhere at all. To make this information available, we need to develop techniques for recovering textual information from in-line Web images. In this paper, we describe two methods for Web image OCR. Recognizing text extracted from in-line Web images is difficult because characters in these images are often rendered at a low spatial resolution. Such images are typically considered to be 'low quality' by traditional OCR technologies. Our proposed methods utilize the information contained in the color bits to compensate for the loss of information due to low sampling resolution. The first method uses a polynomial surface fitting technique for object recognition. The second method is based on the traditional n-tuple technique. We collected a small set of character samples from Web documents and tested the two algorithms. Preliminary experimental results show that our n-tuple method works quite well. However, the surface fitting method performs rather poorly due to the coarseness and small number of color shades used in the text."
            },
            "slug": "OCR-for-World-Wide-Web-images-Zhou-Lopresti",
            "title": {
                "fragments": [],
                "text": "OCR for World Wide Web images"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Two methods for Web image OCR based on the traditional n-tuple technique, which utilize the information contained in the color bits to compensate for the loss of information due to low sampling resolution."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1868906"
                        ],
                        "name": "J. Hobby",
                        "slug": "J.-Hobby",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hobby",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hobby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795578"
                        ],
                        "name": "T. Ho",
                        "slug": "T.-Ho",
                        "structuredName": {
                            "firstName": "Tin",
                            "lastName": "Ho",
                            "middleNames": [
                                "Kam"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ho"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 148
                            }
                        ],
                        "text": "Hobby presents a method to enhance degraded document images via bitmap clustering and averaging for better display quality and recognition accuracy [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2682428,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2d375b85a3fdf928a5aa0672894f61276003d69f",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Proper display and accurate recognition of document images are often hampered by degradations caused by poor scanning or transmission conditions. The authors propose a method to enhance such degraded document images for better display quality and recognition accuracy. The essence of the method is in finding and averaging bitmaps of the same symbol that are scattered across a text page. Outline descriptions of the symbols are then obtained that can be rendered at arbitrary solution. The paper describes details of the algorithm and an experiment to demonstrate its capabilities using fax images."
            },
            "slug": "Enhancing-degraded-document-images-via-bitmap-and-Hobby-Ho",
            "title": {
                "fragments": [],
                "text": "Enhancing degraded document images via bitmap clustering and averaging"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A method in finding and averaging bitmaps of the same symbol that are scattered across a text page that can be rendered at arbitrary solution for better display quality and recognition accuracy is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fourth International Conference on Document Analysis and Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780258"
                        ],
                        "name": "Jisheng Liang",
                        "slug": "Jisheng-Liang",
                        "structuredName": {
                            "firstName": "Jisheng",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jisheng Liang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710238"
                        ],
                        "name": "R. Haralick",
                        "slug": "R.-Haralick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Haralick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haralick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744200"
                        ],
                        "name": "I. T. Phillips",
                        "slug": "I.-T.-Phillips",
                        "structuredName": {
                            "firstName": "Ihsin",
                            "lastName": "Phillips",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. T. Phillips"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 46058079,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ece5a909ef252c9bfbecfdec120a3e365b08a9a",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper discusses a method for binary morphological filter design to restore document images degraded by subtractive or additive noise, given a constraint on the size of filters. With a filter size restriction (for example 3 by 3), each pixel in output image depends only on its (3 by 3) neighborhood of input image. Therefore, we can construct a look-up table between input and output. Each output image pixel is determined by this table. So the filter design becomes the search for the optimal look-up table. By considering the degradation condition of the input image, we provide a methodology for knowledge based look-up table design, to achieve computational tractability. The methodology can be applied iteratively so that the final output image is the input image after being transformed through successive 3 by 3 operations. An experimental protocol is developed for restoring degraded document images, and improving the corresponding recognition accuracy rates of an OCR algorithm. We present results for a set of real images which are manually ground-truthed. The performance of each filter is evaluated by the OCR accuracy."
            },
            "slug": "Document-image-restoration-using-binary-filters-Liang-Haralick",
            "title": {
                "fragments": [],
                "text": "Document image restoration using binary morphological filters"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A method for binary morphological filter design to restore document images degraded by subtractive or additive noise, given a constraint on the size of filters is discussed."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2020816517"
                        ],
                        "name": "K. Hannu",
                        "slug": "K.-Hannu",
                        "structuredName": {
                            "firstName": "K\u00e4\u00e4ri\u00e4inen",
                            "lastName": "Hannu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hannu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "107963401"
                        ],
                        "name": "S. Jaakko",
                        "slug": "S.-Jaakko",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Jaakko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Jaakko"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 195594272,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a6add242a2cb19f1dfa535c6a7f65a97e2e1101",
            "isKey": false,
            "numCitedBy": 1,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a new approach for automated quality improvement of grey-scale document images, called STORM. The grey-scale images are first adaptively partitioned into representative regions, whose content is analyzed using a set of document image features developed and adapted for the purpose. The document condition and quality information is evaluated for defect pattern classification in a given entity. This data is then processed using a neural network classifier to expose and prioritize the image defects, if any. The evaluation information is further partitioned using the soft control technique by mapping and parametrising the evaluation classes into available image operation techniques. The document type and domain characteristics are used to bias these operations. The experiments cover over 1000 document images in different categories having degradation types in various degree. The outcome shows good results in most of these domains with an automated process."
            },
            "slug": "An-automated-defect-management-for-document-images-Hannu-Jaakko",
            "title": {
                "fragments": [],
                "text": "An automated defect management for document images"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A new approach for automated quality improvement of grey-scale document images, called STORM, which covers over 1000 document images in different categories having degradation types in various degree and shows good results."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "108000705"
                        ],
                        "name": "D. Hobby",
                        "slug": "D.-Hobby",
                        "structuredName": {
                            "firstName": "Dj",
                            "lastName": "Hobby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hobby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2074018608"
                        ],
                        "name": "S. Henry",
                        "slug": "S.-Henry",
                        "structuredName": {
                            "firstName": "Sonia",
                            "lastName": "Henry",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Henry"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2079824980"
                        ],
                        "name": "BairdAbstractThe",
                        "slug": "BairdAbstractThe",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "BairdAbstractThe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "BairdAbstractThe"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 190
                            }
                        ],
                        "text": "proximate the character\u2019s ideal artwork by seeking to minimize the discrepancy between the approximation and the ideal measured as the worst-case Euclidean distance between their boundaries [3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16772769,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "36e087f60d950b8ae3594916ee993e863ad37b60",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The design and analysis of an algorithm for the restoration of degraded images of machine-printed characters is presented. The input is a set of degraded bilevel images of a single unknown character ; the output is an approximation to the charac-ter's ideal artwork. The algorithm seeks to minimize the discrepancy between the approximation and the ideal, measured as the worst-case Euclidean distance between their boundaries. We investigate a family of algorithms which superimpose the input images, add up the intensities at each point, and threshold the result. We show that, under degradations due to random spatial sampling error, signiicant asymp-totic improvements can be achieved by suitably pre-processing each input image and postprocessing the nal result. Experimental trials on special test shapes and Latin characters are discussed."
            },
            "slug": "Degraded-Character-Image-RestorationJohn-Hobby-Henry",
            "title": {
                "fragments": [],
                "text": "Degraded Character Image RestorationJohn"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that, under degradations due to random spatial sampling error, signiicant asymp-totic improvements can be achieved by suitably pre-processing each input image and postprocessing the result."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067184490"
                        ],
                        "name": "V.",
                        "slug": "V.",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "V.",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V."
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "65990038"
                        ],
                        "name": "Donghyeok An",
                        "slug": "Donghyeok-An",
                        "structuredName": {
                            "firstName": "Donghyeok",
                            "lastName": "An",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donghyeok An"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2070444158"
                        ],
                        "name": "J.",
                        "slug": "J.",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "J.",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J."
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 97
                            }
                        ],
                        "text": "In the literature numerous methods have been reported including global and adaptive thresholding [1, 12, 14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14814398,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "dd9b0c0e19c20afc51698626832d189a2388d86f",
            "isKey": false,
            "numCitedBy": 133,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "There has been recent interest in the segmentation of images by thresholding. We present several model based algorithms for threshold selection. We concentrate on the important two population univariate case when an image contains an object and background. However the methods are applicable to multispectral k-population images. We show how the main ideas behind two important nonspatial thresholding algorithms follow from classical discriminant analysis. We then give various new thresholding algorithms which make use of available IocaVspatial information. We consider one FLIR image and two artificial examples. A comparative study indicates that a new \" alternating mean thresholding and median filtering \" algorithm provides an acceptable method when the image is relatively highly contaminated. This method seems to depend less on initial values. I. INTRODUCTION We will consider the problem of image segmentation by thresh-olding. This problem and its importance were fully described recently in [l], [2], and [3]. Our main interest is with the k = 2 population case which is related to object identification. However, we shall also consider the extension to the general k-population problem. We first show that the threshold value in the segmentation algorithms of [ l ] and [2] can be deduced from the well-known statistical discriminant rule. Unlike [3], their rule is not spatial, i.e., it does not use contextual information. We give a spatial allocation rule based on the work of [4] and [ 5 ]. This is utilized to give a new thresholding algorithm. We also consider the iterated conditional modes (ICM) method [6]. Section I1 describes the nonspatial allocation rule, and shows how the allocation rules of [I] and [2] are particular cases. We summarize their iterative thresholding method in Section 111. In Section IV we give a spatial allocation rule which takes into account the spatial relationship between neighboring pixels and describe the modified iterative algorithms in Section V. In Section VI we describe ICM and its implementation. The methods are compared using synthetic images (following [3]) and one \" real \" FLIR (forward looking infrared) image. We conclude with a discussion of the methods in Section VIII. Our method follows naturally from a model introduced in Section 11. The method in [3] is not discussed here since it is not model orientated. Also our method applies to multispectral data, i.e., color images. In all the algorithms considered here, we do not require any prior"
            },
            "slug": "A-Spatial-Thresholding-Method-for-Image-V.-An",
            "title": {
                "fragments": [],
                "text": "A Spatial Thresholding Method for Image Segmentation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "89125003"
                        ],
                        "name": "Andr\u00e9 Marion",
                        "slug": "Andr\u00e9-Marion",
                        "structuredName": {
                            "firstName": "Andr\u00e9",
                            "lastName": "Marion",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andr\u00e9 Marion"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6825818,
            "fieldsOfStudy": [
                "Mathematics",
                "Art"
            ],
            "id": "731cc6cf4f60a6cf6dc96b925940c16b59858a0e",
            "isKey": false,
            "numCitedBy": 334,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1 The image as an analogue signal.- 2 Scanning of an image by an aperture.- 3 Extension of the aperture notion.- 4 Photographic images.- 5 Digitizing and reconstructing images.- 6 Basic techniques of digital image processing.- 7 Algebraic operations between images.- 8 Coloured images.- 9 Linear processing of signals and images."
            },
            "slug": "Introduction-to-Image-Processing-Marion",
            "title": {
                "fragments": [],
                "text": "Introduction to Image Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "The image as an analogue signal, scanning of an image by an aperture, and extension of the aperture notion are illustrated."
            },
            "venue": {
                "fragments": [],
                "text": "Springer US"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2141915"
                        ],
                        "name": "W. Niblack",
                        "slug": "W.-Niblack",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "Niblack",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Niblack"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60929037,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "43678765df1d0b4594f7a49298cf27d75e174787",
            "isKey": false,
            "numCitedBy": 1417,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-introduction-to-digital-image-processing-Niblack",
            "title": {
                "fragments": [],
                "text": "An introduction to digital image processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2304070"
                        ],
                        "name": "H. Kauniskangas",
                        "slug": "H.-Kauniskangas",
                        "structuredName": {
                            "firstName": "Hannu",
                            "lastName": "Kauniskangas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kauniskangas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704694"
                        ],
                        "name": "J. Sauvola",
                        "slug": "J.-Sauvola",
                        "structuredName": {
                            "firstName": "Jaakko",
                            "lastName": "Sauvola",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sauvola"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 56
                            }
                        ],
                        "text": "mated quality improvement of grey-scale document images [5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 33185282,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "f623d46bd6893bb4af5fe8f7a644aad8a3d497dd",
            "isKey": false,
            "numCitedBy": 1,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-automated-defect-management-for-document-images-Kauniskangas-Sauvola",
            "title": {
                "fragments": [],
                "text": "An automated defect management for document images"
            },
            "venue": {
                "fragments": [],
                "text": "ICPR"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 218
                            }
                        ],
                        "text": "The main contribution of this paper is that we use multiple frame information to enhance the textual image so the text can be easily separated from background while the previous work focuses primarily on single frames [5, 8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic text ex-  traction from video for content-based annotation and re-  trieval"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings of ICPR,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 97
                            }
                        ],
                        "text": "In the literature numerous methods have been reported including global and adaptive thresholding [1, 12, 14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "In An introduction to image processing, pages 115\u2013116"
            },
            "venue": {
                "fragments": [],
                "text": "Englewood Cliffs, N.J.:Prentice Hall,"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dynamic thresholding of grey-level images"
            },
            "venue": {
                "fragments": [],
                "text": "Dynamic thresholding of grey-level images"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 67
                            }
                        ],
                        "text": "In the literature numerous thresholding methods have been reported [1, 6, 7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A spatial threshold-  ing method for image segmentation"
            },
            "venue": {
                "fragments": [],
                "text": "In PAMI(10),"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 19
                            }
                        ],
                        "text": "As we addressed in [8], increasing the image resolution by a factor of 2 is a good balance between computation cost and OCR accuracy, corresponding to 0."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Text extraction and recognition in digital video"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings of DAS,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 67
                            }
                        ],
                        "text": "In the literature numerous thresholding methods have been reported [1, 6, 7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An introduction to image processing,  pages 115{116"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "OCR performance evaluation software \u2013 user's manual"
            },
            "venue": {
                "fragments": [],
                "text": "OCR performance evaluation software \u2013 user's manual"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "OCR performance evaluation software \u2013 user \u2019 s manual"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 67
                            }
                        ],
                        "text": "In the literature numerous thresholding methods have been reported [1, 6, 7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dynamic thresholding of grey-level im-  ages"
            },
            "venue": {
                "fragments": [],
                "text": "In ICPR86,"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 97
                            }
                        ],
                        "text": "In the literature numerous methods have been reported including global and adaptive thresholding [1, 12, 14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dynamic thresholding of greylevel images"
            },
            "venue": {
                "fragments": [],
                "text": "In ICPR86,"
            },
            "year": 1986
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 10,
            "methodology": 12
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 28,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Text-enhancement-in-digital-video-using-multiple-Li-Doermann/1484e9fc337e99fbd70ece46005c7cf6a8541bb7?sort=total-citations"
}