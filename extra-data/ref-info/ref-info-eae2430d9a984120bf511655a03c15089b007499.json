{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166443669"
                        ],
                        "name": "M. Jones",
                        "slug": "M.-Jones",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Jones"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 125993244,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "3be0c6a742f89a9ef648e443e5d042587d07e5e9",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We had previously shown that regularization principles lead to approximation schemes which are equivalent to networks with one layer of hidden units, called regularization networks. We summarize some recent results (Girosi, Jones and Poggio, 1993) that show that regularization networks encompass a much broader range of approximation schemes, including many of the popular general additive models and some of the neural networks. In particular, additive splines as well as some tensor product splines can be obtained from appropriate classes of smoothness functionals. Furthermore the same extension that extends radial basis functions to hyper basis functions leads from additive models to ridge approximation models, containing as special cases Breiman's hinge functions and some forms of projection pursuit regression. We propose to use the term generalized regularization networks for this broad class of approximation schemes that follow from an extension of regularization."
            },
            "slug": "From-regularization-to-radial,-tensor-and-additive-Poggio-Girosi",
            "title": {
                "fragments": [],
                "text": "From regularization to radial, tensor and additive splines"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The term generalized regularization networks is proposed to be used for this broad class of approximation schemes that follow from an extension of regularization, including many of the popular general additive models and some of the neural networks."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166443669"
                        ],
                        "name": "M. Jones",
                        "slug": "M.-Jones",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 53854,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "1b76ef6f839cc03559ae7ce5ded915c55c2214ab",
            "isKey": false,
            "numCitedBy": 111,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Poggio and Girosi showed that regularization principles lead to approximation schemes which are equivalent to networks with one layer of hidden units, called regularization networks. They summarize their results (1993) that show that regularization networks encompass a much broader range of approximation schemes, including many of the general additive models and some of the neural networks. In particular, additive splines as well as some tensor product splines can be obtained from appropriate classes of smoothness functionals. The same extension that extends radial basis functions to hyper basis functions leads from additive models to ridge approximation models, containing as special cases Breiman's hinge functions and some forms of projection pursuit regression. The authors propose to use the term generalized regularization networks for this broad class of approximation schemes that follow from an extension of regularization.<<ETX>>"
            },
            "slug": "From-regularization-to-radial,-tensor-and-additive-Poggio-Girosi",
            "title": {
                "fragments": [],
                "text": "From regularization to radial, tensor and additive splines"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The authors propose to use the term generalized regularization networks for this broad class of approximation schemes that follow from an extension of regularization, including many of the general additive models and some of the neural networks."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks for Signal Processing III - Proceedings of the 1993 IEEE-SP Workshop"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 42
                            }
                        ],
                        "text": "For example in entries 5 and 6 of Table 3 (Girosi and Anzellotti 1992, 1993; Girosi 1993) the result holds in H2\u201dJ(Rd), that is the Sobolev space of functions whose derivatives up to order 2m are integrable (Ziemer 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 116357204,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "739ea1973d3eaadd46e259b2162f2cfa804088b8",
            "isKey": false,
            "numCitedBy": 91,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper consists of two parts. In the first part we consider the problem of learning from examples in the setting of the theory of the approximation of multivariate functions from sparse data. We first will show how an approach based on regularization theory leads to develop a family of approximation techniques, including Radial Basis Functions, and some tensor product and additive splines. Then we will show how this fairly classical approach has to be extended in order to cope with special features of the problem of learning of examples, such as high dimensionality and strong anisotropics. Furthermore, the same extension that leads from Radial Basis Functions (RBF) to Hyper Basis Functions (HBF) also leads from additive models to ridge approximation models, such as some forms of Projection Pursuit Regression."
            },
            "slug": "Regularization-Theory,-Radial-Basis-Functions-and-Girosi",
            "title": {
                "fragments": [],
                "text": "Regularization Theory, Radial Basis Functions and Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper will show how an approach based on regularization theory leads to develop a family of approximation techniques, including Radial Basis Functions, and some tensor product and additive splines, and how this fairly classical approach has to be extended in order to cope with special features of the problem of learning of examples."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6635519,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4abd4e51705e74f1739bd3a1e47ac10e45f6468b",
            "isKey": false,
            "numCitedBy": 1171,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning an input-output mapping from a set of examples, of the type that many neural networks have been constructed to perform, can be regarded as synthesizing an approximation of a multidimensional function (that is, solving the problem of hypersurface reconstruction). From this point of view, this form of learning is closely related to classical approximation techniques, such as generalized splines and regularization theory. A theory is reported that shows the equivalence between regularization and a class of three-layer networks called regularization networks or hyper basis functions. These networks are not only equivalent to generalized splines but are also closely related to the classical radial basis functions used for interpolation tasks and to several pattern recognition and neural network algorithms. They also have an interesting interpretation in terms of prototypes that are synthesized and optimally combined during the learning stage."
            },
            "slug": "Regularization-Algorithms-for-Learning-That-Are-to-Poggio-Girosi",
            "title": {
                "fragments": [],
                "text": "Regularization Algorithms for Learning That Are Equivalent to Multilayer Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A theory is reported that shows the equivalence between regularization and a class of three-layer networks called regularization networks or hyper basis functions."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14892653,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "089a76dbc62a06ad30ae1925530e8733e850268e",
            "isKey": false,
            "numCitedBy": 3701,
            "numCiting": 96,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of the approximation of nonlinear mapping, (especially continuous mappings) is considered. Regularization theory and a theoretical framework for approximation (based on regularization techniques) that leads to a class of three-layer networks called regularization networks are discussed. Regularization networks are mathematically related to the radial basis functions, mainly used for strict interpolation tasks. Learning as approximation and learning as hypersurface reconstruction are discussed. Two extensions of the regularization approach are presented, along with the approach's corrections to splines, regularization, Bayes formulation, and clustering. The theory of regularization networks is generalized to a formulation that includes task-dependent clustering and dimensionality reduction. Applications of regularization networks are discussed. >"
            },
            "slug": "Networks-for-approximation-and-learning-Poggio-Girosi",
            "title": {
                "fragments": [],
                "text": "Networks for approximation and learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145717635"
                        ],
                        "name": "H. Mhaskar",
                        "slug": "H.-Mhaskar",
                        "structuredName": {
                            "firstName": "Hrushikesh",
                            "lastName": "Mhaskar",
                            "middleNames": [
                                "Narhar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Mhaskar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9891530,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "0e168de5975a9dfbe12837ef19a0366d938bc1d7",
            "isKey": false,
            "numCitedBy": 307,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We prove that neural networks with a single hidden layer are capable of providing an optimal order of approximation for functions assumed to possess a given number of derivatives, if the activation function evaluated by each principal element satisfies certain technical conditions. Under these conditions, it is also possible to construct networks that provide a geometric order of approximation for analytic target functions. The permissible activation functions include the squashing function (1 ex)1 as well as a variety of radial basis functions. Our proofs are constructive. The weights and thresholds of our networks are chosen independently of the target function; we give explicit formulas for the coefficients as simple, continuous, linear functionals of the target function."
            },
            "slug": "Neural-Networks-for-Optimal-Approximation-of-Smooth-Mhaskar",
            "title": {
                "fragments": [],
                "text": "Neural Networks for Optimal Approximation of Smooth and Analytic Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "It is proved that neural networks with a single hidden layer are capable of providing an optimal order of approximation for functions assumed to possess a given number of derivatives, if the activation function evaluated by each principal element satisfies certain technical conditions."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 205119351,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "77fdd39ab366b65a617015a72fe8dc9d0b394d64",
            "isKey": false,
            "numCitedBy": 716,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Connectionist-nonparametric-regression:-Multilayer-White",
            "title": {
                "fragments": [],
                "text": "Connectionist nonparametric regression: Multilayer feedforward networks can learn arbitrary mappings"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2076400"
                        ],
                        "name": "B. Caprile",
                        "slug": "B.-Caprile",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Caprile",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Caprile"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10243731,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "898c01de58eb3b8e790b60e0fe0db2230d88f15b",
            "isKey": false,
            "numCitedBy": 699,
            "numCiting": 152,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning an input-output mapping from a set of examples, of the type that many neural networks have been constructed to perform, can be regarded as synthesizing an approximation of a multi-dimensional function. We develop a theoretical framework for approximation based on regularization techniques that leads to a class of three-layer networks that we call Generalized Radial Basis Functions (GRBF). GRBF networks are not only equivalent to generalized splines, but are also closely related to several pattern recognition methods and neural network algorithms. The paper introduces several extensions and applications of the technique and discusses intriguing analogies with neurobiological data."
            },
            "slug": "Extensions-of-a-Theory-of-Networks-for-and-Learning-Girosi-Poggio",
            "title": {
                "fragments": [],
                "text": "Extensions of a Theory of Networks for Approximation and Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A theoretical framework for approximation based on regularization techniques that leads to a class of three-layer networks that is called Generalized Radial Basis Functions (GRBF), which is not only equivalent to generalized splines, but is closely related to several pattern recognition methods and neural network algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16804370,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a27406efbf76f1d3538643135bcb13248567a41",
            "isKey": false,
            "numCitedBy": 187,
            "numCiting": 138,
            "paperAbstract": {
                "fragments": [],
                "text": "Feedforward networks together with their training algorithms are a class of regression techniques that can be used to learn to perform some task from a set of examples. The question of generalization of network performance from a finite training set to unseen data is clearly of crucial importance. In this article we first show that the generalization error can be decomposed into two terms: the approximation error, due to the insufficient representational capacity of a finite sized network, and the estimation error, due to insufficient information about the target function because of the finite number of samples. We then consider the problem of learning functions belonging to certain Sobolev spaces with gaussian radial basis functions. Using the above-mentioned decomposition we bound the generalization error in terms of the number of basis functions and number of examples. While the bound that we derive is specific for radial basis functions, a number of observations deriving from it apply to any approximation technique. Our result also sheds light on ways to choose an appropriate network architecture for a particular problem and the kinds of problems that can be effectively solved with finite resources, i.e., with a finite number of parameters and finite amounts of data."
            },
            "slug": "On-the-Relationship-between-Generalization-Error,-Niyogi-Girosi",
            "title": {
                "fragments": [],
                "text": "On the Relationship between Generalization Error, Hypothesis Complexity, and Sample Complexity for Radial Basis Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This article shows that the generalization error can be decomposed into two terms: the approximation error, due to the insufficient representational capacity of a finite sized network, and the estimation error,due to insufficient information about the target function because of the finite number of samples."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20658113"
                        ],
                        "name": "A. Barron",
                        "slug": "A.-Barron",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barron",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barron"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15383918,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04113e8974341f97258800126d05fd8df2751b7e",
            "isKey": false,
            "numCitedBy": 2593,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Approximation properties of a class of artificial neural networks are established. It is shown that feedforward networks with one layer of sigmoidal nonlinearities achieve integrated squared error of order O(1/n), where n is the number of nodes. The approximated function is assumed to have a bound on the first moment of the magnitude distribution of the Fourier transform. The nonlinear parameters associated with the sigmoidal nodes, as well as the parameters of linear combination, are adjusted in the approximation. In contrast, it is shown that for series expansions with n terms, in which only the parameters of linear combination are adjusted, the integrated squared approximation error cannot be made smaller than order 1/n/sup 2/d/ uniformly for functions satisfying the same smoothness assumption, where d is the dimension of the input to the function. For the class of functions examined, the approximation rate and the parsimony of the parameterization of the networks are shown to be advantageous in high-dimensional settings. >"
            },
            "slug": "Universal-approximation-bounds-for-superpositions-a-Barron",
            "title": {
                "fragments": [],
                "text": "Universal approximation bounds for superpositions of a sigmoidal function"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "The approximation rate and the parsimony of the parameterization of the networks are shown to be advantageous in high-dimensional settings and the integrated squared approximation error cannot be made smaller than order 1/n/sup 2/d/ uniformly for functions satisfying the same smoothness assumption."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145768864"
                        ],
                        "name": "D. Broomhead",
                        "slug": "D.-Broomhead",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Broomhead",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Broomhead"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144159852"
                        ],
                        "name": "D. Lowe",
                        "slug": "D.-Lowe",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lowe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lowe"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117200472,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5558a34dfd1dbb572895664d38fca04029a99cb",
            "isKey": false,
            "numCitedBy": 2933,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : The relationship between 'learning' in adaptive layered networks and the fitting of data with high dimensional surfaces is discussed. This leads naturally to a picture of 'generalization in terms of interpolation between known data points and suggests a rational approach to the theory of such networks. A class of adaptive networks is identified which makes the interpolation scheme explicit. This class has the property that learning is equivalent to the solution of a set of linear equations. These networks thus represent nonlinear relationships while having a guaranteed learning rule. Great Britain."
            },
            "slug": "Radial-Basis-Functions,-Multi-Variable-Functional-Broomhead-Lowe",
            "title": {
                "fragments": [],
                "text": "Radial Basis Functions, Multi-Variable Functional Interpolation and Adaptive Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "The relationship between 'learning' in adaptive layered networks and the fitting of data with high dimensional surfaces is discussed, leading naturally to a picture of 'generalization in terms of interpolation between known data points and suggests a rational approach to the theory of such networks."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145016534"
                        ],
                        "name": "J. Moody",
                        "slug": "J.-Moody",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Moody",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Moody"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60756461,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d6d0053f5f32ef87b60435e04ea5e0d81fad4ec",
            "isKey": false,
            "numCitedBy": 159,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The author proposes a new estimate of generalization performance for nonlinear learning systems called the generalized prediction error (GPE) which is based upon the notion of the effective number of parameters p/sub eff/( lambda ). GPE does not require the use of a test set or computationally intensive cross validation and generalizes previously proposed model selection criteria (such as GCV, FPE, AIC, and PSE) in that it is formulated to include biased, nonlinear models (such as back propagation networks) which may incorporate weight decay or other regularizers. The effective number of parameters p/sub eff/( lambda ) depends upon the amount of bias and smoothness (as determined by the regularization parameter lambda ) in the model, but generally differs from the number of weights p. Construction of an optimal architecture thus requires not just finding the weights w/sub lambda /* which minimize the training function U( lambda , w) but also the lambda which minimizes GPE( lambda ).<<ETX>>"
            },
            "slug": "Note-on-generalization,-regularization-and-in-Moody",
            "title": {
                "fragments": [],
                "text": "Note on generalization, regularization and architecture selection in nonlinear learning systems"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The author proposes a new estimate of generalization performance for nonlinear learning systems called the generalized prediction error (GPE) which is based upon the notion of the effective number of parameters p/sub eff/( lambda )."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks for Signal Processing Proceedings of the 1991 IEEE Workshop"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145717635"
                        ],
                        "name": "H. Mhaskar",
                        "slug": "H.-Mhaskar",
                        "structuredName": {
                            "firstName": "Hrushikesh",
                            "lastName": "Mhaskar",
                            "middleNames": [
                                "Narhar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Mhaskar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122396506,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "be866f9e928956545fb05ce9047a53b3c096f17d",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of constructing universal networks capable of approximating all functions having bounded derivatives is discussed. It is demonstrated that, using standard ideas from the theory of spline approximation, it is possible to construct such networks to provide localized approximation. The networks can be used to implement multivariate analogues of the Chui-Wang wavelets (1990) and also for the simultaneous approximation of a function and its derivative. The number of neurons required to yield the desired approximation at any point does not depend upon the degree of accuracy desired.<<ETX>>"
            },
            "slug": "Neural-networks-for-localized-approximation-of-real-Mhaskar",
            "title": {
                "fragments": [],
                "text": "Neural networks for localized approximation of real functions"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It is demonstrated that, using standard ideas from the theory of spline approximation, it is possible to construct such networks to provide localized approximation and can be used for the simultaneous approximation of a function and its derivative."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks for Signal Processing III - Proceedings of the 1993 IEEE-SP Workshop"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145717635"
                        ],
                        "name": "H. Mhaskar",
                        "slug": "H.-Mhaskar",
                        "structuredName": {
                            "firstName": "Hrushikesh",
                            "lastName": "Mhaskar",
                            "middleNames": [
                                "Narhar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Mhaskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1996365"
                        ],
                        "name": "N. Hahm",
                        "slug": "N.-Hahm",
                        "structuredName": {
                            "firstName": "Nahmwoo",
                            "lastName": "Hahm",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Hahm"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17955940,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "4de48f21660c1d2ab1f05646dbc9e6fcdec2f8d6",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We construct generalized translation networks to approximate uniformly a class of nonlinear, continuous functionals defined on Lp(1, 1s) for integer s 1, 1 p < , or C(1, 1s). We obtain lower bounds on the possible order of approximation for such functionals in terms of any approximation process depending continuously on a given number of parameters. Our networks almost achieve this order of approximation in terms of the number of parameters (neurons) involved in the network. The training is simple and noniterative; in particular, we avoid any optimization such as that involved in the usual backpropagation."
            },
            "slug": "Neural-Networks-for-Functional-Approximation-and-Mhaskar-Hahm",
            "title": {
                "fragments": [],
                "text": "Neural Networks for Functional Approximation and System Identification"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "These networks construct generalized translation networks to approximate uniformly a class of nonlinear, continuous functionals defined on Lp(1, 1s) for integer s 1, 1 p < , or C(1), 1s."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144564069"
                        ],
                        "name": "C. Chui",
                        "slug": "C.-Chui",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Chui",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Chui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2153898598"
                        ],
                        "name": "Xin Li",
                        "slug": "Xin-Li",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xin Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145717635"
                        ],
                        "name": "H. Mhaskar",
                        "slug": "H.-Mhaskar",
                        "structuredName": {
                            "firstName": "Hrushikesh",
                            "lastName": "Mhaskar",
                            "middleNames": [
                                "Narhar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Mhaskar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 27727180,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "70da97f4fb509689837818956db40dcf63cf9564",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "AbstractLets\u22651 be an integer andW be the class of all functions having integrable partial derivatives on [0, 1]s. We are interested in the minimum number of neurons in a neural network with a single hidden layer required in order to provide a mean approximation order of a preassigned\u03b5>0 to each function inW. We prove that this number cannot be\n$$\\mathcal{O}( \\in ^{ - s} log(1/ \\in ))$$\n if a spline-like localization is required. This cannot be improved even if one allows different neurons to evaluate different activation functions, even depending upon the target function. Nevertheless, for any\u03b4>0, a network with\n$$\\mathcal{O}( \\in ^{ - s - \\delta } )$$\n neurons can be constructed to provide this order of approximation, with localization. Analogous results are also valid for otherLp norms."
            },
            "slug": "Limitations-of-the-approximation-capabilities-of-Chui-Li",
            "title": {
                "fragments": [],
                "text": "Limitations of the approximation capabilities of neural networks with one hidden layer"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "It is proved that the minimum number of neurons in a neural network with a single hidden layer required in order to provide a mean approximation order of a preassigned\u03b5>0 to each function inW cannot be $$\\mathcal{O}( \\in ^{ - s} log(1/ \\in ))$$  if a spline-like localization is required."
            },
            "venue": {
                "fragments": [],
                "text": "Adv. Comput. Math."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3175428"
                        ],
                        "name": "Lizhong Wu",
                        "slug": "Lizhong-Wu",
                        "structuredName": {
                            "firstName": "Lizhong",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lizhong Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145016534"
                        ],
                        "name": "J. Moody",
                        "slug": "J.-Moody",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Moody",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Moody"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17028860,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "dc1684ba99e7eac5c977cb1721219b88a4e7cd93",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive a smoothing regularizer for dynamic network models by requiring robustness in prediction performance to perturbations of the training data. The regularizer can be viewed as a generalization of the first-order Tikhonov stabilizer to dynamic models. For two layer networks with recurrent connections described by the training criterion with the regularizer is where = {U, V, W} is the network parameter set, Z(t) are the targets, I(t) = {X(s), s = 1,2, , t} represents the current and all historical input information, N is the size of the training data set, is the regularizer, and is a regularization parameter. The closed-form expression for the regularizer for time-lagged recurrent networks is where is the Euclidean matrix norm and is a factor that depends upon the maximal value of the first derivatives of the internal unit activations f(). Simplifications of the regularizer are obtained for simultaneous recurrent nets ( 0), two-layer feedforward nets, and one layer linear nets. We have successfully tested this regularizer in a number of case studies and found that it performs better than standard quadratic weight decay."
            },
            "slug": "A-Smoothing-Regularizer-for-Feedforward-and-Neural-Wu-Moody",
            "title": {
                "fragments": [],
                "text": "A Smoothing Regularizer for Feedforward and Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A smoothing regularizer for dynamic network models is derived by requiring robustness in prediction performance to perturbations of the training data and found that it performs better than standard quadratic weight decay."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20658113"
                        ],
                        "name": "A. Barron",
                        "slug": "A.-Barron",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barron",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barron"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 22935660,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a9209f90c02a378720879a3bb93aa2f7181cf5f2",
            "isKey": false,
            "numCitedBy": 383,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "AbstractFor a common class of artificial neural networks, the mean integrated squared error between the estimated network and a target function f is shown to be bounded by \n$${\\text{O}}\\left( {\\frac{{C_f^2 }}{n}} \\right) + O(\\frac{{ND}}{N}\\log N)$$\nwhere n is the number of nodes, d is the input dimension of the function, N is the number of training observations, and Cf is the first absolute moment of the Fourier magnitude distribution of f. The two contributions to this total risk are the approximation error and the estimation error. Approximation error refers to the distance between the target function and the closest neural network function of a given architecture and estimation error refers to the distance between this ideal network function and an estimated network function. With n ~ Cf(N/(dlog N))1/2 nodes, the order of the bound on the mean integrated squared error is optimized to be O(Cf((d/N)log N)1/2). The bound demonstrates surprisingly favorable properties of network estimation compared to traditional series and nonparametric curve estimation techniques in the case that d is moderately large. Similar bounds are obtained when the number of nodes n is not preselected as a function of Cf (which is generally not known a priori), but rather the number of nodes is optimized from the observed data by the use of a complexity regularization or minimum description length criterion. The analysis involves Fourier techniques for the approximation error, metric entropy considerations for the estimation error, and a calculation of the index of resolvability of minimum complexity estimation of the family of networks."
            },
            "slug": "Approximation-and-Estimation-Bounds-for-Artificial-Barron",
            "title": {
                "fragments": [],
                "text": "Approximation and Estimation Bounds for Artificial Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "Binding of the mean integrated squared error between the estimated network and a target function f is shown to be bounded by O(Cf((d/N)log N)1/2), which demonstrates surprisingly favorable properties of network estimation compared to traditional series and nonparametric curve estimation techniques in the case that d is moderately large."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '91"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145491571"
                        ],
                        "name": "L. Jones",
                        "slug": "L.-Jones",
                        "structuredName": {
                            "firstName": "Lee",
                            "lastName": "Jones",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 122240265,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7e7f56734291de81e99976d092b58e4e4a2b6f60",
            "isKey": false,
            "numCitedBy": 519,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "A general convergence criterion for certain iterative sequences in Hilbert space is presented. For an important subclass of these sequences, estimates of the rate of convergence are given. Under very mild assumptions these results establish an 0(1/ F4n) nonsampling convergence rate for projection pursuit regression and neural network training; where n represents the number of ridge functions, neurons or coefficients in a greedy basis expansion."
            },
            "slug": "A-Simple-Lemma-on-Greedy-Approximation-in-Hilbert-Jones",
            "title": {
                "fragments": [],
                "text": "A Simple Lemma on Greedy Approximation in Hilbert Space and Convergence Rates for Projection Pursuit Regression and Neural Network Training"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2416462"
                        ],
                        "name": "G. Cybenko",
                        "slug": "G.-Cybenko",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Cybenko",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Cybenko"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3958369,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8da1dda34ecc96263102181448c94ec7d645d085",
            "isKey": false,
            "numCitedBy": 6387,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks."
            },
            "slug": "Approximation-by-superpositions-of-a-sigmoidal-Cybenko",
            "title": {
                "fragments": [],
                "text": "Approximation by superpositions of a sigmoidal function"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "It is demonstrated that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Control. Signals Syst."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145016534"
                        ],
                        "name": "J. Moody",
                        "slug": "J.-Moody",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Moody",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Moody"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2509066"
                        ],
                        "name": "N. Yarvin",
                        "slug": "N.-Yarvin",
                        "structuredName": {
                            "firstName": "Norman",
                            "lastName": "Yarvin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Yarvin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6153045,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4cc51c5c86c0c0dd5cbc78b1735e91f3e56f143b",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Feedforward networks composed of units which compute a sigmoidal function of a weighted sum of their inputs have been much investigated. We tested the approximation and estimation capabilities of networks using functions more complex than sigmoids. Three classes of functions were tested: polynomials, rational functions, and flexible Fourier series. Unlike sigmoids, these classes can fit non-monotonic functions. They were compared on three problems: prediction of Boston housing prices, the sunspot count, and robot arm inverse dynamics. The complex units attained clearly superior performance on the robot arm problem, which is a highly non-monotonic, pure approximation problem. On the noisy and only mildly nonlinear Boston housing and sunspot problems, differences among the complex units were revealed; polynomials did poorly, whereas rationals and flexible Fourier series were comparable to sigmoids."
            },
            "slug": "Networks-with-Learned-Unit-Response-Functions-Moody-Yarvin",
            "title": {
                "fragments": [],
                "text": "Networks with Learned Unit Response Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Differences among the complex units of networks using functions more complex than sigmoids were revealed; polynomials did poorly, whereas rationals and flexible Fourier series were comparable to sigmoid."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145717635"
                        ],
                        "name": "H. Mhaskar",
                        "slug": "H.-Mhaskar",
                        "structuredName": {
                            "firstName": "Hrushikesh",
                            "lastName": "Mhaskar",
                            "middleNames": [
                                "Narhar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Mhaskar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 41177320,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "9134c6e67cca05c93bf8a865b1960049c1064025",
            "isKey": false,
            "numCitedBy": 143,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We prove that an artificial neural network with multiple hidden layers and akth-order sigmoidal response function can be used to approximate any continuous function on any compact subset of a Euclidean space so as to achieve the Jackson rate of approximation. Moreover, if the function to be approximated has an analytic extension, then a nearly geometric rate of approximation can be achieved. We also discuss the problem of approximation of a compact subset of a Euclidean space with such networks with a classical sigmoidal response function."
            },
            "slug": "Approximation-properties-of-a-multilayered-neural-Mhaskar",
            "title": {
                "fragments": [],
                "text": "Approximation properties of a multilayered feedforward artificial neural network"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "It is proved that an artificial neural network with multiple hidden layers and akth-order sigmoidal response function can be used to approximate any continuous function on any compact subset of a Euclidean space so as to achieve the Jackson rate of approximation."
            },
            "venue": {
                "fragments": [],
                "text": "Adv. Comput. Math."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145016534"
                        ],
                        "name": "J. Moody",
                        "slug": "J.-Moody",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Moody",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Moody"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2319258"
                        ],
                        "name": "C. Darken",
                        "slug": "C.-Darken",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Darken",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Darken"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 31251383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e7c4f513f24c3b82a1138b9f22ed87ed00cbe76",
            "isKey": false,
            "numCitedBy": 4527,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a network architecture which uses a single internal layer of locally-tuned processing units to learn both classification tasks and real-valued function approximations (Moody and Darken 1988). We consider training such networks in a completely supervised manner, but abandon this approach in favor of a more computationally efficient hybrid learning method which combines self-organized and supervised learning. Our networks learn faster than backpropagation for two reasons: the local representations ensure that only a few units respond to any given input, thus reducing computational overhead, and the hybrid learning rules are linear rather than nonlinear, thus leading to faster convergence. Unlike many existing methods for data analysis, our network architecture and learning rules are truly adaptive and are thus appropriate for real-time use."
            },
            "slug": "Fast-Learning-in-Networks-of-Locally-Tuned-Units-Moody-Darken",
            "title": {
                "fragments": [],
                "text": "Fast Learning in Networks of Locally-Tuned Processing Units"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This work proposes a network architecture which uses a single internal layer of locally-tuned processing units to learn both classification tasks and real-valued function approximations (Moody and Darken 1988)."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145016534"
                        ],
                        "name": "J. Moody",
                        "slug": "J.-Moody",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Moody",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Moody"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 609306,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7e0dab4fe4299bc2f8b4b18f82702af717cf3924",
            "isKey": false,
            "numCitedBy": 559,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an analysis of how the generalization performance (expected test set error) relates to the expected training set error for nonlinear learning systems, such as multilayer perceptrons and radial basis functions. The principal result is the following relationship (computed to second order) between the expected test set and training set errors: \u2329etest(\u03bb)\u232a\u03be\u03be\u2032 \u2248 \u2329etrain(\u03bb)\u232a\u03be + 2\u03c3eff2 peff(\u03bb)/n (1) Here, n is the size of the training sample \u03be, \u03c3eff2 is the effective noise variance in the response variable(s), \u03bb, is a regularization or weight decay parameter, and Peff(\u03bb) is the effective number of parameters in the nonlinear model. The expectations \u2329 \u232a of training set and test set errors are taken over possible training sets \u03be and training and test sets \u03be\u2032 respectively. The effective number of parameters peff(\u03bb) usually differs from the true number of model parameters p for nonlinear or regularized models; this theoretical conclusion is supported by Monte Carlo experiments. In addition to the surprising result that peff(\u03bb) \u2260 p, we propose an estimate of (1) called the generalized prediction error (GPE) which generalizes well established estimates of prediction risk such as Akaike's F P E and AIC, Mallows Cp, and Barron's P S E to the nonlinear setting."
            },
            "slug": "The-Effective-Number-of-Parameters:-An-Analysis-of-Moody",
            "title": {
                "fragments": [],
                "text": "The Effective Number of Parameters: An Analysis of Generalization and Regularization in Nonlinear Learning Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The surprising result that peff(\u03bb) \u2260 p is proposed, called the generalized prediction error (GPE) which generalizes well established estimates of prediction risk such as Akaike's F P E and AIC, Mallows Cp, and Barron's P S E to the nonlinear setting."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 43711678,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "656a33c1db546da8490d6eba259e2a849d73a001",
            "isKey": false,
            "numCitedBy": 1012,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "The premise of this article is that learning procedures used to train artificial neural networks are inherently statistical techniques. It follows that statistical theory can provide considerable insight into the properties, advantages, and disadvantages of different network learning methods. We review concepts and analytical results from the literatures of mathematical statistics, econometrics, systems identification, and optimization theory relevant to the analysis of learning in artificial neural networks. Because of the considerable variety of available learning procedures and necessary limitations of space, we cannot provide a comprehensive treatment. Our focus is primarily on learning procedures for feedforward networks. However, many of the concepts and issues arising in this framework are also quite broadly relevant to other network learning paradigms. In addition to providing useful insights, the material reviewed here suggests some potentially useful new training methods for artificial neural networks."
            },
            "slug": "Learning-in-Artificial-Neural-Networks:-A-White",
            "title": {
                "fragments": [],
                "text": "Learning in Artificial Neural Networks: A Statistical Perspective"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Concepts and analytical results from the literatures of mathematical statistics, econometrics, systems identification, and optimization theory relevant to the analysis of learning in artificial neural networks are reviewed."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2085069"
                        ],
                        "name": "C. Rabut",
                        "slug": "C.-Rabut",
                        "structuredName": {
                            "firstName": "Christophe",
                            "lastName": "Rabut",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rabut"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 121343367,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3b2627c636e72b807fa626df22ef601511a9c540",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "AN-INTRODUCTION-TO-SCHOENBERG'S-APPROXIMATION-Rabut",
            "title": {
                "fragments": [],
                "text": "AN INTRODUCTION TO SCHOENBERG'S APPROXIMATION"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194361"
                        ],
                        "name": "S. Geman",
                        "slug": "S.-Geman",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2246319"
                        ],
                        "name": "E. Bienenstock",
                        "slug": "E.-Bienenstock",
                        "structuredName": {
                            "firstName": "Elie",
                            "lastName": "Bienenstock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Bienenstock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2330895"
                        ],
                        "name": "R. Doursat",
                        "slug": "R.-Doursat",
                        "structuredName": {
                            "firstName": "Ren\u00e9",
                            "lastName": "Doursat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Doursat"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14215320,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "a34e35dbbc6911fa7b94894dffdc0076a261b6f0",
            "isKey": false,
            "numCitedBy": 3532,
            "numCiting": 151,
            "paperAbstract": {
                "fragments": [],
                "text": "Feedforward neural networks trained by error backpropagation are examples of nonparametric regression estimators. We present a tutorial on nonparametric inference and its relation to neural networks, and we use the statistical viewpoint to highlight strengths and weaknesses of neural models. We illustrate the main points with some recognition experiments involving artificial data as well as handwritten numerals. In way of conclusion, we suggest that current-generation feedforward neural networks are largely inadequate for difficult problems in machine perception and machine learning, regardless of parallel-versus-serial hardware or other implementation issues. Furthermore, we suggest that the fundamental challenges in neural modeling are about representation rather than learning per se. This last point is supported by additional experiments with handwritten numerals."
            },
            "slug": "Neural-Networks-and-the-Bias/Variance-Dilemma-Geman-Bienenstock",
            "title": {
                "fragments": [],
                "text": "Neural Networks and the Bias/Variance Dilemma"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is suggested that current-generation feedforward neural networks are largely inadequate for difficult problems in machine perception and machine learning, regardless of parallel-versus-serial hardware or other implementation issues."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145717635"
                        ],
                        "name": "H. Mhaskar",
                        "slug": "H.-Mhaskar",
                        "structuredName": {
                            "firstName": "Hrushikesh",
                            "lastName": "Mhaskar",
                            "middleNames": [
                                "Narhar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Mhaskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708279"
                        ],
                        "name": "C. Micchelli",
                        "slug": "C.-Micchelli",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Micchelli",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Micchelli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 121886933,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ebf2bc6f41d19d5fe39f93006be5677d28db2ac1",
            "isKey": false,
            "numCitedBy": 268,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Approximation-by-superposition-of-sigmoidal-and-Mhaskar-Micchelli",
            "title": {
                "fragments": [],
                "text": "Approximation by superposition of sigmoidal and radial basis functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103290696"
                        ],
                        "name": "Byandreas Buja",
                        "slug": "Byandreas-Buja",
                        "structuredName": {
                            "firstName": "Byandreas",
                            "lastName": "Buja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Byandreas Buja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784682"
                        ],
                        "name": "T. Hastie",
                        "slug": "T.-Hastie",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Hastie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hastie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761784"
                        ],
                        "name": "R. Tibshirani",
                        "slug": "R.-Tibshirani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tibshirani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tibshirani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 45
                            }
                        ],
                        "text": "Additive models are well known in statistics (Hastie and Tibshirani 1986, 1987, 1990; Stone 1985; Wahba 1990; Buja et al. 1989) and can be considered as a generalization of linear models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15949963,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "643c4d5e57b605cc100370669d0d1f1fb775a96b",
            "isKey": false,
            "numCitedBy": 1000,
            "numCiting": 108,
            "paperAbstract": {
                "fragments": [],
                "text": "We study linear smoothers and their use in building nonparametric regression models. In the first part of this paper we examine certain aspects of linear smoothers for scatterplots; examples of these are the running-mean and running-line, kernel and cubic spline smoothers. The eigenvalue and singular value decompositions of the corresponding smoother matrix are used to describe qualitatively a smoother, and several other topics such as the number of degrees of freedom of a smoother are discussed. In the second part of the paper we describe how linear smoothers can be used to estimate the additive model, a powerful nonparametric regression model, using the \"backfitting algorithm.\" We show that backfitting is the Gauss-Seidel iterative method for solving a set of normal equations associated with the additive model. We provide conditions for consistency and nondegeneracy and prove convergence for the backfitting and related algorithms for a class of smoothers that includes cubic spline smoothers."
            },
            "slug": "Linear-Smoothers-and-Additive-Models-Buja-Hastie",
            "title": {
                "fragments": [],
                "text": "Linear Smoothers and Additive Models"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that backfitting is the Gauss-Seidel iterative method for solving a set of normal equations associated with the additive model and conditions for consistency and nondegeneracy are provided and convergence is proved for the backfitting and related algorithms for a class of smoothers that includes cubic spline smoothers."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145717635"
                        ],
                        "name": "H. Mhaskar",
                        "slug": "H.-Mhaskar",
                        "structuredName": {
                            "firstName": "Hrushikesh",
                            "lastName": "Mhaskar",
                            "middleNames": [
                                "Narhar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Mhaskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708279"
                        ],
                        "name": "C. Micchelli",
                        "slug": "C.-Micchelli",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Micchelli",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Micchelli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2424388,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "35083a44c25d2aec5feb850ae7efb0f643d96644",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the complexity problem in artificial feedforward neural networks designed to approximate real valued functions of several real variables; i.e., we estimate the number of neurons in a network required to ensure a given degree of approximation to every function in a given function class. We indicate how to construct networks with the indicated number of neurons evaluating standard activation functions. Our general theorem shows that the smoother the activation function, the better the rate of approximation."
            },
            "slug": "How-to-Choose-an-Activation-Function-Mhaskar-Micchelli",
            "title": {
                "fragments": [],
                "text": "How to Choose an Activation Function"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "This work studies the complexity problem in artificial feedforward neural networks designed to approximate real valued functions of several real variables and shows that the smoother the activation function, the better the rate of approximation."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2020074"
                        ],
                        "name": "A. Lapedes",
                        "slug": "A.-Lapedes",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Lapedes",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lapedes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2542113"
                        ],
                        "name": "R. Farber",
                        "slug": "R.-Farber",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Farber",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Farber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18474528,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2d7de94252e5040a38ebaaf535841d3500791c79",
            "isKey": false,
            "numCitedBy": 373,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "There is presently great interest in the abilities of neural networks to mimic \"qualitative reasoning\" by manipulating neural incodings of symbols. Less work has been performed on using neural networks to process floating point numbers and it is sometimes stated that neural networks are somehow inherently inaccurate and therefore best suited for \"fuzzy\" qualitative reasoning. Nevertheless, the potential speed of massively parallel operations make neural net \"number crunching\" an interesting topic to explore. In this paper we discuss some of our work in which we demonstrate that for certain applications neural networks can achieve significantly higher numerical accuracy than more conventional techniques. In particular, prediction of future values of a chaotic time series can be performed with exceptionally high accuracy. We analyze how a neural net is able to do this, and in the process show that a large class of functions from Rn \u2192 Rm may be accurately approximated by a backpropagation neural net with just two \"hidden\" layers. The network uses this functional approximation to perform either interpolation (signal processing applications) or extrapolation (symbol processing applications). Neural nets therefore use quite familiar methods to perform their tasks. The geometrical viewpoint advocated here seems to be a useful approach to analyzing neural network operation and relates neural networks to well studied topics in functional approximation."
            },
            "slug": "How-Neural-Nets-Work-Lapedes-Farber",
            "title": {
                "fragments": [],
                "text": "How Neural Nets Work"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper demonstrates that for certain applications neural networks can achieve significantly higher numerical accuracy than more conventional techniques, and shows that prediction of future values of a chaotic time series can be performed with exceptionally high accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13814513,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "1ddf7c046e26060ae1bfa32e6fba093970a1870b",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Some-extensions-of-radial-basis-functions-and-their-Girosi",
            "title": {
                "fragments": [],
                "text": "Some extensions of radial basis functions and their applications in artificial intelligence"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1899177"
                        ],
                        "name": "Ken-ichi Funahashi",
                        "slug": "Ken-ichi-Funahashi",
                        "structuredName": {
                            "firstName": "Ken-ichi",
                            "lastName": "Funahashi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ken-ichi Funahashi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10203109,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "386cbc45ceb59a7abb844b5078e5c944f17723b4",
            "isKey": false,
            "numCitedBy": 4188,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-approximate-realization-of-continuous-by-Funahashi",
            "title": {
                "fragments": [],
                "text": "On the approximate realization of continuous mappings by neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50415833"
                        ],
                        "name": "T. Gasser",
                        "slug": "T.-Gasser",
                        "structuredName": {
                            "firstName": "Theo",
                            "lastName": "Gasser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Gasser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2151194700"
                        ],
                        "name": "H. M\u00fcller",
                        "slug": "H.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Hans-Georg",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. M\u00fcller"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117504785,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f812b5c9f34673526a091a30c7aeb2712f447ca0",
            "isKey": false,
            "numCitedBy": 468,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "A kernel estimate is introduced for obtaining a nonparametric estimate of a regression function, as well as of its derivatives. In many fields of engineering and biomedicine, the estimation of velocity and acceleration is of great importance in addition to obtaining a smoothed curve from the measurements themselves. Some asymptotic properties, as, for example, weak and strong consistency and asymptotic normality are derived, as well as asymptotic expressions for bias and variance. A solution for treating the extremities of the data is investigated in order to obtain the asymptotic integrated mean squared error, and also in order to facilitate data analysis over the range of abscissae investigated. Some finite sample results, and two examples of application in biomedicine, render some insight into the potential usefulness of this approach."
            },
            "slug": "Estimating-regression-functions-and-their-by-the-Gasser-M\u00fcller",
            "title": {
                "fragments": [],
                "text": "Estimating regression functions and their derivatives by the kernel method"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403454"
                        ],
                        "name": "E. Baum",
                        "slug": "E.-Baum",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Baum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Baum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15659829,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "25406e6733a698bfc4ac836f8e74f458e75dad4f",
            "isKey": false,
            "numCitedBy": 1696,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the question of when a network can be expected to generalize from m random training examples chosen from some arbitrary probability distribution, assuming that future test examples are drawn from the same distribution. Among our results are the following bounds on appropriate sample vs. network size. Assume 0 < \u220a 1/8. We show that if m O(W/\u220a log N/\u220a) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 \u220a/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 1 \u220a of future test examples drawn from the same distribution. Conversely, for fully-connected feedforward nets with one hidden layer, any learning algorithm using fewer than (W/\u220a) random training examples will, for some distributions of examples consistent with an appropriate weight choice, fail at least some fixed fraction of the time to find a weight choice that will correctly classify more than a 1 \u220a fraction of the future test examples."
            },
            "slug": "What-Size-Net-Gives-Valid-Generalization-Baum-Haussler",
            "title": {
                "fragments": [],
                "text": "What Size Net Gives Valid Generalization?"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that if m O(W/ \u220a log N/\u220a) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 \u220a/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 2 \u220a of future test examples drawn from the same distribution."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "92579735"
                        ],
                        "name": "Peter Craven",
                        "slug": "Peter-Craven",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Craven",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Craven"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 321,
                                "start": 220
                            }
                        ],
                        "text": "The first term is enforcing closeness to the data, and the second smoothness, while the regularization parameter controls the trade-off between these two terms, and can be chosen according to cross-validation techniques (Allen 1974; Wahba and Wold 1975; Golub et al. 1979; Craven and Wahba 1979; Utreras 1979; Wahba 1985) or to some other principle, such as structural risk minimization (Vapnik 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14094416,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b477dd12dd49e44a62c1a303501df5fb6706c7e9",
            "isKey": false,
            "numCitedBy": 3541,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "SummarySmoothing splines are well known to provide nice curves which smooth discrete, noisy data. We obtain a practical, effective method for estimating the optimum amount of smoothing from the data. Derivatives can be estimated from the data by differentiating the resulting (nearly) optimally smoothed spline.We consider the modelyi(ti)+\u03b5i,i=1, 2, ...,n,ti\u2208[0, 1], whereg\u2208W2(m)={f:f,f\u2032, ...,f(m\u22121) abs. cont.,f(m)\u2208\u21122[0,1]}, and the {\u03b5i} are random errors withE\u03b5i=0,E\u03b5i\u03b5j=\u03c32\u03b4ij. The error variance \u03c32 may be unknown. As an estimate ofg we take the solutiongn, \u03bb to the problem: Findf\u2208W2(m) to minimize\n$$\\frac{1}{n}\\sum\\limits_{j = 1}^n {(f(t_j ) - y_j )^2 + \\lambda \\int\\limits_0^1 {(f^{(m)} (u))^2 du} }$$\n. The functiongn, \u03bb is a smoothing polynomial spline of degree 2m\u22121. The parameter \u03bb controls the tradeoff between the \u201croughness\u201d of the solution, as measured by\n$$\\int\\limits_0^1 {[f^{(m)} (u)]^2 du}$$\n, and the infidelity to the data as measured by\n$$\\frac{1}{n}\\sum\\limits_{j = 1}^n {(f(t_j ) - y_j )^2 }$$\n, and so governs the average square errorR(\u03bb; g)=R(\u03bb) defined by\n$$R(\\lambda ) = \\frac{1}{n}\\sum\\limits_{j = 1}^n {(g_{n,\\lambda } (t_j ) - g(t_j ))^2 }$$\n. We provide an estimate\n$$\\hat \\lambda$$\n, called the generalized cross-validation estimate, for the minimizer ofR(\u03bb). The estimate\n$$\\hat \\lambda$$\n is the minimizer ofV(\u03bb) defined by\n$$V(\\lambda ) = \\frac{1}{n}\\parallel (I - A(\\lambda ))y\\parallel ^2 /\\left[ {\\frac{1}{n}{\\text{Trace(}}I - A(\\lambda ))} \\right]^2$$\n, wherey=(y1, ...,yn)t andA(\u03bb) is then\u00d7n matrix satisfying(gn, \u03bb (t1), ...,gn, \u03bb (tn))t=A (\u03bb) y. We prove that there exist a sequence of minimizers\n$$\\tilde \\lambda = \\tilde \\lambda (n)$$\n ofEV(\u03bb), such that as the (regular) mesh{ti}i=1n becomes finer,\n$$\\mathop {\\lim }\\limits_{n \\to \\infty } ER(\\tilde \\lambda )/\\mathop {\\min }\\limits_\\lambda ER(\\lambda ) \\downarrow 1$$\n. A Monte Carlo experiment with several smoothg's was tried withm=2,n=50 and several values of \u03c32, and typical values of\n$$R(\\hat \\lambda )/\\mathop {\\min }\\limits_\\lambda R(\\lambda )$$\n were found to be in the range 1.01\u20131.4. The derivativeg\u2032 ofg can be estimated by\n$$g'_{n,\\hat \\lambda } (t)$$\n. In the Monte Carlo examples tried, the minimizer of\n$$R_D (\\lambda ) = \\frac{1}{n}\\sum\\limits_{j = 1}^n {(g'_{n,\\lambda } (t_j ) - } g'(t_j ))$$\n tended to be close to the minimizer ofR(\u03bb), so that\n$$\\hat \\lambda$$\n was also a good value of the smoothing parameter for estimating the derivative."
            },
            "slug": "Smoothing-noisy-data-with-spline-functions-Craven-Wahba",
            "title": {
                "fragments": [],
                "text": "Smoothing noisy data with spline functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784682"
                        ],
                        "name": "T. Hastie",
                        "slug": "T.-Hastie",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Hastie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hastie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761784"
                        ],
                        "name": "R. Tibshirani",
                        "slug": "R.-Tibshirani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tibshirani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tibshirani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122507462,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4da96b51b9599b335f7391f79457dbaa8a38c176",
            "isKey": false,
            "numCitedBy": 560,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Generalized additive models have the form \u03b7(x) = \u03b1 + \u03c3 fj (x j ), where \u03b7 might be the regression function in a multiple regression or the logistic transformation of the posterior probability Pr(y = 1 | x) in a logistic regression. In fact, these models generalize the whole family of generalized linear models \u03b7(x) = \u03b2\u2032x, where \u03b7(x) = g(\u03bc(x)) is some transformation of the regression function. We use the local scoring algorithm to estimate the functions fj (xj ) nonparametrically, using a scatterplot smoother as a building block. We demonstrate the models in two different analyses: a nonparametric analysis of covariance and a logistic regression. The procedure can be used as a diagnostic tool for identifying parametric transformations of the covariates in a standard linear analysis. A variety of inferential tools have been developed to aid the analyst in assessing the relevance and significance of the estimated functions: these include confidence curves, degrees of freedom estimates, and approximat..."
            },
            "slug": "Generalized-Additive-Models:-Some-Applications-Hastie-Tibshirani",
            "title": {
                "fragments": [],
                "text": "Generalized Additive Models: Some Applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2848403"
                        ],
                        "name": "M. Buhmann",
                        "slug": "M.-Buhmann",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Buhmann",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Buhmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 121204061,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ac168e8a779572c2f789d6f6094046d5e772bab8",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract It has been known since 1987 that quasi-interpolation with radial functions on the integer grid can be exact for certain order polynomials. If, however, we require that the basis functions of the quasi-interpolants be finite linear combinations of translates of the radial functions, then this can be done only in spaces whose dimension has a prescribed parity. In this paper we show how infinite linear combinations of translates of a given radial function can be found that provide polynomial exactness in spaces whose dimensions do not have this prescribed parity. These infinite linear combinations are of a simple form. They are, in particular, easier to find than the cardinal functions of radial basis function interpolation, which provide polynomial exactness in all dimensions. The techniques that are used in this work also give rise to some remarks about interpolation with radial functions both on the integers and on the nonnegative integers."
            },
            "slug": "On-quasi-interpolation-with-radial-basis-functions-Buhmann",
            "title": {
                "fragments": [],
                "text": "On quasi-interpolation with radial basis functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2848403"
                        ],
                        "name": "M. Buhmann",
                        "slug": "M.-Buhmann",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Buhmann",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Buhmann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117653567,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4cf6226fa2d10f9837d56c99e50b1464ba5812b3",
            "isKey": false,
            "numCitedBy": 171,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "AbstractFor a radial-basis function\u03d5\u2236\u211b\u2192\u211b we consider interpolation on an infinite regular lattice, tof\u2236\u211bn\u2192\u211b, whereh is the spacing between lattice points and the cardinal function, satisfiesX(j)=\u03b4oj for allj\u2208\u2112n. We prove existence and uniqueness of such cardinal functionsX, and we establish polynomial precision properties ofIh for a class of radial-basis functions which includes\n$$\\varphi (r) = r^{2q + 1} $$\n,\n$$\\varphi (r) = r^{2q} \\log r,\\varphi (r) = \\sqrt {r^2 + c^2 } $$\n, and\n$$\\varphi (r) = 1/\\sqrt {r^2 + c^2 } $$\n whereq\u2208\u2112+. We also deduce convergence orders ofIhf to sufficiently differentiable functionsf whenh\u21920."
            },
            "slug": "Multivariate-cardinal-interpolation-with-functions-Buhmann",
            "title": {
                "fragments": [],
                "text": "Multivariate cardinal interpolation with radial-basis functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706122"
                        ],
                        "name": "N. Dyn",
                        "slug": "N.-Dyn",
                        "structuredName": {
                            "firstName": "Nira",
                            "lastName": "Dyn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dyn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30912435"
                        ],
                        "name": "I. Jackson",
                        "slug": "I.-Jackson",
                        "structuredName": {
                            "firstName": "Irh",
                            "lastName": "Jackson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Jackson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145343509"
                        ],
                        "name": "D. Levin",
                        "slug": "D.-Levin",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Levin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Levin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145428225"
                        ],
                        "name": "A. Ron",
                        "slug": "A.-Ron",
                        "structuredName": {
                            "firstName": "Amos",
                            "lastName": "Ron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ron"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15138913,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "de7216b19e67edd11613f504edbda2c15774e661",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Approximation properties of the dilations of the integer translates of a smooth function, with some derivatives vanishing at infinity, are studied. The results apply to fundamental solutions of homogeneous elliptic operators and to \u201cshifted\u201d fundamental solutions of the iterated Laplacian. Following the approach from spline theory, the question of polynomial reproduction by quasi-interpolation is addressed first. The analysis makes an essential use of the structure of the generalized Fourier transform of the basis function. In contrast with spline theory, polynomial reproduction is not sufficient for the derivation of exact order of convergence by dilated quasi-interpolants. These convergence orders are established by a careful and quite involved examination of the decay rates of the basis function. Furthermore, it is shown that the same approximation orders are obtained with quasi-interpolants defined on a bounded domain."
            },
            "slug": "On-multivariate-approximation-by-integer-translates-Dyn-Jackson",
            "title": {
                "fragments": [],
                "text": "On multivariate approximation by integer translates of a basis function"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403454"
                        ],
                        "name": "E. Baum",
                        "slug": "E.-Baum",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Baum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Baum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 37243943,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d20eff70cb168111fb5cc320cb692a11f1adf62",
            "isKey": false,
            "numCitedBy": 329,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-capabilities-of-multilayer-perceptrons-Baum",
            "title": {
                "fragments": [],
                "text": "On the capabilities of multilayer perceptrons"
            },
            "venue": {
                "fragments": [],
                "text": "J. Complex."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764952"
                        ],
                        "name": "K. Hornik",
                        "slug": "K.-Hornik",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Hornik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hornik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2964655"
                        ],
                        "name": "M. Stinchcombe",
                        "slug": "M.-Stinchcombe",
                        "structuredName": {
                            "firstName": "Maxwell",
                            "lastName": "Stinchcombe",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stinchcombe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2757547,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "f22f6972e66bdd2e769fa64b0df0a13063c0c101",
            "isKey": false,
            "numCitedBy": 17354,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multilayer-feedforward-networks-are-universal-Hornik-Stinchcombe",
            "title": {
                "fragments": [],
                "text": "Multilayer feedforward networks are universal approximators"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14921581,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "fedfc9fbcfe46d50b81078560bce724678f90176",
            "isKey": false,
            "numCitedBy": 979,
            "numCiting": 124,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Decision-Theoretic-Generalizations-of-the-PAC-Model-Haussler",
            "title": {
                "fragments": [],
                "text": "Decision Theoretic Generalizations of the PAC Model for Neural Net and Other Learning Applications"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Comput."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2285810"
                        ],
                        "name": "D. Specht",
                        "slug": "D.-Specht",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Specht",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Specht"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6266210,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "45f43abc49a8a60e6b43ddbda5af9fc6c88d663d",
            "isKey": false,
            "numCitedBy": 3774,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "A memory-based network that provides estimates of continuous variables and converges to the underlying (linear or nonlinear) regression surface is described. The general regression neural network (GRNN) is a one-pass learning algorithm with a highly parallel structure. It is shown that, even with sparse data in a multidimensional measurement space, the algorithm provides smooth transitions from one observed value to another. The algorithmic form can be used for any regression problem in which an assumption of linearity is not justified."
            },
            "slug": "A-general-regression-neural-network-Specht",
            "title": {
                "fragments": [],
                "text": "A general regression neural network"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The general regression neural network (GRNN) is a one-pass learning algorithm with a highly parallel structure that provides smooth transitions from one observed value to another."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69954310"
                        ],
                        "name": "W. H\u00c3\u00a4rdle",
                        "slug": "W.-H\u00c3\u00a4rdle",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "H\u00c3\u00a4rdle",
                            "middleNames": [
                                "Karl"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. H\u00c3\u00a4rdle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 123570082,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "966c4df32a99dae7cec240e71e96ccdb5e3806c4",
            "isKey": false,
            "numCitedBy": 1588,
            "numCiting": 136,
            "paperAbstract": {
                "fragments": [],
                "text": "Applied Nonparametric Regression is the first book to bring together in one place the techniques for regression curve smoothing involving more than one variable. The computer and the development of interactive graphics programs have made curve estimation possible. This volume focuses on the applications and practical problems of two central aspects of curve smoothing: the choice of smoothing parameters and the construction of confidence bounds. HA\u00a4rdle argues that all smoothing methods are based on a local averaging mechanism and can be seen as essentially equivalent to kernel smoothing. To simplify the exposition, kernel smoothers are introduced and discussed in great detail. Building on this exposition, various other smoothing methods (among them splines and orthogonal polynomials) are presented and their merits discussed. All the methods presented can be understood on an intuitive level; however, exercises and supplemental materials are provided for those readers desiring a deeper understanding of the techniques. The methods covered in this text have numerous applications in many areas using statistical analysis. Examples are drawn from economics as well as from other disciplines including medicine and engineering."
            },
            "slug": "Applied-Nonparametric-Regression-H\u00c3\u00a4rdle",
            "title": {
                "fragments": [],
                "text": "Applied Nonparametric Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "Applied Nonparametric Regression is the first book to bring together in one place the techniques for regression curve smoothing involving more than one variable and argues that all smoothing methods are based on a local averaging mechanism and can be seen as essentially equivalent to kernel smoothing."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2156574"
                        ],
                        "name": "J. Meinguet",
                        "slug": "J.-Meinguet",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Meinguet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Meinguet"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122834657,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "398a9f4efcbca25c02bc35188fc9b55d7a9f943b",
            "isKey": false,
            "numCitedBy": 456,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The concrete method of \u2018surface spline interpolation\u2019 is closely connected with the classical problem of minimizing a Sobolev seminorm under interpolatory constraints; the intrinsic structure of surface splines is accordingly that of a multivariate extension of natural splines. The proper abstract setting is a Hilbert function space whose reproducing kernel involves no functions more complicated than logarithms and is easily coded. Convenient representation formulas are given, as also a practical multivariate extension of the Peano kernel theorem. Owing to the numerical stability of Cholesky factorization of positive definite symmetric matrices, the whole construction process of a surface spline can be described as a recursive algorithm, the data relative to the various interpolation points being exploited in sequence.R\u00e9sum\u00e9La m\u00e9thode concr\u00e8te d'interpolation par surfaces-spline est \u00e9troitement li\u00e9e au probl\u00e8me classique de la minimisation d'une semi-norme de Soboleff sous des contraintes d'interpolation; la structure intrins\u00e8que des surfaces-spline est d\u00e8s lors celle d'une extension multivari\u00e9e des fonctions-spline naturelles. Le cadre abstrait ad\u00e9quat est un espace fonctionnel hilbertien dont le noyau reproduisant ne fait pas intervenir de fonctions plus compliqu\u00e9es que des logarithmes et est ais\u00e9 \u00e0 programmer. Des formules commodes de repr\u00e9sentation sont donn\u00e9es, ainsi qu'une extension multivari\u00e9e d'int\u00e9r\u00eat pratique du th\u00e9or\u00e8me du noyau de Peano. Gr\u00e2ce \u00e0 la stabilit\u00e9 num\u00e9rique de la factorisation de Cholesky des matrices sym\u00e9triques d\u00e9finies positives, la construction d'une surface-spline peut se faire en exploitant point apr\u00e8s point les donn\u00e9es d'interpolation."
            },
            "slug": "Multivariate-interpolation-at-arbitrary-points-made-Meinguet",
            "title": {
                "fragments": [],
                "text": "Multivariate interpolation at arbitrary points made simple"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 11074771,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "2c598e80a7c70cf974fe1e57b5741aeb2fecbd00",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Given n noisy observations g i of the same quantity f, i t i s c o m m o n use to give an estimate of f by minimizing the function P n i=1 (g i ;f) 2. From a statistical point of view this corresponds to computing the Maximum Likelihood estimate, under the assumption of Gaussian noise. Howeve r , i t i s w ell known that this choice leads to results that are very sensitive to the presence of outliers in the data. For this reason it has been proposed to minimize functions of the form P n i=1 V (g i ; f), where V is a function that increases less rapidly than the square. Several choices for V have been proposed and successfully used to obtain \\robust\" estimates. In this paper we show that, for a class of functions V , using these robust estimators corresponds to assuming that data are corrupted by Gaussian noise whose variance uctuates according to some given probability distribution, that uniquely determines the shape of V ."
            },
            "slug": "Models-of-Noise-and-Robust-Estimates-Girosi",
            "title": {
                "fragments": [],
                "text": "Models of Noise and Robust Estimates"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that, for a class of functions V, using robust estimators corresponds to assuming that data are corrupted by Gaussian noise whose variance uctuates according to some given probability distribution, that uniquely determines the shape of V."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2753465"
                        ],
                        "name": "H. Voorhees",
                        "slug": "H.-Voorhees",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Voorhees",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Voorhees"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6006893,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6577a2344e4a0ea5992787352d03bf2478773239",
            "isKey": false,
            "numCitedBy": 180,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-regularized-solution-to-edge-detection-Poggio-Voorhees",
            "title": {
                "fragments": [],
                "text": "A regularized solution to edge detection"
            },
            "venue": {
                "fragments": [],
                "text": "J. Complex."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46933679"
                        ],
                        "name": "M. Bertero",
                        "slug": "M.-Bertero",
                        "structuredName": {
                            "firstName": "Mario",
                            "lastName": "Bertero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bertero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145730991"
                        ],
                        "name": "V. Torre",
                        "slug": "V.-Torre",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Torre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Torre"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14285485,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "31b5a06273e75f159d5d9e42bc5bdfd7fd4b625e",
            "isKey": false,
            "numCitedBy": 855,
            "numCiting": 108,
            "paperAbstract": {
                "fragments": [],
                "text": "Mathematical results on ill-posed and ill-conditioned problems are reviewed and the formal aspects of regularization theory in the linear case are introduced. Specific topics in early vision and their regularization are then analyzed rigorously, characterizing existence, uniqueness, and stability of solutions. A fundamental difficulty that arises in almost every vision problem is scale, that is, the resolution at which to operate. Methods that have been proposed to deal with the problem include scale-space techniques that consider the behavior of the result across a continuum of scales. From the point of view of regulation theory, the concept of scale is related quite directly to the regularization parameter lambda . It suggested that methods used to obtained the optimal value of lambda may provide, either directly or after suitable modification, the optimal scale associated with the specific instance of certain problems. >"
            },
            "slug": "Ill-posed-problems-in-early-vision-Bertero-Poggio",
            "title": {
                "fragments": [],
                "text": "Ill-posed problems in early vision"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738481"
                        ],
                        "name": "T. Johansen",
                        "slug": "T.-Johansen",
                        "structuredName": {
                            "firstName": "Tor",
                            "lastName": "Johansen",
                            "middleNames": [
                                "Arne"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Johansen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15430871,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ba86df01679630e875908199af61b4219a35068b",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Identification-of-non-linear-systems-using-data-and-Johansen",
            "title": {
                "fragments": [],
                "text": "Identification of non-linear systems using empirical data and prior knowledge - an optimization approach"
            },
            "venue": {
                "fragments": [],
                "text": "Autom."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 189781595,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "02cb8ef325adcc6f29e0b1759920527836ea8b2b",
            "isKey": false,
            "numCitedBy": 1334,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "It is shown how to choose the smoothing parameter when a smoothing periodic spline of degree 2m\u22121 is used to reconstruct a smooth periodic curve from noisy ordinate data. The noise is assumed \u201cwhite\u201d, and the true curve is assumed to be in the Sobolev spaceW2(2m) of periodic functions with absolutely continuousv-th derivative,v=0, 1, ..., 2m\u22121 and square integrable 2m-th derivative. The criteria is minimum expected square error, averaged over the data points. The dependency of the optimum smoothing parameter on the sample size, the noise variance, and the smoothness of the true curve is found explicitly."
            },
            "slug": "Smoothing-noisy-data-with-spline-functions-Wahba",
            "title": {
                "fragments": [],
                "text": "Smoothing noisy data with spline functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 205001834,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "052b1d8ce63b07fec3de9dbb583772d860b7c769",
            "isKey": false,
            "numCitedBy": 20333,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1."
            },
            "slug": "Learning-representations-by-back-propagating-errors-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning representations by back-propagating errors"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "Back-propagation repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector, which helps to represent important features of the task domain."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145627013"
                        ],
                        "name": "J. Stewart",
                        "slug": "J.-Stewart",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Stewart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Stewart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 120093770,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "400ca5d045588b887ec8949662dff5436bbc2461",
            "isKey": false,
            "numCitedBy": 189,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "2 fi COS Xi + i X 6 S i n i = \u00b0i = l ' ' i = l ' Likewise it is easily verified directly that e is p.d. for real \\ , but it is not so straightforward to see that such functions as e~H e~*, and (1 4x)\" ? e p.d. These and other examples are discussed in \u00a7 3. Positive definite functions and their various analogues and generalizations have arisen in diverse parts of mathematics since the beginning of this century. They occur naturally in Fourier analysis, probability theory, operator theory, complex function-theory, moment problems, integral equations, boundary-value problems for partial differential equations, embedding problems, information theory, and other areas. Their history constitutes a good illustration of the words of Hobson [51, p. 290] : \"Not only are special results, obtained independently of one another, frequently seen to be really included in"
            },
            "slug": "Positive-definite-functions-and-generalizations,-an-Stewart",
            "title": {
                "fragments": [],
                "text": "Positive definite functions and generalizations, an historical survey"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2122942"
                        ],
                        "name": "B. Ripley",
                        "slug": "B.-Ripley",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Ripley",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ripley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 86569949,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2004356cf836faf746f713c6fb888c506c9c8a91",
            "isKey": false,
            "numCitedBy": 570,
            "numCiting": 136,
            "paperAbstract": {
                "fragments": [],
                "text": "Feed-forward neural networks are now widely used in classification problems, whereas nonlinear methods of discrimination developed in the statistical field are much less widely known. A general framework for classification is set up within which methods from statistics, neural networks, pattern recognition and machine learning can be compared. Neural networks emerge as one of a class of flexible non-linear regression methods which can be used to classify via regression. Many interesting issues remain, including parameter estimation, the assessment of the classifiers and in algorithm development."
            },
            "slug": "Neural-Networks-and-Related-Methods-for-Ripley",
            "title": {
                "fragments": [],
                "text": "Neural Networks and Related Methods for Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A general framework for classification is set up within which methods from statistics, neural networks, pattern recognition and machine learning can be compared, and neural networks emerge as one of a class of flexible non-linear regression methods which can be used to classify via regression."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706122"
                        ],
                        "name": "N. Dyn",
                        "slug": "N.-Dyn",
                        "structuredName": {
                            "firstName": "Nira",
                            "lastName": "Dyn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dyn"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117038933,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "376ea27808150869d2ed43692ca88f27bfa2430a",
            "isKey": false,
            "numCitedBy": 136,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Interpolation-of-scattered-Data-by-radial-Functions-Dyn",
            "title": {
                "fragments": [],
                "text": "Interpolation of scattered Data by radial Functions"
            },
            "venue": {
                "fragments": [],
                "text": "Topics in Multivariate Approximation"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 7035291,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ca97e1668e305fb719845f84a05a62dfb946a5d",
            "isKey": false,
            "numCitedBy": 578,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Very rarely are training data evenly distributed in the input space. Local learning algorithms attempt to locally adjust the capacity of the training system to the properties of the training set in each area of the input space. The family of local learning algorithms contains known methods, like the k-nearest neighbors method (kNN) or the radial basis function networks (RBF), as well as new algorithms. A single analysis models some aspects of these algorithms. In particular, it suggests that neither kNN or RBF, nor nonlocal classifiers, achieve the best compromise between locality and capacity. A careful control of these parameters in a simple local learning algorithm has provided a performance breakthrough for an optical character recognition problem. Both the error rate and the rejection performance have been significantly improved."
            },
            "slug": "Local-Learning-Algorithms-Bottou-Vapnik",
            "title": {
                "fragments": [],
                "text": "Local Learning Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A single analysis suggests that neither kNN or RBF, nor nonlocal classifiers, achieve the best compromise between locality and capacity."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2085069"
                        ],
                        "name": "C. Rabut",
                        "slug": "C.-Rabut",
                        "structuredName": {
                            "firstName": "Christophe",
                            "lastName": "Rabut",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rabut"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 115392048,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "11e314cd16608c458f9220d1af750c70c729039f",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "How-to-Build-Quasi-Interpolants:-Application-to-Rabut",
            "title": {
                "fragments": [],
                "text": "How to Build Quasi-Interpolants: Application to Polyharmonic B-Splines"
            },
            "venue": {
                "fragments": [],
                "text": "Curves and Surfaces"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144990248"
                        ],
                        "name": "R. Lippmann",
                        "slug": "R.-Lippmann",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Lippmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lippmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15297702,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5b280cbc18c3cd305529226c8d99329a362ce21e",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "A taxonomy of neural network pattern classifiers is presented which includes four major groupings. Global discriminant classifiers use sigmoid or polynomial computing elements that have 'high' nonzero outputs over most of their input space. Local discriminant classifiers use Gaussian or other localized computing elements that have 'high' nonzero outputs over only a small localized region of their input space. Nearest neighbor classifiers compute the distance to stored exemplar patterns and rule forming classifiers use binary threshold-logic computing elements to produce binary outputs. Results of experiments are presented which demonstrate that neural network classifiers provide error rates which are equivalent to and sometimes lower than those of more conventional Gaussian. Gaussian mixture, and binary three classifiers using the same amount of training data.<<ETX>>"
            },
            "slug": "A-critical-overview-of-neural-network-pattern-Lippmann",
            "title": {
                "fragments": [],
                "text": "A critical overview of neural network pattern classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Results of experiments are presented which demonstrate that neural network classifiers provide error rates which are equivalent to and sometimes lower than those of more conventional Gaussian."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks for Signal Processing Proceedings of the 1991 IEEE Workshop"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16697538"
                        ],
                        "name": "N. Aronszajn",
                        "slug": "N.-Aronszajn",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Aronszajn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Aronszajn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 54040858,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "fe697b4e2cb4c132da39aed8b8266a0e6113f9f2",
            "isKey": false,
            "numCitedBy": 5083,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : The present paper may be considered as a sequel to our previous paper in the Proceedings of the Cambridge Philosophical Society, Theorie generale de noyaux reproduisants-Premiere partie (vol. 39 (1944)) which was written in 1942-1943. In the introduction to this paper we outlined the plan of papers which were to follow. In the meantime, however, the general theory has been developed in many directions, and our original plans have had to be changed. Due to wartime conditions we were not able, at the time of writing the first paper, to take into account all the earlier investigations which, although sometimes of quite a different character, were, nevertheless, related to our subject. Our investigation is concerned with kernels of a special type which have been used under different names and in different ways in many domains of mathematical research. We shall therefore begin our present paper with a short historical introduction in which we shall attempt to indicate the different manners in which these kernels have been used by various investigators, and to clarify the terminology. We shall also discuss the more important trends of the application of these kernels without attempting, however, a complete bibliography of the subject matter. (KAR) P. 2"
            },
            "slug": "Theory-of-Reproducing-Kernels.-Aronszajn",
            "title": {
                "fragments": [],
                "text": "Theory of Reproducing Kernels."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1950
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2459996"
                        ],
                        "name": "E. Kansa",
                        "slug": "E.-Kansa",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Kansa",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Kansa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 121196186,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2b37df7dcaef4766969bad8662a53760fb119ba2",
            "isKey": false,
            "numCitedBy": 1818,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multiquadrics\u2014A-scattered-data-approximation-scheme-Kansa",
            "title": {
                "fragments": [],
                "text": "Multiquadrics\u2014A scattered data approximation scheme with applications to computational fluid-dynamics\u2014I surface approximations and partial derivative estimates"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706122"
                        ],
                        "name": "N. Dyn",
                        "slug": "N.-Dyn",
                        "structuredName": {
                            "firstName": "Nira",
                            "lastName": "Dyn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dyn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145343509"
                        ],
                        "name": "D. Levin",
                        "slug": "D.-Levin",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Levin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Levin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47204630"
                        ],
                        "name": "S. Rippa",
                        "slug": "S.-Rippa",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Rippa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Rippa"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121300866,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "79332b153d9a94868f0f6d84897fd7f8f8e75dfc",
            "isKey": false,
            "numCitedBy": 411,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In many applications one encounters the problem of approximating surfaces from data given on a set of scattered points in a two-dimensional domain. The global interpolation methods with Duchon's \u201cthin plate splines\u201d and Hardy's multiquadrics are considered to be of high quality; however, their application is limited, due to computational difficulties, to $ \\sim 150$ data points. In this work we develop some efficient iterative schemes for computing global approximation surfaces interpolating a given smooth data. The suggested iterative procedures can, in principle, handle any number of data points, according to computer capacity. These procedures are extensions of a previous work by Dyn and Levin on iterative methods for computing thin-plate spline interpolants for data given on a square grid. Here the procedures are improved significantly and generalized to the case of data given in a general configuration.The major theme of this work is the development of an iterative scheme for the construction of a smooth surface, presented by global basis functions, which approximates only the smooth components of a set of scattered noisy data. The novelty in the suggested method is in the construction of an iterative procedure for low-pass filtering based on detailed spectral properties of a preconditioned matrix. The general concepts of this approach can also be used in designing iterative computation procedures for many other problems.The interpolation and smoothing procedures are tested, and the theoretical results are verified, by many numerical experiments."
            },
            "slug": "Numerical-Procedures-for-Surface-Fitting-of-Data-by-Dyn-Levin",
            "title": {
                "fragments": [],
                "text": "Numerical Procedures for Surface Fitting of Scattered Data by Radial Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The major theme of this work is the development of an iterative scheme for the construction of a smooth surface, presented by global basis functions, which approximates only the smooth components of a set of scattered noisy data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12319558,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "81a1e67bb9a2cc7a94acbfa4c9174ddbd22ce705",
            "isKey": false,
            "numCitedBy": 484,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "A hinge function y=h(x) consists of two hyperplanes continuously joined together at a hinge. In regression (prediction), classification (pattern recognition), and noiseless function approximation, use of sums of hinge functions gives a powerful and efficient alternative to neural networks with computation times several orders of magnitude less than is obtained by fitting neural networks with a comparable number of parameters. A simple and effective method for finding good hinges is presented. >"
            },
            "slug": "Hinging-hyperplanes-for-regression,-classification,-Breiman",
            "title": {
                "fragments": [],
                "text": "Hinging hyperplanes for regression, classification, and function approximation"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "A simple and effective method for finding good hinges is presented and it is shown that use of sums of hinge functions gives a powerful and efficient alternative to neural networks with computation times several orders of magnitude less than is obtained by fitting neural Networks with a comparable number of parameters."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40380553"
                        ],
                        "name": "G. Drago",
                        "slug": "G.-Drago",
                        "structuredName": {
                            "firstName": "Gian",
                            "lastName": "Drago",
                            "middleNames": [
                                "Paolo"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Drago"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751728"
                        ],
                        "name": "S. Ridella",
                        "slug": "S.-Ridella",
                        "structuredName": {
                            "firstName": "Sandro",
                            "lastName": "Ridella",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ridella"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 39824049,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8f5f1a34a0d9022bff6c366597ab24f730e0ebe",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-convergence-of-a-growing-topology-neural-Drago-Ridella",
            "title": {
                "fragments": [],
                "text": "On the convergence of a growing topology neural algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Neurocomputing"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145730991"
                        ],
                        "name": "V. Torre",
                        "slug": "V.-Torre",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Torre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Torre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145624227"
                        ],
                        "name": "C. Koch",
                        "slug": "C.-Koch",
                        "structuredName": {
                            "firstName": "Christof",
                            "lastName": "Koch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Koch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4346156,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4c6bd3b8d35c4fc14360160efc9c66727abac9df",
            "isKey": false,
            "numCitedBy": 1264,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Descriptions of physical properties of visible surfaces, such as their distance and the presence of edges, must be recovered from the primary image data. Computational vision aims to understand how such descriptions can be obtained from inherently ambiguous and noisy data. A recent development in this field sees early vision as a set of ill-posed problems, which can be solved by the use of regularization methods. These lead to algorithms and parallel analog circuits that can solve \u2018ill-posed problems\u2019 and which are suggestive of neural equivalents in the brain."
            },
            "slug": "Computational-vision-and-regularization-theory-Poggio-Torre",
            "title": {
                "fragments": [],
                "text": "Computational vision and regularization theory"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "Descriptions of physical properties of visible surfaces, such as their distance and the presence of edges, must be recovered from the primary image data and algorithms and parallel analog circuits that can solve \u2018ill-posed problems\u2019 and which are suggestive of neural equivalents in the brain are proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143615848"
                        ],
                        "name": "Rajesh P. N. Rao",
                        "slug": "Rajesh-P.-N.-Rao",
                        "structuredName": {
                            "firstName": "Rajesh",
                            "lastName": "Rao",
                            "middleNames": [
                                "P.",
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rajesh P. N. Rao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691804"
                        ],
                        "name": "D. Ballard",
                        "slug": "D.-Ballard",
                        "structuredName": {
                            "firstName": "Dana",
                            "lastName": "Ballard",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ballard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7136784,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "e3a83c2ed3af29a23ab342212d1ae9650a0c64a1",
            "isKey": false,
            "numCitedBy": 317,
            "numCiting": 167,
            "paperAbstract": {
                "fragments": [],
                "text": "The responses of visual cortical neurons during fixation tasks can be significantly modulated by stimuli from beyond the classical receptive field. Modulatory effects in neural responses have also been recently reported in a task where a monkey freely views a natural scene. In this article, we describe a hierarchical network model of visual recognition that explains these experimental observations by using a form of the extended Kalman filter as given by the minimum description length (MDL) principle. The model dynamically combines input-driven bottom-up signals with expectation-driven top-down signals to predict current recognition state. Synaptic weights in the model are adapted in a Hebbian manner according to a learning rule also derived from the MDL principle. The resulting prediction-learning scheme can be viewed as implementing a form of the expectation-maximization (EM) algorithm. The architecture of the model posits an active computational role for the reciprocal connections between adjoining visual cortical areas in determining neural response properties. In particular, the model demonstrates the possible role of feedback from higher cortical areas in mediating neurophysiological effects due to stimuli from beyond the classical receptive field. Simulations of the model are provided that help explain the experimental observations regarding neural responses in both free viewing and fixating conditions."
            },
            "slug": "Dynamic-Model-of-Visual-Recognition-Predicts-Neural-Rao-Ballard",
            "title": {
                "fragments": [],
                "text": "Dynamic Model of Visual Recognition Predicts Neural Response Properties in the Visual Cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A hierarchical network model of visual recognition that explains experimental observations regarding neural responses in both free viewing and fixating conditions by using a form of the extended Kalman filter as given by the minimum description length (MDL) principle is described."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2459996"
                        ],
                        "name": "E. Kansa",
                        "slug": "E.-Kansa",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Kansa",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Kansa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 121904890,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "04db2b2062c57ff736f919ac30cbcb7bdea6aa9b",
            "isKey": false,
            "numCitedBy": 1769,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "MULTIQUADRICS--A-SCATTERED-DATA-APPROXIMATION-WITH-Kansa",
            "title": {
                "fragments": [],
                "text": "MULTIQUADRICS--A SCATTERED DATA APPROXIMATION SCHEME WITH APPLICATIONS TO COMPUTATIONAL FLUID-DYNAMICS-- II SOLUTIONS TO PARABOLIC, HYPERBOLIC AND ELLIPTIC PARTIAL DIFFERENTIAL EQUATIONS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2378798"
                        ],
                        "name": "L. Schumaker",
                        "slug": "L.-Schumaker",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Schumaker",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Schumaker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 102
                            }
                        ],
                        "text": "The regularization networks with radial stabilizers we studied include many classical one-dimensional (Schumaker 1981; de Boor 1978) as well as multidimensional splines and approximation tech-"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 123077071,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8fa7e5d7ef68525b53a2de82c7b7dfdfeb781511",
            "isKey": false,
            "numCitedBy": 2927,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This classic work continues to offer a comprehensive treatment of the theory of univariate and tensor-product splines. It will be of interest to researchers and students working in applied analysis, numerical analysis, computer science, and engineering. The material covered provides the reader with the necessary tools for understanding the many applications of splines in such diverse areas as approximation theory, computer-aided geometric design, curve and surface design and fitting, image processing, numerical solution of differential equations, and increasingly in business and the biosciences. This new edition includes a supplement outlining some of the major advances in the theory since 1981, and some 250 new references. It can be used as the main or supplementary text for courses in splines, approximation theory or numerical analysis."
            },
            "slug": "Spline-Functions:-Basic-Theory-Schumaker",
            "title": {
                "fragments": [],
                "text": "Spline Functions: Basic Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The material covered provides the reader with the necessary tools for understanding the many applications of splines in such diverse areas as approximation theory, computer-aided geometric design, curve and surface design and fitting, image processing, numerical solution of differential equations, and increasingly in business and the biosciences."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144990248"
                        ],
                        "name": "R. Lippmann",
                        "slug": "R.-Lippmann",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Lippmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lippmann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 39206632,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1528918ae0c9f70ba6da030cb8dbc72f71bc198b",
            "isKey": false,
            "numCitedBy": 433,
            "numCiting": 107,
            "paperAbstract": {
                "fragments": [],
                "text": "The performance of current speech recognition systems is far below that of humans. Neural nets offer the potential of providing massive parallelism, adaptation, and new algorithmic approaches to problems in speech recognition. Initial studies have demonstrated that multilayer networks with time delays can provide excellent discrimination between small sets of pre-segmented difficult-to-discriminate words, consonants, and vowels. Performance for these small vocabularies has often exceeded that of more conventional approaches. Physiological front ends have provided improved recognition accuracy in noise and a cochlea filter-bank that could be used in these front ends has been implemented using micro-power analog VLSI techniques. Techniques have been developed to scale networks up in size to handle larger vocabularies, to reduce training time, and to train nets with recurrent connections. Multilayer perceptron classifiers are being integrated into conventional continuous-speech recognizers. Neural net architectures have been developed to perform the computations required by vector quantizers, static pattern classifiers, and the Viterbi decoding algorithm. Further work is necessary for large-vocabulary continuous-speech problems, to develop training algorithms that progressively build internal word models, and to develop compact VLSI neural net hardware."
            },
            "slug": "Review-of-Neural-Networks-for-Speech-Recognition-Lippmann",
            "title": {
                "fragments": [],
                "text": "Review of Neural Networks for Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Further work is necessary for large-vocabulary continuous-speech problems, to develop training algorithms that progressively build internal word models, and to develop compact VLSI neural net hardware."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143797114"
                        ],
                        "name": "A. Pinkus",
                        "slug": "A.-Pinkus",
                        "structuredName": {
                            "firstName": "Allan",
                            "lastName": "Pinkus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pinkus"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 119692918,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c58148ce71dc46f4e5e590e584c795071273fc3f",
            "isKey": false,
            "numCitedBy": 860,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "I. Introduction.- II. Basic Properties of n-Widths.- 1. Properties of dn.- 2. Existence of Optimal Subspaces for dn.- 3. Properties of dn.- 4. Properties of ?n.- 5. Inequalities Between n-Widths.- 6. Duality Between dn and dn.- 7. n-Widths of Mappings of the Unit Ball.- 8. Some Relationships Between dn(T), dn(T) and ?n(T).- Notes and References.- III. Tchebycheff Systems and Total Positivity.- 1. Tchebycheff Systems.- 2. Matrices.- 3. Kernels.- 4. More on Kernels.- IV. n-Widths in Hilbert Spaces.- 1. Introduction.- 2. n-Widths of Compact Linear Operators.- 3. n-Widths, with Constraints.- 3.1 Restricted Approximating Subspaces.- 3.2 Restricting the Unit Ball and Optimal Recovery.- 3.3 n-Widths Under a Pair of Constraints.- 3.4 A Theorem of Ismagilov.- 4. n-Widths of Compact Periodic Convolution Operators.- 4.1 n-Widths as Fourier Coefficients.- 4.2 A Return to Ismagilov's Theorem.- 4.3 Bounded mth Modulus of Continuity.- 5. n-Widths of Totally Positive Operators in L2.- 5.1 The Main Theorem.- 5.2 Restricted Approximating Subspaces.- 6. Certain Classes of Periodic Functions.- 6.1 n-Widths of Cyclic Variation Diminishing Operators.- 6.2 n-Widths for Kernels Satisfying Property B.- Notes and References.- V. Exact n-Widths of Integral Operators.- 1. Introduction.- 2. Exact n-Widths of K? in Lq and Kp in L1.- 3. Exact n-Widths of K?r in Lq and Kpr in L1.- 4. Exact n-Widths for Periodic Functions.- 5. n-Widths of Rank n + 1 Kernels.- Notes and References.- VI. Matrices and n-Widths.- 1. Introduction and General Remarks.- 2. n-Widths of Diagonal Matrices.- 2.1 The Exact Solution for q ? p and p = 1, q = 2.- 2.2 Various Estimates for p = 1, q = ?.- 3. n-Widths of Strictly Totally Positive Matrices.- Notes and References.- VII. Asymptotic Estimates for n-Widths of Sobolev Spaces.- 1. Introduction.- 2. Optimal Lower Bounds.- 3. Optimal Upper Bounds.- 4. Another Look at ?n(B1(r) L?).- Notes and References.- VIII. n-Widths of Analytic Functions.- 1. Introduction.- 2. n-Widths of Analytic Functions with Bounded mth Derivative.- 3. n-Widths of Analytic Functions in H2.- 4. n-Widths of Analytic Functions in H?.- 5. n-Widths of a Class of Entire Functions.- Notes and References.- Glossary of Selected Symbols.- Author Index."
            },
            "slug": "n-Widths-in-Approximation-Theory-Pinkus",
            "title": {
                "fragments": [],
                "text": "n-Widths in Approximation Theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144134805"
                        ],
                        "name": "T. Chow",
                        "slug": "T.-Chow",
                        "structuredName": {
                            "firstName": "Tommy",
                            "lastName": "Chow",
                            "middleNames": [
                                "W.",
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Chow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144259201"
                        ],
                        "name": "C. Leung",
                        "slug": "C.-Leung",
                        "structuredName": {
                            "firstName": "Chi-Tat",
                            "lastName": "Leung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Leung"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 60716127,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "52305f010c982a4c69e6e9693c255e496e9c52b4",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "NONLINEAR-DILATION-NETWORK-FOR-PREDICTION-Chow-Leung",
            "title": {
                "fragments": [],
                "text": "NONLINEAR DILATION NETWORK FOR PREDICTION APPLICATIONS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143689789"
                        ],
                        "name": "D. Pollard",
                        "slug": "D.-Pollard",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Pollard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pollard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122104450,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "01a1d065a5292be740e75029622a3ab5e71e3150",
            "isKey": false,
            "numCitedBy": 1852,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "I Functional on Stochastic Processes.- 1. Stochastic Processes as Random Functions.- Notes.- Problems.- II Uniform Convergence of Empirical Measures.- 1. Uniformity and Consistency.- 2. Direct Approximation.- 3. The Combinatorial Method.- 4. Classes of Sets with Polynomial Discrimination.- 5. Classes of Functions.- 6. Rates of Convergence.- Notes.- Problems.- III Convergence in Distribution in Euclidean Spaces.- 1. The Definition.- 2. The Continuous Mapping Theorem.- 3. Expectations of Smooth Functions.- 4. The Central Limit Theorem.- 5. Characteristic Functions.- 6. Quantile Transformations and Almost Sure Representations.- Notes.- Problems.- IV Convergence in Distribution in Metric Spaces.- 1. Measurability.- 2. The Continuous Mapping Theorem.- 3. Representation by Almost Surely Convergent Sequences.- 4. Coupling.- 5. Weakly Convergent Subsequences.- Notes.- Problems.- V The Uniform Metric on Spaces of Cadlag Functions.- 1. Approximation of Stochastic Processes.- 2. Empirical Processes.- 3. Existence of Brownian Bridge and Brownian Motion.- 4. Processes with Independent Increments.- 5. Infinite Time Scales.- 6. Functional of Brownian Motion and Brownian Bridge.- Notes.- Problems.- VI The Skorohod Metric on D(0, ?).- 1. Properties of the Metric.- 2. Convergence in Distribution.- Notes.- Problems.- VII Central Limit Theorems.- 1. Stochastic Equicontinuity.- 2. Chaining.- 3. Gaussian Processes.- 4. Random Covering Numbers.- 5. Empirical Central Limit Theorems.- 6. Restricted Chaining.- Notes.- Problems.- VIII Martingales.- 1. A Central Limit Theorem for Martingale-Difference Arrays.- 2. Continuous Time Martingales.- 3. Estimation from Censored Data.- Notes.- Problems.- Appendix A Stochastic-Order Symbols.- Appendix B Exponential Inequalities.- Notes.- Problems.- Appendix C Measurability.- Notes.- Problems.- References.- Author Index."
            },
            "slug": "Convergence-of-stochastic-processes-Pollard",
            "title": {
                "fragments": [],
                "text": "Convergence of stochastic processes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1872039"
                        ],
                        "name": "R. Mammone",
                        "slug": "R.-Mammone",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Mammone",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mammone"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 60390258,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "597e860ffdec5af954bd30f23b5fcd9b5cddbb4d",
            "isKey": false,
            "numCitedBy": 187,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Artificial-neural-networks-for-speech-and-vision-Mammone",
            "title": {
                "fragments": [],
                "text": "Artificial neural networks for speech and vision"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691804"
                        ],
                        "name": "D. Ballard",
                        "slug": "D.-Ballard",
                        "structuredName": {
                            "firstName": "Dana",
                            "lastName": "Ballard",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ballard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 53550263,
            "fieldsOfStudy": [
                "Biology",
                "Computer Science",
                "Psychology"
            ],
            "id": "902a9f6fd1b17f0d0bc9de54cccd4baa179a7845",
            "isKey": false,
            "numCitedBy": 342,
            "numCiting": 237,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract The cerebral cortex is a rich and diverse structure that is the basis of intelligent behavior. One of the deepest mysteries of the function of cortex is that neural processing times are only about one hundred times as fast as the fastest response times for complex behavior. At the very least, this would seem to indicate that the cortex does massive amounts of parallel computation. This paper explores the hypothesis that an important part of the cortex can be modeled as a connectionist computer that is especially suited for parallel problem solving. The connectionist computer uses a special representation, termed value unit encoding, that represents small subsets of parameters in a way that allows parallel access to many different parameter values. This computer can be thought of as computing hierarchies of sensorimotor invariants. The neural substrate can be interpreted as a commitment to data structures and algorithms that compute invariants fast enough to explain the behavioral response times. A detailed consideration of this model has several implications for the underlying anatomy and physiology."
            },
            "slug": "Cortical-connections-and-parallel-processing:-and-Ballard",
            "title": {
                "fragments": [],
                "text": "Cortical connections and parallel processing: Structure and function"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The hypothesis that an important part of the cortex can be modeled as a connectionist computer that is especially suited for parallel problem solving is explored."
            },
            "venue": {
                "fragments": [],
                "text": "Behavioral and Brain Sciences"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2076400"
                        ],
                        "name": "B. Caprile",
                        "slug": "B.-Caprile",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Caprile",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Caprile"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 116890063,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e2a58fafbcb90dbd359f8f8cb13a9ae746dfac1",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of minimizing a multivariate function is recurrent in many disciplines as Physics, Mathematics, Engeneering and, of course, Computer Science. In this paper we describe a simple nondeterministic algorithm which is based on the idea of adaptive noise, and that proved to be particularly effective in the minimization of a class of multivariate, {\\it continuous valued, smooth functions}, associated with some recent extension of regularization theory by Poggio and Girosi (1990). Results obtained by using this method and a more traditional gradient descent technique are also compared."
            },
            "slug": "A-Nondeterministic-Minimization-Algorithm-Caprile-Girosi",
            "title": {
                "fragments": [],
                "text": "A Nondeterministic Minimization Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A simple nondeterministic algorithm which is based on the idea of adaptive noise that proved to be particularly effective in the minimization of a class of multivariate functions associated with some recent extension of regularization theory by Poggio and Girosi (1990)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684073"
                        ],
                        "name": "A. Hurlbert",
                        "slug": "A.-Hurlbert",
                        "structuredName": {
                            "firstName": "Anya",
                            "lastName": "Hurlbert",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hurlbert"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 2441722,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "3ee07fc62801c8e76c8d31292c120580b03b2176",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 87,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper sketches a hypothetical cortical architecture for visual 3D object recognition based on a recent computational model. The view-centered scheme relies on modules for learning from examples, such as Hyperbf-like networks. Such models capture a class of explanations we call Memory-Based Models (MBM) that contains sparse population coding, memory-based recognition, and codebooks of prototypes. Unlike the sigmoidal units of some artificial neural networks, the units of MBMs are consistent with the description of cortical neurons. We describe how an example of MBM may be realized in terms of cortical circuitry and biophysical mechanisms, consistent with psychophysical and physiological data."
            },
            "slug": "Observations-on-Cortical-Mechanisms-for-Object-and-Poggio-Hurlbert",
            "title": {
                "fragments": [],
                "text": "Observations on Cortical Mechanisms for Object Recognition and Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This paper sketches a hypothetical cortical architecture for visual 3D object recognition based on a recent computational model that contains sparse population coding, memory-based recognition, and codebooks of prototypes that is consistent with the description of cortical neurons."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2634385"
                        ],
                        "name": "W. Madych",
                        "slug": "W.-Madych",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Madych",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Madych"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143758670"
                        ],
                        "name": "S. Nelson",
                        "slug": "S.-Nelson",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Nelson",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nelson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 122338574,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d47d29417ba4e516ca5a4889180536a4497825d2",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Polyharmonic-cardinal-splines:-a-minimization-Madych-Nelson",
            "title": {
                "fragments": [],
                "text": "Polyharmonic cardinal splines: a minimization property"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2093644"
                        ],
                        "name": "Bartlett W. Mel",
                        "slug": "Bartlett-W.-Mel",
                        "structuredName": {
                            "firstName": "Bartlett",
                            "lastName": "Mel",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bartlett W. Mel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6639160,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "39c390e6694f86b6c7b8b4321b4cee1b537e5fc5",
            "isKey": false,
            "numCitedBy": 159,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Compartmental simulations of an anatomically characterized cortical pyramidal cell were carried out to study the integrative behavior of a complex dendritic tree. Previous theoretical (Feldman and Ballard 1982; Durbin and Rumelhart 1989; Mel 1990; Mel and Koch 1990; Poggio and Girosi 1990) and compartmental modeling (Koch et al. 1983; Shepherd et al. 1985; Koch and Poggio 1987; Rall and Segev 1987; Shepherd and Brayton 1987; Shepherd et al. 1989; Brown et al. 1991) work had suggested that multiplicative interactions among groups of neighboring synapses could greatly enhance the processing power of a neuron relative to a unit with only a single global firing threshold. This issue was investigated here, with a particular focus on the role of voltage-dependent N-methyl-D-asparate (NMDA) channels in the generation of cell responses. First, it was found that when a large proportion of the excitatory synaptic input to dendritic spines is carried by NMDA channels, the pyramidal cell responds preferentially to spatially clustered, rather than random, distributions of activated synapses. Second, based on this mechanism, the NMDA-rich neuron is shown to be capable of solving a nonlinear pattern discrimination task. We propose that manipulation of the spatial ordering of afferent synaptic connections onto the dendritic arbor is a possible biological strategy for pattern information storage during learning."
            },
            "slug": "NMDA-Based-Pattern-Discrimination-in-a-Modeled-Mel",
            "title": {
                "fragments": [],
                "text": "NMDA-Based Pattern Discrimination in a Modeled Cortical Neuron"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It was found that when a large proportion of the excitatory synaptic input to dendritic spines is carried by NMDA channels, the pyramidal cell responds preferentially to spatially clustered, rather than random, distributions of activated synapses, and the NMDA-rich neuron is shown to be capable of solving a nonlinear pattern discrimination task."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3140752"
                        ],
                        "name": "Charles R. Rosenberg",
                        "slug": "Charles-R.-Rosenberg",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Rosenberg",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles R. Rosenberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12926318,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de996c32045df6f7b404dda2a753b6a9becf3c08",
            "isKey": false,
            "numCitedBy": 1885,
            "numCiting": 229,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes NETtalk, a class of massively-parallel network systems that learn to convert English text to speech. The memory representations for pronunciations are learned by practice and are shared among many processing units. The performance of NETtalk has some similarities with observed human performance. (i) The learning follows a power law. (ii) The more words the network learns, the better it is at generalizing and correctly pronouncing new words, (iii) The performance of the network degrades very slowly as connections in the network are damaged: no single link or processing unit is essential. (iv) Relearning after damage is much faster than learning during the original training. (v) Distributed or spaced practice is more effective for long-term retention than massed practice. Network models can be constructed that have the same performance and learning characteristics on a particular task, but differ completely at the levels of synaptic strengths and single-unit responses. However, hierarchical clustering techniques applied to NETtalk reveal that these different networks have similar internal representations of letter-to-sound correspondences within groups of processing units. This suggests that invariant internal representations may be found in assemblies of neurons intermediate in size between highly localized and completely distributed representations."
            },
            "slug": "Parallel-Networks-that-Learn-to-Pronounce-English-Sejnowski-Rosenberg",
            "title": {
                "fragments": [],
                "text": "Parallel Networks that Learn to Pronounce English Text"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "H hierarchical clustering techniques applied to NETtalk reveal that these different networks have similar internal representations of letter-to-sound correspondences within groups of processing units, which suggests that invariant internal representations may be found in assemblies of neurons intermediate in size between highly localized and completely distributed representations."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684300"
                        ],
                        "name": "W. Stuetzle",
                        "slug": "W.-Stuetzle",
                        "structuredName": {
                            "firstName": "Werner",
                            "lastName": "Stuetzle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Stuetzle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14183758,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "589b8659007e1124f765a5d1bd940b2bf4d79054",
            "isKey": false,
            "numCitedBy": 2178,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract A new method for nonparametric multiple regression is presented. The procedure models the regression surface as a sum of general smooth functions of linear combinations of the predictor variables in an iterative manner. It is more general than standard stepwise and stagewise regression procedures, does not require the definition of a metric in the predictor space, and lends itself to graphical interpretation."
            },
            "slug": "Projection-Pursuit-Regression-Friedman-Stuetzle",
            "title": {
                "fragments": [],
                "text": "Projection Pursuit Regression"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684073"
                        ],
                        "name": "A. Hurlbert",
                        "slug": "A.-Hurlbert",
                        "structuredName": {
                            "firstName": "Anya",
                            "lastName": "Hurlbert",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hurlbert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 450,
                                "start": 118
                            }
                        ],
                        "text": "The interpretation of an approximation scheme in terms of networks and vice versa has also been extensively discussed (Barron and Barron 1988; Poggio and Girosi 1989, 1990a,b; Girosi 1992; Broomhead and Lowe 1988; Moody and Darken 1988, 1989; White 1989, 1990; Ripley 1994; Omohundro 1987; Kohonen 1990; Lapedes and Farber 1988; Rumelhart et al. 1986; Hertz et al. 1991; Kung 1993; Sejnowski and Rosenberg 1987; Hurlbert and Poggio 1988; Poggio 1975)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 19636254,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a63f35c6f8ecf6491fccdcf0a1156ce61c2405a4",
            "isKey": false,
            "numCitedBy": 136,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A lightness algorithm that separates surface reflectance from illumination in a Mondrian world is synthesized automatically from a set of examples, which consist of pairs of input (intensity signal) and desired output (surface reflectance) images. The algorithm, which resembles a new lightness algorithm recently proposed by Land, is approximately equivalent to filtering the image through a center-surround receptive field in individual chromatic channels. The synthesizing technique, optimal linear estimation, requires only one assumption, that the operator that transforms input into output is linear. This assumption is true for a certain class of early vision algorithms that may therefore be synthesized in a similar way from examples. Other methods of synthesizing algorithms from examples, or \"learning,\" such as back-propagation, do not yield a significantly better lightness algorithm."
            },
            "slug": "Synthesizing-a-color-algorithm-from-examples.-Hurlbert-Poggio",
            "title": {
                "fragments": [],
                "text": "Synthesizing a color algorithm from examples."
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A lightness algorithm that separates surface reflectance from illumination in a Mondrian world is synthesized automatically from a set of examples, which consist of pairs of input and desired output (surface reflectance) images."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144035095"
                        ],
                        "name": "N. Sivakumar",
                        "slug": "N.-Sivakumar",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Sivakumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Sivakumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32586775"
                        ],
                        "name": "J. Ward",
                        "slug": "J.-Ward",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Ward",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ward"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 150
                            }
                        ],
                        "text": "A complete theoretical analysis has not yet been given, but some results, in the case in which the matrix W is set to identity, are already available (Sivakumar and Ward 1991; Poggio and Girosi 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120000058,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "95088062883ac48933c99aeb04c5d659a9692509",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "SummaryThis paper investigates some aspects of discrete least squares approximation by translates of certain classes of radial functions. Its specific aims are (i) to provide conditions under which the associated least squares matrix is invertible and (ii) to give upper bounds for the Euclidean norms of the inverses of these matrices (when they exist)."
            },
            "slug": "On-the-least-squares-fit-by-radial-functions-to-Sivakumar-Ward",
            "title": {
                "fragments": [],
                "text": "On the least squares fit by radial functions to multidimensional scattered data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719838"
                        ],
                        "name": "W. Grimson",
                        "slug": "W.-Grimson",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Grimson",
                            "middleNames": [
                                "Eric",
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Grimson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 66
                            }
                        ],
                        "text": "niques, such as radial and nonradial gaussian, thin-plate splines (Duchon 1977; Meinguet 1979; Grimson 1982; Cox 1984; Eubank 1988) and multiquadric functions (Hardy 1971,1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 74
                            }
                        ],
                        "text": "2 yields the so-called \"thin plate\" basis function G(x) = llx112 In IIxJJ (Harder and Desmarais 1972; Grimson 1982)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 46108912,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d27b1205d722be5ec477c476770b7e2753d26b17",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Computational theories of structure-from-motion and stereo vision only specify the computation of three-dimensional surface information at special points in the image. Yet the visual perception is clearly of complete surfaces. To account for this a computational theory of the interpolation of surfaces from visual information is presented. The problem is constrained by the fact that the surface must agree with the information from stereo or motion correspondence, and not vary radically between these points. Using the image irradiance equation, an explicit form of this surface consistency constraint can be derived. To determine which of two possible surfaces is more consistent with the surface consistency constraint, one must be able to compare the two surfaces. To do this, a functional from the space of possible functions to the real numbers is required. In this way, the surface most consistent with the visual information will be that which minimizes the functional. To ensure that the functional has a unique minimal surface, conditions on the form of the functional are derived. In particular, if the functional is a complete semi-norm that satisfies the parallelogram law, or the space of functions is a semi-Hilbert space and the functional is a semi-inner product, then there is a unique (to within possibly an element of the null space of the functional) surface that is most consistent with the visual information. It can be shown, based on the above conditions plus a condition of rotational symmetry, that there is a vector space of possible functionals that measure surface consistency, this vector space being spanned by the functional of quadratic variation and the functional of square Laplacian. Arguments based on the null spaces of the respective functionals are used to justify the choice of the quadratic variation as the optimal functional. Possible refinements to the theory, concerning the role of discontinuities in depth and the effects of applying the interpolation process to scenes containing more than one object, are discussed."
            },
            "slug": "A-computational-theory-of-visual-surface-Grimson",
            "title": {
                "fragments": [],
                "text": "A computational theory of visual surface interpolation."
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A computational theory of the interpolation of surfaces from visual information is presented, and it is shown that there is a vector space of possible functionals that measure surface consistency, this vector space being spanned by thefunctional of quadratic variation and the functional of square Laplacian."
            },
            "venue": {
                "fragments": [],
                "text": "Philosophical transactions of the Royal Society of London. Series B, Biological sciences"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2634385"
                        ],
                        "name": "W. Madych",
                        "slug": "W.-Madych",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Madych",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Madych"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143758670"
                        ],
                        "name": "S. Nelson",
                        "slug": "S.-Nelson",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Nelson",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nelson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 121800597,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "fa6bcb4e71e511decfd6053995b0390dc19bf3e1",
            "isKey": false,
            "numCitedBy": 124,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Polyharmonic-cardinal-splines-Madych-Nelson",
            "title": {
                "fragments": [],
                "text": "Polyharmonic cardinal splines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2306567"
                        ],
                        "name": "F. Utreras",
                        "slug": "F.-Utreras",
                        "structuredName": {
                            "firstName": "Florencio",
                            "lastName": "Utreras",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Utreras"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122510637,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "15d4e28e4d8e4b0a951a3fce2eaab89a58306d4b",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper aims to present some results on the asymptotic behaviour of a matrix associated with certain types of spline functions and shows how these results can be used to obtain a fast algorithm for choosing the smoothing parameter in the smoothing of noisy data by splines."
            },
            "slug": "Cross-validation-techniques-for-smoothing-spline-in-Utreras",
            "title": {
                "fragments": [],
                "text": "Cross-validation techniques for smoothing spline functions in one or two dimensions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122927965,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "ae6ad5780d3562b0fb4af3fa4e17998fef53a2bc",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Least squares estimators are very common in statistics, but they lead to results that are very sensitive to outliers, and it has been proposed to minimize other measures of error, that lead to ``robust'''' estimates. In this paper we show that using these robust estimators corresponds to assuming that data are corrupted by Gaussian noise whose variance fluctuates according to some given probability distribution, that uniquely determines the estimator."
            },
            "slug": "Models-of-Noise-and-Robust-Estimation-Girosi",
            "title": {
                "fragments": [],
                "text": "Models of Noise and Robust Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper shows that using robust least squares estimators corresponds to assuming that data are corrupted by Gaussian noise whose variance fluctuates according to some given probability distribution, that uniquely determines the estimator."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2637505"
                        ],
                        "name": "D. Cox",
                        "slug": "D.-Cox",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "Cox",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cox"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 66
                            }
                        ],
                        "text": "niques, such as radial and nonradial gaussian, thin-plate splines (Duchon 1977; Meinguet 1979; Grimson 1982; Cox 1984; Eubank 1988) and multiquadric functions (Hardy 1971,1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 119965968,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "b14391b13324eb91b7ed1cf4c4a29e0bc47373ae",
            "isKey": false,
            "numCitedBy": 106,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Given data $z_i = g(t_i ) + \\varepsilon _i , 1 \\leqq i \\leqq n$, where g is the unknown function, the $t_i $ are known d-dimensional variables in a domain $\\Omega $, and the $\\varepsilon _i $ are i..."
            },
            "slug": "MULTIVARIATE-SMOOTHING-SPLINE-FUNCTIONS-Cox",
            "title": {
                "fragments": [],
                "text": "MULTIVARIATE SMOOTHING SPLINE FUNCTIONS"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "Given data, g is the unknown function, the t_i and the varepsilon variables are known d-dimensional variables in a domain $\\Omega $, and the $\\varpsilon _i $ are i..."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2074730628"
                        ],
                        "name": "J. Duchon",
                        "slug": "J.-Duchon",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Duchon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Duchon"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123055447,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "dd573acce51d862cfb576bf9588f7ccad07b7272",
            "isKey": false,
            "numCitedBy": 1451,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We define a family of semi-norms \u2016\u03bc\u2016m,s=(\u222b\u211d n\u2223\u03c4\u22232s\u2223\u2131 Dmu(\u03c4)\u22232 d\u03c4)1/2 Minimizing such semi-norms, subject to some interpolating conditions, leads to functions of very simple forms, providing interpolation methods that: 1\u00b0) preserve polynomials of degree\u2264m\u22121; 2\u00b0) commute with similarities as well as translations and rotations of \u211dn; and 3\u00b0) converge in Sobolev spaces Hm+s(\u03a9)."
            },
            "slug": "Splines-minimizing-rotation-invariant-semi-norms-in-Duchon",
            "title": {
                "fragments": [],
                "text": "Splines minimizing rotation-invariant semi-norms in Sobolev spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A family of semi-norms is defined, subject to some interpolating conditions, providing interpolation methods that preserve polynomials of degree\u2264m\u22121 and converge in Sobolev spaces Hm+s(\u03a9)."
            },
            "venue": {
                "fragments": [],
                "text": "Constructive Theory of Functions of Several Variables"
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2190508"
                        ],
                        "name": "N. Grzywacz",
                        "slug": "N.-Grzywacz",
                        "structuredName": {
                            "firstName": "Norberto",
                            "lastName": "Grzywacz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Grzywacz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 106
                            }
                        ],
                        "text": "where /3 is a fixed positive parameter, has G(s) = e-llsl12/@ and as basis function the gaussian function (Poggio and Girosi 1989; Yuille and Grzywacz 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 39137023,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "884886d3c701104083311ce57af00c9091bc1a37",
            "isKey": false,
            "numCitedBy": 148,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Tliere are a number of important phenom- ena in motion perception involving colicrcnce. Examples include motion capture and motion cooperativity. We propose a theoretical model, called the motion coherence tlieory, that gives a possible explanation for these effects (Yuille and Grzywacz, 1988a,b). In this framework, the aperture problem can also be thought of as a problem of coherence and given a similar explanation. We propose the concept of a velocity field dcfined everywhere in the image, even where there is no explicit motion information available. Through a cost function, tlie model imposes smoothness on the velocity field in a more general way than previous theories. In this paper, we provide a de- tailed theoretical analysis of the motion coherence theory. We discuss its relations with previous theories and show that some of t1ic.m arc approximations to it. A sccorid pa- per (Grzywacz, Smith, and Yuillc, 1088) provides exten- sions and cletnilcd comparisons to psychophysical plienom- cna. Tlic theory applies to both short-range and long- range motion. It places them in the same computational framework aiid provides a way to define interactions be- twcr:11 the two 1)roccsses."
            },
            "slug": "The-Motion-Coherence-Theory-Yuille-Grzywacz",
            "title": {
                "fragments": [],
                "text": "The Motion Coherence Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper proposes the concept of a velocity field everywhere in the image, even where there is no explicit motion information available, through a cost function, which imposes smoothness on the velocity field in a more general way than previous theories."
            },
            "venue": {
                "fragments": [],
                "text": "[1988 Proceedings] Second International Conference on Computer Vision"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724922"
                        ],
                        "name": "Bunpei Irie",
                        "slug": "Bunpei-Irie",
                        "structuredName": {
                            "firstName": "Bunpei",
                            "lastName": "Irie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bunpei Irie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3126340"
                        ],
                        "name": "S. Miyake",
                        "slug": "S.-Miyake",
                        "structuredName": {
                            "firstName": "Sei",
                            "lastName": "Miyake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Miyake"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15092998,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb73ff39bf5e42e03b5428ce03c43f451288d534",
            "isKey": false,
            "numCitedBy": 418,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "A theorem is proved to the effect that three-layered perceptrons with an infinite number of computing units can represent arbitrary mapping if the desired mapping and the input-output characteristics of the computing units satisfy some constraints. The proof is constructive, and each coefficient is explicitly presented. The theorem theoretically guarantees a kind of universality for three-layered perceptrons. Although two-layered perceptrons (simple perceptrons) cannot represent arbitrary functions, three-layers prove necessary and sufficient. The relationship between the model used in the proof and the distributed storage and processing of information is also discussed.<<ETX>>"
            },
            "slug": "Capabilities-of-three-layered-perceptrons-Irie-Miyake",
            "title": {
                "fragments": [],
                "text": "Capabilities of three-layered perceptrons"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A theorem is proved that three-layered perceptrons with an infinite number of computing units can represent arbitrary mapping if the desired mapping and the input-output characteristics of the computing units satisfy some constraints."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE 1988 International Conference on Neural Networks"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2477489"
                        ],
                        "name": "L. Devroye",
                        "slug": "L.-Devroye",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Devroye",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Devroye"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1940156"
                        ],
                        "name": "T. Wagner",
                        "slug": "T.-Wagner",
                        "structuredName": {
                            "firstName": "Terry",
                            "lastName": "Wagner",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Wagner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 120504471,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "051c6e55c017c83d75e9425df224716f55d1a73c",
            "isKey": false,
            "numCitedBy": 238,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "first proposed by Watson (1964) and Nadaraya (1964) . In (1) k is a nonnegative function on R\" and {h} n is a sequence of positive numbers . The pointwise consistency of (1) is discussed by Watson (1964), Nadaraya (1964, 1965), Rosenblatt (1969), Schuster (1972), Greblicki (1974) and Noda (1976) . The uniform consistency is treated in the papers by Nadaraya (1965, 1970), Greblicki (1974) and Devroye (1978a) ."
            },
            "slug": "Distribution-Free-Consistency-Results-in-and-Devroye-Wagner",
            "title": {
                "fragments": [],
                "text": "Distribution-Free Consistency Results in Nonparametric Discrimination and Regression Function Estimation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2634385"
                        ],
                        "name": "W. Madych",
                        "slug": "W.-Madych",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Madych",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Madych"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143758670"
                        ],
                        "name": "S. Nelson",
                        "slug": "S.-Nelson",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Nelson",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nelson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 40802283,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "da041bbf08388c7f83982acfe0c8bfc23b12d4c6",
            "isKey": false,
            "numCitedBy": 562,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We continue an earlier study of certain spaces that provide a variational framework for multivariate interpolation. Using the Fourier transform to analyze these spaces, we obtain error estimates of arbitrarily high order for a class of interpolation methods that includes multiquadrics"
            },
            "slug": "Multivariate-interpolation-and-condi-tionally-Madych-Nelson",
            "title": {
                "fragments": [],
                "text": "Multivariate interpolation and condi-tionally positive definite functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2630782"
                        ],
                        "name": "J. Marroqu\u00edn",
                        "slug": "J.-Marroqu\u00edn",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Marroqu\u00edn",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Marroqu\u00edn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689724"
                        ],
                        "name": "S. Mitter",
                        "slug": "S.-Mitter",
                        "structuredName": {
                            "firstName": "Sanjoy",
                            "lastName": "Mitter",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mitter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14692859,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "97c12f5ddfdf036eb6cf935a679d1e4ba2fe535f",
            "isKey": false,
            "numCitedBy": 865,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We formulate several problems in early vision as inverse problems. Among the solution methods we review standard regularization theory, discuss its limitations, and present new stochastic (in particular, Bayesian) techniques based on Markov Random Field models for their solution. We derive efficient algorithms and describe parallel implementations on digital parallel SIMD architectures, as well as a new class of parallel hybrid computers that mix digital with analog components."
            },
            "slug": "Probabilistic-Solution-of-Ill-Posed-Problems-in-Marroqu\u00edn-Mitter",
            "title": {
                "fragments": [],
                "text": "Probabilistic Solution of Ill-Posed Problems in Computational Vision"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work derives efficient algorithms and describes parallel implementations on digital parallel SIMD architectures, as well as a new class of parallel hybrid computers that mix digital with analog components."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144410963"
                        ],
                        "name": "S. Kung",
                        "slug": "S.-Kung",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Kung",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kung"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 39783408,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e404e0459c34749519c1f248f771b86aab7d2729",
            "isKey": false,
            "numCitedBy": 373,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Digital-neural-networks-Kung",
            "title": {
                "fragments": [],
                "text": "Digital neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Prentice Hall Information and System Sciences Series"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49800308"
                        ],
                        "name": "B. Silverman",
                        "slug": "B.-Silverman",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Silverman",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Silverman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 119733144,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "69e535bb138886d10cdba8a53d0b297a5444a86a",
            "isKey": false,
            "numCitedBy": 439,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "On considere une approche spline pour la regression non parametrique et l'estimation de courbe. On montre que dans un certain sens, le lissage spline correspond approximativement au lissage par une methode du noyau avec une largeur de bande dependant de la densite locale des points du plan de regression"
            },
            "slug": "Spline-Smoothing:-The-Equivalent-Variable-Kernel-Silverman",
            "title": {
                "fragments": [],
                "text": "Spline Smoothing: The Equivalent Variable Kernel Method"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700754"
                        ],
                        "name": "Volker Tresp",
                        "slug": "Volker-Tresp",
                        "structuredName": {
                            "firstName": "Volker",
                            "lastName": "Tresp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Volker Tresp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2648836"
                        ],
                        "name": "J. Hollatz",
                        "slug": "J.-Hollatz",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Hollatz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hollatz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109973472"
                        ],
                        "name": "Subutai Ahmad",
                        "slug": "Subutai-Ahmad",
                        "structuredName": {
                            "firstName": "Subutai",
                            "lastName": "Ahmad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Subutai Ahmad"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9586140,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d3fc3bedad659b7a3b45ce9dfa86850d72980af9",
            "isKey": false,
            "numCitedBy": 115,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We demonstrate in this paper how certain forms of rule-based knowledge can be used to prestructure a neural network of normalized basis functions and give a probabilistic interpretation of the network architecture. We describe several ways to assure that rule-based knowledge is preserved during training and present a method for complexity reduction that tries to minimize the number of rules and the number of conjuncts. After training the refined rules are extracted and analyzed."
            },
            "slug": "Network-Structuring-and-Training-Using-Rule-Based-Tresp-Hollatz",
            "title": {
                "fragments": [],
                "text": "Network Structuring and Training Using Rule-Based Knowledge"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "It is demonstrated in this paper how certain forms of rule-based knowledge can be used to prestructure a neural network of normalized basis functions and give a probabilistic interpretation of the network architecture."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "26373808"
                        ],
                        "name": "R. DeVore",
                        "slug": "R.-DeVore",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "DeVore",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. DeVore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145604371"
                        ],
                        "name": "R. Howard",
                        "slug": "R.-Howard",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Howard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Howard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708279"
                        ],
                        "name": "C. Micchelli",
                        "slug": "C.-Micchelli",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Micchelli",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Micchelli"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120895852,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7b3a41df5bc4c1cc0f07d0554130ec160cd936f4",
            "isKey": false,
            "numCitedBy": 208,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a definition of nonlinear n-widths and then determine the n-widths of the unit ball of the Sobolev spaceWpr inLq. We prove that in the sense of these widths the manifold of splines of fixed degree with n free knots is optimal for approximating functions in these Sobolev spaces."
            },
            "slug": "Optimal-nonlinear-approximation-DeVore-Howard",
            "title": {
                "fragments": [],
                "text": "Optimal nonlinear approximation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143701969"
                        ],
                        "name": "M. Priestley",
                        "slug": "M.-Priestley",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Priestley",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Priestley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144505576"
                        ],
                        "name": "M. Chao",
                        "slug": "M.-Chao",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Chao",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Chao"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 283,
                                "start": 163
                            }
                        ],
                        "text": "This form of approximation is known as kernel regression, or Nadaraya-Watson estimator, and it has been the subject of extensive study in the statistics community (Nadaraya 1964; Watson 1964; Rosenblatt 1971; Priestley and Chao 1972; Gasser and Miiller 1985; Devroye and Wagner 1980)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 125981674,
            "fieldsOfStudy": [
                "Mathematics",
                "Philosophy"
            ],
            "id": "b62ee373df05db99b4ac9698468e22c79595ee7a",
            "isKey": false,
            "numCitedBy": 384,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY In this note we consider the problem of fitting a general functional relationship between two variables. We require only that the function to be fitted is, in some sense, \"smooth\", and do not assume that it has a known mathematical form involving only a finite number of unknown parameters."
            },
            "slug": "Non\u2010Parametric-Function-Fitting-Priestley-Chao",
            "title": {
                "fragments": [],
                "text": "Non\u2010Parametric Function Fitting"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727567"
                        ],
                        "name": "R. Solomonoff",
                        "slug": "R.-Solomonoff",
                        "structuredName": {
                            "firstName": "Ray",
                            "lastName": "Solomonoff",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Solomonoff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1673415,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0453028e68581624ec68cfec70214231da8dbce7",
            "isKey": false,
            "numCitedBy": 397,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "In 1964 the author proposed as an explication of {\\em a priori} probability the probability measure induced on output strings by a universal Turing machine with unidirectional output tape and a randomly coded unidirectional input tape. Levin has shown that if tilde{P}'_{M}(x) is an unnormalized form of this measure, and P(x) is any computable probability measure on strings, x , then \\tilde{P}'_{M}\\geqCP(x) where C is a constant independent of x . The corresponding result for the normalized form of this measure, P'_{M} , is directly derivable from Willis' probability measures on nonuniversal machines. If the conditional probabilities of P'_{M} are used to approximate those of P , then the expected value of the total squared error in these conditional probabilities is bounded by -(1/2) \\ln C . With this error criterion, and when used as the basis of a universal gambling scheme, P'_{M} is superior to Cover's measure b\\ast . When H\\ast\\equiv -\\log_{2} P'_{M} is used to define the entropy of a rmite sequence, the equation H\\ast(x,y)= H\\ast(x)+H^{\\ast}_{x}(y) holds exactly, in contrast to Chaitin's entropy definition, which has a nonvanishing error term in this equation."
            },
            "slug": "Complexity-based-induction-systems:-Comparisons-and-Solomonoff",
            "title": {
                "fragments": [],
                "text": "Complexity-based induction systems: Comparisons and convergence theorems"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Levin has shown that if tilde{P}'_{M}(x) is an unnormalized form of this measure, and P( x) is any computable probability measure on strings, x, then \\tilde{M}'_M}\\geqCP (x) where C is a constant independent of x ."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "There are several biophysically plausible ways to implement gaussian RBF-like units (see Poggio and Girosi 1989; Poggio 1990), but none is particularly simple."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In general, the circuits required for the various schemes described in this paper are reasonable from a biological point of view (Poggio and Girosi 1989; Poggio 1990)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3648459,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb2cca58ac24c45210079378e3085a55401b9c81",
            "isKey": false,
            "numCitedBy": 341,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "I propose a speculative new version of the grandmother cell theory to explain how the brain may work, discuss how the visual system may learn to recognize 3D objects, and relate our theory to existing models of the cerebellum and motor control. The main points are: 1) the brain uses modules for multivariate function approximation as components of several information processing subsystems, 2) these modules are realized as HyperBF networks, and 3) HyperBF networks can be implemented in terms of biologically plausible mechanisms and circuitry. The theory predicts a type of population coding that represents an extension of schemes such as look-up tables."
            },
            "slug": "A-theory-of-how-the-brain-might-work.-Poggio",
            "title": {
                "fragments": [],
                "text": "A theory of how the brain might work."
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A speculative new version of the grandmother cell theory is proposed to explain how the brain may work, discuss how the visual system may learn to recognize 3D objects, and relate the theory to existing models of the cerebellum and motor control."
            },
            "venue": {
                "fragments": [],
                "text": "Cold Spring Harbor symposia on quantitative biology"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109466578"
                        ],
                        "name": "Zhiyong Yang",
                        "slug": "Zhiyong-Yang",
                        "structuredName": {
                            "firstName": "Zhiyong",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhiyong Yang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123224094,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "e84649b26c2acd59fe67b67265911554a594bdf6",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Nonlinear-superposition-of-receptive-fields-and-Yang",
            "title": {
                "fragments": [],
                "text": "Nonlinear superposition of receptive fields and phase transitions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40770452"
                        ],
                        "name": "L. Goddard",
                        "slug": "L.-Goddard",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Goddard",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Goddard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4192258,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6bef91d0c32549f07c969322cb6d28f1fd96ec9d",
            "isKey": false,
            "numCitedBy": 515,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Theory of Approximation of Functions of a Real VariableBy A. F. Timan. Translated by J. Berry. English translation edited and editorial preface by J. Cossar. (International Series of Monographs on Pure and Applied Mathematics, Vol. 34.) Pp. xii + 631. (London and New York: Pergamon Press, 1963.) 100s. net."
            },
            "slug": "Approximation-of-Functions-Goddard",
            "title": {
                "fragments": [],
                "text": "Approximation of Functions"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145343252"
                        ],
                        "name": "S. Wold",
                        "slug": "S.-Wold",
                        "structuredName": {
                            "firstName": "Svante",
                            "lastName": "Wold",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Wold"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120116272,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "99a0f1bc2b4535406d780957cc5cc937d987d176",
            "isKey": false,
            "numCitedBy": 396,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The cross validation mean square error technique is used to determine the correct degree of smoothing, in fitting smoothing solines to discrete, noisy observations from some unknown smooth function. Monte Cario results snow amazing success in estimating the true smooth function as well as its derivative."
            },
            "slug": "A-completely-automatic-french-curve:-fitting-spline-Wahba-Wold",
            "title": {
                "fragments": [],
                "text": "A completely automatic french curve: fitting spline functions by cross validation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34923849"
                        ],
                        "name": "G. Lorentz",
                        "slug": "G.-Lorentz",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Lorentz",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lorentz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122692032,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b800abb5dd52984199b89f9a446e51bdcd958b90",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : An introduction is given to some recent developments connected with properties of compact sets of continuous functions. Complete proofs are not given, but their main ideas are explained. (Author)"
            },
            "slug": "METRIC-ENTROPY,-WIDTHS,-AND-SUPERPOSITIONS-OF-Lorentz",
            "title": {
                "fragments": [],
                "text": "METRIC ENTROPY, WIDTHS, AND SUPERPOSITIONS OF FUNCTIONS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117752873,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1b4e5472a0c0cf122db65922eb8d879dcdb63359",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "The method of weighted cross-validation is applied to the problem of solving linear integral equations of the first kind with noisy data. Numerical results illustrating its efficacy are given for estimating derivatives and for solving Fujita\u2019s equation."
            },
            "slug": "Smoothing-and-Ill-Posed-Problems-Wahba",
            "title": {
                "fragments": [],
                "text": "Smoothing and Ill-Posed Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "The method of weighted cross- validation is applied to the problem of solving linear integral equations of the first kind with noisy data and results are given for estimating derivatives and for solving Fujita's equation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 30140639,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d382b9c11e5c6a8e173fbeb442545e3be8d3e3a5",
            "isKey": false,
            "numCitedBy": 6261,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Modeling-By-Shortest-Data-Description*-Rissanen",
            "title": {
                "fragments": [],
                "text": "Modeling By Shortest Data Description*"
            },
            "venue": {
                "fragments": [],
                "text": "Autom."
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2093644"
                        ],
                        "name": "Bartlett W. Mel",
                        "slug": "Bartlett-W.-Mel",
                        "structuredName": {
                            "firstName": "Bartlett",
                            "lastName": "Mel",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bartlett W. Mel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10734619,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "10eaee6baae02e13f1e65b09d42bfab8a2d1ab9d",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "MURPHY consists of a camera looking at a robot arm, with a connectionist network architecture situated in between. By moving its arm through a small, representative sample of the 1 billion possible joint configurations, MURPHY learns the relationships, backwards and forwards, between the positions of its joints and the state of its visual field. MURPHY can use its internal model in the forward direction to \"envision\" sequences of actions for planning purposes, such as in grabbing a visually presented object, or in the reverse direction to \"imitate\", with its arm, autonomous activity in its visual field. Furthermore, by taking explicit advantage of continuity in the mappings between visual space and joint space, MURPHY is able to learn non-linear mappings with only a single layer of modifiable weights."
            },
            "slug": "MURPHY:-A-Robot-that-Learns-by-Doing-Mel",
            "title": {
                "fragments": [],
                "text": "MURPHY: A Robot that Learns by Doing"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "By taking explicit advantage of continuity in the mappings between visual space and joint space, MURPHY is able to learn non-linear mappings with only a single layer of modifiable weights."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101750654"
                        ],
                        "name": "G. Kimeldorf",
                        "slug": "G.-Kimeldorf",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Kimeldorf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kimeldorf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 120654716,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5f75d859e750961d1d094f166fc3b564d9cfe99b",
            "isKey": false,
            "numCitedBy": 975,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : The report presents classes of prior distributions for which the Bayes' estimate of an unknown function given certain observations is a spline function. (Author)"
            },
            "slug": "A-Correspondence-Between-Bayesian-Estimation-on-and-Kimeldorf-Wahba",
            "title": {
                "fragments": [],
                "text": "A Correspondence Between Bayesian Estimation on Stochastic Processes and Smoothing by Splines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71339631"
                        ],
                        "name": "R. L. Harder",
                        "slug": "R.-L.-Harder",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Harder",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. L. Harder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69374968"
                        ],
                        "name": "R. N. Desmarais",
                        "slug": "R.-N.-Desmarais",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Desmarais",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. N. Desmarais"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 119719484,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "67942e80e20420504cc4fa1c030700a9bbdeeb4e",
            "isKey": false,
            "numCitedBy": 761,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "A surface spline is a mathematical tool for interpolating a function of two variables. It is based upon the small deflection equation of an infinite plate. The surface spline depends upon the solution of a system of linear equations, and thus, will ordinarily require the use of a digital computer. The closed form solution involves no functions more complicated than logarithms, and is easily coded. Several modifications which can be incorporated are discussed."
            },
            "slug": "Interpolation-using-surface-splines.-Harder-Desmarais",
            "title": {
                "fragments": [],
                "text": "Interpolation using surface splines."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 121176122,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "88ce531f22108f687cbb576bcb0cd660b2a694bc",
            "isKey": false,
            "numCitedBy": 539,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "On considere des procedures de lissage spline etudiees en 1978 et 1983 et leur extension a la resolution d'equations d'operateurs lineaires avec donnees bruitees"
            },
            "slug": "A-Comparison-of-GCV-and-GML-for-Choosing-the-in-the-Wahba",
            "title": {
                "fragments": [],
                "text": "A Comparison of GCV and GML for Choosing the Smoothing Parameter in the Generalized Spline Smoothing Problem"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143915028"
                        ],
                        "name": "R. Franke",
                        "slug": "R.-Franke",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Franke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Franke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8290519,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ada9b35388f4cf3b0aa54109e3e87165066842d1",
            "isKey": false,
            "numCitedBy": 2103,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Absract. This paper is concerned with the evaluation of methods for scattered data interpolation and some of the results of the tests when applied to a number of methods. The process involves evaluation of the methods in terms of timing, storage, accuracy, visual pleasantness of the surface, and ease of implementation. To indicate the flavor of the type of results obtained, we give a summary table and representative perspective plots of several surfaces."
            },
            "slug": "Scattered-data-interpolation:-tests-of-some-methods-Franke",
            "title": {
                "fragments": [],
                "text": "Scattered data interpolation: tests of some methods"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2259351"
                        ],
                        "name": "C. D. Boor",
                        "slug": "C.-D.-Boor",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Boor",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. D. Boor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16203596,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "db3cf1fb1036cc13ef4c6a64b96f2ca899df774a",
            "isKey": false,
            "numCitedBy": 108,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "The determination of the approximation power of spaces of multivariate splines with the aid of quasiinterpolants is reviewed. In the process, a streamlined description of the existing quasiinterpolant theory is given."
            },
            "slug": "Quasiinterpolants-and-Approximation-Power-of-Boor",
            "title": {
                "fragments": [],
                "text": "Quasiinterpolants and Approximation Power of Multivariate Splines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103006272"
                        ],
                        "name": "R. L. Hardy",
                        "slug": "R.-L.-Hardy",
                        "structuredName": {
                            "firstName": "Rolland",
                            "lastName": "Hardy",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. L. Hardy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 129508657,
            "fieldsOfStudy": [
                "Mathematics",
                "Geology"
            ],
            "id": "6d4fe26165974d9364a2a19d1f8fc3bc43ee7396",
            "isKey": false,
            "numCitedBy": 2452,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "A new analytical method of representing irregular surfaces that involves the summation of equations of quadric surfaces having unknown coefficients is described. The quadric surfaces are located at significant points throughout the region to be mapped. Procedures are given for solving multiquadric equations of topography that are based on coordinate data. Contoured multiquadric surfaces are compared with topography and other irregular surfaces from which the multiquadric equation was derived."
            },
            "slug": "Multiquadric-equations-of-topography-and-other-Hardy",
            "title": {
                "fragments": [],
                "text": "Multiquadric equations of topography and other irregular surfaces"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144159852"
                        ],
                        "name": "D. Lowe",
                        "slug": "D.-Lowe",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lowe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lowe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111800548"
                        ],
                        "name": "Robert Matthews",
                        "slug": "Robert-Matthews",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Matthews",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Matthews"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 23368580,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "874b821b1c4d40f6bbe028062f324a38b893214f",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we show, for the first time, how Radial Basis Function (RBF) network techniques can be used to explore questions surrounding authorship of historic documents. The paper illustrates the technical and practical aspects of RBF's, using data extracted from works written in the early 17th century by William Shakespeare and his contemporary John Fletcher. We also present benchmark comparisons with other standard techniques for contrast and comparison."
            },
            "slug": "Shakespeare-vs.-fletcher:-A-stylometric-analysis-by-Lowe-Matthews",
            "title": {
                "fragments": [],
                "text": "Shakespeare vs. fletcher: A stylometric analysis by radial basis functions"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "It is shown, for the first time, how Radial Basis Function (RBF) network techniques can be used to explore questions surrounding authorship of historic documents."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Humanit."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143915028"
                        ],
                        "name": "R. Franke",
                        "slug": "R.-Franke",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Franke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Franke"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122229458,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a8e58090877fc629526ea2c1e4aa9b716a91055b",
            "isKey": false,
            "numCitedBy": 72,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Recent-Advances-in-the-Approximation-of-surfaces-Franke",
            "title": {
                "fragments": [],
                "text": "Recent Advances in the Approximation of surfaces from scattered Data"
            },
            "venue": {
                "fragments": [],
                "text": "Topics in Multivariate Approximation"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083605154"
                        ],
                        "name": "D. M. Allen",
                        "slug": "D.-M.-Allen",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Allen",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. M. Allen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121351491,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c08cae9bd11e903e301e1f69ef28eb3cba663fb2",
            "isKey": false,
            "numCitedBy": 1252,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "We show that data augmentation provides a rather general formulation for the study of biased prediction techniques using multiple linear regression. Variable selection is a limiting case, and Ridge regression is a special case of data augmentation. We propose a way to obtain predictors given a credible criterion of good prediction."
            },
            "slug": "The-Relationship-Between-Variable-Selection-and-and-Allen",
            "title": {
                "fragments": [],
                "text": "The Relationship Between Variable Selection and Data Agumentation and a Method for Prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "It is shown that data augmentation provides a rather general formulation for the study of biased prediction techniques using multiple linear regression and a way to obtain predictors given a credible criterion of good prediction is proposed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8142232,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a36b028d024bf358c4af1a5e1dc3ca0aed23b553",
            "isKey": false,
            "numCitedBy": 3710,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter reproduces the English translation by B. Seckler of the paper by Vapnik and Chervonenkis in which they gave proofs for the innovative results they had obtained in a draft form in July 1966 and announced in 1968 in their note in Soviet Mathematics Doklady. The paper was first published in Russian as \u0412\u0430\u043f\u043d\u0438\u043a \u0412. \u041d. and \u0427\u0435\u0440\u0432\u043e\u043d\u0435\u043d\u043a\u0438\u0441 \u0410. \u042f. \u041e \u0440\u0430\u0432\u043d\u043e\u043c\u0435\u0440\u043d\u043e\u0419 \u0441\u0445\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u0438 \u0447\u0430\u0441\u0442\u043e\u0442 \u043f\u043e\u044f\u0432\u043b\u0435\u043d\u0438\u044f \u0441\u043e\u0431\u044b\u0442\u0438\u0419 \u043a \u0438\u0445 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044f\u043c. \u0422\u0435\u043e\u0440\u0438\u044f \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0435\u0419 \u0438 \u0435\u0435 \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u044f 16(2), 264\u2013279 (1971)."
            },
            "slug": "Chervonenkis:-On-the-uniform-convergence-of-of-to-Vapnik",
            "title": {
                "fragments": [],
                "text": "Chervonenkis: On the uniform convergence of relative frequencies of events to their probabilities"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This chapter reproduces the English translation by B. Seckler of the paper by Vapnik and Chervonenkis in which they gave proofs for the innovative results they had obtained in a draft form in July 1966 and announced in 1968 in their note in Soviet Mathematics Doklady."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708279"
                        ],
                        "name": "C. Micchelli",
                        "slug": "C.-Micchelli",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Micchelli",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Micchelli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14461054,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9d700e611ee7ffdf54873684a9e8883d3da0bcd7",
            "isKey": false,
            "numCitedBy": 1193,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Among other things, we prove that multiquadric surface interpolation is always solvable, thereby settling a conjecture of R. Franke."
            },
            "slug": "Interpolation-of-scattered-data:-Distance-matrices-Micchelli",
            "title": {
                "fragments": [],
                "text": "Interpolation of scattered data: Distance matrices and conditionally positive definite functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145052965"
                        ],
                        "name": "G. Parisi",
                        "slug": "G.-Parisi",
                        "structuredName": {
                            "firstName": "Giorgio",
                            "lastName": "Parisi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Parisi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48215291"
                        ],
                        "name": "R. Shankar",
                        "slug": "R.-Shankar",
                        "structuredName": {
                            "firstName": "Ramamurti",
                            "lastName": "Shankar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Shankar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 119396873,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "cf2af6e45d25f62bc52bb1ecae86a2c3d0aae72b",
            "isKey": false,
            "numCitedBy": 865,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Classical equilibrium statistical mechanics magnetic systems the Ising model the low-temperature and high-temperature expansions the Landau-Ginsberg model near the transition the renormalization group perturbative evaluation of the critical exponents near four dimensions on spontaneous symmetry breaking other models the transfer matrix path integrals for quantum mechanics semiclassical methods relativistic quantum field theory particle-field duality time dependent correlations the approach to equilibrium the stochastic approach to equilibrium the stochastic approach computer simulation."
            },
            "slug": "Statistical-Field-Theory-Parisi-Shankar",
            "title": {
                "fragments": [],
                "text": "Statistical Field Theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2856152"
                        ],
                        "name": "E. Parzen",
                        "slug": "E.-Parzen",
                        "structuredName": {
                            "firstName": "Emanuel",
                            "lastName": "Parzen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Parzen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 122932724,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "de28c165623adabcdba0fdb18b65eba685aaf31d",
            "isKey": false,
            "numCitedBy": 9493,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Given a sequence of independent identically distributed random variables with a common probability density function, the problem of the estimation of a probability density function and of determining the mode of a probability function are discussed. Only estimates which are consistent and asymptotically normal are constructed. (Author)"
            },
            "slug": "On-Estimation-of-a-Probability-Density-Function-and-Parzen",
            "title": {
                "fragments": [],
                "text": "On Estimation of a Probability Density Function and Mode"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740300"
                        ],
                        "name": "D. Beymer",
                        "slug": "D.-Beymer",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Beymer",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Beymer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 56956813,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "7a0ae56fe2f5434fc1de4969867881a65680ab82",
            "isKey": false,
            "numCitedBy": 317,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The mind cannot make sense of the visual world out of raw image data alone. In an approach to visual processing known as learning from examples, computational neural networks and physiological studies suggest how neurons and machines adapt to novel images on the basis of past experience."
            },
            "slug": "Learning-to-see-Poggio-Beymer",
            "title": {
                "fragments": [],
                "text": "Learning to see"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "In an approach to visual processing known as learning from examples, computational neural networks and physiological studies suggest how neurons and machines adapt to novel images on the basis of past experience."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69859375"
                        ],
                        "name": "E. Stein",
                        "slug": "E.-Stein",
                        "structuredName": {
                            "firstName": "Elias",
                            "lastName": "Stein",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Stein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\"The function c is the standard sigmoidal function, the function IxI+ in the third entry is the ramp function, and the function G, in the fifth entry is a Bessel potential, that is the Fourier transform of (1 + 11s112)-m/2 (Stein 1970)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 115462014,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "443a33713552aa46f7e0b03a4429585147394c20",
            "isKey": false,
            "numCitedBy": 5145,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A plurality of disks include solid laserable material and are in parallel spaced apart relation within a transparent tubular enclosure. Spaces between the disks constitute portions of a collant fluid passage within the tubular enclosure."
            },
            "slug": "Singular-Integrals-and-Dierentiability-Properties-Stein",
            "title": {
                "fragments": [],
                "text": "Singular Integrals and Di?erentiability Properties of Functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30821484"
                        ],
                        "name": "E. Nadaraya",
                        "slug": "E.-Nadaraya",
                        "structuredName": {
                            "firstName": "Elizbar",
                            "lastName": "Nadaraya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Nadaraya"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120067924,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "05175204318c3c01e3301fd864553071039605d2",
            "isKey": false,
            "numCitedBy": 3287,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A study is made of certain properties of an approximation to the regression line on the basis of sampling data when the sample size increases unboundedly."
            },
            "slug": "On-Estimating-Regression-Nadaraya",
            "title": {
                "fragments": [],
                "text": "On Estimating Regression"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102672172"
                        ],
                        "name": "A. Timan",
                        "slug": "A.-Timan",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Timan",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Timan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "However, if we measure performances by how quickly the approximation error goes to zero when the number of parameters of the approximation scheme goes to infinity, very general results from the theory of linear and nonlinear widths (Timan 1963; Pinkus 1986; Lorentz 1962, 1986; DeVore et al. 1989; DeVore 1991; DeVore and Yu 1991) suggest that all techniques share the same limitations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 118819329,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4ec58364169040afb497464c45b12ef85f91529b",
            "isKey": false,
            "numCitedBy": 998,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Theory-of-Approximation-of-Functions-of-a-Real-Timan",
            "title": {
                "fragments": [],
                "text": "Theory of Approximation of Functions of a Real Variable"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 75
                            }
                        ],
                        "text": "We refer the reader to the current literature for the mathematical details (Wahba 1990; Madych and Nelson 1990a; Dyn 1987)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 199
                            }
                        ],
                        "text": "We then apply the regularization approach and impose a smoothness constraint, not on the function f as a whole, but on each single additive component, through a regularization functional of the form (Wahba 1990; Hastie and Tibshirani 1990):"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 112
                            }
                        ],
                        "text": "We present here a simple derivation and refer the reader to the current literature for the mathematical details (Wahba 1990; Madych and Nelson 1990; Dyn 1987)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 45
                            }
                        ],
                        "text": "Additive models are well known in statistics (Hastie and Tibshirani 1986, 1987, 1990; Stone 1985; Wahba 1990; Buja et al. 1989) and can be considered as a generalization of linear models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 49
                            }
                        ],
                        "text": "In the Bayesian interpretation of regularization (see Kimeldorf and Wahba 1971; Wahba 1990; Bertero ef al. 1988; Marroquin et al. 1987; Poggio et al. 1985) the stabilizer corresponds to a smoothness prior, and the error term to a model of the noise in the data (usually gaussian and additive)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 170
                            }
                        ],
                        "text": "These basis functions are radial and conditionally positive definite, so that they represent just particular instances of the well known radial basis functions technique (Micchelli 1986; Wahba 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 195857038,
            "fieldsOfStudy": [],
            "id": "b7294ac444ae927f71ac442372903ff5e2a763e1",
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Spline Models for Observational Data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 149
                            }
                        ],
                        "text": "The fourth and the fifth models were trained by fitting one basis function at a time according to the following recursive algorithm with backfitting (Friedman and Stuezle 1981; Hastie and Tibshirani 1990; Breiman 1993)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 70
                            }
                        ],
                        "text": "The parameters were learned using the iterative backfitting algorithm (Friedman and Stuezle 1981; Hastie and Tibshirani 1990; Breiman 1993) that will be described in Section 7."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 48
                            }
                        ],
                        "text": "A similar phenomenon appears in entries 2 and 3 (Barron 1991, 1993; Breiman 1993), but in a less obvious way."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hinging hyperplanes for regression"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50456127"
                        ],
                        "name": "I. J. Schoenberg",
                        "slug": "I.-J.-Schoenberg",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Schoenberg",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. J. Schoenberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 125923957,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "731ecc3d6cde71cfde719ae809e2bfb1a852a1fd",
            "isKey": false,
            "numCitedBy": 736,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Contributions-to-the-problem-of-approximation-of-by-Schoenberg",
            "title": {
                "fragments": [],
                "text": "Contributions to the problem of approximation of equidistant data by analytic functions. Part A. On the problem of smoothing or graduation. A first class of analytic approximation formulae"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1946
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143929773"
                        ],
                        "name": "M. C. Jones",
                        "slug": "M.-C.-Jones",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Jones",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. C. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33734211"
                        ],
                        "name": "R. Sibson",
                        "slug": "R.-Sibson",
                        "structuredName": {
                            "firstName": "Robin",
                            "lastName": "Sibson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sibson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 125481163,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "1ebb53a7e5cff86b2b42d1108a0fa81f571d8894",
            "isKey": false,
            "numCitedBy": 1404,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "What-is-projection-pursuit-Jones-Sibson",
            "title": {
                "fragments": [],
                "text": "What is projection pursuit"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48566708"
                        ],
                        "name": "G. S. Watson",
                        "slug": "G.-S.-Watson",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Watson",
                            "middleNames": [
                                "Stuart"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. S. Watson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 283,
                                "start": 163
                            }
                        ],
                        "text": "This form of approximation is known as kernel regression, or Nadaraya-Watson estimator, and it has been the subject of extensive study in the statistics community (Nadaraya 1964; Watson 1964; Rosenblatt 1971; Priestley and Chao 1972; Gasser and Miiller 1985; Devroye and Wagner 1980)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 124218927,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "14821ac1bf09890a857fca2a6c324e8c85f2c0d0",
            "isKey": false,
            "numCitedBy": 2957,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Smooth-regression-analysis-Watson",
            "title": {
                "fragments": [],
                "text": "Smooth regression analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50456127"
                        ],
                        "name": "I. J. Schoenberg",
                        "slug": "I.-J.-Schoenberg",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Schoenberg",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. J. Schoenberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 122552054,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8cff1bb84a84c047a0157fbe078738760a06a2e1",
            "isKey": false,
            "numCitedBy": 327,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Cardinal-interpolation-and-spline-functions-Schoenberg",
            "title": {
                "fragments": [],
                "text": "Cardinal interpolation and spline functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2364792"
                        ],
                        "name": "I. Johnstone",
                        "slug": "I.-Johnstone",
                        "structuredName": {
                            "firstName": "Iain",
                            "lastName": "Johnstone",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Johnstone"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 121324601,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71cad2c82858cbafcfafe43c16c74782615ad19f",
            "isKey": false,
            "numCitedBy": 136,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Projection-Based-Approximation-and-a-Duality-with-Donoho-Johnstone",
            "title": {
                "fragments": [],
                "text": "Projection-Based Approximation and a Duality with Kernel Methods"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064415384"
                        ],
                        "name": "A. Tikhonov",
                        "slug": "A.-Tikhonov",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Tikhonov",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tikhonov"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121508623,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "16758935de5c84616bd1454ec91a9351ee49dbb6",
            "isKey": false,
            "numCitedBy": 2347,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Solution-of-Incorrectly-Formulated-Problems-and-the-Tikhonov",
            "title": {
                "fragments": [],
                "text": "Solution of Incorrectly Formulated Problems and the Regularization Method"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1963
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49909289"
                        ],
                        "name": "V. K. Murthy",
                        "slug": "V.-K.-Murthy",
                        "structuredName": {
                            "firstName": "Vrudhula",
                            "lastName": "Murthy",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. K. Murthy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 119696612,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "07a515ad059da0ecd9944b41ac1c42714e1ea3cc",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Estimation-of-Probability-Density-Murthy",
            "title": {
                "fragments": [],
                "text": "Estimation of Probability Density"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103006272"
                        ],
                        "name": "R. L. Hardy",
                        "slug": "R.-L.-Hardy",
                        "structuredName": {
                            "firstName": "Rolland",
                            "lastName": "Hardy",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. L. Hardy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 120513957,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "563aaa7f64937382504d37e06377e5a61c3795cb",
            "isKey": false,
            "numCitedBy": 723,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Theory-and-applications-of-the-method-:-20-years-of-Hardy",
            "title": {
                "fragments": [],
                "text": "Theory and applications of the multiquadric-biharmonic method : 20 years of discovery 1968-1988"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1828999"
                        ],
                        "name": "W. Ziemer",
                        "slug": "W.-Ziemer",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Ziemer",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Ziemer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 117034470,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2e15272f1d17efcea6eabdc06d3f4cf2f930be5e",
            "isKey": false,
            "numCitedBy": 684,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Weakly-Differentiable-Functions:-Sobolev-Spaces-and-Ziemer",
            "title": {
                "fragments": [],
                "text": "Weakly Differentiable Functions: Sobolev Spaces and Functions of Bounded Variation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2294095"
                        ],
                        "name": "M. Powell",
                        "slug": "M.-Powell",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Powell",
                            "middleNames": [
                                "J.",
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Powell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118224933,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c71ca26b183025b9f39f940f5e730f2c9a64e414",
            "isKey": false,
            "numCitedBy": 1425,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Radial-basis-functions-for-multivariable-a-review-Powell",
            "title": {
                "fragments": [],
                "text": "Radial basis functions for multivariable interpolation: a review"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46933679"
                        ],
                        "name": "M. Bertero",
                        "slug": "M.-Bertero",
                        "structuredName": {
                            "firstName": "Mario",
                            "lastName": "Bertero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bertero"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117703131,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "bd7efb0ea1893522220b4cf54d71269b77e80deb",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Regularization-methods-for-linear-inverse-problems-Bertero",
            "title": {
                "fragments": [],
                "text": "Regularization methods for linear inverse problems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2796350"
                        ],
                        "name": "P. Diaconis",
                        "slug": "P.-Diaconis",
                        "structuredName": {
                            "firstName": "Persi",
                            "lastName": "Diaconis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Diaconis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32854168"
                        ],
                        "name": "D. Freedman",
                        "slug": "D.-Freedman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Freedman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Freedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 85511682,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "76b0a46ee00bc34fcc8ebd4b22e1fe672cdd652c",
            "isKey": false,
            "numCitedBy": 577,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Asymptotics-of-Graphical-Projection-Pursuit-Diaconis-Freedman",
            "title": {
                "fragments": [],
                "text": "Asymptotics of Graphical Projection Pursuit"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144873562"
                        ],
                        "name": "R. Bellman",
                        "slug": "R.-Bellman",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Bellman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bellman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62668616,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4a0cd8a28b7668fcd38b98b8e1598c33e1852077",
            "isKey": false,
            "numCitedBy": 846,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "V.-Adaptive-Control-Processes-Bellman",
            "title": {
                "fragments": [],
                "text": "V. Adaptive Control Processes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48410472"
                        ],
                        "name": "C. deBoor",
                        "slug": "C.-deBoor",
                        "structuredName": {
                            "firstName": "C",
                            "lastName": "deBoor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. deBoor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 116206019,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6668f5ec1b6caebd1cfc8bd92ee607684654aace",
            "isKey": false,
            "numCitedBy": 4099,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-practical-guide-to-splines-deBoor",
            "title": {
                "fragments": [],
                "text": "A practical guide to splines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123402533"
                        ],
                        "name": "M. C. Jones",
                        "slug": "M.-C.-Jones",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Jones",
                            "middleNames": [
                                "Chris"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. C. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1859519"
                        ],
                        "name": "R. Eubank",
                        "slug": "R.-Eubank",
                        "structuredName": {
                            "firstName": "Randy",
                            "lastName": "Eubank",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Eubank"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 124078690,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "616989e3eeb160c42ecb10eba0e62a495c7d01b9",
            "isKey": false,
            "numCitedBy": 1081,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Spline-Smoothing-and-Nonparametric-Regression.-Jones-Eubank",
            "title": {
                "fragments": [],
                "text": "Spline Smoothing and Nonparametric Regression."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145768864"
                        ],
                        "name": "D. Broomhead",
                        "slug": "D.-Broomhead",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Broomhead",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Broomhead"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144159852"
                        ],
                        "name": "D. Lowe",
                        "slug": "D.-Lowe",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lowe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lowe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3686496,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "b08ba914037af6d88d16e2657a65cd9dc5cf5da1",
            "isKey": false,
            "numCitedBy": 2307,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multivariable-Functional-Interpolation-and-Adaptive-Broomhead-Lowe",
            "title": {
                "fragments": [],
                "text": "Multivariable Functional Interpolation and Adaptive Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808760"
                        ],
                        "name": "S. Omohundro",
                        "slug": "S.-Omohundro",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Omohundro",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Omohundro"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 450,
                                "start": 118
                            }
                        ],
                        "text": "The interpretation of an approximation scheme in terms of networks and vice versa has also been extensively discussed (Barron and Barron 1988; Poggio and Girosi 1989, 1990a,b; Girosi 1992; Broomhead and Lowe 1988; Moody and Darken 1988, 1989; White 1989, 1990; Ripley 1994; Omohundro 1987; Kohonen 1990; Lapedes and Farber 1988; Rumelhart et al. 1986; Hertz et al. 1991; Kung 1993; Sejnowski and Rosenberg 1987; Hurlbert and Poggio 1988; Poggio 1975)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 44717168,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff80b7820fbc54926946c245e139c382266489ae",
            "isKey": false,
            "numCitedBy": 213,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Efficient-Algorithms-with-Neural-Network-Behavior-Omohundro",
            "title": {
                "fragments": [],
                "text": "Efficient Algorithms with Neural Network Behavior"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103556459"
                        ],
                        "name": "C. J. Stone",
                        "slug": "C.-J.-Stone",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Stone",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. J. Stone"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 45
                            }
                        ],
                        "text": "Additive models are well known in statistics (Hastie and Tibshirani 1986, 1987, 1990; Stone 1985; Wahba 1990; Buja et al. 1989) and can be considered as a generalization of linear models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 43491035,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "072f3eba9251c60e21935821a2410e77492fb93a",
            "isKey": false,
            "numCitedBy": 1199,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Additive-Regression-and-Other-Nonparametric-Models-Stone",
            "title": {
                "fragments": [],
                "text": "Additive Regression and Other Nonparametric Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2085838398"
                        ],
                        "name": "I. Omiaj",
                        "slug": "I.-Omiaj",
                        "structuredName": {
                            "firstName": "I",
                            "lastName": "Omiaj",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Omiaj"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2092794854"
                        ],
                        "name": "O. Grant",
                        "slug": "O.-Grant",
                        "structuredName": {
                            "firstName": "Oliver",
                            "lastName": "Grant",
                            "middleNames": [
                                "David",
                                "Lester"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Grant"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082977778"
                        ],
                        "name": "Eleueit",
                        "slug": "Eleueit",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Eleueit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eleueit"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17910142,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "347fd669a5a14967e1fc3692acec33031895111a",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Extensions-of-a-Theory-of-Networks-for-and-Learning-Omiaj-Grant",
            "title": {
                "fragments": [],
                "text": "Extensions of a Theory of Networks for Approximation and Learning : dimensionality reduction and clustering"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48566708"
                        ],
                        "name": "G. S. Watson",
                        "slug": "G.-S.-Watson",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Watson",
                            "middleNames": [
                                "Stuart"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. S. Watson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46567779"
                        ],
                        "name": "M. R. Leadbetter",
                        "slug": "M.-R.-Leadbetter",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Leadbetter",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. R. Leadbetter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 94
                            }
                        ],
                        "text": "If this option is chosen, one could use the regularization approach to probability estimation (Vapnik and Stefanyuk 1978; Aidu and Vapnik 1989; Vapnik 1982) that leads to the well-known technique of Parzen windows."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 61
                            }
                        ],
                        "text": "17, can be derived in the framework of regularization theory (Vapnik and Stefanyuk 1978; Aidu and Vapnik 1989; Vapnik 1982) under a smoothness assumption on the probability distribution that has to be estimated."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8357602,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a27140f31c7ea6f6afdf51280c9dc23d923198b6",
            "isKey": false,
            "numCitedBy": 255,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-Estimation-of-the-Probability-Density,-I-Watson-Leadbetter",
            "title": {
                "fragments": [],
                "text": "On the Estimation of the Probability Density, I"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1963
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Curve estimates"
            },
            "venue": {
                "fragments": [],
                "text": "Ann. Math. Statist"
            },
            "year": 1971
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "How to build quasi-interpolants"
            },
            "venue": {
                "fragments": [],
                "text": "applications to polyharmonic B-splines. In"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Degree of nonlinear approximation"
            },
            "venue": {
                "fragments": [],
                "text": "Approximation Theory, VI, C. K. Chui, L. L. Schumaker, and D. J. Ward, eds., pp. 175-201. Academic Press, New York."
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The necessary and sufficient conditions for the uniform convergence of averages to their expected values"
            },
            "venue": {
                "fragments": [],
                "text": "Teor. Veroyat. Primen"
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The theory of radial basis functions approximation in 1990"
            },
            "venue": {
                "fragments": [],
                "text": "Wavelets, Subdivision Algorithms and Radial Basis Functions"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Introduction to the TheoryofNeural Computation"
            },
            "venue": {
                "fragments": [],
                "text": "Addison-Wesley, Redwood City, CA. Hornik, K., Stinchcombe, M., and White, W. 1989. Multilayer feedforward networks are universal approximators."
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 120
                            }
                        ],
                        "text": "Notice that HBF networks and ridge approximation networks are directly related in the special case of normalized inputs (Maruyama et al. 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A connection between HBF"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 64
                            }
                        ],
                        "text": "1 can be derived not only in the context of functional analysis (Tikhonov and Arsenin 1977), but also in a probabilistic framework (Kimeldorf and Wahba 1971; Wahba 1980, 1990; Poggio et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Solutions of 111-Posed Problems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theory and applications of the multiquadric-biharmonic method"
            },
            "venue": {
                "fragments": [],
                "text": "Computers Math. Applic"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning with localized receptive fields"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1988 Connectionist Models Summer School, G. Hinton, T. Sejnowski, and D. Touretzsky, eds., pp. 133-143. Palo Alto, CA. Moody,"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Universal approximation bounds for superpositions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Priors, stabilizers and basis functions: From regularization to radial, tensor and additive splines. A.I. Memo No. 1430"
            },
            "venue": {
                "fragments": [],
                "text": "Ad-F. Girosi, M. Jones, and T. Poggio uances in Neural Information Processings Systems"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 0
                            }
                        ],
                        "text": "(Vapnik 1982; Vapnik and Chervonenkis 1991), the conditional probability distribution of class 1, P(l I x)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The necessary and sufficient conditions for consistency in the empirical risk minimization method"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recog. Image Anal"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Approximation by superposition of a sigmoidal function"
            },
            "venue": {
                "fragments": [],
                "text": "Math. Control Systems Signals"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Connectionist nonparametric regression: Multilayer perceptrons can learn arbitrary mappings"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning with localized receptive fields"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1988 Connectionist Models"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multiquadrics-A scattered data approximation scheme with applications to computational fluid dynamics-11"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Math. Applic"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 392,
                                "start": 129
                            }
                        ],
                        "text": "Much attention has been dedicated to this case, and the corresponding approximation technique is known as radial basis functions (Powell 1987, 1990; Franke 1982, 1987; Micchelli 1986; Kansa, 1990a,b; Madych and Nelson 1990a; Dyn 1987,1991; Hardy 1971,1990; Buhmann 1990; Lancaster and Salkauskas 1986; Broomhead and Lowe 1988; Moody and Darken 1988, 1989; Poggio and Girosi 1990; Girosi 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 487,
                                "start": 222
                            }
                        ],
                        "text": "In particular, we described how a certain class of radial stabilizers-and the associated priors in the equivalent Bayesian formulation-lead to a subclass of regularization networks, the alreadyknown radial basis functions (Powell 1987, 1992; Franke 1982, 1987; Micchelli 1986; Kansa 1990a,b; Madych and Nelson 1990a,b; Dyn 1987, 1991; Hardy 1971,1990; Buhmann 1990; Lancaster and Salkauskas 1986; Broomhead and Lowe 1988; Moody and Darken 1988, 1989; Poggio and Girosi 1990; Girosi 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Regularization algorithms for learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 321,
                                "start": 220
                            }
                        ],
                        "text": "The first term is enforcing closeness to the data, and the second smoothness, while the regularization parameter controls the trade-off between these two terms, and can be chosen according to cross-validation techniques (Allen 1974; Wahba and Wold 1975; Golub et al. 1979; Craven and Wahba 1979; Utreras 1979; Wahba 1985) or to some other principle, such as structural risk minimization (Vapnik 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A completely automatic French curve"
            },
            "venue": {
                "fragments": [],
                "text": "Commun"
            },
            "year": 1975
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The sigma-pi column: A model of associative learning in cerebral neocortex"
            },
            "venue": {
                "fragments": [],
                "text": "Tech. Rep. 6, California Institute of Technology. Mel, B. W. 1992. NMDA-based pattern-discrimination in a modeled cortical neuron."
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On multivariate approx"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Methods for Solving lncorrectly Posed Problems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Spline bases, regularization, and generalized cross-validation for solving approximation problems with large quantities of noisy data"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the International Conference on Approximation Theory in Honour of George Lorenz"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The sigma-pi column: A model of associative learning in cerebral neocortex"
            },
            "venue": {
                "fragments": [],
                "text": "Tech. Rep"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multivariable functional interpolation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Approximation by superposition of a sigmoidal function"
            },
            "venue": {
                "fragments": [],
                "text": "Math. Control Systems Signals"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Projection pursuit regression. 1"
            },
            "venue": {
                "fragments": [],
                "text": "Am. Statist. Assoc"
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Extension of a theory of networks for approx"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 283,
                                "start": 163
                            }
                        ],
                        "text": "This form of approximation is known as kernel regression, or Nadaraya-Watson estimator, and it has been the subject of extensive study in the statistics community (Nadaraya 1964; Watson 1964; Rosenblatt 1971; Priestley and Chao 1972; Gasser and Miiller 1985; Devroye and Wagner 1980)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Distribution-free consistency results"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the uniform convergence of relative frequences of events to their probabilities"
            },
            "venue": {
                "fragments": [],
                "text": "Th. Prob. Applic"
            },
            "year": 1971
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Degree of nonlinear approximation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 345,
                                "start": 214
                            }
                        ],
                        "text": "The case in which the data points {xi} coincide with the multi-integers Zd, where 2 is the set of integers number, has been extensively studied in the literature, and it is also known as Schoenberg\u2019s approximation (Schoenberg 1946a, 1969; Rabut 1991, 1992; Madych and Nelson 1990a; Jackson 1988; de Boor 1990; Buhmann 1990,1991; Dyn et al. 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Radial basis functions methodsfor multivariate approximation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theory and applications of the multiquadric-biharmonic"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Rates of convergence of approximation by"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Rates of convergence for radial basis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Spline minimizing rotation-invariant semi-norms"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A nondeterministic minimization algorithm. A.I. Memo 1254, Artificial Intelligence Laboratory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Rates of convergence of approximation by translates. A.I. Memo 1288"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Weakly DifferentiableFunctions: Sobolev Spaces and Functions of Bounded Variation"
            },
            "venue": {
                "fragments": [],
                "text": "New York. matics"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 321,
                                "start": 220
                            }
                        ],
                        "text": "The first term is enforcing closeness to the data, and the second smoothness, while the regularization parameter controls the trade-off between these two terms, and can be chosen according to cross-validation techniques (Allen 1974; Wahba and Wold 1975; Golub et al. 1979; Craven and Wahba 1979; Utreras 1979; Wahba 1985) or to some other principle, such as structural risk minimization (Vapnik 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The relationship between variable selection and data augmen"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Approximation by superposition of a sigmoidal function"
            },
            "venue": {
                "fragments": [],
                "text": "Adu. Appl. Math"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A connection between HBF and MLP. A.I. Memo No. 1291"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 94
                            }
                        ],
                        "text": "If this option is chosen, one could use the regularization approach to probability estimation (Vapnik and Stefanyuk 1978; Aidu and Vapnik 1989; Vapnik 1982) that leads to the well-known technique of Parzen windows."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 61
                            }
                        ],
                        "text": "17, can be derived in the framework of regularization theory (Vapnik and Stefanyuk 1978; Aidu and Vapnik 1989; Vapnik 1982) under a smoothness assumption on the probability distribution that has to be estimated."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nonparametric methods for restoring probability densities"
            },
            "venue": {
                "fragments": [],
                "text": "Automat. Telemek"
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Methods for Solving lncorrectly Posed Problems"
            },
            "venue": {
                "fragments": [],
                "text": "SpringerVerlag, Berlin."
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalized Additive Models, Vol"
            },
            "venue": {
                "fragments": [],
                "text": "43 of Monographs"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nonlinear n-widths in Besov spaces"
            },
            "venue": {
                "fragments": [],
                "text": "Approximation Theory, VI , C. K. Chui, L. L. Schumaker, and D. J. Ward, eds., pp. 203-206. Academic Press, New York. DeVore,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nonlinear n-widths in Besov spaces"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimation of probability density on the basis of the method of stochastic regularization"
            },
            "venue": {
                "fragments": [],
                "text": "Avtom. Telemek"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Introduction to the TheoryofNeural Computation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A theory of networks for approximation and learning. A.I. Memo No. 1140"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 131
                            }
                        ],
                        "text": "1 can be derived not only in the context of functional analysis (Tikhonov and Arsenin 1977), but also in a probabilistic framework (Kimeldorf and Wahba 1971; Wahba 1980, 1990; Poggio et al. 1985; Marroquin et al. 1987; Bertero et al. 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A correspondence between Bayesian"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Connectionist nonparametric regression: Multilayer percep"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Interpolation and approximation by radial and related functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 450,
                                "start": 118
                            }
                        ],
                        "text": "The interpretation of an approximation scheme in terms of networks and vice versa has also been extensively discussed (Barron and Barron 1988; Poggio and Girosi 1989, 1990a,b; Girosi 1992; Broomhead and Lowe 1988; Moody and Darken 1988, 1989; White 1989, 1990; Ripley 1994; Omohundro 1987; Kohonen 1990; Lapedes and Farber 1988; Rumelhart et al. 1986; Hertz et al. 1991; Kung 1993; Sejnowski and Rosenberg 1987; Hurlbert and Poggio 1988; Poggio 1975)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistical learning networks: A unifying view"
            },
            "venue": {
                "fragments": [],
                "text": "Symposium on the Interface: Statistics and Computing Science"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The theory of radial basis functions approximation in 1990"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Numerical Analysis Volume 11: Wavelets, Subdivision Algorithms and Radial Basis Functions, W. A. Light, ed., pp. 105-210. Oxford University Press, Oxford."
            },
            "year": 1992
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 22,
            "methodology": 10
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 199,
        "totalPages": 20
    },
    "page_url": "https://www.semanticscholar.org/paper/Regularization-Theory-and-Neural-Networks-Girosi-Jones/eae2430d9a984120bf511655a03c15089b007499?sort=total-citations"
}