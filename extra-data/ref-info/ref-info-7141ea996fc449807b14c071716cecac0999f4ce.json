{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2939803"
                        ],
                        "name": "Ronan Collobert",
                        "slug": "Ronan-Collobert",
                        "structuredName": {
                            "firstName": "Ronan",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronan Collobert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751569"
                        ],
                        "name": "Samy Bengio",
                        "slug": "Samy-Bengio",
                        "structuredName": {
                            "firstName": "Samy",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samy Bengio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 30
                            }
                        ],
                        "text": "In a recent technical report (Collobert & Bengio, 2000), we have shown that our algorithm converges in the case where the working set size is equal to 2 and without shrinking, for any kernel that veri es Mercer's conditions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 168
                            }
                        ],
                        "text": "However, Nodelib gave slightly better results in terms of the objective function (probably due to the fact that their termination criterion is stronger than ours; see (Collobert & Bengio, 2000) for a comparison between termination criteria), but not in terms of test error."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 18542582,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "d76508262ef4e8fc1982919299fef9bf5ff7de43",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Keywords: learning Reference EPFL-REPORT-82616 URL: http://publications.idiap.ch/downloads/reports/2000/rr00-24.pdf Record created on 2006-03-10, modified on 2017-05-10"
            },
            "slug": "On-the-Convergence-of-SVMTorch,-an-Algorithm-for-Collobert-Bengio",
            "title": {
                "fragments": [],
                "text": "On the Convergence of SVMTorch, an Algorithm for Large-Scale Regression Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "The EPFL-REPORT-82616 serves as a reference for further studies on how language and language-based pedagogical techniques can be applied in the context of professional and post-graduate education."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050470845"
                        ],
                        "name": "H. Drucker",
                        "slug": "H.-Drucker",
                        "structuredName": {
                            "firstName": "Harris",
                            "lastName": "Drucker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Drucker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10706913"
                        ],
                        "name": "L. Kaufman",
                        "slug": "L.-Kaufman",
                        "structuredName": {
                            "firstName": "Linda",
                            "lastName": "Kaufman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Kaufman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 135
                            }
                        ],
                        "text": "It has yielded excellent performance on many regression and time series prediction problems (see for instance M uller et al., 1997, or Drucker et al., 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 743542,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e52fb14e4beccc5e88a33c1fe5c7d6e780831ae1",
            "isKey": false,
            "numCitedBy": 3694,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "A new regression technique based on Vapnik's concept of support vectors is introduced. We compare support vector regression (SVR) with a committee regression technique (bagging) based on regression trees and ridge regression done in feature space. On the basis of these experiments, it is expected that SVR will have advantages in high dimensionality space because SVR optimization does not depend on the dimensionality of the input space."
            },
            "slug": "Support-Vector-Regression-Machines-Drucker-Burges",
            "title": {
                "fragments": [],
                "text": "Support Vector Regression Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "This work compares support vector regression (SVR) with a committee regression technique (bagging) based on regression trees and ridge regression done in feature space and expects that SVR will have advantages in high dimensionality space because SVR optimization does not depend on the dimensionality of the input space."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1772326"
                        ],
                        "name": "S. Shevade",
                        "slug": "S.-Shevade",
                        "structuredName": {
                            "firstName": "Shirish",
                            "lastName": "Shevade",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Shevade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144106136"
                        ],
                        "name": "S. Keerthi",
                        "slug": "S.-Keerthi",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Keerthi",
                            "middleNames": [
                                "Sathiya"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Keerthi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145880755"
                        ],
                        "name": "C. Bhattacharyya",
                        "slug": "C.-Bhattacharyya",
                        "structuredName": {
                            "firstName": "Chiranjib",
                            "lastName": "Bhattacharyya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bhattacharyya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38445965"
                        ],
                        "name": "K. Murthy",
                        "slug": "K.-Murthy",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Murthy",
                            "middleNames": [
                                "R.",
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murthy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 108
                            }
                        ],
                        "text": "Note also that Nodelib includes some enhancements compared to SMO which are di erent from those proposed by Shevade et al. (2000). Both these algorithms use an internal cache in order to be able to solve large-scale problems."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 14
                            }
                        ],
                        "text": "For instance, Shevade et al. (2000) proposed two modi cations of the SMO algorithm from Platt (1999) for regression, based on a previous paper from the same team (Keerthi et al., 1999) for classi cation problems."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 108
                            }
                        ],
                        "text": "Note also that Nodelib includes some enhancements compared to SMO which are di erent from those proposed by Shevade et al. (2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12742298,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc340ca364cac1763c8831970aa89466c07873c1",
            "isKey": false,
            "numCitedBy": 793,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper points out an important source of inefficiency in Smola and Sch\u00f6lkopf's sequential minimal optimization (SMO) algorithm for support vector machine (SVM) regression that is caused by the use of a single threshold value. Using clues from the KKT conditions for the dual problem, two threshold parameters are employed to derive modifications of SMO for regression. These modified algorithms perform significantly faster than the original SMO on the datasets tried."
            },
            "slug": "Improvements-to-the-SMO-algorithm-for-SVM-Shevade-Keerthi",
            "title": {
                "fragments": [],
                "text": "Improvements to the SMO algorithm for SVM regression"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Using clues from the KKT conditions for the dual problem, two threshold parameters are employed to derive modifications of SMO for regression that perform significantly faster than the original SMO on the datasets tried."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks Learn. Syst."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 61
                            }
                        ],
                        "text": "This algorithm followed the same principles as those used by Joachims (1999) in his classi cation algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 516,
                                "start": 106
                            }
                        ],
                        "text": "In this paper, we propose a decomposition algorithm, SVMTorch1, which is similar to SVM-Light proposed by Joachims (1999) for classi cation problems, but adapted to regression problems. With this algorithm, one can now e ciently solve large-scale regression problems (more than 20000 examples). Comparisons with Nodelib, another publicly available SVM algorithm for large-scale regression problems from Flake and Lawrence (2000) yielded signi cant time improvements. Finally, based on a recent paper from Lin (2000), we show that a convergence proof exists for our algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 185
                            }
                        ],
                        "text": "The originality of our algorithm is thus to select independently the variables i and ?i , which has the side e ect of e ciently adapting the shrinking step proposed in classi cation by Joachims (1999) (it is indeed less easy to think of an e cient shrinking method in the context of pairs of variables)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 63
                            }
                        ],
                        "text": "In order to select such a working set, we use the same idea as Joachims (1999): simply search for the optimal gradient-descent direction p which is feasible and which has only q non-null components."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 964,
                                "start": 131
                            }
                        ],
                        "text": "In this paper, we propose a method to solve such problems e ciently using a decomposition algorithm similar to the one proposed by Joachims (1999) in the context of classi cation problems. In the next section, we give the general algorithm and explain in more detail each of its main steps and how they di er from other published algorithms, as well as a discussion on convergence and on some important implementation issues, such as a way to e ciently handle the kernel matrix computation. In the experiment section, we compare this new algorithm on small and large datasets to Nodelib, another SVM algorithm for large-scale regression problems proposed by Flake and Lawrence (2000), then show how the size of the internal memory allocated to the resolution of the problem is related to the time needed to solve it, and nally how our algorithm scales with respect to l. 2. The Decomposition Algorithm As in the classi cation algorithm proposed by Joachims (1999), which was based on a idea from Osuna et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 606,
                                "start": 106
                            }
                        ],
                        "text": "In this paper, we propose a decomposition algorithm, SVMTorch1, which is similar to SVM-Light proposed by Joachims (1999) for classi cation problems, but adapted to regression problems. With this algorithm, one can now e ciently solve large-scale regression problems (more than 20000 examples). Comparisons with Nodelib, another publicly available SVM algorithm for large-scale regression problems from Flake and Lawrence (2000) yielded signi cant time improvements. Finally, based on a recent paper from Lin (2000), we show that a convergence proof exists for our algorithm. 1. Introduction Vapnik (1995) has proposed a method to solve regression problems using support vector machines."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 131
                            }
                        ],
                        "text": "In this paper, we propose a method to solve such problems e ciently using a decomposition algorithm similar to the one proposed by Joachims (1999) in the context of classi cation problems."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1933,
                                "start": 131
                            }
                        ],
                        "text": "In this paper, we propose a method to solve such problems e ciently using a decomposition algorithm similar to the one proposed by Joachims (1999) in the context of classi cation problems. In the next section, we give the general algorithm and explain in more detail each of its main steps and how they di er from other published algorithms, as well as a discussion on convergence and on some important implementation issues, such as a way to e ciently handle the kernel matrix computation. In the experiment section, we compare this new algorithm on small and large datasets to Nodelib, another SVM algorithm for large-scale regression problems proposed by Flake and Lawrence (2000), then show how the size of the internal memory allocated to the resolution of the problem is related to the time needed to solve it, and nally how our algorithm scales with respect to l. 2. The Decomposition Algorithm As in the classi cation algorithm proposed by Joachims (1999), which was based on a idea from Osuna et al. (1997), our regression algorithm is subdivided into the following four steps, which are explained afterward in the following subsections: 1. Select q variables i or ?i as the new working set, called S. 2. Fix the other variables F to their current values and solve the problem (3) with respect to S. 3. Search for variables whose values have been at 0 or C for a long time and that will probably not change anymore. This optional step is the shrinking phase, as these variables are removed from the problem. 4. Test whether the optimization is nished; if not, return to the rst step. Many other decomposition algorithms for regression have been published recently, and a comparison is given later in section 2.6. 2.1 Selection of a New Working Set We propose to select a new set of q variables such that the overall criterion will be optimized. In order to select such a working set, we use the same idea as Joachims (1999): simply search for the optimal gradient-descent direction p which is feasible and which has only q non-null components."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 106
                            }
                        ],
                        "text": "In this paper, we propose a decomposition algorithm, SVMTorch1, which is similar to SVM-Light proposed by Joachims (1999) for classi cation problems, but adapted to regression problems."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1016,
                                "start": 131
                            }
                        ],
                        "text": "In this paper, we propose a method to solve such problems e ciently using a decomposition algorithm similar to the one proposed by Joachims (1999) in the context of classi cation problems. In the next section, we give the general algorithm and explain in more detail each of its main steps and how they di er from other published algorithms, as well as a discussion on convergence and on some important implementation issues, such as a way to e ciently handle the kernel matrix computation. In the experiment section, we compare this new algorithm on small and large datasets to Nodelib, another SVM algorithm for large-scale regression problems proposed by Flake and Lawrence (2000), then show how the size of the internal memory allocated to the resolution of the problem is related to the time needed to solve it, and nally how our algorithm scales with respect to l. 2. The Decomposition Algorithm As in the classi cation algorithm proposed by Joachims (1999), which was based on a idea from Osuna et al. (1997), our regression algorithm is subdivided into the following four steps, which are explained afterward in the following subsections: 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 46
                            }
                        ],
                        "text": "As in the classi cation algorithm proposed by Joachims (1999), which was based on a idea from Osuna et al. (1997), our regression algorithm is subdivided into the following four steps, which are explained afterward in the following subsections:\n1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 429,
                                "start": 106
                            }
                        ],
                        "text": "In this paper, we propose a decomposition algorithm, SVMTorch1, which is similar to SVM-Light proposed by Joachims (1999) for classi cation problems, but adapted to regression problems. With this algorithm, one can now e ciently solve large-scale regression problems (more than 20000 examples). Comparisons with Nodelib, another publicly available SVM algorithm for large-scale regression problems from Flake and Lawrence (2000) yielded signi cant time improvements."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 171
                            }
                        ],
                        "text": "\u2026of our algorithm is thus to select independently the variables i and ? i , which has the side e ect of e ciently adapting the shrinking step proposed in classi cation by Joachims (1999) (it is indeed less easy to think of an e cient shrinking method in the context of pairs of variables)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 684,
                                "start": 131
                            }
                        ],
                        "text": "In this paper, we propose a method to solve such problems e ciently using a decomposition algorithm similar to the one proposed by Joachims (1999) in the context of classi cation problems. In the next section, we give the general algorithm and explain in more detail each of its main steps and how they di er from other published algorithms, as well as a discussion on convergence and on some important implementation issues, such as a way to e ciently handle the kernel matrix computation. In the experiment section, we compare this new algorithm on small and large datasets to Nodelib, another SVM algorithm for large-scale regression problems proposed by Flake and Lawrence (2000), then show how the size of the internal memory allocated to the resolution of the problem is related to the time needed to solve it, and nally how our algorithm scales with respect to l."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60502770,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb99668d4df98a3f6ff0b9fa3402e09008f22e2c",
            "isKey": true,
            "numCitedBy": 1838,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Training a support vector machine (SVM) leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples, oo-the-shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. SV M light1 is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for SV M light V2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains."
            },
            "slug": "Making-large-scale-support-vector-machine-learning-Joachims",
            "title": {
                "fragments": [],
                "text": "Making large-scale support vector machine learning practical"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This chapter presents algorithmic and computational results developed for SV M light V2.0, which make large-scale SVM training more practical and give guidelines for the application of SVMs to large domains."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152597562"
                        ],
                        "name": "Gunnar R\u00e4tsch",
                        "slug": "Gunnar-R\u00e4tsch",
                        "structuredName": {
                            "firstName": "Gunnar",
                            "lastName": "R\u00e4tsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gunnar R\u00e4tsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2633352"
                        ],
                        "name": "J. Kohlmorgen",
                        "slug": "J.-Kohlmorgen",
                        "structuredName": {
                            "firstName": "Jens",
                            "lastName": "Kohlmorgen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kohlmorgen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5398743,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f43840dc1638a18eb6178f1060dc5f41af1c5ac7",
            "isKey": false,
            "numCitedBy": 975,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Support Vector Machines are used for time series prediction and compared to radial basis function networks. We make use of two different cost functions for Support Vectors: training with (i) an e insensitive loss and (ii) Huber's robust loss function and discuss how to choose the regularization parameters in these models. Two applications are considered: data from (a) a noisy (normal and uniform noise) Mackey Glass equation and (b) the Santa Fe competition (set D). In both cases Support Vector Machines show an excellent performance. In case (b) the Support Vector approach improves the best known result on the benchmark by a factor of 29%."
            },
            "slug": "Predicting-Time-Series-with-Support-Vector-Machines-M\u00fcller-Smola",
            "title": {
                "fragments": [],
                "text": "Predicting Time Series with Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Two different cost functions for Support Vectors are made use: training with an e insensitive loss and Huber's robust loss function and how to choose the regularization parameters in these models are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781874"
                        ],
                        "name": "E. Osuna",
                        "slug": "E.-Osuna",
                        "structuredName": {
                            "firstName": "Edgar",
                            "lastName": "Osuna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Osuna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052797964"
                        ],
                        "name": "R. Freund",
                        "slug": "R.-Freund",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Freund",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 94
                            }
                        ],
                        "text": "As in the classi cation algorithm proposed by Joachims (1999), which was based on a idea from Osuna et al. (1997), our regression algorithm is subdivided into the following four steps, which are explained afterward in the following subsections:\n1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5667586,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a61a3bf41fc770186a58fa34466af337e997ef6",
            "isKey": false,
            "numCitedBy": 1235,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the problem of training a support vector machine (SVM) on a very large database in the case in which the number of support vectors is also very large. Training a SVM is equivalent to solving a linearly constrained quadratic programming (QP) problem in a number of variables equal to the number of data points. This optimization problem is known to be challenging when the number of data points exceeds few thousands. In previous work done by us as well as by other researchers, the strategy used to solve the large scale QP problem takes advantage of the fact that the expected number of support vectors is small (<3,000). Therefore, the existing algorithms cannot deal with more than a few thousand support vectors. In this paper we present a decomposition algorithm that is guaranteed to solve the QP problem and that does not make assumptions on the expected number of support vectors. In order to present the feasibility of our approach we consider a foreign exchange rate time series database with 110,000 data points that generates 100,000 support vectors."
            },
            "slug": "An-improved-training-algorithm-for-support-vector-Osuna-Freund",
            "title": {
                "fragments": [],
                "text": "An improved training algorithm for support vector machines"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper presents a decomposition algorithm that is guaranteed to solve the QP problem and that does not make assumptions on the expected number of support vectors."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks for Signal Processing VII. Proceedings of the 1997 IEEE Signal Processing Society Workshop"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189092"
                        ],
                        "name": "John C. Platt",
                        "slug": "John-C.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Platt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 88
                            }
                        ],
                        "text": "For instance, Shevade et al. (2000) proposed two modi cations of the SMO algorithm from Platt (1999) for regression, based on a previous paper from the same team (Keerthi et al., 1999) for classi cation problems."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 117
                            }
                        ],
                        "text": "Finally, Flake and Lawrence (2000) proposed a modi cation of SMO for regression that uses the heuristics proposed by Platt (1999) and those from Smola and Sch olkopf (1998), but their modi cation uses a new variable i = i ? i ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 85
                            }
                        ],
                        "text": "We have shown how to solve analytically subproblems of size\n2, as it is done in SMO (Platt, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 41
                            }
                        ],
                        "text": "Moreover, following Platt's idea in SMO (Platt, 1999), if one xes the size of the working set S to 2, the problem can also be solved analytically."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1099857,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4de39c94e340a108fff01a90a67b0c17c86fb981",
            "isKey": true,
            "numCitedBy": 5910,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter describes a new algorithm for training Support Vector Machines: Sequential Minimal Optimization, or SMO. Training a Support Vector Machine (SVM) requires the solution of a very large quadratic programming (QP) optimization problem. SMO breaks this large QP problem into a series of smallest possible QP problems. These small QP problems are solved analytically, which avoids using a time-consuming numerical QP optimization as an inner loop. The amount of memory required for SMO is linear in the training set size, which allows SMO to handle very large training sets. Because large matrix computation is avoided, SMO scales somewhere between linear and quadratic in the training set size for various test problems, while a standard projected conjugate gradient (PCG) chunking algorithm scales somewhere between linear and cubic in the training set size. SMO's computation time is dominated by SVM evaluation, hence SMO is fastest for linear SVMs and sparse data sets. For the MNIST database, SMO is as fast as PCG chunking; while for the UCI Adult database and linear SVMs, SMO can be more than 1000 times faster than the PCG chunking algorithm."
            },
            "slug": "Fast-training-of-support-vector-machines-using-in-Platt",
            "title": {
                "fragments": [],
                "text": "Fast training of support vector machines using sequential minimal optimization, advances in kernel methods"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "SMO breaks this large quadratic programming problem into a series of smallest possible QP problems, which avoids using a time-consuming numerical QP optimization as an inner loop and hence SMO is fastest for linear SVMs and sparse data sets."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144106136"
                        ],
                        "name": "S. Keerthi",
                        "slug": "S.-Keerthi",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Keerthi",
                            "middleNames": [
                                "Sathiya"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Keerthi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1772326"
                        ],
                        "name": "S. Shevade",
                        "slug": "S.-Shevade",
                        "structuredName": {
                            "firstName": "Shirish",
                            "lastName": "Shevade",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Shevade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145880755"
                        ],
                        "name": "C. Bhattacharyya",
                        "slug": "C.-Bhattacharyya",
                        "structuredName": {
                            "firstName": "Chiranjib",
                            "lastName": "Bhattacharyya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bhattacharyya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38445965"
                        ],
                        "name": "K. Murthy",
                        "slug": "K.-Murthy",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Murthy",
                            "middleNames": [
                                "R.",
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murthy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 17
                            }
                        ],
                        "text": "To do so, we have used a theorem proved by Keerthi and Gilbert (2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 163
                            }
                        ],
                        "text": "For instance, Shevade et al. (2000) proposed two modi cations of the SMO algorithm from Platt (1999) for regression, based on a previous paper from the same team (Keerthi et al., 1999) for classi cation problems."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1536643,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d95b96d71669f3f4edfcc95cacd428b62b3fcde",
            "isKey": false,
            "numCitedBy": 1804,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This article points out an important source of inefficiency in Platt's sequential minimal optimization (SMO) algorithm that is caused by the use of a single threshold value. Using clues from the KKT conditions for the dual problem, two threshold parameters are employed to derive modifications of SMO. These modified algorithms perform significantly faster than the original SMO on all benchmark data sets tried."
            },
            "slug": "Improvements-to-Platt's-SMO-Algorithm-for-SVM-Keerthi-Shevade",
            "title": {
                "fragments": [],
                "text": "Improvements to Platt's SMO Algorithm for SVM Classifier Design"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Using clues from the KKT conditions for the dual problem, two threshold parameters are employed to derive modifications of SMO that perform significantly faster than the original SMO on all benchmark data sets tried."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754215"
                        ],
                        "name": "P. Laskov",
                        "slug": "P.-Laskov",
                        "structuredName": {
                            "firstName": "Pavel",
                            "lastName": "Laskov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Laskov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Laskov (2000) proposed also a decomposition method for regression problems which is very similar to the second modi cation proposed by Shevade et al. In fact, it is easy to see that Laskov's method with a subproblem of size 2 uses the same selection algorithm as well as the same termination\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2732134,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1c15e30eaa290f6fac5975242121f152ee9c624a",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "A new decomposition algorithm for training regression Support Vector Machines (SVM) is presented. The algorithm builds on the basic principles of decomposition proposed by Osuna et. al., and addresses the issue of optimal working set selection. The new criteria for testing optimality of a working set are derived. Based on these criteria, the principle of \"maximal inconsistency\" is proposed to form (approximately) optimal working sets. Experimental results show superior performance of the new algorithm in comparison with traditional training of regression SVM without decomposition. Similar results have been previously reported on decomposition algorithms for pattern recognition SVM. The new algorithm is also applicable to advanced SVM formulations based on regression, such as density estimation and integral equation SVM."
            },
            "slug": "An-Improved-Decomposition-Algorithm-for-Regression-Laskov",
            "title": {
                "fragments": [],
                "text": "An Improved Decomposition Algorithm for Regression Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A new decomposition algorithm for training regression Support Vector Machines (SVM) that builds on the basic principles of decomposition proposed by Osuna et."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711460"
                        ],
                        "name": "Chih-Jen Lin",
                        "slug": "Chih-Jen-Lin",
                        "structuredName": {
                            "firstName": "Chih-Jen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Jen Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 33
                            }
                        ],
                        "text": "Note however that more recently, Lin (2000) has shown the convergence of our algorithm for any value of the working set size (but again without shrinking), under the following hypothesis:\nAssumption 1 The matrix K satis es\nmin I (min(eig(KII ))) > 0\nwhere I is any subset of f1; : : : ; lg with jIj\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 148
                            }
                        ],
                        "text": "\u2026this seems to be a small di erence, let us note that i ? i = 0 8i at the optimal solution as well as during the optimization process, as proved by Lin (2000) in the case of algorithms such as the one we propose here.4 Thus, one of the two variables i or ? i is always equal to 0, and choosing the\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7157087,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "1226a673ce3f69738366be7b53eaf82dd33e290d",
            "isKey": false,
            "numCitedBy": 245,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "The decomposition method is currently one of the major methods for solving support vector machines (SVM). Its convergence properties have not been fully understood. The general asymptotic convergence was first proposed by Chang et al. However, their working set selection does not coincide with existing implementation. A later breakthrough by Keerthi and Gilbert (2000, 2002) proved the convergence finite termination for practical cases while the size of the working set is restricted to two. In this paper, we prove the asymptotic convergence of the algorithm used by the software SVM(light) and other later implementation. The size of the working set can be any even number. Extensions to other SVM formulations are also discussed."
            },
            "slug": "On-the-convergence-of-the-decomposition-method-for-Lin",
            "title": {
                "fragments": [],
                "text": "On the convergence of the decomposition method for support vector machines"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The asymptotic convergence of the algorithm used by the software SVM(light) and other later implementation is proved and the size of the working set can be any even number."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Vapnik (1995) has proposed a method to solve regression problems using support vector machines."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7138354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8213dbed4db44e113af3ed17d6dad57471a0c048",
            "isKey": false,
            "numCitedBy": 38756,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?."
            },
            "slug": "The-Nature-of-Statistical-Learning-Theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics for Engineering and Information Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144733293"
                        ],
                        "name": "R. Fletcher",
                        "slug": "R.-Fletcher",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Fletcher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fletcher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 190
                            }
                        ],
                        "text": "Minimizing (18) under the constraints (19) and (20) can be realized using a constrained quadratic optimizer, such as a conjugate gradient method with projection or an interior point method (Fletcher, 1987)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 617,
                                "start": 190
                            }
                        ],
                        "text": "Minimizing (18) under the constraints (19) and (20) can be realized using a constrained quadratic optimizer, such as a conjugate gradient method with projection or an interior point method (Fletcher, 1987). Moreover, following Platt's idea in SMO (Platt, 1999), if one xes the size of the working set S to 2, the problem can also be solved analytically. This particular case is important because experimental results show that it always gives the fastest convergence times. We explain it here because it is a di erent minimization problem from the one proposed by previous authors such as Smola and Sch\u007folkopf (1998); in fact, it is easier because it only has 2 variables and not 4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 123487779,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b84b383ad59f79e607ad0f08a8a10876631a0cd",
            "isKey": false,
            "numCitedBy": 9912,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface Table of Notation Part 1: Unconstrained Optimization Introduction Structure of Methods Newton-like Methods Conjugate Direction Methods Restricted Step Methods Sums of Squares and Nonlinear Equations Part 2: Constrained Optimization Introduction Linear Programming The Theory of Constrained Optimization Quadratic Programming General Linearly Constrained Optimization Nonlinear Programming Other Optimization Problems Non-Smooth Optimization References Subject Index."
            },
            "slug": "Practical-Methods-of-Optimization-Fletcher",
            "title": {
                "fragments": [],
                "text": "Practical Methods of Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The aim of this book is to provide a Discussion of Constrained Optimization and its Applications to Linear Programming and Other Optimization Problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Vapnik (1995) has proposed a method to solve regression problems using support vector machines."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 198120202,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "15cbb7b85622db0d17ade1d24fa7cfa321d16bd6",
            "isKey": false,
            "numCitedBy": 388,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Nature-of-Statistical-Learning-Theory,-Second-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning Theory, Second Edition"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics for Engineering and Information Science"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Available at http:::www.csie.ntu.edu.tww~cjlinpapersconv.ps Predicting time series with support vector machines"
            },
            "venue": {
                "fragments": [],
                "text": "Artiicial Neural Networks -ICANN'97 pp. 9999 1004"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Available on ftp://ftp.ics.uci.edu/pub/machine-learning-databases/covtype/covtype"
            },
            "venue": {
                "fragments": [],
                "text": "Available on ftp://ftp.ics.uci.edu/pub/machine-learning-databases/covtype/covtype"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 43
                            }
                        ],
                        "text": "To do so, we have used a theorem proved by Keerthi and Gilbert (2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Convergence of a Generalized SMO Algorithm for SVM Classiier Design Tech"
            },
            "venue": {
                "fragments": [],
                "text": "Convergence of a Generalized SMO Algorithm for SVM Classiier Design Tech"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 43
                            }
                        ],
                        "text": "To do so, we have used a theorem proved by Keerthi and Gilbert (2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Convergence of a Generalized SMO Algorithm for SVM Classiier Design (Tech. Rep. No. CD-00-01) Control Division"
            },
            "venue": {
                "fragments": [],
                "text": "Convergence of a Generalized SMO Algorithm for SVM Classiier Design (Tech. Rep. No. CD-00-01) Control Division"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "For 5000 examples, we used -clever -best -lazy, while for more than 5000 examples, we used -clever -best -lazy -ssz 200"
            },
            "venue": {
                "fragments": [],
                "text": "For 5000 examples, we used -clever -best -lazy, while for more than 5000 examples, we used -clever -best -lazy -ssz 200"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 150
                            }
                        ],
                        "text": "\u2026section, we compare this new algorithm on small and large datasets to Nodelib, another SVM algorithm for large-scale regression problems proposed by Flake and Lawrence (2000), then show how the size of the internal memory allocated to the resolution of the problem is related to the time needed to\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 86
                            }
                        ],
                        "text": "We compared our SVM implementation for regression problems (SVMTorch) to the one from Flake and Lawrence (2000) using their publicly available software Nodelib."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 9
                            }
                        ],
                        "text": "Finally, Flake and Lawrence (2000) proposed a modi cation of SMO for regression that uses the heuristics proposed by Platt (1999) and those from Smola and Sch olkopf (1998), but their modi cation uses a new variable i = i ? i ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "EEcient SVM regression training with SMO. Submitted to Machine Learning"
            },
            "venue": {
                "fragments": [],
                "text": "EEcient SVM regression training with SMO. Submitted to Machine Learning"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 12
                            }
                        ],
                        "text": "To do so, we have used a theorem proved by Keerthi and Gilbert (2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 163
                            }
                        ],
                        "text": "For instance, Shevade et al. (2000) proposed two modi cations of the SMO algorithm from Platt (1999) for regression, based on a previous paper from the same team (Keerthi et al., 1999) for classi cation problems."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improvements to Platt's SMO Algorithm for SVM Classiier Design Tech"
            },
            "venue": {
                "fragments": [],
                "text": "To appear in Neural Computation. Available at http"
            },
            "year": 1999
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 3,
            "methodology": 14,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 20,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/SVMTorch:-Support-Vector-Machines-for-Large-Scale-Collobert-Bengio/7141ea996fc449807b14c071716cecac0999f4ce?sort=total-citations"
}