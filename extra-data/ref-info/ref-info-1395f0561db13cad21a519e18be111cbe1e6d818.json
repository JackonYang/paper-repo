{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710872"
                        ],
                        "name": "T. Brox",
                        "slug": "T.-Brox",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Brox",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Brox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1769383"
                        ],
                        "name": "Lubomir D. Bourdev",
                        "slug": "Lubomir-D.-Bourdev",
                        "structuredName": {
                            "firstName": "Lubomir",
                            "lastName": "Bourdev",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lubomir D. Bourdev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35208858"
                        ],
                        "name": "Subhransu Maji",
                        "slug": "Subhransu-Maji",
                        "structuredName": {
                            "firstName": "Subhransu",
                            "lastName": "Maji",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Subhransu Maji"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[9] align a predicted object mask to the image contours, apply variational smoothing and assign figure/ground superpixels using self-similarity."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 64
                            }
                        ],
                        "text": "Top: Original image and activations using the part detectors of [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 61
                            }
                        ],
                        "text": "In the experiments, we use pre-trained models and masks from [9], in order to make a fair comparison with that work."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 135
                            }
                        ],
                        "text": "The left panel shows the most up-todate version of entries in comp5 (train on VOC data), and the right panel shows our results next to [9], the only enTable 3."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 44
                            }
                        ],
                        "text": "VOC(%) [18] [10] [21] [5] SRL UC3M TTI [23] [9] FULL FULL"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 77
                            }
                        ],
                        "text": "Some inspiring early works are [20, 6], which have recently been followed by [3, 37, 24, 18, 9, 29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 55
                            }
                        ],
                        "text": "Since we use as input pre-trained poselet detectors of [9], the two methods can be directly compared."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2634569,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d2efe575c931cf923e47ec5c7f444d53aae549cd",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose techniques to make use of two complementary bottom-up features, image edges and texture patches, to guide top-down object segmentation towards higher precision. We build upon the part-based pose-let detector, which can predict masks for numerous parts of an object. For this purpose we extend poselets to 19 other categories apart from person. We non-rigidly align these part detections to potential object contours in the image, both to increase the precision of the predicted object mask and to sort out false positives. We spatially aggregate object information via a variational smoothing technique while ensuring that object regions do not overlap. Finally, we propose to refine the segmentation based on self-similarity defined on small image patches. We obtain competitive results on the challenging Pascal VOC benchmark. On four classes we achieve the best numbers to-date."
            },
            "slug": "Object-segmentation-by-alignment-of-poselet-to-Brox-Bourdev",
            "title": {
                "fragments": [],
                "text": "Object segmentation by alignment of poselet activations to image contours"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper builds upon the part-based pose-let detector, which can predict masks for numerous parts of an object, and extends poselets to 19 other categories apart from person."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145854440"
                        ],
                        "name": "M. Maire",
                        "slug": "M.-Maire",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Maire",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Maire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107881808"
                        ],
                        "name": "Stella X. Yu",
                        "slug": "Stella-X.-Yu",
                        "structuredName": {
                            "firstName": "Stella",
                            "lastName": "Yu",
                            "middleNames": [
                                "X."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stella X. Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[24] solve the combined problem of detection and segmentation of people by incorporating outputs of top down part-detectors in a generalized eigen-problem and obtaining parts and pixel groupings."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 77
                            }
                        ],
                        "text": "Some inspiring early works are [20, 6], which have recently been followed by [3, 37, 24, 18, 9, 29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13031234,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "18ccd8bd64b50c1b6a83a71792fd808da7076bc9",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new framework in which image segmentation, figure/ground organization, and object detection all appear as the result of solving a single grouping problem. This framework serves as a perceptual organization stage that integrates information from low-level image cues with that of high-level part detectors. Pixels and parts each appear as nodes in a graph whose edges encode both affinity and ordering relationships. We derive a generalized eigen-problem from this graph and read off an interpretation of the image from the solution eigenvectors. Combining an off-the-shelf top-down part-based person detector with our low-level cues and grouping formulation, we demonstrate improvements to object detection and segmentation."
            },
            "slug": "Object-detection-and-segmentation-from-joint-of-and-Maire-Yu",
            "title": {
                "fragments": [],
                "text": "Object detection and segmentation from joint embedding of parts and pixels"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A new framework in which image segmentation, figure/ground organization, and object detection all appear as the result of solving a single grouping problem is presented, and improvements to object detection and segmentation are demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2143685864"
                        ],
                        "name": "Yi Yang",
                        "slug": "Yi-Yang",
                        "structuredName": {
                            "firstName": "Yi",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2555602"
                        ],
                        "name": "S. Hallman",
                        "slug": "S.-Hallman",
                        "structuredName": {
                            "firstName": "Sam",
                            "lastName": "Hallman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hallman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143800213"
                        ],
                        "name": "Charless C. Fowlkes",
                        "slug": "Charless-C.-Fowlkes",
                        "structuredName": {
                            "firstName": "Charless",
                            "lastName": "Fowlkes",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charless C. Fowlkes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 2
                            }
                        ],
                        "text": ", [31, 20, 1, 27, 15, 10, 37, 5, 12, 21]; the second one is addressed naturally in a multiscale scanning window detec-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[37], use object detector outputs to estimate object shape, appearance and depth ordering in order to facilitate segmentation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 77
                            }
                        ],
                        "text": "Some inspiring early works are [20, 6], which have recently been followed by [3, 37, 24, 18, 9, 29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3505099,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3044cb9dc9a364c6ec8d85fdc6dc4f9d14efcc20",
            "isKey": false,
            "numCitedBy": 113,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We formulate a layered model for object detection and multi-class segmentation. Our system uses the output of a bank of object detectors in order to define shape priors for support masks and then estimates appearance, depth ordering and labeling of pixels in the image. We train our system on the PASCAL segmentation challenge dataset and show good test results with state of the art performance in several categories including segmenting humans."
            },
            "slug": "Layered-object-detection-for-multi-class-Yang-Hallman",
            "title": {
                "fragments": [],
                "text": "Layered object detection for multi-class segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A layered model for object detection and multi-class segmentation that uses the output of a bank of object detectors in order to define shape priors for support masks and then estimates appearance, depth ordering and labeling of pixels in the image."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790580"
                        ],
                        "name": "Bharath Hariharan",
                        "slug": "Bharath-Hariharan",
                        "structuredName": {
                            "firstName": "Bharath",
                            "lastName": "Hariharan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bharath Hariharan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778133"
                        ],
                        "name": "Pablo Arbel\u00e1ez",
                        "slug": "Pablo-Arbel\u00e1ez",
                        "structuredName": {
                            "firstName": "Pablo",
                            "lastName": "Arbel\u00e1ez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pablo Arbel\u00e1ez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1769383"
                        ],
                        "name": "Lubomir D. Bourdev",
                        "slug": "Lubomir-D.-Bourdev",
                        "structuredName": {
                            "firstName": "Lubomir",
                            "lastName": "Bourdev",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lubomir D. Bourdev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35208858"
                        ],
                        "name": "Subhransu Maji",
                        "slug": "Subhransu-Maji",
                        "structuredName": {
                            "firstName": "Subhransu",
                            "lastName": "Maji",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Subhransu Maji"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "As an additional source of top-down information, we consider the method of [16], which produces a soft map of category-specific contours for each of the PASCAL categories."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6683607,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82fae97673a353271b1d4c001afda1af6ef6dc23",
            "isKey": false,
            "numCitedBy": 1058,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the challenging problem of localizing and classifying category-specific object contours in real world images. For this purpose, we present a simple yet effective method for combining generic object detectors with bottom-up contours to identify object contours. We also provide a principled way of combining information from different part detectors and across categories. In order to study the problem and evaluate quantitatively our approach, we present a dataset of semantic exterior boundaries on more than 20, 000 object instances belonging to 20 categories, using the images from the VOC2011 PASCAL challenge [7]."
            },
            "slug": "Semantic-contours-from-inverse-detectors-Hariharan-Arbel\u00e1ez",
            "title": {
                "fragments": [],
                "text": "Semantic contours from inverse detectors"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A simple yet effective method for combining generic object detectors with bottom-up contours to identify object contours is presented and a principled way of combining information from different part detectors and across categories is provided."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2831988"
                        ],
                        "name": "Ian Endres",
                        "slug": "Ian-Endres",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Endres",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian Endres"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2433269"
                        ],
                        "name": "Derek Hoiem",
                        "slug": "Derek-Hoiem",
                        "structuredName": {
                            "firstName": "Derek",
                            "lastName": "Hoiem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Derek Hoiem"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 0
                            }
                        ],
                        "text": "[10, 12, 2] design a ranking function for object segment proposals."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 2
                            }
                        ],
                        "text": ", [31, 20, 1, 27, 15, 10, 37, 5, 12, 21]; the second one is addressed naturally in a multiscale scanning window detec-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 22
                            }
                        ],
                        "text": "Note that compared to [10, 12] our region generation does not use any mid-level cues or additional training."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 697224,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f0640da2565bad7037d969e9f07276c10102083d",
            "isKey": false,
            "numCitedBy": 495,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a category-independent method to produce a bag of regions and rank them, such that top-ranked regions are likely to be good segmentations of different objects. Our key objectives are completeness and diversity: every object should have at least one good proposed region, and a diverse set should be top-ranked. Our approach is to generate a set of segmentations by performing graph cuts based on a seed region and a learned affinity function. Then, the regions are ranked using structured learning based on various cues. Our experiments on BSDS and PASCAL VOC 2008 demonstrate our ability to find most objects within a small bag of proposed regions."
            },
            "slug": "Category-Independent-Object-Proposals-Endres-Hoiem",
            "title": {
                "fragments": [],
                "text": "Category Independent Object Proposals"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A category-independent method to produce a bag of regions and rank them, such that top-ranked regions are likely to be good segmentations of different objects, and the ability to find most objects within a small bag of proposed regions is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35681810"
                        ],
                        "name": "Jo\u00e3o Carreira",
                        "slug": "Jo\u00e3o-Carreira",
                        "structuredName": {
                            "firstName": "Jo\u00e3o",
                            "lastName": "Carreira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jo\u00e3o Carreira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3141988"
                        ],
                        "name": "Fuxin Li",
                        "slug": "Fuxin-Li",
                        "structuredName": {
                            "firstName": "Fuxin",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fuxin Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781120"
                        ],
                        "name": "C. Sminchisescu",
                        "slug": "C.-Sminchisescu",
                        "structuredName": {
                            "firstName": "Cristian",
                            "lastName": "Sminchisescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Sminchisescu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 22
                            }
                        ],
                        "text": "Note that compared to [10, 12] our region generation does not use any mid-level cues or additional training."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[10] use regression on the responses of multiple figure-ground (FG) hypotheses generated from"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 0
                            }
                        ],
                        "text": "[10, 12, 2] design a ranking function for object segment proposals."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 12
                            }
                        ],
                        "text": "VOC(%) [18] [10] [21] [5] SRL UC3M TTI [23] [9] FULL FULL"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 87
                            }
                        ],
                        "text": "We extract the following 16 generic geometric properties for each region, similarly to [10]: normalized area, normalized centroid, eccentricity, normalized length of equivalent diameter, extent, solidity, normalized length of axis, principal orientation, ratio of lengths of minor/major axes, aspect ratio, average strength of UCM inside, outside and on the boundary, and normalized length of perimeter."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 2
                            }
                        ],
                        "text": ", [31, 20, 1, 27, 15, 10, 37, 5, 12, 21]; the second one is addressed naturally in a multiscale scanning window detec-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 30
                            }
                        ],
                        "text": "[17] use the FG hypotheses of [10] to come up with image tilings and produce semantic segmentations by constructing a joint distribution over image tilings and tile labelings in [18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5875591,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "140d2acd4cdbc30b102dac34f4c68f279ace6a26",
            "isKey": true,
            "numCitedBy": 124,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an approach to visual object-class segmentation and recognition based on a pipeline that combines multiple figure-ground hypotheses with large object spatial support, generated by bottom-up computational processes that do not exploit knowledge of specific categories, and sequential categorization based on continuous estimates of the spatial overlap between the image segment hypotheses and each putative class. We differ from existing approaches not only in our seemingly unreasonable assumption that good object-level segments can be obtained in a feed-forward fashion, but also in formulating recognition as a regression problem. Instead of focusing on a one-vs.-all winning margin that may not preserve the ordering of segment qualities inside the non-maximum (non-winning) set, our learning method produces a globally consistent ranking with close ties to segment quality, hence to the extent entire object or part hypotheses are likely to spatially overlap the ground truth. We demonstrate results beyond the current state of the art for image classification, object detection and semantic segmentation, in a number of challenging datasets including Caltech-101, ETHZ-Shape as well as PASCAL VOC 2009 and 2010."
            },
            "slug": "Object-Recognition-by-Sequential-Figure-Ground-Carreira-Li",
            "title": {
                "fragments": [],
                "text": "Object Recognition by Sequential Figure-Ground Ranking"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "This work presents an approach to visual object-class segmentation and recognition based on a pipeline that combines multiple figure-ground hypotheses with large object spatial support, generated by bottom-up computational processes that do not exploit knowledge of specific categories, and sequential categorization based on continuous estimates of the spatial overlap between the image segment hypotheses and each putative class."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39599498"
                        ],
                        "name": "Chunhui Gu",
                        "slug": "Chunhui-Gu",
                        "structuredName": {
                            "firstName": "Chunhui",
                            "lastName": "Gu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunhui Gu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109780936"
                        ],
                        "name": "Joseph J. Lim",
                        "slug": "Joseph-J.-Lim",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Lim",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph J. Lim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778133"
                        ],
                        "name": "Pablo Arbel\u00e1ez",
                        "slug": "Pablo-Arbel\u00e1ez",
                        "structuredName": {
                            "firstName": "Pablo",
                            "lastName": "Arbel\u00e1ez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pablo Arbel\u00e1ez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652147"
                        ],
                        "name": "J. Malik",
                        "slug": "J.-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 2
                            }
                        ],
                        "text": ", [31, 20, 1, 27, 15, 10, 37, 5, 12, 21]; the second one is addressed naturally in a multiscale scanning window detec-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[15] start with regions and vote for object detections using a generalized hough transform."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2100273,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "42a5da01c672b2d37e76c62f851c1f88e6b988c0",
            "isKey": false,
            "numCitedBy": 433,
            "numCiting": 112,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a unified framework for object detection, segmentation, and classification using regions. Region features are appealing in this context because: (1) they encode shape and scale information of objects naturally; (2) they are only mildly affected by background clutter. Regions have not been popular as features due to their sensitivity to segmentation errors. In this paper, we start by producing a robust bag of overlaid regions for each image using Arbeldez et al., CVPR 2009. Each region is represented by a rich set of image cues (shape, color and texture). We then learn region weights using a max-margin framework. In detection and segmentation, we apply a generalized Hough voting scheme to generate hypotheses of object locations, scales and support, followed by a verification classifier and a constrained segmenter on each hypothesis. The proposed approach significantly outperforms the state of the art on the ETHZ shape database(87.1% average detection rate compared to Ferrari et al. 's 67.2%), and achieves competitive performance on the Caltech 101 database."
            },
            "slug": "Recognition-using-regions-Gu-Lim",
            "title": {
                "fragments": [],
                "text": "Recognition using regions"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "This paper presents a unified framework for object detection, segmentation, and classification using regions using a generalized Hough voting scheme to generate hypotheses of object locations, scales and support, followed by a verification classifier and a constrained segmenter on each hypothesis."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3045340"
                        ],
                        "name": "Tomasz Malisiewicz",
                        "slug": "Tomasz-Malisiewicz",
                        "structuredName": {
                            "firstName": "Tomasz",
                            "lastName": "Malisiewicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomasz Malisiewicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 2
                            }
                        ],
                        "text": ", [31, 20, 1, 27, 15, 10, 37, 5, 12, 21]; the second one is addressed naturally in a multiscale scanning window detec-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 580381,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "88482475e5dffab106149c7b358732e6c973e611",
            "isKey": false,
            "numCitedBy": 163,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We pose the recognition problem as data association. In this setting, a novel object is explained solely in terms of a small set of exemplar objects to which it is visually similar. Inspired by the work of Frome et al., we learn separate distance functions for each exemplar; however, our distances are interpretable on an absolute scale and can be thresholded to detect the presence of an object. Our exemplars are represented as image regions and the learned distances capture the relative importance of shape, color, texture, and position features for that region. We use the distance functions to detect and segment objects in novel images by associating the bottom-up segments obtained from multiple image segmentations with the exemplar regions. We evaluate the detection and segmentation performance of our algorithm on real-world outdoor scenes from the LabelMe (B. Russel, et al., 2007) dataset and also show some promising qualitative image parsing results."
            },
            "slug": "Recognition-by-association-via-learning-distances-Malisiewicz-Efros",
            "title": {
                "fragments": [],
                "text": "Recognition by association via learning per-exemplar distances"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This work uses the distance functions to detect and segment objects in novel images by associating the bottom-up segments obtained from multiple image segmentations with the exemplar regions and learns separate distance functions for each exemplar."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756979"
                        ],
                        "name": "K. V. D. Sande",
                        "slug": "K.-V.-D.-Sande",
                        "structuredName": {
                            "firstName": "Koen",
                            "lastName": "Sande",
                            "middleNames": [
                                "E.",
                                "A.",
                                "van",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. V. D. Sande"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1823362"
                        ],
                        "name": "J. Uijlings",
                        "slug": "J.-Uijlings",
                        "structuredName": {
                            "firstName": "Jasper",
                            "lastName": "Uijlings",
                            "middleNames": [
                                "R.",
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Uijlings"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695527"
                        ],
                        "name": "T. Gevers",
                        "slug": "T.-Gevers",
                        "structuredName": {
                            "firstName": "Theo",
                            "lastName": "Gevers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Gevers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144638781"
                        ],
                        "name": "A. Smeulders",
                        "slug": "A.-Smeulders",
                        "structuredName": {
                            "firstName": "Arnold",
                            "lastName": "Smeulders",
                            "middleNames": [
                                "W.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Smeulders"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11442196,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "37e41557932cc0035eab23fd767bde68f6475c3a",
            "isKey": false,
            "numCitedBy": 711,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "For object recognition, the current state-of-the-art is based on exhaustive search. However, to enable the use of more expensive features and classifiers and thereby progress beyond the state-of-the-art, a selective search strategy is needed. Therefore, we adapt segmentation as a selective search by reconsidering segmentation: We propose to generate many approximate locations over few and precise object delineations because (1) an object whose location is never generated can not be recognised and (2) appearance and immediate nearby context are most effective for object recognition. Our method is class-independent and is shown to cover 96.7% of all objects in the Pascal VOC 2007 test set using only 1,536 locations per image. Our selective search enables the use of the more expensive bag-of-words method which we use to substantially improve the state-of-the-art by up to 8.5% for 8 out of 20 classes on the Pascal VOC 2010 detection challenge."
            },
            "slug": "Segmentation-as-selective-search-for-object-Sande-Uijlings",
            "title": {
                "fragments": [],
                "text": "Segmentation as selective search for object recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work adapt segmentation as a selective search by reconsidering segmentation to generate many approximate locations over few and precise object delineations because an object whose location is never generated can not be recognised and appearance and immediate nearby context are most effective for object recognition."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2259154"
                        ],
                        "name": "Sudheendra Vijayanarasimhan",
                        "slug": "Sudheendra-Vijayanarasimhan",
                        "structuredName": {
                            "firstName": "Sudheendra",
                            "lastName": "Vijayanarasimhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sudheendra Vijayanarasimhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794409"
                        ],
                        "name": "K. Grauman",
                        "slug": "K.-Grauman",
                        "structuredName": {
                            "firstName": "Kristen",
                            "lastName": "Grauman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Grauman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[35] group over-segmented regions to maximize classifier scores."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6417661,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d8e2a74b265788a9f029a97ce0d1aa4a11ca1618",
            "isKey": false,
            "numCitedBy": 90,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a branch-and-cut strategy for efficient region-based object detection. Given an oversegmented image, our method determines the subset of spatially contiguous regions whose collective features will maximize a classifier's score. We formulate the objective as an instance of the prize-collecting Steiner tree problem, and show that for a family of additive classifiers this enables fast search for the optimal object region via a branch-and-cut algorithm. Unlike existing branch-and-bounddetection methods designed for bounding boxes, our approach allows scoring of irregular shapes \u2014 which is especially critical for objects that do not conform to a rectangular window. We provide results on three challenging object detection datasets, and demonstrate the advantage of rapidly seeking best-scoring regions rather than subwindow rectangles."
            },
            "slug": "Efficient-region-search-for-object-detection-Vijayanarasimhan-Grauman",
            "title": {
                "fragments": [],
                "text": "Efficient region search for object detection"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work formulate the objective as an instance of the prize-collecting Steiner tree problem, and shows that for a family of additive classifiers this enables fast search for the optimal object region via a branch-and-cut algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "25633106"
                        ],
                        "name": "Eran Borenstein",
                        "slug": "Eran-Borenstein",
                        "structuredName": {
                            "firstName": "Eran",
                            "lastName": "Borenstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eran Borenstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743045"
                        ],
                        "name": "S. Ullman",
                        "slug": "S.-Ullman",
                        "structuredName": {
                            "firstName": "Shimon",
                            "lastName": "Ullman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ullman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 31
                            }
                        ],
                        "text": "Some inspiring early works are [20, 6], which have recently been followed by [3, 37, 24, 18, 9, 29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2279060,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "35da186e1d3e6a95cc605331a1fc1c3859685cbc",
            "isKey": false,
            "numCitedBy": 179,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "We construct a segmentation scheme that combines top-down with bottom-up processing. In the proposed scheme, segmentation and recognition are intertwined rather than proceeding in a serial manner. The top-down part applies stored knowledge about object shapes acquired through learning, whereas the bottom-up part creates a hierarchy of segmented regions based on uniformity criteria. Beginning with unsegmented training examples of class and non-class images, the algorithm constructs a bank of class-specific fragments and determines their figure-ground segmentation. This bank is then used to segment novel images in a top-down manner: the fragments are first used to recognize images containing class objects, and then to create a complete cover that best approximates these objects. The resulting segmentation is then integrated with bottom-up multi-scale grouping to better delineate the object boundaries. Our experiments, applied to a large set of four classes (horses, pedestrians, cars, faces), demonstrate segmentation results that surpass those achieved by previous top-down or bottom-up schemes. The main novel aspects of this work are the fragment learning phase, which efficiently learns the figure-ground labeling of segmentation fragments, even in training sets with high object and background variability; combining the top-down segmentation with bottom-up criteria to draw on their relative merits; and the use of segmentation to improve recognition."
            },
            "slug": "Combined-Top-Down/Bottom-Up-Segmentation-Borenstein-Ullman",
            "title": {
                "fragments": [],
                "text": "Combined Top-Down/Bottom-Up Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The main novel aspects of this work are the fragment learning phase, which efficiently learns the figure-ground labeling of segmentation fragments, even in training sets with high object and background variability; combining the top-down segmentation with bottom-up criteria to draw on their relative merits; and the use of segmentsation to improve recognition."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3717791"
                        ],
                        "name": "M. P. Kumar",
                        "slug": "M.-P.-Kumar",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Kumar",
                            "middleNames": [
                                "Pawan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. P. Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143635539"
                        ],
                        "name": "P. Torr",
                        "slug": "P.-Torr",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Torr",
                            "middleNames": [
                                "H.",
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Torr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 2
                            }
                        ],
                        "text": ", [31, 20, 1, 27, 15, 10, 37, 5, 12, 21]; the second one is addressed naturally in a multiscale scanning window detec-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 31
                            }
                        ],
                        "text": "Some inspiring early works are [20, 6], which have recently been followed by [3, 37, 24, 18, 9, 29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7854172,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "72a2c172cf49edb4a33708e05f53938f4d475432",
            "isKey": false,
            "numCitedBy": 403,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a principled Bayesian method for detecting and segmenting instances of a particular object category within an image, providing a coherent methodology for combining top down and bottom up cues. The work draws together two powerful formulations: pictorial structures (PS) and Markov random fields (MRFs) both of which have efficient algorithms for their solution. The resulting combination, which we call the object category specific MRF, suggests a solution to the problem that has long dogged MRFs namely that they provide a poor prior for specific shapes. In contrast, our model provides a prior that is global across the image plane using the PS. We develop an efficient method, OBJ CUT, to obtain segmentations using this model. Novel aspects of this method include an efficient algorithm for sampling the PS model, and the observation that the expected log likelihood of the model can be increased by a single graph cut. Results are presented on two object categories, cows and horses. We compare our methods to the state of the art in object category specific image segmentation and demonstrate significant improvements."
            },
            "slug": "OBJ-CUT-Kumar-Torr",
            "title": {
                "fragments": [],
                "text": "OBJ CUT"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A principled Bayesian method for detecting and segmenting instances of a particular object category within an image, providing a coherent methodology for combining top down and bottom up cues and developing an efficient method, OBJ CUT, to obtain segmentations using this model."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144135837"
                        ],
                        "name": "Adrian Ion",
                        "slug": "Adrian-Ion",
                        "structuredName": {
                            "firstName": "Adrian",
                            "lastName": "Ion",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adrian Ion"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35681810"
                        ],
                        "name": "Jo\u00e3o Carreira",
                        "slug": "Jo\u00e3o-Carreira",
                        "structuredName": {
                            "firstName": "Jo\u00e3o",
                            "lastName": "Carreira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jo\u00e3o Carreira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781120"
                        ],
                        "name": "C. Sminchisescu",
                        "slug": "C.-Sminchisescu",
                        "structuredName": {
                            "firstName": "Cristian",
                            "lastName": "Sminchisescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Sminchisescu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 7
                            }
                        ],
                        "text": "VOC(%) [18] [10] [21] [5] SRL UC3M TTI [23] [9] FULL FULL"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 77
                            }
                        ],
                        "text": "Some inspiring early works are [20, 6], which have recently been followed by [3, 37, 24, 18, 9, 29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 178
                            }
                        ],
                        "text": "[17] use the FG hypotheses of [10] to come up with image tilings and produce semantic segmentations by constructing a joint distribution over image tilings and tile labelings in [18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11335146,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f05c39403e9a00b07dbf8479cae4660d7a20fbd",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a joint image segmentation and labeling model (JSL) which, given a bag of figure-ground segment hypotheses extracted at multiple image locations and scales, constructs a joint probability distribution over both the compatible image interpretations (tilings or image segmentations) composed from those segments, and over their labeling into categories. The process of drawing samples from the joint distribution can be interpreted as first sampling tilings, modeled as maximal cliques, from a graph connecting spatially non-overlapping segments in the bag [1], followed by sampling labels for those segments, conditioned on the choice of a particular tiling. We learn the segmentation and labeling parameters jointly, based on Maximum Likelihood with a novel Incremental Saddle Point estimation procedure. The partition function over tilings and labelings is increasingly more accurately approximated by including incorrect configurations that a not-yet-competent model rates probable during learning. We show that the proposed methodology matches the current state of the art in the Stanford dataset [2], as well as in VOC2010, where 41.7% accuracy on the test set is achieved."
            },
            "slug": "Probabilistic-Joint-Image-Segmentation-and-Labeling-Ion-Carreira",
            "title": {
                "fragments": [],
                "text": "Probabilistic Joint Image Segmentation and Labeling"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "A joint image segmentation and labeling model which, given a bag of figure-ground segment hypotheses extracted at multiple image locations and scales, constructs a joint probability distribution over both the compatible image interpretations (tilings or image segmentations) composed from those segments, and over their labeling into categories."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3188342"
                        ],
                        "name": "O. Parkhi",
                        "slug": "O.-Parkhi",
                        "structuredName": {
                            "firstName": "Omkar",
                            "lastName": "Parkhi",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Parkhi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687524"
                        ],
                        "name": "A. Vedaldi",
                        "slug": "A.-Vedaldi",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Vedaldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vedaldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694502"
                        ],
                        "name": "C. Jawahar",
                        "slug": "C.-Jawahar",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Jawahar",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jawahar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[29] use appearance similarity to expand top-down detections of heads to detect cats and dogs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 77
                            }
                        ],
                        "text": "Some inspiring early works are [20, 6], which have recently been followed by [3, 37, 24, 18, 9, 29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14022400,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb9c06521b5a3e07c8106873a46bf2d5fb8ce3f4",
            "isKey": false,
            "numCitedBy": 134,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Template-based object detectors such as the deformable parts model of Felzenszwalb et al. [11] achieve state-of-the-art performance for a variety of object categories, but are still outperformed by simpler bag-of-words models for highly flexible objects such as cats and dogs. In these cases we propose to use the template-based model to detect a distinctive part for the class, followed by detecting the rest of the object via segmentation on image specific information learnt from that part. This approach is motivated by two ob- servations: (i) many object classes contain distinctive parts that can be detected very reliably by template-based detec- tors, whilst the entire object cannot; (ii) many classes (e.g. animals) have fairly homogeneous coloring and texture that can be used to segment the object once a sample is provided in an image. We show quantitatively that our method substantially outperforms whole-body template-based detectors for these highly deformable object categories, and indeed achieves accuracy comparable to the state-of-the-art on the PASCAL VOC competition, which includes other models such as bag-of-words."
            },
            "slug": "The-truth-about-cats-and-dogs-Parkhi-Vedaldi",
            "title": {
                "fragments": [],
                "text": "The truth about cats and dogs"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This approach proposes to use the template-based model to detect a distinctive part for the class, followed by detecting the rest of the object via segmentation on image specific information learnt from that part, and achieves accuracy comparable to the state-of-the-art on the PASCAL VOC competition, which includes other models such as bag- of-words."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2365442"
                        ],
                        "name": "B. Alexe",
                        "slug": "B.-Alexe",
                        "structuredName": {
                            "firstName": "Bogdan",
                            "lastName": "Alexe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Alexe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1879646"
                        ],
                        "name": "Thomas Deselaers",
                        "slug": "Thomas-Deselaers",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Deselaers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Deselaers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143865718"
                        ],
                        "name": "V. Ferrari",
                        "slug": "V.-Ferrari",
                        "structuredName": {
                            "firstName": "Vittorio",
                            "lastName": "Ferrari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Ferrari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11515509,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2dd55b3bcaf50c1228569d0efe5620a910c1cd07",
            "isKey": false,
            "numCitedBy": 933,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a generic objectness measure, quantifying how likely it is for an image window to contain an object of any class. We explicitly train it to distinguish objects with a well-defined boundary in space, such as cows and telephones, from amorphous background elements, such as grass and road. The measure combines in a Bayesian framework several image cues measuring characteristics of objects, such as appearing different from their surroundings and having a closed boundary. This includes an innovative cue measuring the closed boundary characteristic. In experiments on the challenging PASCAL VOC 07 dataset, we show this new cue to outperform a state-of-the-art saliency measure [17], and the combined measure to perform better than any cue alone. Finally, we show how to sample windows from an image according to their objectness distribution and give an algorithm to employ them as location priors for modern class-specific object detectors. In experiments on PASCAL VOC 07 we show this greatly reduces the number of windows evaluated by class-specific object detectors."
            },
            "slug": "What-is-an-object-Alexe-Deselaers",
            "title": {
                "fragments": [],
                "text": "What is an object?"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A generic objectness measure, quantifying how likely it is for an image window to contain an object of any class, is presented, combining in a Bayesian framework several image cues measuring characteristics of objects, such as appearing different from their surroundings and having a closed boundary."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144135837"
                        ],
                        "name": "Adrian Ion",
                        "slug": "Adrian-Ion",
                        "structuredName": {
                            "firstName": "Adrian",
                            "lastName": "Ion",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adrian Ion"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35681810"
                        ],
                        "name": "Jo\u00e3o Carreira",
                        "slug": "Jo\u00e3o-Carreira",
                        "structuredName": {
                            "firstName": "Jo\u00e3o",
                            "lastName": "Carreira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jo\u00e3o Carreira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781120"
                        ],
                        "name": "C. Sminchisescu",
                        "slug": "C.-Sminchisescu",
                        "structuredName": {
                            "firstName": "Cristian",
                            "lastName": "Sminchisescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Sminchisescu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 104
                            }
                        ],
                        "text": "Note we don\u2019t impose any particular structure on our object candidates, such as parsing the image as in [17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[17] use the FG hypotheses of [10] to come up with image tilings and produce semantic segmentations by constructing a joint distribution over image tilings and tile labelings in [18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12481211,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7437955c2284f93e9919839a1e0f1967c1591843",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a mid-level statistical model for image segmentation that composes multiple figure-ground hypotheses (FG) obtained by applying constraints at different locations and scales, into larger interpretations (tilings) of the entire image. Inference is cast as optimization over sets of maximal cliques sampled from a graph connecting all non-overlapping figure-ground segment hypotheses. Potential functions over cliques combine unary, Gestalt-based figure qualities, and pairwise compatibilities among spatially neighboring segments, constrained by T-junctions and the boundary interface statistics of real scenes. Learning the model parameters is based on maximum likelihood, alternating between sampling image tilings and optimizing their potential function parameters. State of the art results are reported on the Berkeley and Stanford segmentation datasets, as well as VOC2009, where a 28% improvement was achieved."
            },
            "slug": "Image-segmentation-by-figure-ground-composition-Ion-Carreira",
            "title": {
                "fragments": [],
                "text": "Image segmentation by figure-ground composition into maximal cliques"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A mid-level statistical model for image segmentation that composes multiple figure-ground hypotheses obtained by applying constraints at different locations and scales, into larger interpretations of the entire image."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114833718"
                        ],
                        "name": "Xiaofeng Ren",
                        "slug": "Xiaofeng-Ren",
                        "structuredName": {
                            "firstName": "Xiaofeng",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaofeng Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 2
                            }
                        ],
                        "text": ", [31, 20, 1, 27, 15, 10, 37, 5, 12, 21]; the second one is addressed naturally in a multiscale scanning window detec-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13571735,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9a9049a50dfe94fa4473880a9b60c99333ade685",
            "isKey": false,
            "numCitedBy": 1644,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a two-class classification model for grouping. Human segmented natural images are used as positive examples. Negative examples of grouping are constructed by randomly matching human segmentations and images. In a preprocessing stage an image is over-segmented into super-pixels. We define a variety of features derived from the classical Gestalt cues, including contour, texture, brightness and good continuation. Information-theoretic analysis is applied to evaluate the power of these grouping cues. We train a linear classifier to combine these features. To demonstrate the power of the classification model, a simple algorithm is used to randomly search for good segmentations. Results are shown on a wide range of images."
            },
            "slug": "Learning-a-classification-model-for-segmentation-Ren-Malik",
            "title": {
                "fragments": [],
                "text": "Learning a classification model for segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "A two-class classification model for grouping is proposed that defines a variety of features derived from the classical Gestalt cues, including contour, texture, brightness and good continuation, and trains a linear classifier to combine these features."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685089"
                        ],
                        "name": "Pedro F. Felzenszwalb",
                        "slug": "Pedro-F.-Felzenszwalb",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Felzenszwalb",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro F. Felzenszwalb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145689002"
                        ],
                        "name": "David A. McAllester",
                        "slug": "David-A.-McAllester",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "McAllester",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. McAllester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3198903,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e79272fe3d65197100eae8be9fec6469107969ae",
            "isKey": false,
            "numCitedBy": 9374,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL data sets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin-sensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI--SVM in terms of latent variables. A latent SVM is semiconvex, and the training problem becomes convex once latent information is specified for the positive examples. This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and optimizing the latent SVM objective function."
            },
            "slug": "Object-Detection-with-Discriminatively-Trained-Part-Felzenszwalb-Girshick",
            "title": {
                "fragments": [],
                "text": "Object Detection with Discriminatively Trained Part Based Models"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "An object detection system based on mixtures of multiscale deformable part models that is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges is described."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749609"
                        ],
                        "name": "S. Lazebnik",
                        "slug": "S.-Lazebnik",
                        "structuredName": {
                            "firstName": "Svetlana",
                            "lastName": "Lazebnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lazebnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189388"
                        ],
                        "name": "J. Ponce",
                        "slug": "J.-Ponce",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Ponce",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ponce"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 98
                            }
                        ],
                        "text": "More specifically, we consider the following first-order appearance cues: Shape: a spatial pyramid[22] representation three levels on the bounding box of the region."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2421251,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6dbaff29d3898cf60f63f5a34cb9610ebb75220c",
            "isKey": false,
            "numCitedBy": 8328,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a method for recognizing scene categories based on approximate global geometric correspondence. This technique works by partitioning the image into increasingly fine sub-regions and computing histograms of local features found inside each sub-region. The resulting \"spatial pyramid\" is a simple and computationally efficient extension of an orderless bag-of-features image representation, and it shows significantly improved performance on challenging scene categorization tasks. Specifically, our proposed method exceeds the state of the art on the Caltech-101 database and achieves high accuracy on a large database of fifteen natural scene categories. The spatial pyramid framework also offers insights into the success of several recently proposed image descriptions, including Torralba\u2019s \"gist\" and Lowe\u2019s SIFT descriptors."
            },
            "slug": "Beyond-Bags-of-Features:-Spatial-Pyramid-Matching-Lazebnik-Schmid",
            "title": {
                "fragments": [],
                "text": "Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This paper presents a method for recognizing scene categories based on approximate global geometric correspondence that exceeds the state of the art on the Caltech-101 database and achieves high accuracy on a large database of fifteen natural scene categories."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1769383"
                        ],
                        "name": "Lubomir D. Bourdev",
                        "slug": "Lubomir-D.-Bourdev",
                        "structuredName": {
                            "firstName": "Lubomir",
                            "lastName": "Bourdev",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lubomir D. Bourdev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35208858"
                        ],
                        "name": "Subhransu Maji",
                        "slug": "Subhransu-Maji",
                        "structuredName": {
                            "firstName": "Subhransu",
                            "lastName": "Maji",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Subhransu Maji"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710872"
                        ],
                        "name": "T. Brox",
                        "slug": "T.-Brox",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Brox",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Brox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 61
                            }
                        ],
                        "text": "For this purpose, we use the poselet framework introduced in [8, 7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 918485,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c8d905b121e3e1f27d1f72195e27b7c8ac1a4386",
            "isKey": false,
            "numCitedBy": 403,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Bourdev and Malik (ICCV 09) introduced a new notion of parts, poselets, constructed to be tightly clustered both in the configuration space of keypoints, as well as in the appearance space of image patches. In this paper we develop a new algorithm for detecting people using poselets. Unlike that work which used 3D annotations of keypoints, we use only 2D annotations which are much easier for naive human annotators. The main algorithmic contribution is in how we use the pattern of poselet activations. Individual poselet activations are noisy, but considering the spatial context of each can provide vital disambiguating information, just as object detection can be improved by considering the detection scores of nearby objects in the scene. This can be done by training a two-layer feed-forward network with weights set using a max margin technique. The refined poselet activations are then clustered into mutually consistent hypotheses where consistency is based on empirically determined spatial keypoint distributions. Finally, bounding boxes are predicted for each person hypothesis and shape masks are aligned to edges in the image to provide a segmentation. To the best of our knowledge, the resulting system is the current best performer on the task of people detection and segmentation with an average precision of 47.8% and 40.5% respectively on PASCAL VOC 2009."
            },
            "slug": "Detecting-People-Using-Mutually-Consistent-Poselet-Bourdev-Maji",
            "title": {
                "fragments": [],
                "text": "Detecting People Using Mutually Consistent Poselet Activations"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new algorithm for detecting people using poselets is developed which uses only 2D annotations which are much easier for naive human annotators and is the current best performer on the task of people detection and segmentation."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9739979"
                        ],
                        "name": "P. Arbel\u00e1ez",
                        "slug": "P.-Arbel\u00e1ez",
                        "structuredName": {
                            "firstName": "Pablo",
                            "lastName": "Arbel\u00e1ez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Arbel\u00e1ez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145854440"
                        ],
                        "name": "M. Maire",
                        "slug": "M.-Maire",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Maire",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Maire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1505240324"
                        ],
                        "name": "C. Fowlkes",
                        "slug": "C.-Fowlkes",
                        "structuredName": {
                            "firstName": "Charlotte",
                            "lastName": "Fowlkes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Fowlkes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787589"
                        ],
                        "name": "J. Malik",
                        "slug": "J.-Malik",
                        "structuredName": {
                            "firstName": "Julien",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 57
                            }
                        ],
                        "text": "When compared to a naive application of the algorithm in [4], our combinatorial multi-scale strategy produces more candidates (1322 per image), but provides a large improvement in their quality."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 88
                            }
                        ],
                        "text": "We generate object candidates building on the publicly available segmentation method of [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 54
                            }
                        ],
                        "text": "We obtain a significant improvement over the baseline [4]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 206764694,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0e5a262bf59b68ba8a7a1103d16fa33a9f5ffc28",
            "isKey": true,
            "numCitedBy": 4197,
            "numCiting": 89,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper investigates two fundamental problems in computer vision: contour detection and image segmentation. We present state-of-the-art algorithms for both of these tasks. Our contour detector combines multiple local cues into a globalization framework based on spectral clustering. Our segmentation algorithm consists of generic machinery for transforming the output of any contour detector into a hierarchical region tree. In this manner, we reduce the problem of image segmentation to that of contour detection. Extensive experimental evaluation demonstrates that both our contour detection and segmentation methods significantly outperform competing algorithms. The automatically generated hierarchical segmentations can be interactively refined by user-specified annotations. Computation at multiple image resolutions provides a means of coupling our system to recognition applications."
            },
            "slug": "Contour-Detection-and-Hierarchical-Image-Arbel\u00e1ez-Maire",
            "title": {
                "fragments": [],
                "text": "Contour Detection and Hierarchical Image Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "This paper investigates two fundamental problems in computer vision: contour detection and image segmentation and presents state-of-the-art algorithms for both of these tasks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1769383"
                        ],
                        "name": "Lubomir D. Bourdev",
                        "slug": "Lubomir-D.-Bourdev",
                        "structuredName": {
                            "firstName": "Lubomir",
                            "lastName": "Bourdev",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lubomir D. Bourdev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 61
                            }
                        ],
                        "text": "For this purpose, we use the poselet framework introduced in [8, 7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9320620,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "55b29a2505149d06d8c1d616cd30edca40cb029c",
            "isKey": false,
            "numCitedBy": 1048,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the classic problems of detection, segmentation and pose estimation of people in images with a novel definition of a part, a poselet. We postulate two criteria (1) It should be easy to find a poselet given an input image (2) it should be easy to localize the 3D configuration of the person conditioned on the detection of a poselet. To permit this we have built a new dataset, H3D, of annotations of humans in 2D photographs with 3D joint information, inferred using anthropometric constraints. This enables us to implement a data-driven search procedure for finding poselets that are tightly clustered in both 3D joint configuration space as well as 2D image appearance. The algorithm discovers poselets that correspond to frontal and profile faces, pedestrians, head and shoulder views, among others. Each poselet provides examples for training a linear SVM classifier which can then be run over the image in a multiscale scanning mode. The outputs of these poselet detectors can be thought of as an intermediate layer of nodes, on top of which one can run a second layer of classification or regression. We show how this permits detection and localization of torsos or keypoints such as left shoulder, nose, etc. Experimental results show that we obtain state of the art performance on people detection in the PASCAL VOC 2007 challenge, among other datasets. We are making publicly available both the H3D dataset as well as the poselet parameters for use by other researchers."
            },
            "slug": "Poselets:-Body-part-detectors-trained-using-3D-pose-Bourdev-Malik",
            "title": {
                "fragments": [],
                "text": "Poselets: Body part detectors trained using 3D human pose annotations"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A new dataset, H3D, is built of annotations of humans in 2D photographs with 3D joint information, inferred using anthropometric constraints, to address the classic problems of detection, segmentation and pose estimation of people in images with a novel definition of a part, a poselet."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE 12th International Conference on Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48950628"
                        ],
                        "name": "N. Dalal",
                        "slug": "N.-Dalal",
                        "structuredName": {
                            "firstName": "Navneet",
                            "lastName": "Dalal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dalal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 96
                            }
                        ],
                        "text": "We then train an initial classifier, retrieve maximal scoring regions in the train images as in [11], and add them as negative training instances."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "tor such as Viola-Jones [36] or Dalal-Triggs [11], complemented with part detectors as in Felszenszwalb et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 277,
                                "start": 265
                            }
                        ],
                        "text": "The first one suggests a region-based approach , e.g., [31, 20, 1, 27, 15, 10, 37, 5, 12, 21]; the second one is addressed naturally in a multiscale scanning window detec-\n\u2020This work was done while the author was at Adobe Systems, Inc.\ntor such as Viola-Jones [36] or Dalal-Triggs [11], complemented with part detectors as in Felszenszwalb et al. [14] or Bourdev et al. [8]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 206590483,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cec734d7097ab6b1e60d95228ffd64248eb89d66",
            "isKey": false,
            "numCitedBy": 29262,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds."
            },
            "slug": "Histograms-of-oriented-gradients-for-human-Dalal-Triggs",
            "title": {
                "fragments": [],
                "text": "Histograms of oriented gradients for human detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection, and the influence of each stage of the computation on performance is studied."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40401747"
                        ],
                        "name": "Aur\u00e9lien Lucchi",
                        "slug": "Aur\u00e9lien-Lucchi",
                        "structuredName": {
                            "firstName": "Aur\u00e9lien",
                            "lastName": "Lucchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aur\u00e9lien Lucchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110423572"
                        ],
                        "name": "Yunpeng Li",
                        "slug": "Yunpeng-Li",
                        "structuredName": {
                            "firstName": "Yunpeng",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yunpeng Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2343486"
                        ],
                        "name": "X. Boix",
                        "slug": "X.-Boix",
                        "structuredName": {
                            "firstName": "Xavier",
                            "lastName": "Boix",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Boix"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145548651"
                        ],
                        "name": "Kevin Smith",
                        "slug": "Kevin-Smith",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Smith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717736"
                        ],
                        "name": "P. Fua",
                        "slug": "P.-Fua",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Fua",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Fua"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7267744,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "31f1c4cf34ce0bb35382c35b2f468cf72bffae0b",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Many state-of-the-art segmentation algorithms rely on Markov or Conditional Random Field models designed to enforce spatial and global consistency constraints. This is often accomplished by introducing additional latent variables to the model, which can greatly increase its complexity. As a result, estimating the model parameters or computing the best maximum a posteriori (MAP) assignment becomes a computationally expensive task. In a series of experiments on the PASCAL and the MSRC datasets, we were unable to find evidence of a significant performance increase attributed to the introduction of such constraints. On the contrary, we found that similar levels of performance can be achieved using a much simpler design that essentially ignores these constraints. This more simple approach makes use of the same local and global features to leverage evidence from the image, but instead directly biases the preferences of individual pixels. While our investigation does not prove that spatial and consistency constraints are not useful in principle, it points to the conclusion that they should be validated in a larger context."
            },
            "slug": "Are-spatial-and-global-constraints-really-necessary-Lucchi-Li",
            "title": {
                "fragments": [],
                "text": "Are spatial and global constraints really necessary for segmentation?"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This investigation was unable to find evidence of a significant performance increase attributed to the introduction of spatial and consistency constraints, and found that similar levels of performance can be achieved using a much simpler design that essentially ignores these constraints."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3045340"
                        ],
                        "name": "Tomasz Malisiewicz",
                        "slug": "Tomasz-Malisiewicz",
                        "structuredName": {
                            "firstName": "Tomasz",
                            "lastName": "Malisiewicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomasz Malisiewicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726095131"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Gupta",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14448882,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d124f004fed2ee15860f624005b2215cfaeff276",
            "isKey": false,
            "numCitedBy": 892,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a conceptually simple but surprisingly powerful method which combines the effectiveness of a discriminative object detector with the explicit correspondence offered by a nearest-neighbor approach. The method is based on training a separate linear SVM classifier for every exemplar in the training set. Each of these Exemplar-SVMs is thus defined by a single positive instance and millions of negatives. While each detector is quite specific to its exemplar, we empirically observe that an ensemble of such Exemplar-SVMs offers surprisingly good generalization. Our performance on the PASCAL VOC detection task is on par with the much more complex latent part-based model of Felzenszwalb et al., at only a modest computational cost increase. But the central benefit of our approach is that it creates an explicit association between each detection and a single training exemplar. Because most detections show good alignment to their associated exemplar, it is possible to transfer any available exemplar meta-data (segmentation, geometric structure, 3D model, etc.) directly onto the detections, which can then be used as part of overall scene understanding."
            },
            "slug": "Ensemble-of-exemplar-SVMs-for-object-detection-and-Malisiewicz-Gupta",
            "title": {
                "fragments": [],
                "text": "Ensemble of exemplar-SVMs for object detection and beyond"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper proposes a conceptually simple but surprisingly powerful method which combines the effectiveness of a discriminative object detector with the explicit correspondence offered by a nearest-neighbor approach."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728641"
                        ],
                        "name": "Lubor Ladicky",
                        "slug": "Lubor-Ladicky",
                        "structuredName": {
                            "firstName": "Lubor",
                            "lastName": "Ladicky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lubor Ladicky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145485799"
                        ],
                        "name": "Chris Russell",
                        "slug": "Chris-Russell",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Russell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143967473"
                        ],
                        "name": "Pushmeet Kohli",
                        "slug": "Pushmeet-Kohli",
                        "structuredName": {
                            "firstName": "Pushmeet",
                            "lastName": "Kohli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pushmeet Kohli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143635540"
                        ],
                        "name": "Philip H. S. Torr",
                        "slug": "Philip-H.-S.-Torr",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Torr",
                            "middleNames": [
                                "H.",
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip H. S. Torr"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2794268,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0095ca57665eae51c9dd7a4ed8a3311aeea1b441",
            "isKey": false,
            "numCitedBy": 365,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Markov and Conditional random fields (CRFs) used in computer vision typically model only local interactions between variables, as this is computationally tractable. In this paper we consider a class of global potentials defined over all variables in the CRF. We show how they can be readily optimised using standard graph cut algorithms at little extra expense compared to a standard pairwise field. \n \nThis result can be directly used for the problem of class based image segmentation which has seen increasing recent interest within computer vision. Here the aim is to assign a label to each pixel of a given image from a set of possible object classes. Typically these methods use random fields to model local interactions between pixels or super-pixels. One of the cues that helps recognition is global object co-occurrence statistics, a measure of which classes (such as chair or motorbike) are likely to occur in the same image together. There have been several approaches proposed to exploit this property, but all of them suffer from different limitations and typically carry a high computational cost, preventing their application on large images. We find that the new model we propose produces an improvement in the labelling compared to just using a pairwise model."
            },
            "slug": "Graph-Cut-Based-Inference-with-Co-occurrence-Ladicky-Russell",
            "title": {
                "fragments": [],
                "text": "Graph Cut Based Inference with Co-occurrence Statistics"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper considers a class of global potentials defined over all variables in the CRF to show how they can be readily optimised using standard graph cut algorithms at little extra expense compared to a standard pairwise field."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145237406"
                        ],
                        "name": "N. Ahuja",
                        "slug": "N.-Ahuja",
                        "structuredName": {
                            "firstName": "Narendra",
                            "lastName": "Ahuja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ahuja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143856428"
                        ],
                        "name": "S. Todorovic",
                        "slug": "S.-Todorovic",
                        "structuredName": {
                            "firstName": "Sinisa",
                            "lastName": "Todorovic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Todorovic"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 2
                            }
                        ],
                        "text": ", [31, 20, 1, 27, 15, 10, 37, 5, 12, 21]; the second one is addressed naturally in a multiscale scanning window detec-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7420545,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee5c2d73d5146f3a56be9b091a9f9ff3109deabd",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a new object representation, called connected segmentation tree (CST), which captures canonical characteristics of the object in terms of the photometric, geometric, and spatial adjacency and containment properties of its constituent image regions. CST is obtained by augmenting the objectpsilas segmentation tree (ST) with inter-region neighbor links, in addition to their recursive embedding structure already present in ST. This makes CST a hierarchy of region adjacency graphs. A regionpsilas neighbors are computed using an extension to regions of the Voronoi diagram for point patterns. Unsupervised learning of the CST model of a category is formulated as matching the CST graph representations of unlabeled training images, and fusing their maximally matching subgraphs. A new learning algorithm is proposed that optimizes the model structure by simultaneously searching for both the most salient nodes (regions) and the most salient edges (containment and neighbor relationships of regions) across the image graphs. Matching of the category model to the CST of a new image results in simultaneous detection, segmentation and recognition of all occurrences of the category, and a semantic explanation of these results."
            },
            "slug": "Connected-Segmentation-Tree-\u2014-A-joint-of-region-and-Ahuja-Todorovic",
            "title": {
                "fragments": [],
                "text": "Connected Segmentation Tree \u2014 A joint representation of region layout and hierarchy"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A new object representation, called connected segmentation tree (CST), is proposed, which captures canonical characteristics of the object in terms of the photometric, geometric, and spatial adjacency and containment properties of its constituent image regions."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2343486"
                        ],
                        "name": "X. Boix",
                        "slug": "X.-Boix",
                        "structuredName": {
                            "firstName": "Xavier",
                            "lastName": "Boix",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Boix"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739551"
                        ],
                        "name": "J. M. Gonfaus",
                        "slug": "J.-M.-Gonfaus",
                        "structuredName": {
                            "firstName": "Josep",
                            "lastName": "Gonfaus",
                            "middleNames": [
                                "Maria"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. M. Gonfaus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2820687"
                        ],
                        "name": "Joost van de Weijer",
                        "slug": "Joost-van-de-Weijer",
                        "structuredName": {
                            "firstName": "Joost",
                            "lastName": "Weijer",
                            "middleNames": [
                                "van",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joost van de Weijer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749498"
                        ],
                        "name": "Andrew D. Bagdanov",
                        "slug": "Andrew-D.-Bagdanov",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Bagdanov",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew D. Bagdanov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3061716"
                        ],
                        "name": "J. Gual",
                        "slug": "J.-Gual",
                        "structuredName": {
                            "firstName": "Joan",
                            "lastName": "Gual",
                            "middleNames": [
                                "Serrat"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gual"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763726"
                        ],
                        "name": "Jordi Gonz\u00e0lez",
                        "slug": "Jordi-Gonz\u00e0lez",
                        "structuredName": {
                            "firstName": "Jordi",
                            "lastName": "Gonz\u00e0lez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jordi Gonz\u00e0lez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18136467,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "673c9465e61e8f8b1bec90ff8ab0a5d2ea5db6f6",
            "isKey": false,
            "numCitedBy": 91,
            "numCiting": 90,
            "paperAbstract": {
                "fragments": [],
                "text": "The Hierarchical Conditional Random Field (HCRF) model have been successfully applied to a number of image labeling problems, including image segmentation. However, existing HCRF models of image segmentation do not allow multiple classes to be assigned to a single region, which limits their ability to incorporate contextual information across multiple scales. At higher scales in the image, this representation yields an oversimplified model since multiple classes can be reasonably expected to appear within large regions. This simplified model particularly limits the impact of information at higher scales. Since class-label information at these scales is usually more reliable than at lower, noisier scales, neglecting this information is undesirable. To address these issues, we propose a new consistency potential for image labeling problems, which we call the harmony potential. It can encode any possible combination of labels, penalizing only unlikely combinations of classes. We also propose an effective sampling strategy over this expanded label set that renders tractable the underlying optimization problem. Our approach obtains state-of-the-art results on two challenging, standard benchmark datasets for semantic image segmentation: PASCAL VOC 2010, and MSRC-21."
            },
            "slug": "Harmony-Potentials-Boix-Gonfaus",
            "title": {
                "fragments": [],
                "text": "Harmony Potentials"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new consistency potential is proposed for image labeling problems that can encode any possible combination of labels, penalizing only unlikely combinations of classes, and an effective sampling strategy is proposed over this expanded label set that renders tractable the underlying optimization problem."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731948"
                        ],
                        "name": "Paul A. Viola",
                        "slug": "Paul-A.-Viola",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Viola",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul A. Viola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111328101"
                        ],
                        "name": "Michael Jones",
                        "slug": "Michael-Jones",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 24
                            }
                        ],
                        "text": "tor such as Viola-Jones [36] or Dalal-Triggs [11], complemented with part detectors as in Felszenszwalb et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 256,
                                "start": 245
                            }
                        ],
                        "text": "The first one suggests a region-based approach , e.g., [31, 20, 1, 27, 15, 10, 37, 5, 12, 21]; the second one is addressed naturally in a multiscale scanning window detec-\n\u2020This work was done while the author was at Adobe Systems, Inc.\ntor such as Viola-Jones [36] or Dalal-Triggs [11], complemented with part detectors as in Felszenszwalb et al. [14] or Bourdev et al. [8]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2796017,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b09ec0b350f8352bce46a2f5bf7ae97c83a7b9ca",
            "isKey": false,
            "numCitedBy": 11227,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a face detection framework that is capable of processing images extremely rapidly while achieving high detection rates. There are three key contributions. The first is the introduction of a new image representation called the \u201cIntegral Image\u201d which allows the features used by our detector to be computed very quickly. The second is a simple and efficient classifier which is built using the AdaBoost learning algorithm (Freund and Schapire, 1995) to select a small number of critical visual features from a very large set of potential features. The third contribution is a method for combining classifiers in a \u201ccascade\u201d which allows background regions of the image to be quickly discarded while spending more computation on promising face-like regions. A set of experiments in the domain of face detection is presented. The system yields face detection performance comparable to the best previous systems (Sung and Poggio, 1998; Rowley et al., 1998; Schneiderman and Kanade, 2000; Roth et al., 2000). Implemented on a conventional desktop, face detection proceeds at 15 frames per second."
            },
            "slug": "Robust-Real-Time-Face-Detection-Viola-Jones",
            "title": {
                "fragments": [],
                "text": "Robust Real-Time Face Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new image representation called the \u201cIntegral Image\u201d is introduced which allows the features used by the detector to be computed very quickly and a method for combining classifiers in a \u201ccascade\u201d which allows background regions of the image to be quickly discarded while spending more computation on promising face-like regions."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35208858"
                        ],
                        "name": "Subhransu Maji",
                        "slug": "Subhransu-Maji",
                        "structuredName": {
                            "firstName": "Subhransu",
                            "lastName": "Maji",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Subhransu Maji"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 128
                            }
                        ],
                        "text": "Our region scoring strategy is tractable thanks to the low dimensionality of the representation and the use of additive kernels [26, 25, 34], which combine the expressive power of non-linear classifiers with efficiency close to linear ones, both in training and testing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13243510,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "03a8f53058127798bc2bc0245d21e78354f6c93b",
            "isKey": false,
            "numCitedBy": 228,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We present methods for training high quality object detectors very quickly. The core contribution is a pair of fast training algorithms for piece-wise linear classifiers, which can approximate arbitrary additive models. The classifiers are trained in a max-margin framework and significantly outperform linear classifiers on a variety of vision datasets. We report experimental results quantifying training time and accuracy on image classification tasks and pedestrian detection, including detection results better than the best previous on the INRIA dataset with faster training."
            },
            "slug": "Max-margin-additive-classifiers-for-detection-Maji-Berg",
            "title": {
                "fragments": [],
                "text": "Max-margin additive classifiers for detection"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "A pair of fast training algorithms for piece-wise linear classifiers, which can approximate arbitrary additive models, are presented, which are trained in a max-margin framework and significantly outperform linear classifier on a variety of vision datasets."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE 12th International Conference on Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756979"
                        ],
                        "name": "K. V. D. Sande",
                        "slug": "K.-V.-D.-Sande",
                        "structuredName": {
                            "firstName": "Koen",
                            "lastName": "Sande",
                            "middleNames": [
                                "E.",
                                "A.",
                                "van",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. V. D. Sande"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695527"
                        ],
                        "name": "T. Gevers",
                        "slug": "T.-Gevers",
                        "structuredName": {
                            "firstName": "Theo",
                            "lastName": "Gevers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Gevers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145404204"
                        ],
                        "name": "Cees G. M. Snoek",
                        "slug": "Cees-G.-M.-Snoek",
                        "structuredName": {
                            "firstName": "Cees",
                            "lastName": "Snoek",
                            "middleNames": [
                                "G.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cees G. M. Snoek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 828465,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1aa5a8ad5b7031ba39e1dc0537484694364a1312",
            "isKey": false,
            "numCitedBy": 2099,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Image category recognition is important to access visual information on the level of objects and scene types. So far, intensity-based descriptors have been widely used for feature extraction at salient points. To increase illumination invariance and discriminative power, color descriptors have been proposed. Because many different descriptors exist, a structured overview is required of color invariant descriptors in the context of image category recognition. Therefore, this paper studies the invariance properties and the distinctiveness of color descriptors (software to compute the color descriptors from this paper is available from http://www.colordescriptors.com) in a structured way. The analytical invariance properties of color descriptors are explored, using a taxonomy based on invariance properties with respect to photometric transformations, and tested experimentally using a data set with known illumination conditions. In addition, the distinctiveness of color descriptors is assessed experimentally using two benchmarks, one from the image domain and one from the video domain. From the theoretical and experimental results, it can be derived that invariance to light intensity changes and light color changes affects category recognition. The results further reveal that, for light intensity shifts, the usefulness of invariance is category-specific. Overall, when choosing a single descriptor and no prior knowledge about the data set and object and scene categories is available, the OpponentSIFT is recommended. Furthermore, a combined set of color descriptors outperforms intensity-based SIFT and improves category recognition by 8 percent on the PASCAL VOC 2007 and by 7 percent on the Mediamill Challenge."
            },
            "slug": "Evaluating-Color-Descriptors-for-Object-and-Scene-Sande-Gevers",
            "title": {
                "fragments": [],
                "text": "Evaluating Color Descriptors for Object and Scene Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "From the theoretical and experimental results, it can be derived that invariance to light intensity changes and light color changes affects category recognition and the usefulness of invariance is category-specific."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2299479"
                        ],
                        "name": "R. Arandjelovi\u0107",
                        "slug": "R.-Arandjelovi\u0107",
                        "structuredName": {
                            "firstName": "Relja",
                            "lastName": "Arandjelovi\u0107",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Arandjelovi\u0107"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 48
                            }
                        ],
                        "text": "First, they should approximate as precisely as possible the ground-truth masks for a large majority of objects."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18276628,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2d0251c64baeafae96be9f33fd6e6a5c5d9a8387",
            "isKey": false,
            "numCitedBy": 99,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a scalable approach to 3D smooth object retrieval which searches for and localizes all the occurrences of a user outlined object in a dataset of images in real time. The approach is illustrated on sculptures."
            },
            "slug": "Smooth-object-retrieval-using-a-bag-of-boundaries-Arandjelovi\u0107-Zisserman",
            "title": {
                "fragments": [],
                "text": "Smooth object retrieval using a bag of boundaries"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A scalable approach to 3D smooth object retrieval which searches for and localizes all the occurrences of a user outlined object in a dataset of images in real time is described."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143967473"
                        ],
                        "name": "Pushmeet Kohli",
                        "slug": "Pushmeet-Kohli",
                        "structuredName": {
                            "firstName": "Pushmeet",
                            "lastName": "Kohli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pushmeet Kohli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3717791"
                        ],
                        "name": "M. P. Kumar",
                        "slug": "M.-P.-Kumar",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Kumar",
                            "middleNames": [
                                "Pawan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. P. Kumar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 65
                            }
                        ],
                        "text": "Ladicky et al. [21] incorporate object cooccurrence in CRFs, and Kohli and Kumar [19] propose a generic representation for higher-order potentials in MRF. Boix et al. [5] propose a new potential which allows multiple class labels to be assigned to a single node."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 66
                            }
                        ],
                        "text": "[21] incorporate object cooccurrence in CRFs, and Kohli and Kumar [19] propose a generic representation for higher-order potentials in MRF."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 168
                            }
                        ],
                        "text": "There are three broad lines of work, and we review them below: CRF Approaches: A popular approach is to encode segment relations using Conditional Random Fields (CRFs) [19, 21, 5, 23]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9193630,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c1592a4dcc17f1874d463f964837bf7f66e0d6a",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Markov random fields with higher order potentials have emerged as a powerful model for several problems in computer vision. In order to facilitate their use, we propose a new representation for higher order potentials as upper and lower envelopes of linear functions. Our representation concisely models several commonly used higher order potentials, thereby providing a unified framework for minimizing the corresponding Gibbs energy functions. We exploit this framework by converting lower envelope potentials to standard pairwise functions with the addition of a small number of auxiliary variables. This allows us to minimize energy functions with lower envelope potentials using conventional algorithms such as BP, TRW and \u03b1-expansion. Furthermore, we show how the minimization of energy functions with upper envelope potentials leads to a difficult minmax problem. We address this difficulty by proposing a new message passing algorithm that solves a linear programming relaxation of the problem. Although this is primarily a theoretical paper, we demonstrate the efficacy of our approach on the binary (fg/bg) segmentation problem."
            },
            "slug": "Energy-minimization-for-linear-envelope-MRFs-Kohli-Kumar",
            "title": {
                "fragments": [],
                "text": "Energy minimization for linear envelope MRFs"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown how the minimization of energy functions with upper envelope potentials leads to a difficult minmax problem, and a new message passing algorithm is proposed that solves a linear programming relaxation of the problem."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687524"
                        ],
                        "name": "A. Vedaldi",
                        "slug": "A.-Vedaldi",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Vedaldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vedaldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 128
                            }
                        ],
                        "text": "Our region scoring strategy is tractable thanks to the low dimensionality of the representation and the use of additive kernels [26, 25, 34], which combine the expressive power of non-linear classifiers with efficiency close to linear ones, both in training and testing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1440386,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "9217484cc329dc0aa37614ebac60f530106706a4",
            "isKey": false,
            "numCitedBy": 574,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Maji and Berg [13] have recently introduced an explicit feature map approximating the intersection kernel. This enables efficient learning methods for linear kernels to be applied to the non-linear intersection kernel, expanding the applicability of this model to much larger problems. In this paper we generalize this idea, and analyse a large family of additive kernels, called homogeneous, in a unified framework. The family includes the intersection, Hellinger's, and \u03c72 kernels commonly employed in computer vision. Using the framework we are able to: (i) provide explicit feature maps for all homogeneous additive kernels along with closed form expression for all common kernels; (ii) derive corresponding approximate finite-dimensional feature maps based on the Fourier sampling theorem; and (iii) quantify the extent of the approximation. We demonstrate that the approximations have indistinguishable performance from the full kernel on a number of standard datasets, yet greatly reduce the train/test times of SVM implementations. We show that the \u03c72 kernel, which has been found to yield the best performance in most applications, also has the most compact feature representation. Given these train/test advantages we are able to obtain a significant performance improvement over current state of the art results based on the intersection kernel."
            },
            "slug": "Efficient-additive-kernels-via-explicit-feature-Vedaldi-Zisserman",
            "title": {
                "fragments": [],
                "text": "Efficient additive kernels via explicit feature maps"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that the \u03c72 kernel, which has been found to yield the best performance in most applications, also has the most compact feature representation, and is able to obtain a significant performance improvement over current state of the art results based on the intersection kernel."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35208858"
                        ],
                        "name": "Subhransu Maji",
                        "slug": "Subhransu-Maji",
                        "structuredName": {
                            "firstName": "Subhransu",
                            "lastName": "Maji",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Subhransu Maji"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 128
                            }
                        ],
                        "text": "Our region scoring strategy is tractable thanks to the low dimensionality of the representation and the use of additive kernels [26, 25, 34], which combine the expressive power of non-linear classifiers with efficiency close to linear ones, both in training and testing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2990061,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7086378e68dae59975cf749c101c53a0fa90eab",
            "isKey": false,
            "numCitedBy": 1082,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Straightforward classification using kernelized SVMs requires evaluating the kernel for a test vector and each of the support vectors. For a class of kernels we show that one can do this much more efficiently. In particular we show that one can build histogram intersection kernel SVMs (IKSVMs) with runtime complexity of the classifier logarithmic in the number of support vectors as opposed to linear for the standard approach. We further show that by precomputing auxiliary tables we can construct an approximate classifier with constant runtime and space requirements, independent of the number of support vectors, with negligible loss in classification accuracy on various tasks. This approximation also applies to 1 - chi2 and other kernels of similar form. We also introduce novel features based on a multi-level histograms of oriented edge energy and present experiments on various detection datasets. On the INRIA pedestrian dataset an approximate IKSVM classifier based on these features has the current best performance, with a miss rate 13% lower at 10-6 False Positive Per Window than the linear SVM detector of Dalal & Triggs. On the Daimler Chrysler pedestrian dataset IKSVM gives comparable accuracy to the best results (based on quadratic SVM), while being 15times faster. In these experiments our approximate IKSVM is up to 2000times faster than a standard implementation and requires 200times less memory. Finally we show that a 50times speedup is possible using approximate IKSVM based on spatial pyramid features on the Caltech 101 dataset with negligible loss of accuracy."
            },
            "slug": "Classification-using-intersection-kernel-support-is-Maji-Berg",
            "title": {
                "fragments": [],
                "text": "Classification using intersection kernel support vector machines is efficient"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that one can build histogram intersection kernel SVMs (IKSVMs) with runtime complexity of the classifier logarithmic in the number of support vectors as opposed to linear for the standard approach."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 22
                            }
                        ],
                        "text": "VOC(%) [18] [10] [21] [5] SRL UC3M TTI [23] [9] FULL FULL"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 2
                            }
                        ],
                        "text": ", [31, 20, 1, 27, 15, 10, 37, 5, 12, 21]; the second one is addressed naturally in a multiscale scanning window detec-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[5] propose a new potential which allows multiple class labels to be assigned to a single node."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 168
                            }
                        ],
                        "text": "There are three broad lines of work, and we review them below: CRF Approaches: A popular approach is to encode segment relations using Conditional Random Fields (CRFs) [19, 21, 5, 23]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Harmony Potentials Fusing Global and Local Scale for Semantic Image Segmentation"
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision,"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2141732326"
                        ],
                        "name": "Jianguo Zhang",
                        "slug": "Jianguo-Zhang",
                        "structuredName": {
                            "firstName": "Jianguo",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianguo Zhang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 63925014,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "0ec48ac86456cea3d6d6172ca81ef68e98b21a61",
            "isKey": false,
            "numCitedBy": 3322,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-PASCAL-Visual-Object-Classes-Challenge-Zhang",
            "title": {
                "fragments": [],
                "text": "The PASCAL Visual Object Classes Challenge"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061462269"
                        ],
                        "name": "J. Platt",
                        "slug": "J.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Platt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 151
                            }
                        ],
                        "text": "We then train additive kernel classifiers for pixels using these 240 scores as features and convert the scores into probabilities using Platt\u2019s method [30]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 56563878,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "384bb3944abe9441dcd2cede5e7cd7353e9ee5f7",
            "isKey": false,
            "numCitedBy": 5100,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Probabilistic-Outputs-for-Support-vector-Machines-Platt",
            "title": {
                "fragments": [],
                "text": "Probabilistic Outputs for Support vector Machines and Comparisons to Regularized Likelihood Methods"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Body part detectors trained using 3 d human pose annotations"
            },
            "venue": {
                "fragments": [],
                "text": "Proc . ICCV"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 0
                            }
                        ],
                        "text": "[10, 12, 2] design a ranking function for object segment proposals."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "What is an object? In Proc"
            },
            "venue": {
                "fragments": [],
                "text": "CVPR, pages 73\u201380"
            },
            "year": 2010
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 22,
            "methodology": 17,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 40,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Semantic-segmentation-using-regions-and-parts-Arbel\u00e1ez-Hariharan/1395f0561db13cad21a519e18be111cbe1e6d818?sort=total-citations"
}