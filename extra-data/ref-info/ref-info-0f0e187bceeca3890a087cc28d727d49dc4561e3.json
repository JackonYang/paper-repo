{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144179113"
                        ],
                        "name": "K. Stevens",
                        "slug": "K.-Stevens",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Stevens",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Stevens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 197
                            }
                        ],
                        "text": "compared to a vanilla HMM baseline and is distinct from any other built so far, though it shares many qualities with those inspired by acoustic phonetics, distinctive features, and event landmarks [1][2]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 94
                            }
                        ],
                        "text": "At these points the features are most strongly expressed; following the general philosophy of [1] and others, we refer to these points in time as landmarks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1811670,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "8f0da8dd0dec86f20d6879d86392935ff81240e8",
            "isKey": false,
            "numCitedBy": 524,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "This article describes a model in which the acoustic speech signal is processed to yield a discrete representation of the speech stream in terms of a sequence of segments, each of which is described by a set (or bundle) of binary distinctive features. These distinctive features specify the phonemic contrasts that are used in the language, such that a change in the value of a feature can potentially generate a new word. This model is a part of a more general model that derives a word sequence from this feature representation, the words being represented in a lexicon by sequences of feature bundles. The processing of the signal proceeds in three steps: (1) Detection of peaks, valleys, and discontinuities in particular frequency ranges of the signal leads to identification of acoustic landmarks. The type of landmark provides evidence for a subset of distinctive features called articulator-free features (e.g., [vowel], [consonant], [continuant]). (2) Acoustic parameters are derived from the signal near the landmarks to provide evidence for the actions of particular articulators, and acoustic cues are extracted by sampling selected attributes of these parameters in these regions. The selection of cues that are extracted depends on the type of landmark and on the environment in which it occurs. (3) The cues obtained in step (2) are combined, taking context into account, to provide estimates of \"articulator-bound\" features associated with each landmark (e.g., [lips], [high], [nasal]). These articulator-bound features, combined with the articulator-free features in (1), constitute the sequence of feature bundles that forms the output of the model. Examples of cues that are used, and justification for this selection, are given, as well as examples of the process of inferring the underlying features for a segment when there is variability in the signal due to enhancement gestures (recruited by a speaker to make a contrast more salient) or due to overlap of gestures from neighboring segments."
            },
            "slug": "Toward-a-model-for-lexical-access-based-on-acoustic-Stevens",
            "title": {
                "fragments": [],
                "text": "Toward a model for lexical access based on acoustic landmarks and distinctive features."
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A model in which the acoustic speech signal is processed to yield a discrete representation of the speech stream in terms of a sequence of segments, each of which is described by a set (or bundle) of binary distinctive features."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of the Acoustical Society of America"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1958344"
                        ],
                        "name": "Zhimin Xie",
                        "slug": "Zhimin-Xie",
                        "structuredName": {
                            "firstName": "Zhimin",
                            "lastName": "Xie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhimin Xie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 47
                            }
                        ],
                        "text": "We employ mel-frequency cepstral coefficients (MFCCs) spanning the full frequency range (0-8 kHz), computed in 10 ms windows every 5 ms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 80
                            }
                        ],
                        "text": "Each HMM was trained on all sx/i TIMIT training sentences, using 39-dimensional MFCCs, 8-mixture observation densities, no skip transitions, and no language model or transition probability rescaling."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 81
                            }
                        ],
                        "text": "For the vowel, approximant, nasal, fricative and silence detectors, we again use MFCCs, but the window size (Twin), step size (Tstep), and frequency range (Frange) parameters vary according to Table 1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 93
                            }
                        ],
                        "text": "Possible approaches include: (i) implementing acoustic parameters as an alternative to MFCCs [7][8]; Table 2."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 87
                            }
                        ],
                        "text": "Possible approaches include: (i) implementing acoustic parameters as an alternative to MFCCs [7][8];\n(ii) individually addressing specific phoneme-level detector inadequacies; (iii) implementing broad class transition detectors; and (iv) using alternative machine learning techniques."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 89
                            }
                        ],
                        "text": "To address this complication, we adapt the recursive \u201cconvex-hull\u201d approach presented in [7] to compute a timedependent baseline."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 7
                            }
                        ],
                        "text": "The 39 MFCCs include one energy and 12 cepstral coefficients, along with their delta and acceleration (double-delta) coefficients."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6345522,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "id": "4c819ca99ce6c2a8b251d46e982d57c5b4b4a303",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we describe a method to detect syllabic nuclei in continuous speech. It employs two basic and robust acoustic features, periodicity and energy, to detect syllable landmarks. This method is evaluated on TIMIT, noise additive TIMIT and NTIMIT datasets with typical total error rates of around 30% in all the datasets, except for extremely adverse 0dB signal-noise-ratio environments, while HMM-based systems degrade rigorously. Based on the landmarks, a vowel classifier is further constructed and achieves the same performance as HMM-based systems."
            },
            "slug": "Robust-acoustic-based-syllable-detection-Xie-Niyogi",
            "title": {
                "fragments": [],
                "text": "Robust acoustic-based syllable detection"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "A vowel classifier is further constructed and achieves the same performance as HMM-based systems, except for extremely adverse 0dB signal-noise-ratio environments, while HMMs degrade rigorously."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35039757"
                        ],
                        "name": "A. Juneja",
                        "slug": "A.-Juneja",
                        "structuredName": {
                            "firstName": "Amit",
                            "lastName": "Juneja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Juneja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398390485"
                        ],
                        "name": "C. Espy-Wilson",
                        "slug": "C.-Espy-Wilson",
                        "structuredName": {
                            "firstName": "Carol",
                            "lastName": "Espy-Wilson",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Espy-Wilson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 200
                            }
                        ],
                        "text": "compared to a vanilla HMM baseline and is distinct from any other built so far, though it shares many qualities with those inspired by acoustic phonetics, distinctive features, and event landmarks [1][2]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 1496041,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d88804353d8eac6c366381911ea4b555b7a3aade",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a method that combines a probabilistic phonetic feature hierarchy with support vector machines for segmentation of continuous speech into five classes - vowel, sonorant consonant, fricative, stop and silence. We show that by using the hierarchy, only four binary classifiers are required to recognize the five classes. Due to the probabilistic nature of the hierarchy, the method overcomes the disadvantage of the traditional acoustic-phonetic methods where the error is carried down the hierarchy. In addition, the hierarchical approach allows the use of comparable amount of training data of two classes that each binary classifier is designed to discriminate. The segmentation method with 13 knowledge based parameters performs considerably better than a context-dependent hidden Markov model (HMM) based approach that uses 39 mel-cepstrum based parameters."
            },
            "slug": "Speech-segmentation-using-probabilistic-phonetic-Juneja-Espy-Wilson",
            "title": {
                "fragments": [],
                "text": "Speech segmentation using probabilistic phonetic feature hierarchy and support vector machines"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The method overcomes the disadvantage of the traditional acoustic-phonetic methods where the error is carried down the hierarchy and performs considerably better than a context-dependent hidden Markov model (HMM) based approach that uses 39 mel-cepstrum based parameters."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the International Joint Conference on Neural Networks, 2003."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116262"
                        ],
                        "name": "Om Deshmukh",
                        "slug": "Om-Deshmukh",
                        "structuredName": {
                            "firstName": "Om",
                            "lastName": "Deshmukh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Om Deshmukh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398390485"
                        ],
                        "name": "C. Espy-Wilson",
                        "slug": "C.-Espy-Wilson",
                        "structuredName": {
                            "firstName": "Carol",
                            "lastName": "Espy-Wilson",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Espy-Wilson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35039757"
                        ],
                        "name": "A. Juneja",
                        "slug": "A.-Juneja",
                        "structuredName": {
                            "firstName": "Amit",
                            "lastName": "Juneja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Juneja"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 96
                            }
                        ],
                        "text": "Possible approaches include: (i) implementing acoustic parameters as an alternative to MFCCs [7][8]; Table 2."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 505994,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "id": "1ef1705a6627d0faa6e7b47821d2ffa036977aa6",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Coping with inter-speaker variability (i.e., differences in the vocal tract characteristics of speakers) is still a major challenge for Automatic Speech Recognizers. In this paper, we discuss a method that compensates for differences in speaker characteristics. In particular, we demonstrate that when continuous density hidden Markov model based system is used as the back-end, a Knowledge-Based Front End (KBFE) can outperform the traditional Mel-Frequency Cepstral Coefficients (MFCCs), particularly when there is a mismatch in the gender and ages of the subjects used to train and test the recognizer. This work was supported by NSF grant # SBR-9729688 and NIH grant # IK02DCOOI49."
            },
            "slug": "Acoustic-phonetic-speech-parameters-for-speech-Deshmukh-Espy-Wilson",
            "title": {
                "fragments": [],
                "text": "Acoustic-phonetic speech parameters for speaker-independent speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is demonstrated that when continuous density hidden Markov model based system is used as the back-end, a Knowledge-Based Front End (KBFE) can outperform the traditional Mel-Frequency Cepstral Coefficients (MFCCs), particularly when there is a mismatch in the gender and ages of the subjects used to train and test the recognizer."
            },
            "venue": {
                "fragments": [],
                "text": "2002 IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34830449"
                        ],
                        "name": "M. Sondhi",
                        "slug": "M.-Sondhi",
                        "structuredName": {
                            "firstName": "Man",
                            "lastName": "Sondhi",
                            "middleNames": [
                                "Mohan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sondhi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 70
                            }
                        ],
                        "text": "For the stop detector, we used the acoustic parameter prescription of [6] as an alternative to the MFCC representation, though we modify the frame rate to reduce computational"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 117
                            }
                        ],
                        "text": "Furthermore, our stop classifier employs energy and Wiener entropy parameters shown to be successful in this setting [6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 27094648,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "60c64829462b9953952898d078355aa4d2c11a6c",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of implementing a detector for stop consonants in continuously spoken speech is considered. The problem is posed as one of finding an optimal filter (linear or nonlinear) that operates on a particular appropriately chosen representation, and ideally outputs a 1 when a stop occurs and 0 otherwise. The performance of several variants of a canonical stop detector is discussed and its implications for human and machine speech recognition is considered."
            },
            "slug": "Detecting-stop-consonants-in-continuous-speech.-Niyogi-Sondhi",
            "title": {
                "fragments": [],
                "text": "Detecting stop consonants in continuous speech."
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "The problem of implementing a detector for stop consonants in continuously spoken speech is considered and the performance of several variants of a canonical stop detector is discussed and its implications for human and machine speech recognition are considered."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of the Acoustical Society of America"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2209643"
                        ],
                        "name": "K. Esser",
                        "slug": "K.-Esser",
                        "structuredName": {
                            "firstName": "Karl-Heinz",
                            "lastName": "Esser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Esser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47644169"
                        ],
                        "name": "C. J. Condon",
                        "slug": "C.-J.-Condon",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Condon",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. J. Condon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2420980"
                        ],
                        "name": "N. Suga",
                        "slug": "N.-Suga",
                        "structuredName": {
                            "firstName": "Nobuo",
                            "lastName": "Suga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Suga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2442926"
                        ],
                        "name": "J. Kanwal",
                        "slug": "J.-Kanwal",
                        "structuredName": {
                            "firstName": "Jagmeet",
                            "lastName": "Kanwal",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kanwal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 158
                            }
                        ],
                        "text": "Such detectors may be analogized to neurons found in animals that fire selectively when certain complex acoustic attributes are present in the input stimulus [4][5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 21851235,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "1081a68e519ca1f28522199c6677118cca3cd4a2",
            "isKey": false,
            "numCitedBy": 147,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Syntax denotes a rule system that allows one to predict the sequencing of communication signals. Despite its significance for both human speech processing and animal acoustic communication, the representation of syntactic structure in the mammalian brain has not been studied electrophysiologically at the single-unit level. In the search for a neuronal correlate for syntax, we used playback of natural and temporally destructured complex species-specific communication calls-so-called composites-while recording extracellularly from neurons in a physiologically well defined area (the FM-FM area) of the mustached bat's auditory cortex. Even though this area is known to be involved in the processing of target distance information for echolocation, we found that units in the FM-FM area were highly responsive to composites. The finding that neuronal responses were strongly affected by manipulation in the time domain of the natural composite structure lends support to the hypothesis that syntax processing in mammals occurs at least at the level of the nonprimary auditory cortex."
            },
            "slug": "Syntax-processing-by-auditory-cortical-neurons-in-Esser-Condon",
            "title": {
                "fragments": [],
                "text": "Syntax processing by auditory cortical neurons in the FM-FM area of the mustached bat Pteronotus parnellii."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The finding that neuronal responses were strongly affected by manipulation in the time domain of the natural composite structure lends support to the hypothesis that syntax processing in mammals occurs at least at the level of the nonprimary auditory cortex."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3140471"
                        ],
                        "name": "Zhiyi Chi",
                        "slug": "Zhiyi-Chi",
                        "structuredName": {
                            "firstName": "Zhiyi",
                            "lastName": "Chi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhiyi Chi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145637814"
                        ],
                        "name": "Wei Wu",
                        "slug": "Wei-Wu",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39799419"
                        ],
                        "name": "Z. Haga",
                        "slug": "Z.-Haga",
                        "structuredName": {
                            "firstName": "Zach",
                            "lastName": "Haga",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Haga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1907395"
                        ],
                        "name": "N. Hatsopoulos",
                        "slug": "N.-Hatsopoulos",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Hatsopoulos",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Hatsopoulos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3027096"
                        ],
                        "name": "D. Margoliash",
                        "slug": "D.-Margoliash",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Margoliash",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Margoliash"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 231796,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "40ef559849e5b97bf4b1356cc28509c1d4817b2f",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Pattern identification for spiking activity, which is central to neurophysiological analysis, is complicated by variability in spiking at multiple timescales. Incorporating likelihood tests on the variability at two timescales, we developed an approach to identifying segments from continuous neurophysiological recordings that match preselected spike \"templates.\" At smaller timescales, each component of the preselected pattern is represented by a linear filter. Local scores to measure the similarities between short data segments and the pattern components are computed as filter responses. At larger timescales, overall scores to measure the similarities between relatively long data segments and the entire pattern are computed by dynamic time warping, which combines the local similarity scores associated with the pattern components, optimizing over a range of intercomponent time intervals. Occurrences of the pattern are identified by local peaks in the overall similarity scores. This approach is developed for point process representations and binary representations of spiking activity, both deriving from a single underlying statistical model. Point process representations are suitable for highly reliable single-unit responses, whereas binary representations are preferred for more variable single-unit responses and multiunit responses. Testing with single units recorded from individual electrodes within the robust nucleus of the arcopallium of zebra finches and with recordings from an array placed within the motor cortex of macaque monkeys demonstrates that the approach can identify occurrences of specified patterns with good time precision in a broad range of neurophysiological data."
            },
            "slug": "Template-based-spike-pattern-identification-with-Chi-Wu",
            "title": {
                "fragments": [],
                "text": "Template-based spike pattern identification with linear convolution and dynamic time warping."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of neurophysiology"
            },
            "year": 2007
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 2,
            "methodology": 3,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 7,
        "totalPages": 1
    },
    "page_url": "https://www.semanticscholar.org/paper/A-hierarchical-point-process-model-for-speech-Jansen-Niyogi/0f0e187bceeca3890a087cc28d727d49dc4561e3?sort=total-citations"
}