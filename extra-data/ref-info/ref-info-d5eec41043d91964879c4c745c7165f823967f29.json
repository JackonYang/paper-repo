{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40984420"
                        ],
                        "name": "A. Yang",
                        "slug": "A.-Yang",
                        "structuredName": {
                            "firstName": "Allen",
                            "lastName": "Yang",
                            "middleNames": [
                                "Yuqing"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143607492"
                        ],
                        "name": "John Wright",
                        "slug": "John-Wright",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Wright",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John Wright"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50032052"
                        ],
                        "name": "Yi Ma",
                        "slug": "Yi-Ma",
                        "structuredName": {
                            "firstName": "Yi",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi Ma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144797536"
                        ],
                        "name": "S. Sastry",
                        "slug": "S.-Sastry",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Sastry",
                            "middleNames": [
                                "Shankar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sastry"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "One can find a more thorough justification for this choice in the companion paper [20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This paper and its companion [20] have introduced a novel and comprehensive approach to face recognition, providing a new perspective on fundamental issues such as feature selection, occlusion, and outlier rejection."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In the companion paper [20], we discuss the ability of sparse representation to adapt to nonlinear distributions such as face images with varying pose."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This issue is discussed in detail in the companion paper [20], which advocates that if the sparsity in the recognition problem is properly harnessed, details of feature selection become less important."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In related work [20], we argue that such a global representation is naturally discriminative, and demonstrate its superiority over local methods for identifying subjects represented in the training set and rejecting outlying images of subjects not included in the training set."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This is the method of choice in the companion paper [20] in which we deal with images or features of much lower dimension, where the signal-to-noise ratio may become considerably lower."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 545915,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b594a2625a5328b835544aea9144a0a74ab1ca13",
            "isKey": false,
            "numCitedBy": 196,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we examine the role of feature selection in face recognition from the perspective of sparse representation. We cast the recognition problem as finding a sparse representation of the test image features w.r.t. the training set. The sparse representation can be accurately and efficiently computed by `-minimization. The proposed simple algorithm generalizes conventional face recognition classifiers such as nearest neighbors and nearest subspaces. Using face recognition under varying illumination and expression as an example, we show that if sparsity in the recognition problem is properly harnessed, the choice of features is no longer critical. What is critical, however, is whether the number of features is sufficient and whether the sparse representation is correctly found. We conduct extensive experiments to validate the significance of imposing sparsity using the Extended Yale B database and the AR database. Our thorough evaluation shows that, using conventional features such as Eigenfaces and facial parts, the proposed algorithm achieves much higher recognition accuracy on face images with variation in either illumination or expression. Furthermore, other unconventional features such as severely downsampled images and randomly projected features perform almost equally well with the increase of feature dimensions. The differences in performance between different features become insignificant as the feature-space dimension is sufficiently large."
            },
            "slug": "Feature-Selection-in-Face-Recognition:-A-Sparse-Yang-Wright",
            "title": {
                "fragments": [],
                "text": "Feature Selection in Face Recognition: A Sparse Representation Perspective"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "If sparsity in the recognition problem is properly harnessed, the choice of features is no longer critical and the differences in performance between different features become insignificant as the feature-space dimension is sufficiently large."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767767"
                        ],
                        "name": "P. Belhumeur",
                        "slug": "P.-Belhumeur",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Belhumeur",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Belhumeur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685300"
                        ],
                        "name": "J. Hespanha",
                        "slug": "J.-Hespanha",
                        "structuredName": {
                            "firstName": "Jo\u00e3o",
                            "lastName": "Hespanha",
                            "middleNames": [
                                "Pedro"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hespanha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765887"
                        ],
                        "name": "D. Kriegman",
                        "slug": "D.-Kriegman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Kriegman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kriegman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "One class of methods extracts holistic face features such as Eigenfaces[23], Fisherfaces [ 24 ], and Laplacianfaces [25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "high-dimensional test image into lower dimensional feature spaces: examples include Eigenfaces [23], Fisherfaces [ 24 ], Laplacianfaces [25], and a host of variants [26], [27]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "the other features because the maximal number of valid Fisherfaces is one less than the number of classes k [ 24 ], 38 in this case."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Subspace models are flexible enough to capture much of the variation in real data sets and are especially well motivated in the context of face recognition, where it has been observed that the images of faces under varying lighting and expression lie on a special low-dimensional subspace [ 24 ], [30], often called a face subspace."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 30582,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "be86da00efdd8c2a7fdeb2334605796c24b370f0",
            "isKey": true,
            "numCitedBy": 11721,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a face recognition algorithm which is insensitive to large variation in lighting direction and facial expression. Taking a pattern classification approach, we consider each pixel in an image as a coordinate in a high-dimensional space. We take advantage of the observation that the images of a particular face, under varying illumination but fixed pose, lie in a 3D linear subspace of the high dimensional image space-if the face is a Lambertian surface without shadowing. However, since faces are not truly Lambertian surfaces and do indeed produce self-shadowing, images will deviate from this linear subspace. Rather than explicitly modeling this deviation, we linearly project the image into a subspace in a manner which discounts those regions of the face with large deviation. Our projection method is based on Fisher's linear discriminant and produces well separated classes in a low-dimensional subspace, even under severe variation in lighting and facial expressions. The eigenface technique, another method based on linearly projecting the image space to a low dimensional subspace, has similar computational requirements. Yet, extensive experimental results demonstrate that the proposed \"Fisherface\" method has error rates that are lower than those of the eigenface technique for tests on the Harvard and Yale face databases."
            },
            "slug": "Eigenfaces-vs.-Fisherfaces:-Recognition-Using-Class-Belhumeur-Hespanha",
            "title": {
                "fragments": [],
                "text": "Eigenfaces vs. Fisherfaces: Recognition Using Class Specific Linear Projection"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A face recognition algorithm which is insensitive to large variation in lighting direction and facial expression is developed, based on Fisher's linear discriminant and produces well separated classes in a low-dimensional subspace, even under severe variations in lighting and facial expressions."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1384255355"
                        ],
                        "name": "Aleix M. Martinez",
                        "slug": "Aleix-M.-Martinez",
                        "structuredName": {
                            "firstName": "Aleix M.",
                            "lastName": "Martinez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aleix M. Martinez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 63
                            }
                        ],
                        "text": "Manuscript received 13 Aug. 2007; revised 18 Jan. 2008; accepted 20 Mar. 2008; published online 26 Mar. 2008."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 11
                            }
                        ],
                        "text": "The role of parsimony in human perception has also been strongly supported by studies of human vision."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13862950,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ed2ad7dfdb82039f63908b20dd736a92b6fdf3d5",
            "isKey": false,
            "numCitedBy": 868,
            "numCiting": 184,
            "paperAbstract": {
                "fragments": [],
                "text": "The classical way of attempting to solve the face (or object) recognition problem is by using large and representative data sets. In many applications, though, only one sample per class is available to the system. In this contribution, we describe a probabilistic approach that is able to compensate for imprecisely localized, partially occluded, and expression-variant faces even when only one single training sample per class is available to the system. To solve the localization problem, we find the subspace (within the feature space, e.g., eigenspace) that represents this error for each of the training images. To resolve the occlusion problem, each face is divided into k local regions which are analyzed in isolation. In contrast with other approaches where a simple voting space is used, we present a probabilistic method that analyzes how \"good\" a local match is. To make the recognition system less sensitive to the differences between the facial expression displayed on the training and the testing images, we weight the results obtained on each local area on the basis of how much of this local area is affected by the expression displayed on the current test image."
            },
            "slug": "Recognizing-Imprecisely-Localized,-Partially-and-a-Martinez",
            "title": {
                "fragments": [],
                "text": "Recognizing Imprecisely Localized, Partially Occluded, and Expression Variant Faces from a Single Sample per Class"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "A probabilistic approach that is able to compensate for imprecisely localized, partially occluded, and expression-variant faces even when only one single training sample per class is available to the system."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144097660"
                        ],
                        "name": "M. Turk",
                        "slug": "M.-Turk",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Turk",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Turk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We test the proposed method with some global features, such as down-sampled images, Eigenfaces [7], Laplacianfaces [8] and compare their performances under sparse representation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16193920,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d2bfef811f514391aa2a7b8f4020d1c9e033016",
            "isKey": false,
            "numCitedBy": 3604,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "An approach to the detection and identification of human faces is presented, and a working, near-real-time face recognition system which tracks a subject's head and then recognizes the person by comparing characteristics of the face to those of known individuals is described. This approach treats face recognition as a two-dimensional recognition problem, taking advantage of the fact that faces are normally upright and thus may be described by a small set of 2-D characteristic views. Face images are projected onto a feature space ('face space') that best encodes the variation among known face images. The face space is defined by the 'eigenfaces', which are the eigenvectors of the set of faces; they do not necessarily correspond to isolated features such as eyes, ears, and noses. The framework provides the ability to learn to recognize new faces in an unsupervised manner.<<ETX>>"
            },
            "slug": "Face-recognition-using-eigenfaces-Turk-Pentland",
            "title": {
                "fragments": [],
                "text": "Face recognition using eigenfaces"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An approach to the detection and identification of human faces is presented, and a working, near-real-time face recognition system which tracks a subject's head and then recognizes the person by comparing characteristics of the face to those of known individuals is described."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110099506"
                        ],
                        "name": "Jongsun Kim",
                        "slug": "Jongsun-Kim",
                        "structuredName": {
                            "firstName": "Jongsun",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jongsun Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31121550"
                        ],
                        "name": "Jongmoo Choi",
                        "slug": "Jongmoo-Choi",
                        "structuredName": {
                            "firstName": "Jongmoo",
                            "lastName": "Choi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jongmoo Choi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144724643"
                        ],
                        "name": "Juneho Yi",
                        "slug": "Juneho-Yi",
                        "structuredName": {
                            "firstName": "Juneho",
                            "lastName": "Yi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Juneho Yi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144097660"
                        ],
                        "name": "M. Turk",
                        "slug": "M.-Turk",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Turk",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Turk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3023129,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e94543dafa59f4c61e6fa2d46568946eb2058588",
            "isKey": false,
            "numCitedBy": 233,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "The performance of face recognition methods using subspace projection is directly related to the characteristics of their basis images, especially in the cases of local distortion or partial occlusion. In order for a subspace projection method to be robust to local distortion and partial occlusion, the basis images generated by the method should exhibit a part-based local representation. We propose an effective part-based local representation method named locally salient ICA (LS-ICA) method for face recognition that is robust to local distortion and partial occlusion. The LS-ICA method only employs locally salient information from important facial parts in order to maximize the benefit of applying the idea of \"recognition by parts\". It creates part-based local basis images by imposing additional localization constraint in the process of computing ICA architecture I basis images. We have contrasted the LS-ICA method with other part-based representations such as LNMF (localized nonnegative matrix factorization) and LFA (local feature analysis). Experimental results show that the LS-ICA method performs better than PCA, ICA architecture I, ICA architecture 11, LFA, and LNMF methods, especially in the cases of partial occlusions and local distortions."
            },
            "slug": "Effective-representation-using-ICA-for-face-robust-Kim-Choi",
            "title": {
                "fragments": [],
                "text": "Effective representation using ICA for face recognition robust to local distortion and partial occlusion"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This work proposes an effective part-based local representation method named locally salient ICA (LS-ICA) method for face recognition that is robust to local distortion and partial occlusion and compares it with other part- based representations such as LNMF (localized nonnegative matrix factorization) and LFA (local feature analysis)."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3230391"
                        ],
                        "name": "A. Georghiades",
                        "slug": "A.-Georghiades",
                        "structuredName": {
                            "firstName": "Athinodoros",
                            "lastName": "Georghiades",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Georghiades"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767767"
                        ],
                        "name": "P. Belhumeur",
                        "slug": "P.-Belhumeur",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Belhumeur",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Belhumeur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765887"
                        ],
                        "name": "D. Kriegman",
                        "slug": "D.-Kriegman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Kriegman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kriegman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 85
                            }
                        ],
                        "text": "The Extended Yale B database consists of 2,414 frontal-face images of 38 individuals [58]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "Following [58], we normalize the image pixels to have zero mean and unit variance before applying PCA."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9234219,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6642e9c6cf7432e2d11b7edf7cd47f1285acd54e",
            "isKey": false,
            "numCitedBy": 4696,
            "numCiting": 160,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a generative appearance-based method for recognizing human faces under variation in lighting and viewpoint. Our method exploits the fact that the set of images of an object in fixed pose, but under all possible illumination conditions, is a convex cone in the space of images. Using a small number of training images of each face taken with different lighting directions, the shape and albedo of the face can be reconstructed. In turn, this reconstruction serves as a generative model that can be used to render (or synthesize) images of the face under novel poses and illumination conditions. The pose space is then sampled and, for each pose, the corresponding illumination cone is approximated by a low-dimensional linear subspace whose basis vectors are estimated using the generative model. Our recognition algorithm assigns to a test image the identity of the closest approximated illumination cone. Test results show that the method performs almost without error, except on the most extreme lighting directions."
            },
            "slug": "From-Few-to-Many:-Illumination-Cone-Models-for-Face-Georghiades-Belhumeur",
            "title": {
                "fragments": [],
                "text": "From Few to Many: Illumination Cone Models for Face Recognition under Variable Lighting and Pose"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A generative appearance-based method for recognizing human faces under variation in lighting and viewpoint that exploits the fact that the set of images of an object in fixed pose but under all possible illumination conditions, is a convex cone in the space of images."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3945955"
                        ],
                        "name": "Xiaofei He",
                        "slug": "Xiaofei-He",
                        "structuredName": {
                            "firstName": "Xiaofei",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaofei He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143653681"
                        ],
                        "name": "Shuicheng Yan",
                        "slug": "Shuicheng-Yan",
                        "structuredName": {
                            "firstName": "Shuicheng",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuicheng Yan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689532"
                        ],
                        "name": "Yuxiao Hu",
                        "slug": "Yuxiao-Hu",
                        "structuredName": {
                            "firstName": "Yuxiao",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuxiao Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144973386"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 148
                            }
                        ],
                        "text": "\u2026atoms has seen a recent surge of interest [9], [10], [11], [12].1 Much of this excitement centers around the discovery that whenever the optimal representation is sufficiently sparse, it can be efficiently computed by convex optimization [9], even though this problem can be extremely difficult\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3147903,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "34860ead8af5cdb2550f2767eaacf71d4a5d0c80",
            "isKey": false,
            "numCitedBy": 3162,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an appearance-based face recognition method called the Laplacianface approach. By using locality preserving projections (LPP), the face images are mapped into a face subspace for analysis. Different from principal component analysis (PCA) and linear discriminant analysis (LDA) which effectively see only the Euclidean structure of face space, LPP finds an embedding that preserves local information, and obtains a face subspace that best detects the essential face manifold structure. The Laplacianfaces are the optimal linear approximations to the eigenfunctions of the Laplace Beltrami operator on the face manifold. In this way, the unwanted variations resulting from changes in lighting, facial expression, and pose may be eliminated or reduced. Theoretical analysis shows that PCA, LDA, and LPP can be obtained from different graph models. We compare the proposed Laplacianface approach with Eigenface and Fisherface methods on three different face data sets. Experimental results suggest that the proposed Laplacianface approach provides a better representation and achieves lower error rates in face recognition."
            },
            "slug": "Face-recognition-using-Laplacianfaces-He-Yan",
            "title": {
                "fragments": [],
                "text": "Face recognition using Laplacianfaces"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Experimental results suggest that the proposed Laplacianface approach provides a better representation and achieves lower error rates in face recognition."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144097660"
                        ],
                        "name": "M. Turk",
                        "slug": "M.-Turk",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Turk",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Turk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 147
                            }
                        ],
                        "text": "\u2026dictionary of base elements or signal atoms has seen a recent surge of interest [9], [10], [11], [12].1 Much of this excitement centers around the discovery that whenever the optimal representation is sufficiently sparse, it can be efficiently computed by convex optimization [9], even though\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 26127529,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6f1dfcc44277d4cfd8507284d994c9283dc3a2f",
            "isKey": false,
            "numCitedBy": 14954,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We have developed a near-real-time computer system that can locate and track a subject's head, and then recognize the person by comparing characteristics of the face to those of known individuals. The computational approach taken in this system is motivated by both physiology and information theory, as well as by the practical requirements of near-real-time performance and accuracy. Our approach treats the face recognition problem as an intrinsically two-dimensional (2-D) recognition problem rather than requiring recovery of three-dimensional geometry, taking advantage of the fact that faces are normally upright and thus may be described by a small set of 2-D characteristic views. The system functions by projecting face images onto a feature space that spans the significant variations among known face images. The significant features are known as \"eigenfaces,\" because they are the eigenvectors (principal components) of the set of faces; they do not necessarily correspond to features such as eyes, ears, and noses. The projection operation characterizes an individual face by a weighted sum of the eigenface features, and so to recognize a particular face it is necessary only to compare these weights to those of known individuals. Some particular advantages of our approach are that it provides for the ability to learn and later recognize new faces in an unsupervised manner, and that it is easy to implement using a neural network architecture."
            },
            "slug": "Eigenfaces-for-Recognition-Turk-Pentland",
            "title": {
                "fragments": [],
                "text": "Eigenfaces for Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A near-real-time computer system that can locate and track a subject's head, and then recognize the person by comparing characteristics of the face to those of known individuals, and that is easy to implement using a neural network architecture."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Cognitive Neuroscience"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794486"
                        ],
                        "name": "M. Savvides",
                        "slug": "M.-Savvides",
                        "structuredName": {
                            "firstName": "Marios",
                            "lastName": "Savvides",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Savvides"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2762416"
                        ],
                        "name": "R. Abiantun",
                        "slug": "R.-Abiantun",
                        "structuredName": {
                            "firstName": "Ramzi",
                            "lastName": "Abiantun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Abiantun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39533703"
                        ],
                        "name": "J. Heo",
                        "slug": "J.-Heo",
                        "structuredName": {
                            "firstName": "Jingu",
                            "lastName": "Heo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Heo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115524519"
                        ],
                        "name": "S. Park",
                        "slug": "S.-Park",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50437434"
                        ],
                        "name": "C. Xie",
                        "slug": "C.-Xie",
                        "structuredName": {
                            "firstName": "Chunyan",
                            "lastName": "Xie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Xie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145132210"
                        ],
                        "name": "B. Kumar",
                        "slug": "B.-Kumar",
                        "structuredName": {
                            "firstName": "B. V. K. Vijaya",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Kumar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 36081611,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0fab51acb8155ae67de337eb7141ec36ac8e50c2",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we investigate how to perform face recognition on the hardest experiment (Exp4) in Face Recognition Grand Challenge(FRGC) phase-II data which deals with subjects captured under uncontrolled conditions such as harsh overhead illumination, some pose variations and facial expressions in both indoor and outdoor environments. Other variations include the presence and absence of eye-glasses. The database consists of a generic dataset of 12,776 images for training a generic face subspace, a target set of 16,028 images and a query set of 8,014 images are given for matching. We propose to use our novel face recognition algorithm using Kernel Correlation Feature Analysis for dimensionality reduction (222 features) coupled with Support Vector Machine discriminative training in the Target KCFA feature set for providing a similarity distance measure of the probe to each target subject. We show that this algorithm configuration yields the best verification rate at 0.1% FAR (87.5%) compared to PCA+SVM, GSLDA+SVM, SVM+SVM, KDA+SVM. Thus we explore with our proposed algorithm which facial regions provide the best discrimination ability, we analyze performing partial face recognition using the eye-region, nose region and mouth region. We empirically find that the eye-region is the most discriminative feature of the faces in FRGC data and yields a verification rate closest to the holistic face recognition of 83.5% @ 0.1% FAR compared to 87.5%. We use Support Vector Machines for fusing these two to boost the performance to ~90@0.1 % FAR on the first large-scale face database such as the FRGC dataset."
            },
            "slug": "Partial-&-Holistic-Face-Recognition-on-FRGC-II-data-Savvides-Abiantun",
            "title": {
                "fragments": [],
                "text": "Partial & Holistic Face Recognition on FRGC-II data using Support Vector Machine"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is empirically found that the eye-region is the most discriminative feature of the faces in FRGC data and yields a verification rate closest to the holistic face recognition of 83.5% @ 0.1% FAR compared to 87.5%."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR Workshops"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780935"
                        ],
                        "name": "B. Moghaddam",
                        "slug": "B.-Moghaddam",
                        "structuredName": {
                            "firstName": "Baback",
                            "lastName": "Moghaddam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Moghaddam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738894"
                        ],
                        "name": "T. Starner",
                        "slug": "T.-Starner",
                        "structuredName": {
                            "firstName": "Thad",
                            "lastName": "Starner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Starner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 11
                            }
                        ],
                        "text": ", [16] and [57]) are less likely to succeed here due to the unpredictable location of the occlusion."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "A somewhat traditional approach (explored in [57] among others) to exploiting this information in face recognition is to partition the image into blocks and process each block independently."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 136280,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0bf5d558220d39698ce96d59ee5772e8e1a0663",
            "isKey": false,
            "numCitedBy": 2234,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe experiments with eigenfaces for recognition and interactive search in a large-scale face database. Accurate visual recognition is demonstrated using a database of O(10/sup 3/) faces. The problem of recognition under general viewing orientation is also examined. A view-based multiple-observer eigenspace technique is proposed for use in face recognition under variable pose. In addition, a modular eigenspace description technique is used which incorporates salient features such as the eyes, nose and mouth, in an eigenfeature layer. This modular representation yields higher recognition rates as well as a more robust framework for face recognition. An automatic feature extraction technique using feature eigentemplates is also demonstrated.<<ETX>>"
            },
            "slug": "View-based-and-modular-eigenspaces-for-face-Pentland-Moghaddam",
            "title": {
                "fragments": [],
                "text": "View-based and modular eigenspaces for face recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A modular eigenspace description technique is used which incorporates salient features such as the eyes, nose and mouth, in an eigenfeature layer, which yields higher recognition rates as well as a more robust framework for face recognition."
            },
            "venue": {
                "fragments": [],
                "text": "1994 Proceedings of IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39664966"
                        ],
                        "name": "C. Liu",
                        "slug": "C.-Liu",
                        "structuredName": {
                            "firstName": "Chengjun",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "distribution of features RAi does not become degenerate [42]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 142
                            }
                        ],
                        "text": "This has led to the development of a wide variety of increasingly complex feature extraction methods, including nonlinear and kernel features [42], [43]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 814756,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "084f1a6c62a3464b1a9b745fee40af2895920301",
            "isKey": false,
            "numCitedBy": 287,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a novel pattern recognition framework by capitalizing on dimensionality increasing techniques. In particular, the framework integrates Gabor image representation, a novel multiclass kernel Fisher analysis (KFA) method, and fractional power polynomial models for improving pattern recognition performance. Gabor image representation, which increases dimensionality by incorporating Gabor filters with different scales and orientations, is characterized by spatial frequency, spatial locality, and orientational selectivity for coping with image variabilities such as illumination variations. The KFA method first performs nonlinear mapping from the input space to a high-dimensional feature space, and then implements the multiclass Fisher discriminant analysis in the feature space. The significance of the nonlinear mapping is that it increases the discriminating power of the KFA method, which is linear in the feature space but nonlinear in the input space. The novelty of the KFA method comes from the fact that 1) it extends the two-class kernel Fisher methods by addressing multiclass pattern classification problems and 2) it improves upon the traditional generalized discriminant analysis (GDA) method by deriving a unique solution (compared to the GDA solution, which is not unique). The fractional power polynomial models further improve performance of the proposed pattern recognition framework. Experiments on face recognition using both the FERET database and the FRGC (face recognition grand challenge) databases show the feasibility of the proposed framework. In particular, experimental results using the FERET database show that the KFA method performs better than the GDA method and the fractional power polynomial models help both the KFA method and the GDA method improve their face recognition performance. Experimental results using the FRGC databases show that the proposed pattern recognition framework improves face recognition performance upon the BEE baseline algorithm and the LDA-based baseline algorithm by large margins."
            },
            "slug": "Capitalize-on-dimensionality-increasing-techniques-Liu",
            "title": {
                "fragments": [],
                "text": "Capitalize on dimensionality increasing techniques for improving face recognition grand challenge performance"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "A novel pattern recognition framework that integrates Gabor image representation, a novel multiclass kernel Fisher analysis (KFA) method, and fractional power polynomial models for improving pattern recognition performance is presented."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112440748"
                        ],
                        "name": "Ke Huang",
                        "slug": "Ke-Huang",
                        "structuredName": {
                            "firstName": "Ke",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ke Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735838"
                        ],
                        "name": "Selin Aviyente",
                        "slug": "Selin-Aviyente",
                        "structuredName": {
                            "firstName": "Selin",
                            "lastName": "Aviyente",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Selin Aviyente"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 72
                            }
                        ],
                        "text": "For feature extraction, we show that if sparsity in the recognition problem is properly harnessed, the choice of features is no longer critical."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1102037,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "63140301f88a0c5223f92afbf2acfec9e537f6be",
            "isKey": false,
            "numCitedBy": 682,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, application of sparse representation (factorization) of signals over an overcomplete basis (dictionary) for signal classification is discussed. Searching for the sparse representation of a signal over an overcomplete dictionary is achieved by optimizing an objective function that includes two terms: one that measures the signal reconstruction error and another that measures the sparsity. This objective function works well in applications where signals need to be reconstructed, like coding and denoising. On the other hand, discriminative methods, such as linear discriminative analysis (LDA), are better suited for classification tasks. However, discriminative methods are usually sensitive to corruption in signals due to lacking crucial properties for signal reconstruction. In this paper, we present a theoretical framework for signal classification with sparse representation. The approach combines the discrimination power of the discriminative methods with the reconstruction property and the sparsity of the sparse representation that enables one to deal with signal corruptions: noise, missing data and outliers. The proposed approach is therefore capable of robust classification with a sparse representation of signals. The theoretical results are demonstrated with signal classification tasks, showing that the proposed approach outperforms the standard discriminative methods and the standard sparse representation in the case of corrupted signals."
            },
            "slug": "Sparse-Representation-for-Signal-Classification-Huang-Aviyente",
            "title": {
                "fragments": [],
                "text": "Sparse Representation for Signal Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The proposed approach combines the discrimination power of the discriminative methods with the reconstruction property and the sparsity of the sparse representation that enables one to deal with signal corruptions: noise, missing data and outliers."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37895334"
                        ],
                        "name": "S. Fidler",
                        "slug": "S.-Fidler",
                        "structuredName": {
                            "firstName": "Sanja",
                            "lastName": "Fidler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fidler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2238343"
                        ],
                        "name": "D. Sko\u010daj",
                        "slug": "D.-Sko\u010daj",
                        "structuredName": {
                            "firstName": "Danijel",
                            "lastName": "Sko\u010daj",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Sko\u010daj"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732672"
                        ],
                        "name": "A. Leonardis",
                        "slug": "A.-Leonardis",
                        "structuredName": {
                            "firstName": "Ale\u0161",
                            "lastName": "Leonardis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Leonardis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In fact, the most popular approach to improving the robustness of feature-based methods is based on randomly sampling individual pixels [28], sometimes in conjunction with statistical techniques such as multivariate trimming [29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ", [29]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "b) Robustness to Occlusion: Occlusion poses a significant obstacle to robust, real-world face recognition [16], [28], [29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This performance exceeds the best known results on the AR dataset [29] to date."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 7847226,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fbaecde38bf43214778193d3728f8f33471b0a91",
            "isKey": true,
            "numCitedBy": 192,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Linear subspace methods that provide sufficient reconstruction of the data, such as PCA, offer an efficient way of dealing with missing pixels, outliers, and occlusions that often appear in the visual data. Discriminative methods, such as LDA, which, on the other hand, are better suited for classification tasks, are highly sensitive to corrupted data. We present a theoretical framework for achieving the best of both types of methods: an approach that combines the discrimination power of discriminative methods with the reconstruction property of reconstructive methods which enables one to work on subsets of pixels in images to efficiently detect and reject the outliers. The proposed approach is therefore capable of robust classification with a high-breakdown point. We also show that subspace methods, such as CCA, which are used for solving regression tasks, can be treated in a similar manner. The theoretical results are demonstrated on several computer vision tasks showing that the proposed approach significantly outperforms the standard discriminative methods in the case of missing pixels and images containing occlusions and outliers."
            },
            "slug": "Combining-reconstructive-and-discriminative-methods-Fidler-Sko\u010daj",
            "title": {
                "fragments": [],
                "text": "Combining reconstructive and discriminative subspace methods for robust classification and regression by subsampling"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work presents a theoretical framework for achieving the best of both types of methods: an approach that combines the discrimination power of discriminative methods with the reconstruction property of reconstructive methods which enables one to work on subsets of pixels in images to efficiently detect and reject the outliers."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2457452"
                        ],
                        "name": "Kuang-chih Lee",
                        "slug": "Kuang-chih-Lee",
                        "structuredName": {
                            "firstName": "Kuang-chih",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kuang-chih Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144850973"
                        ],
                        "name": "J. Ho",
                        "slug": "J.-Ho",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Ho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765887"
                        ],
                        "name": "D. Kriegman",
                        "slug": "D.-Kriegman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Kriegman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kriegman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2458350,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f5b5e5b70ef61b90a030dfc26815deb6846b57e",
            "isKey": false,
            "numCitedBy": 2294,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous work has demonstrated that the image variation of many objects (human faces in particular) under variable lighting can be effectively modeled by low-dimensional linear spaces, even when there are multiple light sources and shadowing. Basis images spanning this space are usually obtained in one of three ways: a large set of images of the object under different lighting conditions is acquired, and principal component analysis (PCA) is used to estimate a subspace. Alternatively, synthetic images are rendered from a 3D model (perhaps reconstructed from images) under point sources and, again, PCA is used to estimate a subspace. Finally, images rendered from a 3D model under diffuse lighting based on spherical harmonics are directly used as basis images. In this paper, we show how to arrange physical lighting so that the acquired images of each object can be directly used as the basis vectors of a low-dimensional linear space and that this subspace is close to those acquired by the other methods. More specifically, there exist configurations of k point light source directions, with k typically ranging from 5 to 9, such that, by taking k images of an object under these single sources, the resulting subspace is an effective representation for recognition under a wide range of lighting conditions. Since the subspace is generated directly from real images, potentially complex and/or brittle intermediate steps such as 3D reconstruction can be completely avoided; nor is it necessary to acquire large numbers of training images or to physically construct complex diffuse (harmonic) light fields. We validate the use of subspaces constructed in this fashion within the context of face recognition."
            },
            "slug": "Acquiring-linear-subspaces-for-face-recognition-Lee-Ho",
            "title": {
                "fragments": [],
                "text": "Acquiring linear subspaces for face recognition under variable lighting"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper shows how to arrange physical lighting so that the acquired images of each object can be directly used as the basis vectors of a low-dimensional linear space and that this subspace is close to those acquired by the other methods."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "98902637"
                        ],
                        "name": "Wen Zhao",
                        "slug": "Wen-Zhao",
                        "structuredName": {
                            "firstName": "Wen",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wen Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9215658"
                        ],
                        "name": "R. Chellappa",
                        "slug": "R.-Chellappa",
                        "structuredName": {
                            "firstName": "Rama",
                            "lastName": "Chellappa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Chellappa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145691986"
                        ],
                        "name": "P. Phillips",
                        "slug": "P.-Phillips",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Phillips",
                            "middleNames": [
                                "Jonathon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Phillips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143766793"
                        ],
                        "name": "A. Rosenfeld",
                        "slug": "A.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Azriel",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Rosenfeld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "A related, but simpler, measure of parsimony in high-dimensional data processing seeks models that depend on only a few of the observations, selecting a small subset of features for classification or visualization (e.g., Sparse PCA [3], [4] among others)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12331515,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "28312c3a47c1be3a67365700744d3d6665b86f22",
            "isKey": false,
            "numCitedBy": 6984,
            "numCiting": 418,
            "paperAbstract": {
                "fragments": [],
                "text": "As one of the most successful applications of image analysis and understanding, face recognition has recently received significant attention, especially during the past several years. At least two reasons account for this trend: the first is the wide range of commercial and law enforcement applications, and the second is the availability of feasible technologies after 30 years of research. Even though current machine recognition systems have reached a certain level of maturity, their success is limited by the conditions imposed by many real applications. For example, recognition of face images acquired in an outdoor environment with changes in illumination and/or pose remains a largely unsolved problem. In other words, current systems are still far away from the capability of the human perception system.This paper provides an up-to-date critical survey of still- and video-based face recognition research. There are two underlying motivations for us to write this survey paper: the first is to provide an up-to-date review of the existing literature, and the second is to offer some insights into the studies of machine recognition of faces. To provide a comprehensive survey, we not only categorize existing recognition techniques but also present detailed descriptions of representative methods within each category. In addition, relevant topics such as psychophysical studies, system evaluation, and issues of illumination and pose variation are covered."
            },
            "slug": "Face-recognition:-A-literature-survey-Zhao-Chellappa",
            "title": {
                "fragments": [],
                "text": "Face recognition: A literature survey"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper provides an up-to-date critical survey of still- and video-based face recognition research, and categorizes existing recognition techniques but also presents detailed descriptions of representative methods within each category."
            },
            "venue": {
                "fragments": [],
                "text": "CSUR"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "114388665"
                        ],
                        "name": "Bo-Gun Park",
                        "slug": "Bo-Gun-Park",
                        "structuredName": {
                            "firstName": "Bo-Gun",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bo-Gun Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2135837"
                        ],
                        "name": "Kyoung Mu Lee",
                        "slug": "Kyoung-Mu-Lee",
                        "structuredName": {
                            "firstName": "Kyoung",
                            "lastName": "Lee",
                            "middleNames": [
                                "Mu"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kyoung Mu Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153310963"
                        ],
                        "name": "Sang Uk Lee",
                        "slug": "Sang-Uk-Lee",
                        "structuredName": {
                            "firstName": "Sang",
                            "lastName": "Lee",
                            "middleNames": [
                                "Uk"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sang Uk Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 20
                            }
                        ],
                        "text": "The role of parsimony in human perception has also been strongly supported by studies of human vision."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10382247,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ebce277a2510701f1a2b77585e4d02c3ae32aebc",
            "isKey": false,
            "numCitedBy": 88,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a novel line feature-based face recognition algorithm. A face is represented by the face-ARG model, where all the geometric quantities and the structural information are encoded in an attributed relational graph (ARG) structure, then the partial ARG matching is done for matching face-ARG's. Experimental results demonstrate that the proposed algorithm is quite robust to various facial expression changes, varying illumination conditions and occlusion, even when a single sample per person is given."
            },
            "slug": "Face-recognition-using-face-ARG-matching-Park-Lee",
            "title": {
                "fragments": [],
                "text": "Face recognition using face-ARG matching"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Experimental results demonstrate that the proposed algorithm is quite robust to various facial expression changes, varying illumination conditions and occlusion, even when a single sample per person is given."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732672"
                        ],
                        "name": "A. Leonardis",
                        "slug": "A.-Leonardis",
                        "structuredName": {
                            "firstName": "Ale\u0161",
                            "lastName": "Leonardis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Leonardis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144746444"
                        ],
                        "name": "H. Bischof",
                        "slug": "H.-Bischof",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Bischof",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bischof"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 69
                            }
                        ],
                        "text": "Manuscript received 13 Aug. 2007; revised 18 Jan. 2008; accepted 20 Mar. 2008; published online 26 Mar. 2008."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8762812,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0039685527d20ee4a100837be9185892f1a4d8bf",
            "isKey": false,
            "numCitedBy": 330,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "The basic limitations of the standard appearance-based matching methods using eigenimages are nonrobust estimation of coefficients and inability to cope with problems related to outliers, occlusions, and varying background. In this paper we present a new approach which successfully solves these problems. The major novelty of our approach lies in the way the coefficients of the eigenimages are determined. Instead of computing the coefficients by a projection of the data onto the eigenimages, we extract them by a robust hypothesize-and-test paradigm using subsets of image points. Competing hypotheses are then subject to a selection procedure based on the Minimum Description Length principle. The approach enables us not only to reject outliers and to deal with occlusions but also to simultaneously use multiple classes of eigenimages."
            },
            "slug": "Robust-Recognition-Using-Eigenimages-Leonardis-Bischof",
            "title": {
                "fragments": [],
                "text": "Robust Recognition Using Eigenimages"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper presents a new approach which successfully solves problems related to outliers, occlusions, and varying background in appearance-based matching methods using eigenimages using a robust hypothesize-and-test paradigm."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753908"
                        ],
                        "name": "Michael Elad",
                        "slug": "Michael-Elad",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Elad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Elad"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 162
                            }
                        ],
                        "text": "In fact, if the columns of A are in general position, then whenever y 1\u20444 Ax for some x with less than m=2 nonzeros, x is the unique sparsest solution: x\u03020 1\u20444 x [33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5724741,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "17e7cca7e795d8ba1fa9d2c88bf2675c2d6ddfe8",
            "isKey": false,
            "numCitedBy": 2842,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Given a dictionary D = {dk} of vectors dk, we seek to represent a signal S as a linear combination S = \u2211k \u03b3(k)dk, with scalar coefficients \u03b3(k). In particular, we aim for the sparsest representation possible. In general, this requires a combinatorial optimization process. Previous work considered the special case where D is an overcomplete system consisting of exactly two orthobases and has shown that, under a condition of mutual incoherence of the two bases, and assuming that S has a sufficiently sparse representation, this representation is unique and can be found by solving a convex optimization problem: specifically, minimizing the \u21131 norm of the coefficients \u03b3\u0331. In this article, we obtain parallel results in a more general setting, where the dictionary D can arise from two or several bases, frames, or even less structured systems. We sketch three applications: separating linear features from planar ones in 3D data, noncooperative multiuser encoding, and identification of over-complete independent component models."
            },
            "slug": "Optimally-sparse-representation-in-general-via-\u21131-Donoho-Elad",
            "title": {
                "fragments": [],
                "text": "Optimally sparse representation in general (nonorthogonal) dictionaries via \u21131 minimization"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This article obtains parallel results in a more general setting, where the dictionary D can arise from two or several bases, frames, or even less structured systems, and sketches three applications: separating linear features from planar ones in 3D data, noncooperative multiuser encoding, and identification of over-complete independent component models."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2417905"
                        ],
                        "name": "H. Ellis",
                        "slug": "H.-Ellis",
                        "structuredName": {
                            "firstName": "Hadyn",
                            "lastName": "Ellis",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ellis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49912554"
                        ],
                        "name": "J. Shepherd",
                        "slug": "J.-Shepherd",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shepherd",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shepherd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116342692"
                        ],
                        "name": "G. Davies",
                        "slug": "G.-Davies",
                        "structuredName": {
                            "firstName": "Graham",
                            "lastName": "Davies",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Davies"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 260,
                                "start": 256
                            }
                        ],
                        "text": "Most Informative Face Regions for Recognition Experiments in human vision indicate that the eye and eyebrow region is most important for face recognition by humans; if the eyebrows are removed, even familiar faces become quite difficult to recognize [27], [28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 38816722,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "79082ee2d817d73e0efd54ba2ce64a7a2f95b9ad",
            "isKey": false,
            "numCitedBy": 653,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Three experiments are reported in which recognition of faces from whole faces or internal or external features was compared. In the first experiment, where the faces were of famous people, an advantage was found for identification from internal features. In the second experiment involving unfamiliar faces, however, no difference was found in recognition rates when subjects were given the internal or the external features. In a third experiment famous faces were presented and mixed with other famous faces for a recognition test. As in experiment 1, better recognition occurred from internal as compared with external features. It is argued that the internal representation for familiar faces may be qualitatively different from that for faces seen just once. In particular some advantage in feature saliency may accrue to the internal or \u2018expressive\u2019 features of familiar faces. The implications of these results are considered in relation to general theories of face perception and recognition."
            },
            "slug": "Identification-of-Familiar-and-Unfamiliar-Faces-and-Ellis-Shepherd",
            "title": {
                "fragments": [],
                "text": "Identification of Familiar and Unfamiliar Faces from Internal and External Features: Some Implications for Theories of Face Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is argued that the internal representation for familiar faces may be qualitatively different from that for faces seen just once and some advantage in feature saliency may accrue to the internal or \u2018expressive\u2019 features of familiar faces."
            },
            "venue": {
                "fragments": [],
                "text": "Perception"
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3342552"
                        ],
                        "name": "I. Ciocoiu",
                        "slug": "I.-Ciocoiu",
                        "structuredName": {
                            "firstName": "Iulian",
                            "lastName": "Ciocoiu",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Ciocoiu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", [16])."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14892725,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5597c8db1f24c0b1be09708a1d48ae140be263ba",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Occluded face recognition performances of four different parts-based representation approaches are presented. The methods under investigation are: a) nonnegative matrix factorization (NMF); b) local NMF (LNMF); c) independent components analysis (ICA); d) neural autoassociators (NA). A systematic comparative analysis is conducted in terms of distance metric used and number of selected features on AR and Olivetti databases. Simulation results indicate superior performances of specific design solutions over classical holistic approaches based on principal components analysis (PCA)."
            },
            "slug": "Occluded-face-recognition-using-parts-based-methods-Ciocoiu",
            "title": {
                "fragments": [],
                "text": "Occluded face recognition using parts-based representation methods"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Simulation results indicate superior performances of specific design solutions over classical holistic approaches based on principal components analysis (PCA) in face recognition performances of four different parts-based representation approaches."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2005 European Conference on Circuit Theory and Design, 2005."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35146156"
                        ],
                        "name": "M. Lades",
                        "slug": "M.-Lades",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Lades",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lades"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3189439"
                        ],
                        "name": "J. Vorbr\u00fcggen",
                        "slug": "J.-Vorbr\u00fcggen",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Vorbr\u00fcggen",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Vorbr\u00fcggen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682548"
                        ],
                        "name": "J. Buhmann",
                        "slug": "J.-Buhmann",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Buhmann",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Buhmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075388397"
                        ],
                        "name": "J. Lange",
                        "slug": "J.-Lange",
                        "structuredName": {
                            "firstName": "J\u00f6rg",
                            "lastName": "Lange",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lange"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704573"
                        ],
                        "name": "C. Malsburg",
                        "slug": "C.-Malsburg",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Malsburg",
                            "middleNames": [
                                "von",
                                "der"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Malsburg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3038814"
                        ],
                        "name": "R. W\u00fcrtz",
                        "slug": "R.-W\u00fcrtz",
                        "structuredName": {
                            "firstName": "Rolf",
                            "lastName": "W\u00fcrtz",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. W\u00fcrtz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34993891"
                        ],
                        "name": "W. Konen",
                        "slug": "W.-Konen",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Konen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Konen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": "Gabor wavelets [56] exhibit similar properties, since they are"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1266405,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4fb52984078d75ec5655962dc94dc7848182286b",
            "isKey": false,
            "numCitedBy": 2069,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "An object recognition system based on the dynamic link architecture, an extension to classical artificial neural networks (ANNs), is presented. The dynamic link architecture exploits correlations in the fine-scale temporal structure of cellular signals to group neurons dynamically into higher-order entities. These entities represent a rich structure and can code for high-level objects. To demonstrate the capabilities of the dynamic link architecture, a program was implemented that can recognize human faces and other objects from video images. Memorized objects are represented by sparse graphs, whose vertices are labeled by a multiresolution description in terms of a local power spectrum, and whose edges are labeled by geometrical distance vectors. Object recognition can be formulated as elastic graph matching, which is performed here by stochastic optimization of a matching cost function. The implementation on a transputer network achieved recognition of human faces and office objects from gray-level camera images. The performance of the program is evaluated by a statistical analysis of recognition results from a portrait gallery comprising images of 87 persons. >"
            },
            "slug": "Distortion-Invariant-Object-Recognition-in-the-Link-Lades-Vorbr\u00fcggen",
            "title": {
                "fragments": [],
                "text": "Distortion Invariant Object Recognition in the Dynamic Link Architecture"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "An object recognition system based on the dynamic link architecture, an extension to classical artificial neural networks (ANNs), is presented and the implementation on a transputer network achieved recognition of human faces and office objects from gray-level camera images."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Computers"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2807217"
                        ],
                        "name": "J. Sadr",
                        "slug": "J.-Sadr",
                        "structuredName": {
                            "firstName": "Javid",
                            "lastName": "Sadr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sadr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4145645"
                        ],
                        "name": "Izzat N. Jarudi",
                        "slug": "Izzat-N.-Jarudi",
                        "structuredName": {
                            "firstName": "Izzat",
                            "lastName": "Jarudi",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Izzat N. Jarudi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46597039"
                        ],
                        "name": "P. Sinha",
                        "slug": "P.-Sinha",
                        "structuredName": {
                            "firstName": "Pawan",
                            "lastName": "Sinha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Sinha"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This corroborates the results on humans [27]; eyes appear to be the most important feature for our algorithm as well."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Most Informative Face Regions for Recognition Experiments in human vision indicate that the eye and eyebrow region is most important for face recognition by humans; if the eyebrows are removed, even familiar faces become quite difficult to recognize [27], [28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6146145,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "51198ada840fb32dcdc9a7f0b94826a3ef576f79",
            "isKey": false,
            "numCitedBy": 283,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "A fundamental challenge in face recognition lies in determining which facial characteristics are important in the identification of faces. Several studies have indicated the significance of certain facial features in this regard, particularly internal ones such as the eyes and mouth. Surprisingly, however, one rather prominent facial feature has received little attention in this domain: the eyebrows. Past work has examined the role of eyebrows in emotional expression and nonverbal communication, as well as in facial aesthetics and sexual dimorphism. However, it has not been made clear whether the eyebrows play an important role in the identification of faces. Here, we report experimental results which suggest that for face recognition the eyebrows may be at least as influential as the eyes. Specifically, we find that the absence of eyebrows in familiar faces leads to a very large and significant disruption in recognition performance. In fact, a significantly greater decrement in face recognition is observed in the absence of eyebrows than in the absence of eyes. These results may have important implications for our understanding of the mechanisms of face recognition in humans as well as for the development of artificial face-recognition systems."
            },
            "slug": "The-Role-of-Eyebrows-in-Face-Recognition-Sadr-Jarudi",
            "title": {
                "fragments": [],
                "text": "The Role of Eyebrows in Face Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Experimental results suggest that for face recognition the eyebrows may be at least as influential as the eyes, and it is found that the absence of eyebrows in familiar faces leads to a very large and significant disruption in recognition performance."
            },
            "venue": {
                "fragments": [],
                "text": "Perception"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34679741"
                        ],
                        "name": "S. Li",
                        "slug": "S.-Li",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150152476"
                        ],
                        "name": "Juwei Lu",
                        "slug": "Juwei-Lu",
                        "structuredName": {
                            "firstName": "Juwei",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Juwei Lu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u00c7"
                    },
                    "intents": []
                }
            ],
            "corpusId": 5959423,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f570e77bc9a6e9362b7eec366e98448d96e4c5a4",
            "isKey": false,
            "numCitedBy": 582,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a novel classification method, called the nearest feature line (NFL), for face recognition. Any two feature points of the same class (person) are generalized by the feature line (FL) passing through the two points. The derived FL can capture more variations of face images than the original points and thus expands the capacity of the available database. The classification is based on the nearest distance from the query feature point to each FL. With a combined face database, the NFL error rate is about 43.7-65.4% of that of the standard eigenface method. Moreover, the NFL achieves the lowest error rate reported to date for the ORL face database."
            },
            "slug": "Face-recognition-using-the-nearest-feature-line-Li-Lu",
            "title": {
                "fragments": [],
                "text": "Face recognition using the nearest feature line method"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A novel classification method, called the nearest feature line (NFL), for face recognition, based on the nearest distance from the query feature point to each FL, which achieves the lowest error rate reported for the ORL face database."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34679741"
                        ],
                        "name": "S. Li",
                        "slug": "S.-Li",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761961"
                        ],
                        "name": "Xinwen Hou",
                        "slug": "Xinwen-Hou",
                        "structuredName": {
                            "firstName": "Xinwen",
                            "lastName": "Hou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinwen Hou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108698841"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758097"
                        ],
                        "name": "Qiansheng Cheng",
                        "slug": "Qiansheng-Cheng",
                        "structuredName": {
                            "firstName": "Qiansheng",
                            "lastName": "Cheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiansheng Cheng"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 2064887,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db53b8465a2d6b9dcba0a9c2ec7f10712c826306",
            "isKey": false,
            "numCitedBy": 853,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a novel method, called local non-negative matrix factorization (LNMF), for learning spatially localized, parts-based subspace representation of visual patterns. An objective function is defined to impose a localization constraint, in addition to the non-negativity constraint in the standard NMF. This gives a set of bases which not only allows a non-subtractive (part-based) representation of images but also manifests localized features. An algorithm is presented for the learning of such basic components. Experimental results are presented to compare LNMF with the NMF and PCA methods for face representation and recognition, which demonstrates advantages of LNMF."
            },
            "slug": "Learning-spatially-localized,-parts-based-Li-Hou",
            "title": {
                "fragments": [],
                "text": "Learning spatially localized, parts-based representation"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A novel method, called local non-negative matrix factorization (LNMF), for learning spatially localized, parts-based subspace representation of visual patterns, which gives a set of bases which not only allows a non-subtractive representation of images but also manifests localized features."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109611348"
                        ],
                        "name": "Haitao Wang",
                        "slug": "Haitao-Wang",
                        "structuredName": {
                            "firstName": "Haitao",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haitao Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34679741"
                        ],
                        "name": "S. Li",
                        "slug": "S.-Li",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146020071"
                        ],
                        "name": "Yangsheng Wang",
                        "slug": "Yangsheng-Wang",
                        "structuredName": {
                            "firstName": "Yangsheng",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yangsheng Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 208
                            }
                        ],
                        "text": "In face recognition, we actually do not need to know whether the linear structure is due to varying illumination or expression, since we do not rely on domain-specific knowledge such as an illumination model [31] to eliminate the variability in the training and testing images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1451602,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1c6be6874e150898d9db984dd546e9e85c85724e",
            "isKey": false,
            "numCitedBy": 149,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a unified framework for modeling intrinsic properties of face images for recognition. It is based on the quotient image (QI) concept, in particular on the existing works of QI, spherical harmonic, image ratio and retinex. Under this framework, we generalize these previous works into two new algorithms: (1) non-point light quotient image (NPL-QI) extends QI to deal with non-point light sources by modeling non-point light directions using spherical harmonic bases; (2) self-quotient image (S-QI) extends QI to perform illumination subtraction without the need for alignment and no shadow assumption. Experimental results show that our algorithms can significantly improve the performance of face recognition under varying illumination conditions."
            },
            "slug": "Generalized-quotient-image-Wang-Li",
            "title": {
                "fragments": [],
                "text": "Generalized quotient image"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Experimental results show that the algorithms presented can significantly improve the performance of face recognition under varying illumination conditions."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48345000"
                        ],
                        "name": "T. Ahonen",
                        "slug": "T.-Ahonen",
                        "structuredName": {
                            "firstName": "Timo",
                            "lastName": "Ahonen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ahonen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144979251"
                        ],
                        "name": "A. Hadid",
                        "slug": "A.-Hadid",
                        "structuredName": {
                            "firstName": "Abdenour",
                            "lastName": "Hadid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hadid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145962204"
                        ],
                        "name": "M. Pietik\u00e4inen",
                        "slug": "M.-Pietik\u00e4inen",
                        "structuredName": {
                            "firstName": "Matti",
                            "lastName": "Pietik\u00e4inen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pietik\u00e4inen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "Local Binary Patterns [55] and Gabor wavelets [56] exhibit similar properties, since they are also computed from local image regions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 369876,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e7c4665ce36a53484f8a7b7dfa821a9f6273eab4",
            "isKey": false,
            "numCitedBy": 5461,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a novel and efficient facial image representation based on local binary pattern (LBP) texture features. The face image is divided into several regions from which the LBP feature distributions are extracted and concatenated into an enhanced feature vector to be used as a face descriptor. The performance of the proposed method is assessed in the face recognition problem under different challenges. Other applications and several extensions are also discussed"
            },
            "slug": "Face-Description-with-Local-Binary-Patterns:-to-Ahonen-Hadid",
            "title": {
                "fragments": [],
                "text": "Face Description with Local Binary Patterns: Application to Face Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "This paper presents a novel and efficient facial image representation based on local binary pattern (LBP) texture features that is assessed in the face recognition problem under different challenges."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40375942"
                        ],
                        "name": "R. Zass",
                        "slug": "R.-Zass",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Zass",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zass"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3140335"
                        ],
                        "name": "A. Shashua",
                        "slug": "A.-Shashua",
                        "structuredName": {
                            "firstName": "Amnon",
                            "lastName": "Shashua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Shashua"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 107
                            }
                        ],
                        "text": "The usefulness of sparsity in detection has been noticed in the work of [17] and more recently explored in [31]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6179957,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d09f9b027cea21f443c331a9aad7bbb2b4ffa13",
            "isKey": false,
            "numCitedBy": 179,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a nonnegative variant of the \"Sparse PCA\" problem. The goal is to create a low dimensional representation from a collection of points which on the one hand maximizes the variance of the projected points and on the other uses only parts of the original coordinates, and thereby creating a sparse representation. What distinguishes our problem from other Sparse PCA formulations is that the projection involves only nonnegative weights of the original coordinates \u2014 a desired quality in various fields, including economics, bioinformatics and computer vision. Adding nonnegativity contributes to sparseness, where it enforces a partitioning of the original coordinates among the new axes. We describe a simple yet efficient iterative coordinate-descent type of scheme which converges to a local optimum of our optimization criteria, giving good results on large real world datasets."
            },
            "slug": "Nonnegative-Sparse-PCA-Zass-Shashua",
            "title": {
                "fragments": [],
                "text": "Nonnegative Sparse PCA"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A simple yet efficient iterative coordinate-descent type of scheme which converges to a local optimum of the optimization criteria, giving good results on large real world datasets."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14489533"
                        ],
                        "name": "D. Geiger",
                        "slug": "D.-Geiger",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Geiger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geiger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805102"
                        ],
                        "name": "Tyng-Luh Liu",
                        "slug": "Tyng-Luh-Liu",
                        "structuredName": {
                            "firstName": "Tyng-Luh",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tyng-Luh Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32498041"
                        ],
                        "name": "M. Donahue",
                        "slug": "M.-Donahue",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Donahue",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Donahue"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 72
                            }
                        ],
                        "text": "The usefulness of sparsity in detection has been noticed in the work in [61] and more recently explored in [62]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11497256,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d9a411221e81eb6043ac4d33c9c37b33da2cf72e",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "AbstractWe are given an image I and a library of templates \n$${\\mathcal{L}}$$\n , such that \n$${\\mathcal{L}}$$\n is an overcomplete basis for I. The templates can represent objects, faces, features, analytical functions, or be single pixel templates (canonical templates). There are infinitely many ways to decompose I as a linear combination of the library templates. Each decomposition defines a representation for the image I, given \n$${\\mathcal{L}}$$\n .What is an optimal representation for I given \n$${\\mathcal{L}}$$\n and how to select it? We are motivated to select a sparse/compact representation for I, and to account for occlusions and noise in the image. We present a concave cost function criterion on the linear decomposition coefficients that satisfies our requirements. More specifically, we study a \u201cweighted L norm\u201d with 0 < p < 1. We prove a result that allows us to generate all local minima for the L norm, and the global minimum is obtained by searching through the local ones. Due to the computational complexity, i.e., the large number of local minima, we also study a greedy and iterative \u201cweighted L Matching Pursuit\u201d strategy."
            },
            "slug": "Sparse-Representations-for-Image-Decompositions-Geiger-Liu",
            "title": {
                "fragments": [],
                "text": "Sparse Representations for Image Decompositions"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work is motivated to select a sparse/compact representation for I, and to account for occlusions and noise in the image, and presents a concave cost function criterion on the linear decomposition coefficients that satisfies the requirements."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144850973"
                        ],
                        "name": "J. Ho",
                        "slug": "J.-Ho",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Ho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715634"
                        ],
                        "name": "Ming-Hsuan Yang",
                        "slug": "Ming-Hsuan-Yang",
                        "structuredName": {
                            "firstName": "Ming-Hsuan",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Hsuan Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153239384"
                        ],
                        "name": "Jongwoo Lim",
                        "slug": "Jongwoo-Lim",
                        "structuredName": {
                            "firstName": "Jongwoo",
                            "lastName": "Lim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jongwoo Lim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2457452"
                        ],
                        "name": "Kuang-chih Lee",
                        "slug": "Kuang-chih-Lee",
                        "structuredName": {
                            "firstName": "Kuang-chih",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kuang-chih Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765887"
                        ],
                        "name": "D. Kriegman",
                        "slug": "D.-Kriegman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Kriegman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kriegman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u00c7"
                    },
                    "intents": []
                }
            ],
            "corpusId": 2667132,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "a94c7e602c2cfd85343a09662b90206f2494d239",
            "isKey": false,
            "numCitedBy": 541,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce two appearance-based methods for clustering a set of images of 3D (three-dimensional) objects, acquired under varying illumination conditions, into disjoint subsets corresponding to individual objects. The first algorithm is based on the concept of illumination cones. According to the theory, the clustering problem is equivalent to finding convex polyhedral cones in the high-dimensional image space. To efficiently determine the conic structures hidden in the image data, we introduce the concept of conic affinity, which measures the likelihood of a pair of images belonging to the same underlying polyhedral cone. For the second method, we introduce another affinity measure based on image gradient comparisons. The algorithm operates directly on the image gradients by comparing the magnitudes and orientations of the image gradient at each pixel. Both methods have clear geometric motivations, and they operate directly on the images without the need for feature extraction or computation of pixel statistics. We demonstrate experimentally that both algorithms are surprisingly effective in clustering images acquired under varying illumination conditions with two large, well-known image data sets."
            },
            "slug": "Clustering-appearances-of-objects-under-varying-Ho-Yang",
            "title": {
                "fragments": [],
                "text": "Clustering appearances of objects under varying illumination conditions"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "Two appearance-based methods for clustering a set of images of 3D (three-dimensional) objects into disjoint subsets corresponding to individual objects, based on the concept of illumination cones and another affinity measure based on image gradient comparisons are introduced."
            },
            "venue": {
                "fragments": [],
                "text": "2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46597039"
                        ],
                        "name": "P. Sinha",
                        "slug": "P.-Sinha",
                        "structuredName": {
                            "firstName": "Pawan",
                            "lastName": "Sinha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Sinha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46609120"
                        ],
                        "name": "B. Balas",
                        "slug": "B.-Balas",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Balas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Balas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7252638"
                        ],
                        "name": "Yuri Ostrovsky",
                        "slug": "Yuri-Ostrovsky",
                        "structuredName": {
                            "firstName": "Yuri",
                            "lastName": "Ostrovsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuri Ostrovsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144575094"
                        ],
                        "name": "Richard Russell",
                        "slug": "Richard-Russell",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Russell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard Russell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 92
                            }
                        ],
                        "text": "This is partly due to the remarkable face recognition capability of the human visual system [21] and partly due to numerous important applications for face recognition technology [22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": ", patches around eyes or nose) [21], [41] (see Fig."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2541311,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5c22cb54bd23f17289c31abb84baaf0cd439540",
            "isKey": false,
            "numCitedBy": 715,
            "numCiting": 206,
            "paperAbstract": {
                "fragments": [],
                "text": "A key goal of computer vision researchers is to create automated face recognition systems that can equal, and eventually surpass, human performance. To this end, it is imperative that computational researchers know of the key findings from experimental studies of face recognition by humans. These findings provide insights into the nature of cues that the human visual system relies upon for achieving its impressive performance and serve as the building blocks for efforts to artificially emulate these abilities. In this paper, we present what we believe are 19 basic results, with implications for the design of computational systems. Each result is described briefly and appropriate pointers are provided to permit an in-depth study of any particular result"
            },
            "slug": "Face-Recognition-by-Humans:-Nineteen-Results-All-Sinha-Balas",
            "title": {
                "fragments": [],
                "text": "Face Recognition by Humans: Nineteen Results All Computer Vision Researchers Should Know About"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Findings from experimental studies of face recognition by humans provide insights into the nature of cues that the human visual system relies upon for achieving its impressive performance and serve as the building blocks for efforts to artificially emulate these abilities."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14208692,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "2805537bec87a6177037b18f9a3a9d3f1038867b",
            "isKey": false,
            "numCitedBy": 3574,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Sparse-coding-with-an-overcomplete-basis-set:-A-by-Olshausen-Field",
            "title": {
                "fragments": [],
                "text": "Sparse coding with an overcomplete basis set: A strategy employed by V1?"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152684324"
                        ],
                        "name": "T. Serre",
                        "slug": "T.-Serre",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Serre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Serre"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Investigators have recently revealed that in both low-level and midlevel human vision [7], [ 8 ], many neurons in the visual pathway are selective for a variety of specific stimuli, such as color, texture, orientation, scale, and even viewtuned object images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 19261154,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "e03bda45248b4169e2a20cb9124ae60440cad2de",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 449,
            "paperAbstract": {
                "fragments": [],
                "text": "In this thesis, I describe a quantitative model that accounts for the circuits and computations of the feedforward path of the ventral stream of visual cortex. This model is consistent with a general theory of visual processing that extends the hierarchical model of [Hubel and Wiesel, 1959] from primary to extrastriate visual areas. It attempts to explain the first few hundred milliseconds of visual processing and \"immediate recognition\". One of the key elements in the approach is the learning of a generic dictionary of shape-components from V2 to IT, which provides an invariant representation to task-specific categorization circuits in higher brain areas. This vocabulary of shape-tuned units is learned in an unsupervised manner from natural images, and constitutes a large and redundant set of image features with different complexities and invariances. This theory significantly extends an earlier approach by [Riesenhuber and Poggio, 1999a] and builds upon several existing neurobiological models and conceptual proposals. \nFirst, I present evidence to show that the model can duplicate the tuning properties of neurons in various brain areas (e.g., V1, V4 and IT). In particular, the model agrees with data from V4 about the response of neurons to combinations of simple two-bar stimuli [Reynolds et al., 1999] (within the receptive field of the S2 units) and some of the C2 units in the model show a tuning for boundary conformations which is consistent with recordings from V4 [Pasupathy and Connor, 2001]. Second, I show that not only can the model duplicate the tuning properties of neurons in various brain areas when probed with artificial stimuli, but it can also handle the recognition of objects in the real-world, to the extent of competing with the best computer vision systems. Third, I describe a comparison between the performance of the model and the performance of human observers in a rapid animal vs. non-animal recognition task for which recognition is fast and cortical back-projections are likely to be inactive. Results indicate that the model predicts human performance extremely well when the delay between the stimulus and the mask is about 50 ms. This suggests that cortical back-projections may not play a significant role when the time interval is in this range, and the model may therefore provide a satisfactory description of the feedforward path. \nTaken together, the evidences suggest that we may have the skeleton of a successful theory of visual cortex. In addition, this may be the first time that a neurobiological model, faithful to the physiology and the anatomy of visual cortex, not only competes with some of the best computer vision systems thus providing a realistic alternative to engineered artificial vision systems, but also achieves performance close to that of humans in a categorization task involving complex natural images. (Copies available exclusively from MIT Libraries, Rm. 14-0551, Cambridge, MA 02139-4307. Ph. 617-253-5668; Fax 617-253-1690.)"
            },
            "slug": "Learning-a-dictionary-of-shape-components-in-visual-Poggio-Serre",
            "title": {
                "fragments": [],
                "text": "Learning a dictionary of shape-components in visual cortex: comparison with neurons, humans and machines"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This may be the first time that a neurobiological model, faithful to the physiology and the anatomy of visual cortex, not only competes with some of the best computer vision systems thus providing a realistic alternative to engineered artificial vision systems, but also achieves performance close to that of humans in a categorization task involving complex natural images."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144908066"
                        ],
                        "name": "Richard Baraniuk",
                        "slug": "Richard-Baraniuk",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Baraniuk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard Baraniuk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686007"
                        ],
                        "name": "M. Wakin",
                        "slug": "M.-Wakin",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Wakin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wakin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 186
                            }
                        ],
                        "text": "Random projection has been previously studied as a general dimensionality-reduction method for numerous clustering problems [47], [48], [49], as well as for learning nonlinear manifolds [50], [51]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11001398,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "202b93afcd3f7bc17bfb305e52bd171ea196760b",
            "isKey": false,
            "numCitedBy": 423,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract\nWe propose a new approach for nonadaptive dimensionality reduction of manifold-modeled data, demonstrating that a small number of random linear projections can preserve key information about a manifold-modeled signal. We center our analysis on the effect of a random linear projection operator \u03a6:\u211dN\u2192\u211dM, M<N, on a smooth well-conditioned K-dimensional submanifold \u2133\u2282\u211dN. As our main theoretical contribution, we establish a sufficient number M of random projections to guarantee that, with high probability, all pairwise Euclidean and geodesic distances between points on \u2133 are well preserved under the mapping \u03a6.\n\nOur results bear strong resemblance to the emerging theory of Compressed Sensing (CS), in which sparse signals can be recovered from small numbers of random linear measurements. As in CS, the random measurements we propose can be used to recover the original data in \u211dN. Moreover, like the fundamental bound in CS, our requisite M is linear in the \u201cinformation level\u201d K and logarithmic in the ambient dimension N; we also identify a logarithmic dependence on the volume and conditioning of the manifold. In addition to recovering faithful approximations to manifold-modeled signals, however, the random projections we propose can also be used to discern key properties about the manifold. We discuss connections and contrasts with existing techniques in manifold learning, a setting where dimensionality reducing mappings are typically nonlinear and constructed adaptively from a set of sampled training data.\n"
            },
            "slug": "Random-Projections-of-Smooth-Manifolds-Baraniuk-Wakin",
            "title": {
                "fragments": [],
                "text": "Random Projections of Smooth Manifolds"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A new approach for nonadaptive dimensionality reduction of manifold-modeled data is proposed, demonstrating that a small number of random linear projections can preserve key information about a manifold- modeled signal."
            },
            "venue": {
                "fragments": [],
                "text": "Found. Comput. Math."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727797"
                        ],
                        "name": "S. Chen",
                        "slug": "S.-Chen",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Chen",
                            "middleNames": [
                                "Saobing"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145621255"
                        ],
                        "name": "M. Saunders",
                        "slug": "M.-Saunders",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Saunders",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Saunders"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "Systems based on conventional classifiers such as NN or NS, often use the residuals ri\u00f0y\u00de for validation, in addition to identification."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2429822,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9af121fbed84c3484ab86df8f17f1f198ed790a0",
            "isKey": false,
            "numCitedBy": 9739,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "The time-frequency and time-scale communities have recently developed a large number of overcomplete waveform dictionaries --- stationary wavelets, wavelet packets, cosine packets, chirplets, and warplets, to name a few. Decomposition into overcomplete systems is not unique, and several methods for decomposition have been proposed, including the method of frames (MOF), Matching pursuit (MP), and, for special dictionaries, the best orthogonal basis (BOB). \nBasis Pursuit (BP) is a principle for decomposing a signal into an \"optimal\" superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions. We give examples exhibiting several advantages over MOF, MP, and BOB, including better sparsity and superresolution. BP has interesting relations to ideas in areas as diverse as ill-posed problems, in abstract harmonic analysis, total variation denoising, and multiscale edge denoising. \nBP in highly overcomplete dictionaries leads to large-scale optimization problems. With signals of length 8192 and a wavelet packet dictionary, one gets an equivalent linear program of size 8192 by 212,992. Such problems can be attacked successfully only because of recent advances in linear programming by interior-point methods. We obtain reasonable success with a primal-dual logarithmic barrier method and conjugate-gradient solver."
            },
            "slug": "Atomic-Decomposition-by-Basis-Pursuit-Chen-Donoho",
            "title": {
                "fragments": [],
                "text": "Atomic Decomposition by Basis Pursuit"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Basis Pursuit (BP) is a principle for decomposing a signal into an \"optimal\" superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Sci. Comput."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13490933"
                        ],
                        "name": "E. Bingham",
                        "slug": "E.-Bingham",
                        "structuredName": {
                            "firstName": "Ella",
                            "lastName": "Bingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Bingham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712654"
                        ],
                        "name": "H. Mannila",
                        "slug": "H.-Mannila",
                        "structuredName": {
                            "firstName": "Heikki",
                            "lastName": "Mannila",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Mannila"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 136
                            }
                        ],
                        "text": "Random projection has been previously studied as a general dimensionality-reduction method for numerous clustering problems [47], [48], [49], as well as for learning nonlinear manifolds [50], [51]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1854295,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3c58166098c07f2efe30651446a0f4f19b9b7ce9",
            "isKey": false,
            "numCitedBy": 1362,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Random projections have recently emerged as a powerful method for dimensionality reduction. Theoretical results indicate that the method preserves distances quite nicely; however, empirical results are sparse. We present experimental results on using random projection as a dimensionality reduction tool in a number of cases, where the high dimensionality of the data would otherwise lead to burden-some computations. Our application areas are the processing of both noisy and noiseless images, and information retrieval in text documents. We show that projecting the data onto a random lower-dimensional subspace yields results comparable to conventional dimensionality reduction methods such as principal component analysis: the similarity of data vectors is preserved well under random projection. However, using random projections is computationally significantly less expensive than using, e.g., principal component analysis. We also show experimentally that using a sparse random matrix gives additional computational savings in random projection."
            },
            "slug": "Random-projection-in-dimensionality-reduction:-to-Bingham-Mannila",
            "title": {
                "fragments": [],
                "text": "Random projection in dimensionality reduction: applications to image and text data"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It is shown that projecting the data onto a random lower-dimensional subspace yields results comparable to conventional dimensionality reduction methods such as principal component analysis: the similarity of data vectors is preserved well under random projection."
            },
            "venue": {
                "fragments": [],
                "text": "KDD '01"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145691986"
                        ],
                        "name": "P. Phillips",
                        "slug": "P.-Phillips",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Phillips",
                            "middleNames": [
                                "Jonathon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Phillips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067993"
                        ],
                        "name": "W. T. Scruggs",
                        "slug": "W.-T.-Scruggs",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Scruggs",
                            "middleNames": [
                                "Todd"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. T. Scruggs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400601690"
                        ],
                        "name": "A. O'Toole",
                        "slug": "A.-O'Toole",
                        "structuredName": {
                            "firstName": "Alice",
                            "lastName": "O'Toole",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. O'Toole"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704876"
                        ],
                        "name": "P. Flynn",
                        "slug": "P.-Flynn",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Flynn",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Flynn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144601844"
                        ],
                        "name": "K. W. Boyer",
                        "slug": "K.-W.-Boyer",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Boyer",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. W. Boyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153531021"
                        ],
                        "name": "Cathy L. Schott",
                        "slug": "Cathy-L.-Schott",
                        "structuredName": {
                            "firstName": "Cathy",
                            "lastName": "Schott",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cathy L. Schott"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055055507"
                        ],
                        "name": "Matthew Sharpe",
                        "slug": "Matthew-Sharpe",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Sharpe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew Sharpe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 58409227,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "189a7b14efb53fc0e66aea1ac005782b8a073e38",
            "isKey": false,
            "numCitedBy": 460,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This report describes the large-scale experimental results from the Face Recognition Vendor Test (FRVT) 2006 and the Iris Challenge Evaluation (ICE) 2006. The FRVT 2006 looks at recognition from high-resolution still images and three-dimensional (3D) face images, and measures performance for still images taken under controlled and uncontrolled illumination. The ICE 2006 reports iris recognition performance from left and right iris images. The FRVT 2006 results from controlled still images and 3D images document an order-of-magnitude improvement in recognition performance over the FRVT 2002. This order-of-magnitude improvement was one of the goals of the preceding technology development effort, the Face Recognition Grand Challenge (FRGC). The FRVT 2006 and the ICE 2006 compared recognition performance from very-high resolution still face images, 3D face images, and single-iris images. On the FRVT 2006 and the ICE 2006 datasets, recognition performance was comparable for all three biometrics. In an experiment comparing human and algorithm performance, the best-performing face recognition algorithms were more accurate than humans. These and other results are discussed in detail. \u2217Please direct correspondence to P. Jonathon Phillips at jonathon@nist.gov. We acknowledge the support of Department of Homeland Security\u2019s Science and Technology Department and Transportation Security Administration (TSA), the Director of National Intelligence\u2019s Information Technology Innovation Center, the Federal Bureau of Investigation (FBI), the National Institute of Justice, and the Technical Support Working Group (TSWG). The identification of any commercial product or trade name does not imply endorsement or recommendation by the National Institute of Standards and Technology, SAIC, Schafer Corp., U. of Texas at Dallas or U. of Notre Dame. 1"
            },
            "slug": "FRVT-2006-and-ICE-2006-large-scale-results-Phillips-Scruggs",
            "title": {
                "fragments": [],
                "text": "FRVT 2006 and ICE 2006 large-scale results"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "On the FRVT 2006 and the ICE 2006 datasets, recognition performance was comparable for all three biometrics and the best-performing face recognition algorithms were more accurate than humans."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11959218,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "577d19a115f9ef6f002483fcf88adbb3b5479556",
            "isKey": false,
            "numCitedBy": 7619,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Independent-component-analysis:-algorithms-and-Hyv\u00e4rinen-Oja",
            "title": {
                "fragments": [],
                "text": "Independent component analysis: algorithms and applications"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144770610"
                        ],
                        "name": "J. Tanner",
                        "slug": "J.-Tanner",
                        "structuredName": {
                            "firstName": "Jared",
                            "lastName": "Tanner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tanner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 263,
                                "start": 259
                            }
                        ],
                        "text": "The results corroborate the theory of compressed sensing: (18) suggests that d 128 random linear measurements should suffice for sparse recovery in the Yale database, while d 88 random linear measurements should suffice for sparse recovery in the AR database [44]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 130
                            }
                        ],
                        "text": "d 2t log\u00f0n=d\u00de \u00f018\u00de random linear measurements are sufficient for \u2018(1)-minimization (17) to recover the correct sparse solution x0 [44]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3350404,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0be0febd785b832d9a90945b25d8a9372b3f7b7f",
            "isKey": false,
            "numCitedBy": 418,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "1.1. Three surprises of high dimensions. This paper develops asymptotic methods to count faces of random high-dimensional polytopes; a seemingly dry and unpromising pursuit. Yet our conclusions have surprising implications - in statistics, probability, information theory, and signal processing - with potential impacts in practical subjects like medical imaging and digital communications. Before involving the reader in our lengthy analysis of high-dimensional face counting, we describe three implications of our results. 1.1.1. Convex Hulls of Gaussian Point Clouds. Consider a random point cloud of n points xi, i = 1, . . . , n, sampled independently and identically from a Gaussian distribution in R d with nonsingular covariance. This is a standard model of multivariate data; its properties are increasingly important in a wide range of applications. At the same time, it is an attractive and in some sense timeless object for theoretical study. Properties of the convex hull of the random point cloud X = {xi} have attracted interest for several decades, increasingly so in recent years; there is a nowvoluminous literature on the subject. The results could be significant for understanding outlier detection, or classification problems in machine learning."
            },
            "slug": "Counting-faces-of-randomly-projected-polytopes-when-Donoho-Tanner",
            "title": {
                "fragments": [],
                "text": "Counting faces of randomly-projected polytopes when the projection radically lowers dimension"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "This paper develops asymptotic methods to count faces of random high-dimensional polytopes; a seemingly dry and unpromising pursuit that has surprising implications in statistics, probability, information theory, and signal processing with potential impacts in practical subjects like medical imaging and digital communications."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145691986"
                        ],
                        "name": "P. Phillips",
                        "slug": "P.-Phillips",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Phillips",
                            "middleNames": [
                                "Jonathon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Phillips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067993"
                        ],
                        "name": "W. T. Scruggs",
                        "slug": "W.-T.-Scruggs",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Scruggs",
                            "middleNames": [
                                "Todd"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. T. Scruggs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400601690"
                        ],
                        "name": "A. O'Toole",
                        "slug": "A.-O'Toole",
                        "structuredName": {
                            "firstName": "Alice",
                            "lastName": "O'Toole",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. O'Toole"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704876"
                        ],
                        "name": "P. Flynn",
                        "slug": "P.-Flynn",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Flynn",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Flynn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143759604"
                        ],
                        "name": "K. Bowyer",
                        "slug": "K.-Bowyer",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Bowyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Bowyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153531021"
                        ],
                        "name": "Cathy L. Schott",
                        "slug": "Cathy-L.-Schott",
                        "structuredName": {
                            "firstName": "Cathy",
                            "lastName": "Schott",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cathy L. Schott"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055055507"
                        ],
                        "name": "Matthew Sharpe",
                        "slug": "Matthew-Sharpe",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Sharpe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew Sharpe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2693161,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "e67981b9454f240df38c0a228be75fb187d6a392",
            "isKey": false,
            "numCitedBy": 392,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the large-scale experimental results from the Face Recognition Vendor Test (FRVT) 2006 and the Iris Challenge Evaluation (ICE) 2006. The FRVT 2006 looked at recognition from high-resolution still frontal face images and 3D face images, and measured performance for still frontal face images taken under controlled and uncontrolled illumination. The ICE 2006 evaluation reported verification performance for both left and right irises. The images in the ICE 2006 intentionally represent a broader range of quality than the ICE 2006 sensor would normally acquire. This includes images that did not pass the quality control software embedded in the sensor. The FRVT 2006 results from controlled still and 3D images document at least an order-of-magnitude improvement in recognition performance over the FRVT 2002. The FRVT 2006 and the ICE 2006 compared recognition performance from high-resolution still frontal face images, 3D face images, and the single-iris images. On the FRVT 2006 and the ICE 2006 data sets, recognition performance was comparable for high-resolution frontal face, 3D face, and the iris images. In an experiment comparing human and algorithms on matching face identity across changes in illumination on frontal face images, the best performing algorithms were more accurate than humans on unfamiliar faces."
            },
            "slug": "FRVT-2006-and-ICE-2006-Large-Scale-Experimental-Phillips-Scruggs",
            "title": {
                "fragments": [],
                "text": "FRVT 2006 and ICE 2006 Large-Scale Experimental Results"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "On the FRVT 2006 and the ICE 2006 data sets, recognition performance was comparable for high-resolution frontal face, 3D face, and the iris images and the best performing algorithms were more accurate than humans on unfamiliar faces."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14241443,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "626703b4b5d8f2188ec53d82d8cb9e6868edc145",
            "isKey": false,
            "numCitedBy": 278,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Consider a d \u00d7 n matrix A, with d < n. The problem of solving for x in y = Ax is underdetermined, and has many possible solutions (if there are any). In several fields it is of interest to find the sparsest solution \u2013 the one with fewest nonzeros \u2013 but in general this involves combinatorial optimization. Let ai denote the i-th column of A, 1 \u2264 i \u2264 n. Associate to A the quotient polytope P formed by taking the convex hull of the 2n points (\u00b1ai) in R. P is centrosymmetric and is called (centrally) k-neighborly if every subset of k + 1 elements (\u00b1ilail) k+1 l=1 are the vertices of a face of P . We show that if P is k-neighborly, then if a system y = Ax has a solution with at most k nonzeros, that solution is also the unique solution of the convex optimization problem min \u2016x\u20161 subject to y = Ax; the converse holds as well. This complete equivalence between the study of sparse solution by ` minimization and neighborliness of convex polytopes immediately gives new results in each field. On the one hand, we get new families of neighborly centrosymmetric polytopes, by exploiting known results about sparsity of ` minimization; on the other, we get new limits on the ability of ` minimization to find sparse solutions, by exploiting known limits on neighborliness of centrally symmetric polytopes. Weaker notions of equivalence between ` and sparse optimization have also been studied recently. These are equivalent to other interesting properties of the quotient polytope. Thus, suppose the columns of A are in general position. Consider the vectors having k < d/2 nonzeros that are simultaneously the sparsest solution of y = Ax and the minimal ` solution. These make up a fraction 1 \u2212 of all vectors with k nonzeros if and only if the quotient polytope P has at least 1\u2212 as many k-dimensional faces as the regular cross polytope C. Combining this with recent work on face numbers of randomly-projected cross-polytopes, we learn that for large d, the overwhelming majority of systems of linear equations with d equations and 4d/3 unknowns have the following property: if there is a solution with fewer than .49d nonzeros, it is the unique minimum ` solution. A stylized application in digital communication is sketched; for large n, it is possible to transmit n/4 pieces of information using a codeword of length n with immunity to .49n gross errors in the received codeword, if the signs and sites of the gross errors are random, and with immunity to .11n gross errors chosen by a malicious opponent. The receiver uses ` minimization."
            },
            "slug": "Neighborly-Polytopes-And-Sparse-Solution-Of-Linear-Donoho",
            "title": {
                "fragments": [],
                "text": "Neighborly Polytopes And Sparse Solution Of Underdetermined Linear Equations"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "For large d, the overwhelming majority of systems of linear equations with d equations and 4d/3 unknowns have the following property: if there is a solution with fewer than .49d nonzeros, it is the unique minimum ` solution."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2293280"
                        ],
                        "name": "Y. Tsaig",
                        "slug": "Y.-Tsaig",
                        "structuredName": {
                            "firstName": "Yaakov",
                            "lastName": "Tsaig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Tsaig"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 127
                            }
                        ],
                        "text": "For example, homotopy algorithms recover solutions with t nonzeros in O(t(3) + n) time, linear in the size of the training set [35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 153
                            }
                        ],
                        "text": "One should distinguish the Lasso optimization problem from the LARS algorithm, which provably solves some instances of Lasso with very sparse optimizers [35]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 225064229,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a64a04c76bc411b57018927e950a5310b42c8125",
            "isKey": false,
            "numCitedBy": 153,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "The minimum -norm solution to an underdetermined system of linear equations is often, remarkably, also the sparsest solution to that system. This sparsity-seeking property is of interest in signal processing and information transmission. However, general-purpose optimizers are much too slow for minimization in many large-scale applications. In this paper, the Homotopy method, originally proposed by Osborne et al. and Efron et al., is applied to the underdetermined -minimization problem subject to . Homotopy is shown to run much more rapidly than general-purpose LP solvers when sufficient sparsity is present. Indeed, the method often has the following -step solution property: if the underlying solution has only nonzeros, the Homotopy method reaches that solution in only iterative steps. This -step solution property is demonstrated for several ensembles of matrices, including incoherent matrices, uniform spherical matrices, and partial orthogonal matrices. These results imply that Homotopy may be used to rapidly decode error-correcting codes in a stylized communication system with a computational budget constraint. The approach also sheds light on the evident parallelism in results on minimization and Orthogonal Matching Pursuit (OMP), and aids in explaining the inherent relations between HOMOTOPY, Least Angle Regression (LARS), OMP, and polytope faces pursuit."
            },
            "slug": "Fast-Solution-of-Norm-Minimization-Problems-When-Be-Donoho-Tsaig",
            "title": {
                "fragments": [],
                "text": "Fast Solution of -Norm Minimization Problems When the Solution May Be Sparse"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Homotopy is shown to run much more rapidly than general-purpose LP solvers when sufficient sparsity is present, implying that Homotopy may be used to rapidly decode error-correcting codes in a stylized communication system with a computational budget constraint."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123423036"
                        ],
                        "name": "E.J. Candes",
                        "slug": "E.J.-Candes",
                        "structuredName": {
                            "firstName": "E.J.",
                            "lastName": "Candes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E.J. Candes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u00c7"
                    },
                    "intents": []
                }
            ],
            "corpusId": 2269521,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f14a338e8837fae059cab41064155cd84cb9cd5",
            "isKey": false,
            "numCitedBy": 2608,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "Conventional wisdom and common practice in acquisition and reconstruction of images from frequency data follow the basic principle of the Nyquist density sampling theory. This principle states that to reconstruct an image, the number of Fourier samples we need to acquire must match the desired resolution of the image, i.e. the number of pixels in the image. This paper surveys an emerging theory which goes by the name of \u201ccompressive sampling\u201d or \u201ccompressed sensing,\u201d and which says that this conventional wisdom is inaccurate. Perhaps surprisingly, it is possible to reconstruct images or signals of scientific interest accurately and sometimes even exactly from a number of samples which is far smaller than the desired resolution of the image/signal, e.g. the number of pixels in the image. It is believed that compressive sampling has far reaching implications. For example, it suggests the possibility of new data acquisition protocols that translate analog information into digital form with fewer sensors than what was considered necessary. This new sampling theory may come to underlie procedures for sampling and compressing data simultaneously. In this short survey, we provide some of the key mathematical insights underlying this new theory, and explain some of the interactions between compressive sampling and other fields such as statistics, information theory, coding theory, and theoretical computer science. Mathematics Subject Classification (2000). Primary 00A69, 41-02, 68P30; Secondary 62C65."
            },
            "slug": "Compressive-sampling-Candes",
            "title": {
                "fragments": [],
                "text": "Compressive sampling"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Some of the key mathematical insights underlying this new sampling theory are provided, and some of the interactions between compressive sampling and other fields such as statistics, information theory, coding theory, and theoretical computer science are explained."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711144"
                        ],
                        "name": "Samuel Kaski",
                        "slug": "Samuel-Kaski",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Kaski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samuel Kaski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13803457,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0e2987872ddc1ba9516aed3f85f8b4b9cf0125fb",
            "isKey": false,
            "numCitedBy": 435,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "When the data vectors are high-dimensional it is computationally infeasible to use data analysis or pattern recognition algorithms which repeatedly compute similarities or distances in the original data space. It is therefore necessary to reduce the dimensionality before, for example, clustering the data. If the dimensionality is very high, like in the WEBSOM method which organizes textual document collections on a self-organizing map, then even the commonly used dimensionality reduction methods like the principal component analysis may be too costly. It is demonstrated that the document classification accuracy obtained after the dimensionality has been reduced using a random mapping method will be almost as good as the original accuracy if the final dimensionality is sufficiently large (about 100 out of 6000). In fact, it can be shown that the inner product (similarity) between the mapped vectors follows closely the inner product of the original vectors."
            },
            "slug": "Dimensionality-reduction-by-random-mapping:-fast-Kaski",
            "title": {
                "fragments": [],
                "text": "Dimensionality reduction by random mapping: fast similarity computation for clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is demonstrated that the document classification accuracy obtained after the dimensionality has been reduced using a random mapping method will be almost as good as the original accuracy if the final dimensionality is sufficiently large."
            },
            "venue": {
                "fragments": [],
                "text": "1998 IEEE International Joint Conference on Neural Networks Proceedings. IEEE World Congress on Computational Intelligence (Cat. No.98CH36227)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2006869"
                        ],
                        "name": "E. Cand\u00e8s",
                        "slug": "E.-Cand\u00e8s",
                        "structuredName": {
                            "firstName": "Emmanuel",
                            "lastName": "Cand\u00e8s",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Cand\u00e8s"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50383069"
                        ],
                        "name": "T. Tao",
                        "slug": "T.-Tao",
                        "structuredName": {
                            "firstName": "Terence",
                            "lastName": "Tao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Tao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1431305,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a898ad13c96e5c068a2e4fc88227278e646b712e",
            "isKey": false,
            "numCitedBy": 6467,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "Suppose we are given a vector f in a class FsubeRopf<sup>N </sup>, e.g., a class of digital signals or digital images. How many linear measurements do we need to make about f to be able to recover f to within precision epsi in the Euclidean (lscr<sub>2</sub>) metric? This paper shows that if the objects of interest are sparse in a fixed basis or compressible, then it is possible to reconstruct f to within very high accuracy from a small number of random measurements by solving a simple linear program. More precisely, suppose that the nth largest entry of the vector |f| (or of its coefficients in a fixed basis) obeys |f|<sub>(n)</sub>lesRmiddotn<sup>-1</sup>p/, where R>0 and p>0. Suppose that we take measurements y<sub>k</sub>=langf<sup># </sup>,X<sub>k</sub>rang,k=1,...,K, where the X<sub>k</sub> are N-dimensional Gaussian vectors with independent standard normal entries. Then for each f obeying the decay estimate above for some 0<p<1 and with overwhelming probability, our reconstruction f<sup>t</sup>, defined as the solution to the constraints y<sub>k</sub>=langf<sup># </sup>,X<sub>k</sub>rang with minimal lscr<sub>1</sub> norm, obeys parf-f<sup>#</sup>par<sub>lscr2</sub>lesC<sub>p </sub>middotRmiddot(K/logN)<sup>-r</sup>, r=1/p-1/2. There is a sense in which this result is optimal; it is generally impossible to obtain a higher accuracy from any set of K measurements whatsoever. The methodology extends to various other random measurement ensembles; for example, we show that similar results hold if one observes a few randomly sampled Fourier coefficients of f. In fact, the results are quite general and require only two hypotheses on the measurement ensemble which are detailed"
            },
            "slug": "Near-Optimal-Signal-Recovery-From-Random-Universal-Cand\u00e8s-Tao",
            "title": {
                "fragments": [],
                "text": "Near-Optimal Signal Recovery From Random Projections: Universal Encoding Strategies?"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "If the objects of interest are sparse in a fixed basis or compressible, then it is possible to reconstruct f to within very high accuracy from a small number of random measurements by solving a simple linear program."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783831"
                        ],
                        "name": "P. Comon",
                        "slug": "P.-Comon",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Comon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Comon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This problem should be solved in polynomial time by standard linear programming methods [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The other algorithm is Independent Component Analysis (ICA) [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18340548,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96a1effa4be3f8caa88270d6d258de418993d2e7",
            "isKey": false,
            "numCitedBy": 8327,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Independent-component-analysis,-A-new-concept-Comon",
            "title": {
                "fragments": [],
                "text": "Independent component analysis, A new concept?"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144012929"
                        ],
                        "name": "Yoav Sharon",
                        "slug": "Yoav-Sharon",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Sharon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoav Sharon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143607492"
                        ],
                        "name": "John Wright",
                        "slug": "John-Wright",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Wright",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John Wright"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 302,
                                "start": 298
                            }
                        ],
                        "text": "While the best known algorithms for exactly computing the neighborliness of a polytope are combinatorial in nature, tighter upper bounds can be obtained by restricting the search for intersections between the nullspace of B and the \u2018(1)-ball to a random subset of the t-faces of the \u2018(1)-ball (see [37] for details)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 131
                            }
                        ],
                        "text": "For this data set, we have estimated that the polytope P 1\u20444 conv\u00f0 B\u00de is approximately 1,185 neighborly (using the method given in [37]), suggesting that perfect reconstruction can be achieved up to 13."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 106
                            }
                        ],
                        "text": "The best known algorithm is combinatorial, and therefore, only practical when the dimension m is moderate [37]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17586951,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d2ab2345700440c0135f955ae77e836523b5ff5",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we investigate the exact conditions under which the ` and ` minimizations arising in the context of sparse error correction or sparse signal reconstruction are equivalent. We present a much simplified condition for verifying equivalence, which leads to a provably correct algorithm that computes the exact sparsity of the error or the signal needed to ensure equivalence. Our algorithm is combinatorial in nature, but for moderate-sized matrices it produces the exact result in a reasonably short time. For `-` equivalence problems involving tall encoding matrices (highly robust error correction) and or wide overcomplete dictionaries (sparse signal reconstruction from few measurements), our algorithm is exponentially faster than the only other algorithm known for this problem. We present an example application that requires such matrices, for which our algorithm can greatly assist with real system design. We also show how, if the encoding matrix is imbalanced, an optimal diagonal rescaling matrix can be computed by linear programming, so that the rescaled system enjoys the widest possible equivalence."
            },
            "slug": "Computation-and-Relaxation-of-Conditions-for-`-1-`-Sharon-Wright",
            "title": {
                "fragments": [],
                "text": "Computation and Relaxation of Conditions for Equivalence between ` 1 and ` 0 Minimization \u2217"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "This paper investigates the exact conditions under which the ` and ` minimizations arising in the context of sparse error correction or sparse signal reconstruction are equivalent, and presents a much simplified condition for verifying equivalence, which leads to a provably correct algorithm that computes the exact sparsity of the error or the signal needed to ensure equivalence."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2293280"
                        ],
                        "name": "Y. Tsaig",
                        "slug": "Y.-Tsaig",
                        "structuredName": {
                            "firstName": "Yaakov",
                            "lastName": "Tsaig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Tsaig"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "example, homotopy algorithms recover solutions with t nonzeros in O(t + n) time, linear in the size of the training set [36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6961958,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b8392a93baaca38810415b1e826fa29d7d5f7af",
            "isKey": false,
            "numCitedBy": 868,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "The minimum lscr1-norm solution to an underdetermined system of linear equations y=Ax is often, remarkably, also the sparsest solution to that system. This sparsity-seeking property is of interest in signal processing and information transmission. However, general-purpose optimizers are much too slow for lscr1 minimization in many large-scale applications.In this paper, the Homotopy method, originally proposed by Osborne et al. and Efron et al., is applied to the underdetermined lscr1-minimization problem min parxpar1 subject to y=Ax. Homotopy is shown to run much more rapidly than general-purpose LP solvers when sufficient sparsity is present. Indeed, the method often has the following k-step solution property: if the underlying solution has only k nonzeros, the Homotopy method reaches that solution in only k iterative steps. This k-step solution property is demonstrated for several ensembles of matrices, including incoherent matrices, uniform spherical matrices, and partial orthogonal matrices. These results imply that Homotopy may be used to rapidly decode error-correcting codes in a stylized communication system with a computational budget constraint. The approach also sheds light on the evident parallelism in results on lscr1 minimization and orthogonal matching pursuit (OMP), and aids in explaining the inherent relations between Homotopy, least angle regression (LARS), OMP, and polytope faces pursuit."
            },
            "slug": "Fast-Solution-of-$\\ell-_{1}$-Norm-Minimization-When-Donoho-Tsaig",
            "title": {
                "fragments": [],
                "text": "Fast Solution of $\\ell _{1}$ -Norm Minimization Problems When the Solution May Be Sparse"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The Homotopy method is applied to the underdetermined lscr1-minimization problem min parxpar1 subject to y=Ax and is shown to run much more rapidly than general-purpose LP solvers when sufficient sparsity is present, implying that homotopy may be used to rapidly decode error-correcting codes in a stylized communication system with a computational budget constraint."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114870993"
                        ],
                        "name": "P. Zhao",
                        "slug": "P.-Zhao",
                        "structuredName": {
                            "firstName": "Peng",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144923779"
                        ],
                        "name": "Bin Yu",
                        "slug": "Bin-Yu",
                        "structuredName": {
                            "firstName": "Bin",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bin Yu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u00c7"
                    },
                    "intents": []
                }
            ],
            "corpusId": 2174351,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc5b06753fac11268bc2300b7c25d50cbbcdeb5c",
            "isKey": false,
            "numCitedBy": 2461,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Sparsity or parsimony of statistical models is crucial for their proper interpretations, as in sciences and social sciences. Model selection is a commonly used method to find such models, but usually involves a computationally heavy combinatorial search. Lasso (Tibshirani, 1996) is now being used as a computationally feasible alternative to model selection. Therefore it is important to study Lasso for model selection purposes. \n \nIn this paper, we prove that a single condition, which we call the Irrepresentable Condition, is almost necessary and sufficient for Lasso to select the true model both in the classical fixed p setting and in the large p setting as the sample size n gets large. Based on these results, sufficient conditions that are verifiable in practice are given to relate to previous works and help applications of Lasso for feature selection and sparse representation. \n \nThis Irrepresentable Condition, which depends mainly on the covariance of the predictor variables, states that Lasso selects the true model consistently if and (almost) only if the predictors that are not in the true model are \"irrepresentable\" (in a sense to be clarified) by predictors that are in the true model. Furthermore, simulations are carried out to provide insights and understanding of this result."
            },
            "slug": "On-Model-Selection-Consistency-of-Lasso-Zhao-Yu",
            "title": {
                "fragments": [],
                "text": "On Model Selection Consistency of Lasso"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is proved that a single condition, which is called the Irrepresentable Condition, is almost necessary and sufficient for Lasso to select the true model both in the classical fixed p setting and in the large p setting as the sample size n gets large."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 62
                            }
                        ],
                        "text": "What is critical, however, is whether the number of features is sufficiently large and whether the sparse representation is correctly computed."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18251470,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "445ad69010658097fc317f7b83f1198179eebae8",
            "isKey": false,
            "numCitedBy": 1840,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper develops the separating capacities of families of nonlinear decision surfaces by a direct application of a theorem in classical combinatorial geometry. It is shown that a family of surfaces having d degrees of freedom has a natural separating capacity of 2d pattern vectors, thus extending and unifying results of Winder and others on the pattern-separating capacity of hyperplanes. Applying these ideas to the vertices of a binary n-cube yields bounds on the number of spherically, quadratically, and, in general, nonlinearly separable Boolean functions of n variables. It is shown that the set of all surfaces which separate a dichotomy of an infinite, random, separable set of pattern vectors can be characterized, on the average, by a subset of only 2d extreme pattern vectors. In addition, the problem of generalizing the classifications on a labeled set of pattern points to the classification of a new point is defined, and it is found that the probability of ambiguous generalization is large unless the number of training patterns exceeds the capacity of the set of separating surfaces."
            },
            "slug": "Geometrical-and-Statistical-Properties-of-Systems-Cover",
            "title": {
                "fragments": [],
                "text": "Geometrical and Statistical Properties of Systems of Linear Inequalities with Applications in Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that a family of surfaces having d degrees of freedom has a natural separating capacity of 2d pattern vectors, thus extending and unifying results of Winder and others on the pattern-separating capacity of hyperplanes."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Electron. Comput."
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8510060,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "3424286d6d39de51080ddd683646565545d015e2",
            "isKey": false,
            "numCitedBy": 2296,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider linear equations y = \u03a6x where y is a given vector in \u211dn and \u03a6 is a given n \u00d7 m matrix with n < m \u2264 \u03c4n, and we wish to solve for x \u2208 \u211dm. We suppose that the columns of \u03a6 are normalized to the unit \ud835\udcc12\u2010norm, and we place uniform measure on such \u03a6. We prove the existence of \u03c1 = \u03c1(\u03c4) > 0 so that for large n and for all \u03a6's except a negligible fraction, the following property holds: For every y having a representation y = \u03a6x0 by a coefficient vector x0 \u2208 \u211dm with fewer than \u03c1 \u00b7 n nonzeros, the solution x1 of the \ud835\udcc11\u2010minimization problem $${\\rm min} \\|x\\|_{1} \\;\\;{subject \\; to}\\;\\; \\Phi x = y$$ is unique and equal to x0. In contrast, heuristic attempts to sparsely solve such systems\u2014greedy algorithms and thresholding\u2014perform poorly in this challenging setting. The techniques include the use of random proportional embeddings and almost\u2010spherical sections in Banach space theory, and deviation bounds for the eigenvalues of random Wishart matrices. \u00a9 2006 Wiley Periodicals, Inc."
            },
            "slug": "For-most-large-underdetermined-systems-of-linear-is-Donoho",
            "title": {
                "fragments": [],
                "text": "For most large underdetermined systems of linear equations the minimal \ud835\udcc11\u2010norm solution is also the sparsest solution"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The techniques include the use of random proportional embeddings and almost\u2010spherical sections in Banach space theory, and deviation bounds for the eigenvalues of random Wishart matrices."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760994"
                        ],
                        "name": "R. Basri",
                        "slug": "R.-Basri",
                        "structuredName": {
                            "firstName": "Ronen",
                            "lastName": "Basri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Basri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34734622"
                        ],
                        "name": "D. Jacobs",
                        "slug": "D.-Jacobs",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Jacobs",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Jacobs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2891906,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "66e6a411a7342203ebbc22fbe9a3740b744d7cbc",
            "isKey": false,
            "numCitedBy": 858,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "We prove that the set of all reflectance functions (the mapping from surface normals to intensities) produced by Lambertian objects under distant, isotropic lighting lies close to a 9D linear subspace. This implies that the images of a convex Lambertian object obtained under a wide variety of lighting conditions can be approximated accurately with a low-dimensional linear subspace, explaining prior empirical results. We also provide a simple analytic characterization of this linear space. We obtain these results by representing lighting using spherical harmonics and describing the effects of Lambertian materials as the analog of a convolution. These results allow us to construct algorithms for object recognition based on linear methods as well as algorithms that use convex optimization to enforce non-negative lighting functions."
            },
            "slug": "Lambertian-reflectance-and-linear-subspaces-Basri-Jacobs",
            "title": {
                "fragments": [],
                "text": "Lambertian reflectance and linear subspaces"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "It is proved that the set of all reflectance functions (the mapping from surface normals to intensities) produced by Lambertian objects under distant, isotropic lighting lies close to a 9D linear subspace, implying that the images of a convex Lambertian object obtained under a wide variety of lighting conditions can be approximated accurately with a low-dimensional linear sub space."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12929381,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "36ab8bd64210ac5c4f7d326ed2c0a5745e91320f",
            "isKey": false,
            "numCitedBy": 1067,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider inexact linear equations y \u2248 \u03a6x where y is a given vector in \u211dn, \u03a6 is a given n \u00d7 m matrix, and we wish to find x0,\u03f5 as sparse as possible while obeying \u2016y \u2212 \u03a6x0,\u03f5\u20162 \u2264 \u03f5. In general, this requires combinatorial optimization and so is considered intractable. On the other hand, the \ud835\udcc11\u2010minimization problem $${\\rm min} \\; \\|x\\|_{1}\\;\\;\\; {\\rm subject \\; to}\\;\\;\\; \\|y - \\Phi{x}\\|_{2} \\leq \\epsilon$$ is convex and is considered tractable. We show that for most \u03a6, if the optimally sparse approximation x0,\u03f5 is sufficiently sparse, then the solution x1,\u03f5 of the \ud835\udcc11\u2010minimization problem is a good approximation to x0,\u03f5."
            },
            "slug": "For-most-large-underdetermined-systems-of-the-the-Donoho",
            "title": {
                "fragments": [],
                "text": "For most large underdetermined systems of equations, the minimal \ud835\udcc11\u2010norm near\u2010solution approximates the sparsest near\u2010solution"
            },
            "tldr": {
                "abstractSimilarityScore": 34,
                "text": "It is shown that for most \u03a6, if the optimally sparse approximation x0,\u03f5 is sufficiently sparse, then the solution x1, \u03f5 of the \ud835\udcc11\u2010minimization problem is a good approximation to x0 ,\u03f5."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784682"
                        ],
                        "name": "T. Hastie",
                        "slug": "T.-Hastie",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Hastie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hastie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761784"
                        ],
                        "name": "R. Tibshirani",
                        "slug": "R.-Tibshirani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tibshirani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tibshirani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14601089,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c8430c5d13834e46631960469d86f7ed4577b0e2",
            "isKey": false,
            "numCitedBy": 733,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the group lasso penalty for the linear model. We note that the standard algorithm for solving the problem assumes that the model matrices in each group are orthonormal. Here we consider a more general penalty that blends the lasso (L1) with the group lasso (\\two-norm\"). This penalty yields solutions that are sparse at both the group and individual feature levels. We derive an ecien t algorithm for the resulting convex problem based on coordinate descent. This algorithm can also be used to solve the general form of the group lasso, with non-orthonormal model matrices."
            },
            "slug": "A-note-on-the-group-lasso-and-a-sparse-group-lasso-Friedman-Hastie",
            "title": {
                "fragments": [],
                "text": "A note on the group lasso and a sparse group lasso"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An ecien t algorithm is derived for the resulting convex problem based on coordinate descent that can be used to solve the general form of the group lasso, with non-orthonormal model matrices."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761784"
                        ],
                        "name": "R. Tibshirani",
                        "slug": "R.-Tibshirani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tibshirani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tibshirani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u00c7"
                    },
                    "intents": []
                }
            ],
            "corpusId": 16162039,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b365b8e45b7d81f081de44ac8f9eadf9144f3ca5",
            "isKey": false,
            "numCitedBy": 36476,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY We propose a new method for estimation in linear models. The 'lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described."
            },
            "slug": "Regression-Shrinkage-and-Selection-via-the-Lasso-Tibshirani",
            "title": {
                "fragments": [],
                "text": "Regression Shrinkage and Selection via the Lasso"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A new method for estimation in linear models called the lasso, which minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant, is proposed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704341"
                        ],
                        "name": "E. Amaldi",
                        "slug": "E.-Amaldi",
                        "structuredName": {
                            "firstName": "Edoardo",
                            "lastName": "Amaldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Amaldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747198"
                        ],
                        "name": "V. Kann",
                        "slug": "V.-Kann",
                        "structuredName": {
                            "firstName": "Viggo",
                            "lastName": "Kann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Kann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 14385658,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d3ada827041115373dc79d2acd76b941c5b2fc0c",
            "isKey": false,
            "numCitedBy": 723,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-Approximability-of-Minimizing-Nonzero-or-in-Amaldi-Kann",
            "title": {
                "fragments": [],
                "text": "On the Approximability of Minimizing Nonzero Variables or Unsatisfied Relations in Linear Systems"
            },
            "venue": {
                "fragments": [],
                "text": "Theor. Comput. Sci."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3023562"
                        ],
                        "name": "H. Rauhut",
                        "slug": "H.-Rauhut",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Rauhut",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Rauhut"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082931"
                        ],
                        "name": "K. Schnass",
                        "slug": "K.-Schnass",
                        "structuredName": {
                            "firstName": "Karin",
                            "lastName": "Schnass",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Schnass"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697397"
                        ],
                        "name": "P. Vandergheynst",
                        "slug": "P.-Vandergheynst",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Vandergheynst",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Vandergheynst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 145
                            }
                        ],
                        "text": "The case where x0 is instead sparse in some overcomplete basis A, and we observe that random measurements ~ y 1\u20444 R Ax0 has also been studied in [45]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15742462,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e2cfa3d7898d6eeac142ab9613f9bb2ea20683c3",
            "isKey": false,
            "numCitedBy": 547,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper extends the concept of compressed sensing to signals that are not sparse in an orthonormal basis but rather in a redundant dictionary. It is shown that a matrix, which is a composition of a random matrix of certain type and a deterministic dictionary, has small restricted isometry constants. Thus, signals that are sparse with respect to the dictionary can be recovered via basis pursuit (BP) from a small number of random measurements. Further, thresholding is investigated as recovery algorithm for compressed sensing, and conditions are provided that guarantee reconstruction with high probability. The different schemes are compared by numerical experiments."
            },
            "slug": "Compressed-Sensing-and-Redundant-Dictionaries-Rauhut-Schnass",
            "title": {
                "fragments": [],
                "text": "Compressed Sensing and Redundant Dictionaries"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "It is shown that a matrix, which is a composition of a random matrix of certain type and a deterministic dictionary, has small restricted isometry constants, and signals that are sparse with respect to the dictionary can be recovered via basis pursuit from a small number of random measurements."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1387902104"
                        ],
                        "name": "A. d'Aspremont",
                        "slug": "A.-d'Aspremont",
                        "structuredName": {
                            "firstName": "Alexandre",
                            "lastName": "d'Aspremont",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. d'Aspremont"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701847"
                        ],
                        "name": "L. Ghaoui",
                        "slug": "L.-Ghaoui",
                        "structuredName": {
                            "firstName": "Laurent",
                            "lastName": "Ghaoui",
                            "middleNames": [
                                "El"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ghaoui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725533"
                        ],
                        "name": "G. Lanckriet",
                        "slug": "G.-Lanckriet",
                        "structuredName": {
                            "firstName": "Gert",
                            "lastName": "Lanckriet",
                            "middleNames": [
                                "R.",
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lanckriet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 34
                            }
                        ],
                        "text": "The Principal Component Analysis (PCA) approach in [23] is not robust to occlusion."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "For PCA, ICA, and LNMF, the number of basis components is chosen to give the optimal test performance over the range f100; 200; 300; 400; 500; 600g.\nocclusion is certainly a worse type of errors for the algorithm."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 34
                            }
                        ],
                        "text": "There are many variations to make PCA robust to corruption or incomplete data, and some have been applied to robust face recognition, e.g., [29]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 228
                            }
                        ],
                        "text": "A related, but simpler, measure of parsimony in high-dimensional data processing seeks models that depend on only a few of the observations, selecting a small subset of features for classification or visualization (e.g., Sparse PCA [3], [4] among others)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 23
                            }
                        ],
                        "text": "Here, we use the basic PCA to provide a standard baseline for comparison.21 The remaining three techniques are designed to be more robust to occlusion."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 98
                            }
                        ],
                        "text": "Following [58], we normalize the image pixels to have zero mean and unit variance before applying PCA."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 186
                            }
                        ],
                        "text": "For comparison, we also considered outlier rejection by thresholding the euclidean distance between (features of) the test image and (features of) the nearest training images within the PCA, ICA, and LNMF feature spaces."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 13
                            }
                        ],
                        "text": ", Sparse PCA [3], [4] amongst others)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5490061,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14e10498f4f12e18aaf4706969746b3cfb36132f",
            "isKey": true,
            "numCitedBy": 750,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "We examine the problem of approximating, in the Frobenius-norm sense, a positive, semidefinite symmetric matrix by a rank-one matrix, with an upper bound on the cardinality of its eigenvector. The problem arises in the decomposition of a covariance matrix into sparse factors, and has wide applications ranging from biology to finance. We use a modification of the classical variational representation of the largest eigenvalue of a symmetric matrix, where cardinality is constrained, and derive a semidefinite programming based relaxation for our problem."
            },
            "slug": "A-Direct-Formulation-for-Sparse-Pca-Using-d'Aspremont-Ghaoui",
            "title": {
                "fragments": [],
                "text": "A Direct Formulation for Sparse Pca Using Semidefinite Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A modification of the classical variational representation of the largest eigenvalue of a symmetric matrix, where cardinality is constrained, is used and derived to derive a semidefinite programming based relaxation for the problem."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM Rev."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2006869"
                        ],
                        "name": "E. Cand\u00e8s",
                        "slug": "E.-Cand\u00e8s",
                        "structuredName": {
                            "firstName": "Emmanuel",
                            "lastName": "Cand\u00e8s",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Cand\u00e8s"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34759333"
                        ],
                        "name": "Paige A. Randall",
                        "slug": "Paige-A.-Randall",
                        "structuredName": {
                            "firstName": "Paige",
                            "lastName": "Randall",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paige A. Randall"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 222
                            }
                        ],
                        "text": "In fact, the problem of solving overdetermined systems y = Ax+e with sparse errors and the problem of solving underdetermined systems y = Bw with a spare solution are equivalent, and can be converted from one to the other [13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 114
                            }
                        ],
                        "text": "z can be explicitly accounted for by replacing the linear program in Algorithm 1 with a second-order cone program [13]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7926497,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "30943e32309a66bf087baf0b7225ffb00b7c0f18",
            "isKey": false,
            "numCitedBy": 176,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper discusses a stylized communications problem where one wishes to transmit a real-valued signal (a block of pieces of information) to a remote receiver. We ask whether it is possible to transmit this information reliably when a fraction of the transmitted codeword is corrupted by arbitrary gross errors, and when in addition, all the entries of the codeword are contaminated by smaller errors (e.g., quantization errors). We show that if one encodes the information as where is a suitable coding matrix, there are two decoding schemes that allow the recovery of the block of pieces of information with nearly the same accuracy as if no gross errors occurred upon transmission (or equivalently as if one had an oracle supplying perfect information about the sites and amplitudes of the gross errors). Moreover, both decoding strategies are very concrete and only involve solving simple convex optimization programs, either a linear program or a second-order cone program. We complement our study with numerical simulations showing that the encoder/decoder pair performs remarkably well."
            },
            "slug": "Highly-Robust-Error-Correction-byConvex-Programming-Cand\u00e8s-Randall",
            "title": {
                "fragments": [],
                "text": "Highly Robust Error Correction byConvex Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "This paper discusses a stylized communications problem where one wishes to transmit a real-valued signal to a remote receiver and shows that if one encodes the information as where is a suitable coding matrix, there are two decoding schemes that allow the recovery of the block of pieces of information with nearly the same accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35674406"
                        ],
                        "name": "Shigeo Abe DrEng",
                        "slug": "Shigeo-Abe-DrEng",
                        "structuredName": {
                            "firstName": "Shigeo",
                            "lastName": "DrEng",
                            "middleNames": [
                                "Abe"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shigeo Abe DrEng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u00c7"
                    },
                    "intents": []
                }
            ],
            "corpusId": 9384346,
            "fieldsOfStudy": [
                "Mathematics",
                "Environmental Science"
            ],
            "id": "65a69968bb8c41aad0113cec4c2d981bddf50bc8",
            "isKey": false,
            "numCitedBy": 13095,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Classification \u2022 Supervised \u2013 parallelpiped \u2013 minimum distance \u2013 maximum likelihood (Bayes Rule) > non-parametric > parametric \u2013 support vector machines \u2013 neural networks \u2013 context classification \u2022 Unsupervised (clustering) \u2013 K-Means \u2013 ISODATA \u2022 Pattern recognition in remote sensing has been based on the intuitive notion that pixels belonging to the same class should have similar gray values in a given band. \u2013 Given two spectral bands, pixels from the same class plotted in a two-dimensional histogram should appear as a localized cluster. \u2013 If n images, each in a different spectral band, are available, pixels from the same class should form a localized cluster in n-space."
            },
            "slug": "Pattern-Classification-DrEng",
            "title": {
                "fragments": [],
                "text": "Pattern Classification"
            },
            "venue": {
                "fragments": [],
                "text": "Springer London"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398381803"
                        ],
                        "name": "G. Piatetsky-Shapiro",
                        "slug": "G.-Piatetsky-Shapiro",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Piatetsky-Shapiro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Piatetsky-Shapiro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404689578"
                        ],
                        "name": "XChange Edward Bosch",
                        "slug": "XChange-Edward-Bosch",
                        "structuredName": {
                            "firstName": "XChange",
                            "lastName": "Bosch",
                            "middleNames": [
                                "Edward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "XChange Edward Bosch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055669385"
                        ],
                        "name": "T. Jung",
                        "slug": "T.-Jung",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jung"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 85
                            }
                        ],
                        "text": "12 This surprising phenomenon has been dubbed the \u201cblessing of dimensionality\u201d [15], [46]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5293263,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "63c68278418b69f60b4814fae8dd15b1b1854295",
            "isKey": false,
            "numCitedBy": 831,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "The coming century is surely the century of data. A combination of blind faith and serious purpose makes our society invest massively in the collection and processing of data of all kinds, on scales unimaginable until recently. Hyperspectral Imagery, Internet Portals, Financial tick-by-tick data, and DNA Microarrays are just a few of the betterknown sources, feeding data in torrential streams into scientific and business databases worldwide. In traditional statistical data analysis, we think of observations of instances of particular phenomena (e.g. instance \u2194 human being), these observations being a vector of values we measured on several variables (e.g. blood pressure, weight, height, ...). In traditional statistical methodology, we assumed many observations and a few, wellchosen variables. The trend today is towards more observations but even more so, to radically larger numbers of variables \u2013 voracious, automatic, systematic collection of hyper-informative detail about each observed instance. We are seeing examples where the observations gathered on individual instances are curves, or spectra, or images, or even movies, so that a single observation has dimensions in the thousands or billions, while there are only tens or hundreds of instances available for study. Classical methods are simply not designed to cope with this kind of explosive growth of dimensionality of the observation vector. We can say with complete confidence that in the coming century, high-dimensional data analysis will be a very significant activity, and completely new methods of high-dimensional data analysis will be developed; we just don\u2019t know what they are yet. Mathematicians are ideally prepared for appreciating the abstract issues involved in finding patterns in such high-dimensional data. Two of the most influential principles in the coming century will be principles originally discovered and cultivated by mathematicians: the blessings of dimensionality and the curse of dimensionality. The curse of dimensionality is a phrase used by several subfields in the mathematical sciences; I use it here to refer to the apparent intractability of systematically searching through a high-dimensional space, the apparent intractability of accurately approximating a general high-dimensional function, the apparent intractability of integrating a high-dimensional function. The blessings of dimensionality are less widely noted, but they include the concentration of measure phenomenon (so-called in the geometry of Banach spaces), which means that certain random fluctuations are very well controlled in high dimensions and the success of asymptotic methods, used widely in mathematical statistics and statistical physics, which suggest that statements about very high-dimensional settings may be made where moderate dimensions would be too complicated. There is a large body of interesting work going on in the mathematical sciences, both to attack the curse of dimensionality in specific ways, and to extend the benefits"
            },
            "slug": "High-Dimensional-Data-Analysis-:-The-Curses-and-of-Piatetsky-Shapiro-Bosch",
            "title": {
                "fragments": [],
                "text": "High-Dimensional Data Analysis : The Curses and Blessings of Dimensionality"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is said with complete confidence that in the coming century, high-dimensional data analysis will be a very significant activity, and completely new methods of high- Dimension Data Analysis will be developed; the authors just don\u2019t know what they are yet."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2074275595"
                        ],
                        "name": "M. Hansen",
                        "slug": "M.-Hansen",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Hansen",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hansen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144923779"
                        ],
                        "name": "Bin Yu",
                        "slug": "Bin-Yu",
                        "structuredName": {
                            "firstName": "Bin",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bin Yu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14460386,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d6ac30380a592f96d1ec43e9cac1a560b3f4fbf4",
            "isKey": false,
            "numCitedBy": 739,
            "numCiting": 196,
            "paperAbstract": {
                "fragments": [],
                "text": "This article reviews the principle of minimum description length (MDL) for problems of model selection. By viewing statistical modeling as a means of generating descriptions of observed data, the MDL framework discriminates between competing models based on the complexity of each description. This approach began with Kolmogorov's theory of algorithmic complexity, matured in the literature on information theory, and has recently received renewed attention within the statistics community. Here we review both the practical and the theoretical aspects of MDL as a tool for model selection, emphasizing the rich connections between information theory and statistics. At the boundary between these two disciplines we find many interesting interpretations of popular frequentist and Bayesian procedures. As we show, MDL provides an objective umbrella under which rather disparate approaches to statistical modeling can coexist and be compared. We illustrate the MDL principle by considering problems in regression, nonparametric curve estimation, cluster analysis, and time series analysis. Because model selection in linear regression is an extremely common problem that arises in many applications, we present detailed derivations of several MDL criteria in this context and discuss their properties through a number of examples. Our emphasis is on the practical application of MDL, and hence we make extensive use of real datasets. In writing this review, we tried to make the descriptive philosophy of MDL natural to a statistics audience by examining classical problems in model selection. In the engineering literature, however, MDL is being applied to ever more exotic modeling situations. As a principle for statistical modeling in general, one strength of MDL is that it can be intuitively extended to provide useful tools for new problems."
            },
            "slug": "Model-Selection-and-the-Principle-of-Minimum-Length-Hansen-Yu",
            "title": {
                "fragments": [],
                "text": "Model Selection and the Principle of Minimum Description Length"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "This article reviews the principle of minimum description length (MDL) for problems of model selection, and illustrates the MDL principle by considering problems in regression, nonparametric curve estimation, cluster analysis, and time series analysis."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705268"
                        ],
                        "name": "D. Achlioptas",
                        "slug": "D.-Achlioptas",
                        "structuredName": {
                            "firstName": "Dimitris",
                            "lastName": "Achlioptas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Achlioptas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 130
                            }
                        ],
                        "text": "Random projection has been previously studied as a general dimensionality-reduction method for numerous clustering problems [47], [48], [49], as well as for learning nonlinear manifolds [50], [51]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2640788,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "d70c182a71aea05a145391b24d6bc3cdeede32a5",
            "isKey": false,
            "numCitedBy": 880,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "A classic result of Johnson and Lindenstrauss asserts that any set of n points in d-dimensional Euclidean space can be embedded into k-dimensional Euclidean space where k is logarithmic in n and independent of d so that all pairwise distances are maintained within an arbitrarily small factor. All known constructions of such embeddings involve projecting the n points onto a random k-dimensional hyperplane. We give a novel construction of the embedding, suitable for database applications, which amounts to computing a simple aggregate over k random attribute partitions."
            },
            "slug": "Database-friendly-random-projections-Achlioptas",
            "title": {
                "fragments": [],
                "text": "Database-friendly random projections"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work gives a novel construction of the embedding of k-dimensional Euclidean space, suitable for database applications, which amounts to computing a simple aggregate over k random attribute partitions."
            },
            "venue": {
                "fragments": [],
                "text": "PODS '01"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2006869"
                        ],
                        "name": "E. Cand\u00e8s",
                        "slug": "E.-Cand\u00e8s",
                        "structuredName": {
                            "firstName": "Emmanuel",
                            "lastName": "Cand\u00e8s",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Cand\u00e8s"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "Our implementation minimizes the `(1) norm via a primal-dual algorithm for linear programming based on [24], [25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1038158,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dd240d485145d12c2c27cfd9a10004484adbcfb7",
            "isKey": false,
            "numCitedBy": 955,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "For maximum computational efficiency, the solvers for each of the seven problems are implemented separately. They all have the same basic structure, however, with the computational bottleneck being the calculation of the Newton step (this is discussed in detail below). The code can be used in either \u201csmall scale\u201d mode, where the system is constructed explicitly and solved exactly, or in \u201clarge scale\u201d mode, where an iterative matrix-free algorithm such as conjugate gradients (CG) is used to approximately solve the system."
            },
            "slug": "11-magic-:-Recovery-of-sparse-signals-via-convex-Cand\u00e8s",
            "title": {
                "fragments": [],
                "text": "11-magic : Recovery of sparse signals via convex programming"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The code can be used in either \u201csmall scale\u201d mode, where the system is constructed explicitly and solved exactly, or in \u201clarge scale\u2019 modes, where an iterative matrix-free algorithm such as conjugate gradients (CG) is used to approximately solve the system."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2006869"
                        ],
                        "name": "E. Cand\u00e8s",
                        "slug": "E.-Cand\u00e8s",
                        "structuredName": {
                            "firstName": "Emmanuel",
                            "lastName": "Cand\u00e8s",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Cand\u00e8s"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777131"
                        ],
                        "name": "M. Rudelson",
                        "slug": "M.-Rudelson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Rudelson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Rudelson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056233969"
                        ],
                        "name": "Terence Tao",
                        "slug": "Terence-Tao",
                        "structuredName": {
                            "firstName": "Terence",
                            "lastName": "Tao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Terence Tao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1806615"
                        ],
                        "name": "R. Vershynin",
                        "slug": "R.-Vershynin",
                        "structuredName": {
                            "firstName": "Roman",
                            "lastName": "Vershynin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Vershynin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, [12] shows that even for a random matrix B drawn from a Gaussian ensemble, EBP(B) > \u03c10m with overwhelming probability, as m\u2192\u221e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Recently, a series of papers [12]\u2013[15] have shown that if the error e is sufficiently sparse, then the ` minimizer x\u03020 is equal to the ` minimizer: x\u03020 = x\u03021, (5)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Sparse representations have attracted a great deal of attention in signal processing and information theory [12]\u2013[15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6000171,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04c89a05810eb787476ac3039bb23582eba100f9",
            "isKey": true,
            "numCitedBy": 175,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": "Suppose we wish to transmit a vector f \u03f5 Rn reliably. A frequently discussed approach consists in encoding f with an m by n coding matrix A. Assume now that a fraction of the entries of Af are corrupted in a completely arbitrary fashion by an error e. We do not know which entries are affected nor do we know how they are affected. Is it possible to recover f exactly from the corrupted m-dimensional vector y = Af + e?"
            },
            "slug": "Error-correction-via-linear-programming-Cand\u00e8s-Rudelson",
            "title": {
                "fragments": [],
                "text": "Error correction via linear programming"
            },
            "venue": {
                "fragments": [],
                "text": "46th Annual IEEE Symposium on Foundations of Computer Science (FOCS'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2006869"
                        ],
                        "name": "E. Cand\u00e8s",
                        "slug": "E.-Cand\u00e8s",
                        "structuredName": {
                            "firstName": "Emmanuel",
                            "lastName": "Cand\u00e8s",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Cand\u00e8s"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735291"
                        ],
                        "name": "J. Romberg",
                        "slug": "J.-Romberg",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Romberg",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Romberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50383069"
                        ],
                        "name": "T. Tao",
                        "slug": "T.-Tao",
                        "structuredName": {
                            "firstName": "Terence",
                            "lastName": "Tao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Tao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "The ability to detect and then reject invalid test samples, or \u201coutliers,\u201d is crucial for recognition systems to work in realworld situations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "\u00c7"
                    },
                    "intents": []
                }
            ],
            "corpusId": 119159284,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "77269f08b025aa4acc8e6039d4b11d17379bb9cd",
            "isKey": false,
            "numCitedBy": 6802,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Suppose we wish to recover a vector x0 \u2208 \u211d\ud835\udcc2 (e.g., a digital signal or image) from incomplete and contaminated observations y = A x0 + e; A is an \ud835\udcc3 \u00d7 \ud835\udcc2 matrix with far fewer rows than columns (\ud835\udcc3 \u226a \ud835\udcc2) and e is an error term. Is it possible to recover x0 accurately based on the data y?"
            },
            "slug": "Stable-signal-recovery-from-incomplete-and-Cand\u00e8s-Romberg",
            "title": {
                "fragments": [],
                "text": "Stable signal recovery from incomplete and inaccurate measurements"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "It is shown that it is possible to recover x0 accurately based on the data y from incomplete and contaminated observations."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1843103"
                        ],
                        "name": "Stephen P. Boyd",
                        "slug": "Stephen-P.-Boyd",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Boyd",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen P. Boyd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2014414"
                        ],
                        "name": "L. Vandenberghe",
                        "slug": "L.-Vandenberghe",
                        "structuredName": {
                            "firstName": "Lieven",
                            "lastName": "Vandenberghe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Vandenberghe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 168
                            }
                        ],
                        "text": "In the computer vision literature, numerous feature extraction schemes have been investigated for finding projections that better separate the classes in lower dimensional spaces, which are often referred to as feature spaces."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 37925315,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4f607f03272e4d62708f5b2441355f9e005cb452",
            "isKey": false,
            "numCitedBy": 38721,
            "numCiting": 276,
            "paperAbstract": {
                "fragments": [],
                "text": "Convex optimization problems arise frequently in many different fields. A comprehensive introduction to the subject, this book shows in detail how such problems can be solved numerically with great efficiency. The focus is on recognizing convex optimization problems and then finding the most appropriate technique for solving them. The text contains many worked examples and homework exercises and will appeal to students, researchers and practitioners in fields such as engineering, computer science, mathematics, statistics, finance, and economics."
            },
            "slug": "Convex-Optimization-Boyd-Vandenberghe",
            "title": {
                "fragments": [],
                "text": "Convex Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A comprehensive introduction to the subject of convex optimization shows in detail how such problems can be solved numerically with great efficiency."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Automatic Control"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144908066"
                        ],
                        "name": "Richard Baraniuk",
                        "slug": "Richard-Baraniuk",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Baraniuk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard Baraniuk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718151"
                        ],
                        "name": "M. Davenport",
                        "slug": "M.-Davenport",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Davenport",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Davenport"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "26373808"
                        ],
                        "name": "R. DeVore",
                        "slug": "R.-DeVore",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "DeVore",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. DeVore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686007"
                        ],
                        "name": "M. Wakin",
                        "slug": "M.-Wakin",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Wakin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wakin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Random projection has been previously studied as a general dimensionality-reduction method for numerous clustering problems [48]\u2013[50], as well as for learning nonlinear manifolds [51], [52]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 440854,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "592bcf994c925e65d91a652eb10358ea6275eb3e",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We show how two fundamental results in analysis related to n-widths and Compressed Sensing are intimately related to the Johnson-Lindenstrauss lemma. Our elementary approach is based on the same concentration inequalities for random inner products that have recently provided simple proofs of the Johnson-Lindenstrauss lemma. We show how these ideas lead to simple proofs of Kashin\u2019s theorems on widths of finite balls in Euclidean space (and their improvements due to Gluskin) as well as the existence of optimal Compressed Sensing measurement matrices. In the process we also prove that these measurement matrices are universal with respect to the sparsity-inducing basis."
            },
            "slug": "The-Johnson-Lindenstrauss-Lemma-Meets-Compressed-Baraniuk-Davenport",
            "title": {
                "fragments": [],
                "text": "The Johnson-Lindenstrauss Lemma Meets Compressed Sensing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2095201599"
                        ],
                        "name": "O. Antoine",
                        "slug": "O.-Antoine",
                        "structuredName": {
                            "firstName": "O",
                            "lastName": "Antoine",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Antoine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2076266876"
                        ],
                        "name": "Berthet",
                        "slug": "Berthet",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Berthet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Berthet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13454875,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9cbc6bc1bec461f1bb7ddc4b68509ae9a4c6ef6b",
            "isKey": false,
            "numCitedBy": 4584,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The field of channel coding started with Claude Shannon's 1948 landmark paper. Fifty years of efforts and invention have finally produced coding schemes that closely approach Shannon's channel capacity limit on AWGN channels, both power-limited and band-limited. Similar gains are being achieved in other important applications, such as wireless channels. This course is divided in two parts. In the first part, we remind students of the basics of the theory of linear codes for conventional memoryless ergodic channels. We then introduce more advanced notions so as to make comprehensible some of the most recent coding schemes proposed in the literature. In the second part, we expound the principles of coded modulations for the Gaussian channel and, if time permits, for Rician and Rayleigh fading channels (fully interleaved). We will conclude the course by evoking some aspects of code design for non-ergodic block-fading channels. Basic definitions \u2013 Classification of channels, random codes Coding theorem for DMC \u2013 Upper bounds on error probability Coding theorem for DMC \u2013 Lower bounds on error probability Strong converse theorem for discrete channels [Wolfowitz] Generalization of results to BIOS memoryless channels Hard or soft decoding, information loss Cutoff rate Lecture 2. Codes with algebraic structures [4][5] Detection and correction capabilities of block codes Lower and upper bounds on code parameters \u2013 General case Linear block codes \u2013 Minimum distance, duality, elementary transformations Lower and upper bounds on code parameters \u2013 GV, Hamming, Plotkin, Singleton Convolutional codes"
            },
            "slug": "Theory-of-Error-correcting-Codes-Antoine-Berthet",
            "title": {
                "fragments": [],
                "text": "Theory of Error-correcting Codes"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This course expounds the principles of coded modulations for the Gaussian channel and, if time permits, for Rician and Rayleigh fading channels (fully interleaved), and reminds students of the basics of the theory of linear codes for conventional memoryless ergodic channels."
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145664300"
                        ],
                        "name": "Peter Bryant",
                        "slug": "Peter-Bryant",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bryant",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Bryant"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1409186107"
                        ],
                        "name": "O. Cordero-Bra\u00f1a",
                        "slug": "O.-Cordero-Bra\u00f1a",
                        "structuredName": {
                            "firstName": "Olga",
                            "lastName": "Cordero-Bra\u00f1a",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Cordero-Bra\u00f1a"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 130
                            }
                        ],
                        "text": "We cast the recognition problem as one of classifying among multiple linear regression models and argue that new theory from sparse signal representation offers the key to addressing this problem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122620509,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f13dec706fe82ed886d4c9abacb212871218a41d",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract The minimum description length (MDL) principle articulated in the last decade by Rissanen and his co-workers yields new criteria for statistical model selection. MDL criteria permit data-based choices from among alternative statistical descriptions of data without necessarily assuming that the data were sampled randomly. This article explains the MDL principle informally, indicates the criteria it yields in the common cases of multinomial distributions and Gaussian regression, and illustrates MDL's use with numerical examples. We hope thereby to stimulate experimentation and debate about the pedagogical and practical implications of the MDL approach."
            },
            "slug": "Model-Selection-using-the-Minimum-Description-Bryant-Cordero-Bra\u00f1a",
            "title": {
                "fragments": [],
                "text": "Model Selection using the Minimum Description Length Principle"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The MDL principle is explained informally, the criteria it yields in the common cases of multinomial distributions and Gaussian regression are indicated, and MDL's use with numerical examples is illustrated."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "One of its most celebrated instantiations, the principle of minimum description length in model selection [ 1 ], [2], stipulates that within a hierarchy of model classes, the model that yields the most compact representation should be preferred for decision-making tasks such as classification."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We test the system\u2019s ability to determine whether a given test subject is in the training database or not by sweeping the threshold \ufffd through a range of values in [0,  1 ], generating the receiver operator characteristic (ROC) curves in Fig. 14. For comparison, we also considered outlier rejection by thresholding the euclidean distance between (features of) the test image and (features of) the nearest training images within the PCA, ICA, and ..."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 30140639,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d382b9c11e5c6a8e173fbeb442545e3be8d3e3a5",
            "isKey": false,
            "numCitedBy": 6261,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Modeling-By-Shortest-Data-Description*-Rissanen",
            "title": {
                "fragments": [],
                "text": "Modeling By Shortest Data Description*"
            },
            "venue": {
                "fragments": [],
                "text": "Autom."
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1432639666"
                        ],
                        "name": "Karl Pearson F.R.S.",
                        "slug": "Karl-Pearson-F.R.S.",
                        "structuredName": {
                            "firstName": "Karl",
                            "lastName": "F.R.S.",
                            "middleNames": [
                                "Pearson"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karl Pearson F.R.S."
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 125037489,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "cac33f91e59f0a137b46176d74cee55c7010c3f8",
            "isKey": false,
            "numCitedBy": 9519,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "(1901). LIII. On lines and planes of closest fit to systems of points in space. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science: Vol. 2, No. 11, pp. 559-572."
            },
            "slug": "LIII.-On-lines-and-planes-of-closest-fit-to-systems-F.R.S.",
            "title": {
                "fragments": [],
                "text": "LIII. On lines and planes of closest fit to systems of points in space"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This paper is concerned with the construction of planes of closest fit to systems of points in space and the relationships between these planes and the planes themselves."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1901
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 54
                            }
                        ],
                        "text": "What is critical, however, is whether the number of features is sufficiently large and whether the sparse representation is correctly computed."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7138354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8213dbed4db44e113af3ed17d6dad57471a0c048",
            "isKey": false,
            "numCitedBy": 38755,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?."
            },
            "slug": "The-Nature-of-Statistical-Learning-Theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics for Engineering and Information Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117070857"
                        ],
                        "name": "A. Mart\u00ednez",
                        "slug": "A.-Mart\u00ednez",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Mart\u00ednez",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mart\u00ednez"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "algorithm\u2019s ability to cope with such real and possibly malicious occlusions using a subset of the AR Face Database [29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 221632808,
            "fieldsOfStudy": [
                "Materials Science"
            ],
            "id": "cd520bc4b5301bc51b8b6bf1226c3f2f88e8e444",
            "isKey": false,
            "numCitedBy": 2634,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-AR-face-databasae-Mart\u00ednez",
            "title": {
                "fragments": [],
                "text": "The AR face databasae"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1384255355"
                        ],
                        "name": "Aleix M. Martinez",
                        "slug": "Aleix-M.-Martinez",
                        "structuredName": {
                            "firstName": "Aleix M.",
                            "lastName": "Martinez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aleix M. Martinez"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 69
                            }
                        ],
                        "text": "For each individual, 26 pictures were taken in two separate sessions [60]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 57227467,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d96f946aaabc734af7fe3fc4454cf8547fcd5ed",
            "isKey": false,
            "numCitedBy": 3767,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-AR-face-database-Martinez",
            "title": {
                "fragments": [],
                "text": "The AR face database"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2586918"
                        ],
                        "name": "D. Stork",
                        "slug": "D.-Stork",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Stork",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Stork"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The proposed classifier can be considered a generalization of popular classifiers such as nearest neighbor (NN) [18] and nearest subspace (NS) [19] (i."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 56972603,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "fb014091607b4c11b21784a66ad9e1518111e3cb",
            "isKey": false,
            "numCitedBy": 6044,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Pattern-Classification-(2nd-ed.)-Duda-Hart",
            "title": {
                "fragments": [],
                "text": "Pattern Classification (2nd ed.)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 135
                            }
                        ],
                        "text": "The case where x0 is instead sparse in some overcomplete basis A, and we observe random measurements \u1ef9 = RAx0 has also been studied in [45]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Compressed sensing and  MANUSCRIPT ACCEPTED BY IEEE TRANS. PAMI, MARCH 2008.  17 redundant dictionaries"
            },
            "venue": {
                "fragments": [],
                "text": "to appear in IEEE Transactions on Information Theory, 2007."
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Robust Face Recognition via Sparse Representation"
            },
            "venue": {
                "fragments": [],
                "text": "press) PAMI"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nat'l Academy of Sciences"
            },
            "venue": {
                "fragments": [],
                "text": "Nat'l Academy of Sciences"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 74
                            }
                        ],
                        "text": "approximately recovery sparse solutions in ensembles of random matrices A [39]: There are constants \u03c1 and \u03b6 such that with overwhelming probability, if \u2016x0\u20160 < \u03c1m and \u2016z\u20162 \u2264 , then the computed x\u03021 satisfies \u2016x\u03021 \u2212 x0\u20162 \u2264 \u03b6 ."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "For most large underdetermined systems of linear equations the minimal `1-norm near solution approximates the sparest solution"
            },
            "venue": {
                "fragments": [],
                "text": "preprint, 2004."
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "For most large underdetermined systems of linear equations the minimal 1 -norm near solution approximates the sparsest solution"
            },
            "venue": {
                "fragments": [],
                "text": "For most large underdetermined systems of linear equations the minimal 1 -norm near solution approximates the sparsest solution"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "For most large underdetermined systems of linear equations the minimal 1 -norm near solution approximates the sparsest solution"
            },
            "venue": {
                "fragments": [],
                "text": "For most large underdetermined systems of linear equations the minimal 1 -norm near solution approximates the sparsest solution"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "SM'06) received his two Bachelors' degrees in Automation and Applied Mathematics from Tsinghua University"
            },
            "venue": {
                "fragments": [],
                "text": "He received an M.S. degree in Electrical Engineering and Computer Science (EECS)"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Electronic privacy information center, face recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Electronic privacy information center, face recognition"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "However, noise and modeling error may lead to small nonzero entries associated with multiple object classes (see Fig."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimal Sparse Representation in General (Nonorthogonal) Dictionaries via ' 1 Minimization Proc"
            },
            "venue": {
                "fragments": [],
                "text": "Optimal Sparse Representation in General (Nonorthogonal) Dictionaries via ' 1 Minimization Proc"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "RECOGNITION RATES OF NEAREST NEIGHBOR (LEFT) AND NEAREST SUBSPACE (RIGHT) ON THE AR DATABASE. THE BOLD NUMBERS INDICATE THE BEST AMONG ALL FEATURES"
            },
            "venue": {
                "fragments": [],
                "text": "RECOGNITION RATES OF NEAREST NEIGHBOR (LEFT) AND NEAREST SUBSPACE (RIGHT) ON THE AR DATABASE. THE BOLD NUMBERS INDICATE THE BEST AMONG ALL FEATURES"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 79
                            }
                        ],
                        "text": "6 The solution gives a provably stable estimate of the desired sparse solution [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 79
                            }
                        ],
                        "text": "However, the geometry and algorithms described are provably stable under noise [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "For most large underdetermined systems of linear equations the minimal `1-norm near solution approximates the sparsest solution"
            },
            "venue": {
                "fragments": [],
                "text": "preprint, http://www-stat.stanford.edu/ donoho/Reports/, 2004."
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 261
                            }
                        ],
                        "text": "\u2026to capture much of the variation in real data sets and are especially well motivated in the context of face recognition, where it has been observed that the images of faces under varying lighting and expression lie on a special low-dimensional subspace [24], [30], often called a face subspace."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Lambertian Reflection and Linear Subspaces"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Compressed Sensing and Redundant Dictionaries , \u201d to appear in IEEE Trans"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans . Pattern Analysis and Machine Intelligence"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 124
                            }
                        ],
                        "text": "Random projection has been previously studied as a general dimensionality-reduction method for numerous clustering problems [47], [48], [49], as well as for learning nonlinear manifolds [50], [51]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dimensionality Reduction by Random Mapping"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE Int'l Joint Conf. Neural Networks"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 37
                            }
                        ],
                        "text": ", patches around eyes or nose) [21], [41] (see Fig."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Partial and Holistic Face Recognition on FRGC-II Data Using Support Vector Machine Kernel Correlation Feature Analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Conf. Computer Vision and Pattern Recognition Workshop (CVPR)"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 112
                            }
                        ],
                        "text": "The proposed classifier can be considered a generalization of popular classifiers such as nearest neighbor (NN) [18] and nearest subspace (NS) [19] (i."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pattern Classification, second ed"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "He is also a member of the boards of the Federation of American Scientists and Embedded Systems Consortium for Hybrid and Embedded Research"
            },
            "venue": {
                "fragments": [],
                "text": "He is also a member of the boards of the Federation of American Scientists and Embedded Systems Consortium for Hybrid and Embedded Research"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 42,
            "methodology": 20,
            "result": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 92,
        "totalPages": 10
    },
    "page_url": "https://www.semanticscholar.org/paper/Robust-Face-Recognition-via-Sparse-Representation-Wright-Yang/d5eec41043d91964879c4c745c7165f823967f29?sort=total-citations"
}