{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46640241"
                        ],
                        "name": "S. Katagiri",
                        "slug": "S.-Katagiri",
                        "structuredName": {
                            "firstName": "Shoji",
                            "lastName": "Katagiri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Katagiri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109156106"
                        ],
                        "name": "C.-H. Lee",
                        "slug": "C.-H.-Lee",
                        "structuredName": {
                            "firstName": "C.-H.",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C.-H. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143604406"
                        ],
                        "name": "B. Juang",
                        "slug": "B.-Juang",
                        "structuredName": {
                            "firstName": "Biing-Hwang",
                            "lastName": "Juang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Juang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60452117,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "60435609231230f19d4574380904cce917e8f75c",
            "isKey": false,
            "numCitedBy": 169,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors developed a generalized probabilistic descent (GPD) method by extending the classical theory on adaptive training by Amari (1967). Their generalization makes it possible to treat dynamic patterns (of a variable duration or dimension) such as speech as well as static patterns (of a fixed duration or dimension), for pattern classification problems. The key ideas of GPD formulations include the embedding of time normalization and the incorporation of smooth classification error functions into the gradient search optimization objectives. As a result, a family of new discriminative training algorithms can be rigorously formulated for various kinds of classifier frameworks, including the popular dynamic time warping (DTW) and hidden Markov model (HMM). Experimental results are also provided to show the superiority of this new family of GPD-based, adaptive training algorithms for speech recognition.<<ETX>>"
            },
            "slug": "New-discriminative-training-algorithms-based-on-the-Katagiri-Lee",
            "title": {
                "fragments": [],
                "text": "New discriminative training algorithms based on the generalized probabilistic descent method"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "A family of new discriminative training algorithms can be rigorously formulated for various kinds of classifier frameworks, including the popular dynamic time warping (DTW) and hidden Markov model (HMM)."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks for Signal Processing Proceedings of the 1991 IEEE Workshop"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688681"
                        ],
                        "name": "T. Kohonen",
                        "slug": "T.-Kohonen",
                        "structuredName": {
                            "firstName": "Teuvo",
                            "lastName": "Kohonen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kohonen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5768521"
                        ],
                        "name": "G. Barna",
                        "slug": "G.-Barna",
                        "structuredName": {
                            "firstName": "Gy\u00f6rgy",
                            "lastName": "Barna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Barna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693259"
                        ],
                        "name": "Ron Chrisley",
                        "slug": "Ron-Chrisley",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Chrisley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ron Chrisley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Recently, due to the advent of artificial neural networks (ANN) [6] and learning vector quantizers (LVQ) [ 7 ], there is a resurgent interest in reexamining the classical techniques to suit the new classifier structures."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The above minimum classification error formulation can be applied to many new classifier structures such as the multilayer perceptron (MLP) [8], learning vector quantizer (LVQ) [ 7 ], and distance network [16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17154105,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6666e8358eed45caf93ada4272f8d63bb1b6b705",
            "isKey": false,
            "numCitedBy": 512,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Three basic types of neural-like networks (backpropagation network, Boltzmann machine, and learning vector quantization), were applied to two representative artificial statistical pattern recognition tasks, each with varying dimensionality. The performance of each network's approach to solving the tasks was evaluated and compared, both to the performance of the other two networks and to the theoretical limit. The learning vector quantization was further benchmarked against the parametric Bayes classifier and the k-nearest-neighbor classifier using natural speech data. A novel learning vector quantization classifier called LVQ2 is introduced.<<ETX>>"
            },
            "slug": "Statistical-pattern-recognition-with-neural-studies-Kohonen-Barna",
            "title": {
                "fragments": [],
                "text": "Statistical pattern recognition with neural networks: benchmarking studies"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Three basic types of neural-like networks, backpropagation network, Boltzmann machine, and learning vector quantization, were applied to two representative artificial statistical pattern recognition tasks, each with varying dimensionality."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE 1988 International Conference on Neural Networks"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46360329"
                        ],
                        "name": "J. Patterson",
                        "slug": "J.-Patterson",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Patterson",
                            "middleNames": [
                                "David"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Patterson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2780316"
                        ],
                        "name": "B. Womack",
                        "slug": "B.-Womack",
                        "structuredName": {
                            "firstName": "Baxter",
                            "lastName": "Womack",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Womack"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "asymptotically as the number of design observations grows [l], [ 13 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 41948214,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8952490c2dcedc7ea87e4a43880f7f4899e33587",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Adaptive pattern classification is the assignment of patterns to classes based on typical patterns or training samples, used by the system to determine the decision procedure. The system is adaptive in the sense that the decision procedure is optimized according to some criterion of the system's performance on the training samples. An adaptive pattern classification system is described that does not require a priori knowledge of the probability density of the pattern vectors for each class, as do the classical statistical techniques. Any decision rule, consisting of a discriminant function, that is a linear combination of arbitrary scalar functions of the pattern vector, may be chosen on the basis of a priori knowledge about the classes, engineering judgment, and economic considerations. The system optimizes itself by adjustment of the decision parameters according to a weighted mean-square-error performance criterion, using a multivariable search technique. The proposed performance criterion is well suited for self-optimizing search procedures. It also has the property that, as the number of training samples approaches infinity, the resulting disciminant function belongs to the class of discriminant functions, chosen at the outset, which approximates the optimum Baye's discriminant function with minimum variance. Some results from simulation studies are presented which include comparison with classical statistical techniques."
            },
            "slug": "An-Adaptive-Pattern-Classification-System-Patterson-Womack",
            "title": {
                "fragments": [],
                "text": "An Adaptive Pattern Classification System"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "An adaptive pattern classification system is described that does not require a priori knowledge of the probability density of the pattern vectors for each class, as do the classical statistical techniques."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Sci. Cybern."
            },
            "year": 1966
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2793874"
                        ],
                        "name": "W. Highleyman",
                        "slug": "W.-Highleyman",
                        "structuredName": {
                            "firstName": "Wilbur",
                            "lastName": "Highleyman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Highleyman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 51642793,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "474166c1754092367f0cb09e31bf12de929b2f19",
            "isKey": false,
            "numCitedBy": 159,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Many pattern recognition machines may be considered to consist of two principal parts, a receptor and a categorizer. The receptor makes certain measurements on the unknown pattern to be recognized; the categorizer determines from these measurements the particular allowable pattern class to which the unknown pattern belongs. This paper is concerned with the study of a particular class of categorizers, the linear decision function. The optimum linear decision function is the best linear approximation to the optimum decision function in the following sense: 1) \"Optimum\" is taken to mean minimum loss (which includes minimum error systems). 2) \"Linear\" is taken to mean that each pair of pattern classes is separated by one and only one hyperplane in the measurement space. This class of categorizers is of practical interest for two reasons: 1) It can be empirically designed without making any assumptions whatsoever about either the distribution of the receptor measurements or the a priori probabilities of occurrence of the pattern classes, providing an appropriate pattern source is available. 2) Its implementation is quite simple and inexpensive. Various properties of linear decision functions are discussed. One such property is that a linear decision function is guaranteed to perform at least as well as a minimum distance categorizer. Procedures are then developed for the estimation (or design) of the optimum linear decision function based upon an appropriate sampling from the pattern classes to be categorized."
            },
            "slug": "Linear-Decision-Functions,-with-Application-to-Highleyman",
            "title": {
                "fragments": [],
                "text": "Linear Decision Functions, with Application to Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper is concerned with the study of a particular class of categorizers, the linear decision function, which can be empirically designed without making any assumptions whatsoever about either the distribution of the receptor measurements or the a priori probabilities of occurrence of the pattern classes, providing an appropriate pattern source is available."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IRE"
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "One proposal by Amari [ 14 ] is to define the misclassification measure by where Sk = { i 1 gi (x; A) > gk (x; A)}, the set of \u201cconfusing classes,\u201d and mk is the number of confusing classes in sk. This misclassification measure apparently is motivated by the Bayes discriminant of (10)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "(The formulation of (14) was also introduced by Amari [ 14 ].) Note that the cost function e, and the misclassification measure dk can be defined individually for each class k for generality."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For minimum error classification, the following cost functions are merely two of several possibilities: a) Exponential [ 14 ]:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 31220579,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1339348aeef592802288d9d929a085cb3ae61c4b",
            "isKey": true,
            "numCitedBy": 451,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes error-correction adjustment procedures for determining the weight vector of linear pattern classifiers under general pattern distribution. It is mainly aimed at clarifying theoretically the performance of adaptive pattern classifiers. In the case where the loss depends on the distance between a pattern vector and a decision boundary and where the average risk function is unimodal, it is proved that, by the procedures proposed here, the weight vector converges to the optimal one even under nonseparable pattern distributions. The speed and the accuracy of convergence are analyzed, and it is shown that there is an important tradeoff between speed and accuracy of convergence. Dynamical behaviors, when the probability distributions of patterns are changing, are also shown. The theory is generalized and made applicable to the case with general discriminant functions, including piecewise-linear discriminant functions."
            },
            "slug": "A-Theory-of-Adaptive-Pattern-Classifiers-Amari",
            "title": {
                "fragments": [],
                "text": "A Theory of Adaptive Pattern Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "It is proved that, by the procedures proposed here, the weight vector converges to the optimal one even under nonseparable pattern distributions, and there is an important tradeoff between speed and accuracy of convergence."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Electron. Comput."
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143604406"
                        ],
                        "name": "B. Juang",
                        "slug": "B.-Juang",
                        "structuredName": {
                            "firstName": "Biing-Hwang",
                            "lastName": "Juang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Juang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712517"
                        ],
                        "name": "L. Rabiner",
                        "slug": "L.-Rabiner",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Rabiner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rabiner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2154317"
                        ],
                        "name": "J. Wilpon",
                        "slug": "J.-Wilpon",
                        "structuredName": {
                            "firstName": "Jay",
                            "lastName": "Wilpon",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Wilpon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1874483,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "8f6ce3c64c24e0f7e3ad8f104e38a7a8190a2e87",
            "isKey": false,
            "numCitedBy": 292,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we extend the interpretation of distortion measures, based upon the observation that measurements of speech spectral envelopes (as normally obtained from analysis procedures) are prone to statistical variations due to window position fluctuations, excitation interference, measurement noise, etc. and may possess spurious characteristics because of analysis model constraints. We have found that these undesirable spectral measurement variations can be controlled (i.e. reduced in the level of variation) through proper cepstral processing and that a statistical model can be established to predict the variances of the cepstral coefficient measurements. The findings lead to the use of a bandpass \"liftering\" process aimed at reducing the variability of the statistical components of spectral measurements. We have applied this liftering process to various speech recognition problems; in particular, vowel recognition and isolated word recognition. With the liftering process, we have been able to achieve an average digit error rate of 1%, which is about half of the previously reported best results, with dynamic time warping in a speaker-independent isolated digit test."
            },
            "slug": "On-the-use-of-bandpass-liftering-in-speech-Juang-Rabiner",
            "title": {
                "fragments": [],
                "text": "On the use of bandpass liftering in speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper has found that a bandpass \"liftering\" process reduces the variability of the statistical components of LPC-based spectral measurements and hence it is desirable to use such a liftering process in a speech recognizer."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144990248"
                        ],
                        "name": "R. Lippmann",
                        "slug": "R.-Lippmann",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Lippmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lippmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Recently, due to the advent of artificial neural networks (ANN) [ 6 ] and learning vector quantizers (LVQ) [7], there is a resurgent interest in reexamining the classical techniques to suit the new classifier structures."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8275028,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8778bb692cf105254fe767ef11a3a8afac4a068",
            "isKey": false,
            "numCitedBy": 3817,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Artificial neural net models have been studied for many years in the hope of achieving human-like performance in the fields of speech and image recognition. These models are composed of many nonlinear computational elements operating in parallel and arranged in patterns reminiscent of biological neural nets. Computational elements or nodes are connected via weights that are typically adapted during use to improve performance. There has been a recent resurgence in the field of artificial neural nets caused by new net topologies and algorithms, analog VLSI implementation techniques, and the belief that massive parallelism is essential for high performance speech and image recognition. This paper provides an introduction to the field of artificial neural nets by reviewing six important neural net models that can be used for pattern classification. These nets are highly parallel building blocks that illustrate neural net components and design principles and can be used to construct more complex systems. In addition to describing these nets, a major emphasis is placed on exploring how some existing classification and clustering algorithms can be performed using simple neuron-like components. Single-layer nets can implement algorithms required by Gaussian maximum-likelihood classifiers and optimum minimum-error classifiers for binary patterns corrupted by noise. More generally, the decision regions required by any classification algorithm can be generated in a straightforward manner by three-layer feed-forward nets."
            },
            "slug": "An-introduction-to-computing-with-neural-nets-Lippmann",
            "title": {
                "fragments": [],
                "text": "An introduction to computing with neural nets"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper provides an introduction to the field of artificial neural nets by reviewing six important neural net models that can be used for pattern classification and exploring how some existing classification and clustering algorithms can be performed using simple neuron-like components."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE ASSP Magazine"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46640241"
                        ],
                        "name": "S. Katagiri",
                        "slug": "S.-Katagiri",
                        "structuredName": {
                            "firstName": "Shoji",
                            "lastName": "Katagiri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Katagiri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109156106"
                        ],
                        "name": "C.-H. Lee",
                        "slug": "C.-H.-Lee",
                        "structuredName": {
                            "firstName": "C.-H.",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C.-H. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14955592"
                        ],
                        "name": "B. Juang",
                        "slug": "B.-Juang",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Juang",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Juang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62360869,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "354fed3e54475445c63b8997f25f7d8e0ab56bb9",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors propose a new family of multi-layer, feed-forward network (FFN) architectures. This framework allows examination of several feed-forward networks, including the well-known multi-layer perceptron (MLP) network, the likelihood network (LNET) and the distance network (DNET), in a unified manner. They then introduce a novel formulation which embeds network parameters into a functional form of the classifier design objective so that the network's parameters can be adjusted by gradient search algorithms, such as the generalized probabilistic descent (GPD) method. They evaluate several discriminative three-layer networks by performing a pattern classification task. They demonstrate that the performance of a network can be significantly improved when discriminative formulations are incorporated into the design of the pattern classification networks.<<ETX>>"
            },
            "slug": "Discriminative-multi-layer-feed-forward-networks-Katagiri-Lee",
            "title": {
                "fragments": [],
                "text": "Discriminative multi-layer feed-forward networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The authors introduce a novel formulation which embeds network parameters into a functional form of the classifier design objective so that the network's parameters can be adjusted by gradient search algorithms, such as the generalized probabilistic descent (GPD) method."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks for Signal Processing Proceedings of the 1991 IEEE Workshop"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1899177"
                        ],
                        "name": "Ken-ichi Funahashi",
                        "slug": "Ken-ichi-Funahashi",
                        "structuredName": {
                            "firstName": "Ken-ichi",
                            "lastName": "Funahashi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ken-ichi Funahashi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10203109,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "386cbc45ceb59a7abb844b5078e5c944f17723b4",
            "isKey": false,
            "numCitedBy": 4188,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-approximate-realization-of-continuous-by-Funahashi",
            "title": {
                "fragments": [],
                "text": "On the approximate realization of continuous mappings by neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145452404"
                        ],
                        "name": "Y. Ho",
                        "slug": "Y.-Ho",
                        "structuredName": {
                            "firstName": "Yu-Chi",
                            "lastName": "Ho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Ho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32583901"
                        ],
                        "name": "R. Kashyap",
                        "slug": "R.-Kashyap",
                        "structuredName": {
                            "firstName": "Rangasami",
                            "lastName": "Kashyap",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kashyap"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 206507307,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a99597a45057ec86c3af110e8145298a9dadc732",
            "isKey": false,
            "numCitedBy": 178,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "An exponentially convergent and finite algorithm is presented for the determination of the solution ? of the linear inequalities A?\u226b0 for a given matrix A, or for determining the non-existence of solution for A?\u226b0. This result is useful in threshold-switching theory and in pattern classification problems. Experiments indicate extremely rapid convergence of the method."
            },
            "slug": "An-Algorithm-for-Linear-Inequalities-and-its-Ho-Kashyap",
            "title": {
                "fragments": [],
                "text": "An Algorithm for Linear Inequalities and its Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "An exponentially convergent and finite algorithm is presented for the determination of the solution ? of the linear inequalities A?\u226b0 for a given matrix A, or for determining the non-existence of solution for A?\u2a7d0."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Electron. Comput."
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31577094"
                        ],
                        "name": "T. Motzkin",
                        "slug": "T.-Motzkin",
                        "structuredName": {
                            "firstName": "Theodore",
                            "lastName": "Motzkin",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Motzkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50456127"
                        ],
                        "name": "I. J. Schoenberg",
                        "slug": "I.-J.-Schoenberg",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Schoenberg",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. J. Schoenberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 123653448,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3bd7fe1a5c67bb2f1d445e95816922c11db78ae8",
            "isKey": false,
            "numCitedBy": 460,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Let A be a closed set of points in the n-dimensional euclidean space En. If p and p 1 are points of En such that 1.1 then p 1 is said to be point-wise closer than p to the set A. If p is such that there is no point p1 which is point-wise closer than p to A, then p is called a closest point to the set A."
            },
            "slug": "The-Relaxation-Method-for-Linear-Inequalities-Motzkin-Schoenberg",
            "title": {
                "fragments": [],
                "text": "The Relaxation Method for Linear Inequalities"
            },
            "venue": {
                "fragments": [],
                "text": "Canadian Journal of Mathematics"
            },
            "year": 1954
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145969445"
                        ],
                        "name": "J. K. Hawkins",
                        "slug": "J.-K.-Hawkins",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Hawkins",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. K. Hawkins"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "P linear discriminant analysis, is a very well-studied topic with most of the original developments completed in the 1960\u2019s (see [1]-[ 5 ])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 51640615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3172e6c45788cfc0fcf6d35d29473d55e1b017e4",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "The class of self-organizing systems represented by networks which learn to recognize patterns is reviewed from an historical standpoint, and some of the behavioral similarities between such nets and biological nervous systems are discussed. Examples and results of several experimental models for alphanumeric character recognition are presented. The network synthesis problem is then recast in terms of redundant information removal, multivariable curve-fitting and expansion in orthonormal functions. Recognition network structures and the learning process are described from these points of view. The potential component and behavioral advantages to be gained from sequential feedback networks are discussed briefly."
            },
            "slug": "Self-Organizing-Systems-A-Review-and-Commentary-Hawkins",
            "title": {
                "fragments": [],
                "text": "Self-Organizing Systems-A Review and Commentary"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The class of self-organizing systems represented by networks which learn to recognize patterns is reviewed from an historical standpoint, and some of the behavioral similarities between such nets and biological nervous systems are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IRE"
            },
            "year": 1961
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 62245742,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "111fd833a4ae576cfdbb27d87d2f8fc0640af355",
            "isKey": false,
            "numCitedBy": 19355,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-internal-representations-by-error-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning internal representations by error propagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144497046"
                        ],
                        "name": "N. Nilsson",
                        "slug": "N.-Nilsson",
                        "structuredName": {
                            "firstName": "Nils",
                            "lastName": "Nilsson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Nilsson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For multicategory considerations, Kesler\u2019s construction of the equivalent problem [ 4 ], of course, is advisable."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 54132942,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d725045ed29d69b7a503896841ef637383376043",
            "isKey": false,
            "numCitedBy": 445,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-Machines:-Foundations-of-Trainable-Systems-Nilsson",
            "title": {
                "fragments": [],
                "text": "Learning Machines: Foundations of Trainable Pattern-Classifying Systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49353544"
                        ],
                        "name": "R. Fisher",
                        "slug": "R.-Fisher",
                        "structuredName": {
                            "firstName": "Rory",
                            "lastName": "Fisher",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fisher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 29084021,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "ab21376e43ac90a4eafd14f0f02a0c87502b6bbf",
            "isKey": false,
            "numCitedBy": 13267,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "THE-USE-OF-MULTIPLE-MEASUREMENTS-IN-TAXONOMIC-Fisher",
            "title": {
                "fragments": [],
                "text": "THE USE OF MULTIPLE MEASUREMENTS IN TAXONOMIC PROBLEMS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1936
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Systematic explanation of learning vector quantization and multilayer perceptron-proposition of distance network,"
            },
            "venue": {
                "fragments": [],
                "text": "IEICE, MBE 88-72,"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The perceptron-a perceiving and recognizing automation,"
            },
            "venue": {
                "fragments": [],
                "text": "Rep. 85-460-1,"
            },
            "year": 1957
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "S\u201979-M\u20198O-SM\u201987-F\u201992) received the B.Sc. degree in electrical engineering from National Taiwan University, Taipei, in 1973 and the M.Sc"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 6
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 18,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Discriminative-learning-for-minimum-error-[pattern-Juang-Katagiri/e142635b24b57cfeb20e0e69bf7836dabe44aa7f?sort=total-citations"
}