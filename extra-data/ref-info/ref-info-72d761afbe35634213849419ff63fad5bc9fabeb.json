{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2416462"
                        ],
                        "name": "G. Cybenko",
                        "slug": "G.-Cybenko",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Cybenko",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Cybenko"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3958369,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8da1dda34ecc96263102181448c94ec7d645d085",
            "isKey": false,
            "numCitedBy": 6388,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks."
            },
            "slug": "Approximation-by-superpositions-of-a-sigmoidal-Cybenko",
            "title": {
                "fragments": [],
                "text": "Approximation by superpositions of a sigmoidal function"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "It is demonstrated that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Control. Signals Syst."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 36671080,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a35203c70c6ed6a95faacd4f1c71a51692af37fb",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The probably approximately correct (PAC) model of learning from examples is generalized. The problem of learning functions from a set X into a set Y is considered, assuming only that the examples are generated by independent draws according to an unknown probability measure on X*Y. The learner's goal is to find a function in a given hypothesis space of functions from X into Y that on average give Y values that are close to those observed in random examples. The discrepancy is measured by a bounded real-valued loss function. The average loss is called the error of the hypothesis. A theorem on the uniform convergence of empirical error estimates to true error rates is given for certain hypothesis spaces, and it is shown how this implies learnability. A generalized notion of VC dimension that applies to classes of real-valued functions and a notion of capacity for classes of functions that map into a bounded metric space are given. These measures are used to bound the rate of convergence of empirical error estimates to true error rates, giving bounds on the sample size needed for learning using hypotheses in these classes. As an application, a distribution-independent uniform convergence result for certain classes of functions computed by feedforward neural nets is obtained. Distribution-specific uniform convergence results for classes of functions that are uniformly continuous on average are also obtained.<<ETX>>"
            },
            "slug": "Generalizing-the-PAC-model:-sample-size-bounds-from-Haussler",
            "title": {
                "fragments": [],
                "text": "Generalizing the PAC model: sample size bounds from metric dimension-based uniform convergence results"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "The probably approximately correct (PAC) model of learning from examples is generalized, and a distribution-independent uniform convergence result for certain classes of functions computed by feedforward neural nets is obtained."
            },
            "venue": {
                "fragments": [],
                "text": "30th Annual Symposium on Foundations of Computer Science"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2477489"
                        ],
                        "name": "L. Devroye",
                        "slug": "L.-Devroye",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Devroye",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Devroye"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6686370,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e85a68602abf92fcc1efb8b7aa90d27d141a80c2",
            "isKey": false,
            "numCitedBy": 150,
            "numCiting": 149,
            "paperAbstract": {
                "fragments": [],
                "text": "A test sequence is used to select the best rule from a class of discrimination rules defined in terms of the training sequence. The Vapnik-Chervonenkis and related inequalities are used to obtain distribution-free bounds on the difference between the probability of error of the selected rule and the probability of error of the best rule in the given class. The bounds are used to prove the consistency and asymptotic optimality for several popular classes, including linear discriminators, nearest-neighbor rules, kernel-based rules, histogram rules, binary tree classifiers, and Fourier series classifiers. In particular, the method can be used to choose the smoothing parameter in kernel-based rules, to choose k in the k-nearest neighbor rule, and to choose between parametric and nonparametric rules. >"
            },
            "slug": "Automatic-Pattern-Recognition:-A-Study-of-the-of-Devroye",
            "title": {
                "fragments": [],
                "text": "Automatic Pattern Recognition: A Study of the Probability of Error"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The Vapnik-Chervonenkis method can be used to choose the smoothing parameter in kernel-based rules, to choose k in the k-nearest neighbor rule, and to choose between parametric and nonparametric rules."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144962533"
                        ],
                        "name": "C. Canuto",
                        "slug": "C.-Canuto",
                        "structuredName": {
                            "firstName": "Claudio",
                            "lastName": "Canuto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Canuto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3167600"
                        ],
                        "name": "A. Quarteroni",
                        "slug": "A.-Quarteroni",
                        "structuredName": {
                            "firstName": "Alfio",
                            "lastName": "Quarteroni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Quarteroni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 32104987,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "dd9691fafbc6225ea50bed3fc7a898fbc6d3192e",
            "isKey": false,
            "numCitedBy": 436,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We analyze the approximation properties of some interpolation operators and some L2-orthogonal projection operators related to systems of polynomials which are orthonormal with respect to a weight function o(x1, . . ., Xd), d > 1. The error estimates for the Legendre system and the Chebyshev system of the first kind are given in the norms of the Sobolev spaces H'. These results are useful in the numerical analysis of the approximation of partial differential equations by spectral methods. 0. Introduction. Spectral methods are a classical and largely used technique to solve differential equations, both theoretically and numerically. During the years they have gained new popularity in automatic computations for a wide class of physical problems (for instance in the fields of fluid and gas dynamics), due to the use of the Fast Fourier Transform algorithm. These methods appear to be competitive with finite difference and finite element methods and they must be decisively preferred to the last ones whenever the solution is highly regular and the geometric dimension of the domain becomes large. Moreover, by these methods it is possible to control easily the solution (filtering) of those numerical problems affected by oscillation and instability phenomena. The use of spectral and pseudo-spectral methods in computations in many fields of engineering has been matched by deeper theoretical studies; let us recall here the pioneering works by Orszag [25], [26], Kreiss and Oliger [14] and the monograph by Gottlieb and Orszag [13]. The theoretical results of such works are mainly concerned with the study of the stability of approximation of parabolic and hyperbolic equations; the solution is assumed to be infinitely differentiable, so that by an analysis of the Fourier coefficients an infinite order of convergence can be achieved. More recently (see Pasciak [27], Canuto and Quarteroni [10], [11], Maday and Quarteroni [20], [211, [22], Mercier [23]), the spectral methods have been studied by the variational techniques typical of functional analysis, to point out the dependence of the approximation error (for instance in the L2-norm, or in the energy norm) on the regularity of the solution of continuous problems and on the discretization parameter (the dimension of the space in which the approximate solution is sought). Indeed, often the solution is not infinitely differentiable; on the other hand, sometimes even if the solution is smooth, its derivatives may have very Received August 9, 1980; revised June 12, 1981. 1980 Mathematics Subject Classification. Primary 41A25; Secondary 41A 10, 41A05. ? 1982 American Mathematical Society 0025-571 8/82/0000-0470/$06.00 (67 This content downloaded from 207.46.13.111 on Tue, 09 Aug 2016 06:29:39 UTC All use subject to http://about.jstor.org/terms 68 C. CANUTO AND A. QUARTERONI large norms which affect negatively the rate of convergence (for instance in problems with boundary layers). Both spectral and pseudo-spectral methods are essentially Ritz-Galerkin methods (combined with some integration formulae in the pseudo-spectral case). It is well known that when Galerkin methods are used the distance between the exact and the discrete solution (approximation error) is bounded by the distance between the exact solution and its orthogonal projection upon the subspace (projection error), or by the distance between the exact solution and its interpolated polynomial at some suitable points (interpolation error). This upper bound is often realistic, in the sense that the asymptotic behavior of the approximation error is not better than the one of the projection (or even the interpolation) error. Even more, in some cases the approximate solution coincides with the projection of the true solution upon the subspace (for instance when linear problems with constant coefficients are approximated by spectral methods). This motivates the interest in evaluating the projection and the interpolation errors in differently weighted Sobolev norms. So we must face a situation different from the one of the classical approximation theory where the properties of approximation of orthogonal function systems, polynomial and trigonometric, are studied in the LP-norms, and mostly in the maximum norm (see, e.g., Butzer and Berens [6], Butzer and Nessel [7], Nikol'skiT [24], Sansone [291, Szego [30], Triebel [31], Zygmund [32]; see also Bube [5]). Approximation results in Sobolev norms for the trigonometric system have been obtained by Kreiss and Oliger [15]. In this paper we consider the systems of Legendre orthogonal polynomials, and of Chebyshev orthogonal polynomials of the first kind in dimension d > 1. The reason for this interest must be sought in the applications to spectral approximations of boundary value problems. Indeed, if the boundary conditions are not periodic, Legendre approximation seems to be the easiest to be investigated (the weight w is equal to 1). On the other hand, the Chebyshev approximation is the most effective for practical computations since it allows the use of the Fast Fourier Transform algorithm. The techniques used to obtain our results are based on the representation of a function in the terms of a series of orthogonal polynomials, on the use of the so-called inverse inequality, and finally on the operator interpolation theory in Banach spaces. For the theory of interpolation we refer for instance to Calderon [8], Lions [17], Lions and Peetre [19], Peetre [28]; a recent survey is given, e.g., by Bergh and Lofstrom [4]. An outline of the paper is as follows. In Section 1 some approximation results for the trigonometric system are recalled; the presentation of the results to the interpolation is made in the spirit of what will be its application to Chebyshev polynomials. In Section 2 we consider the La-projection operator upon the space of polynomials of degree at most N in any variable (w denotes the Chebyshev or Legendre weight). In Section 3 a general interpolation operator, built up starting by integration formulas which are not necessarily the same in different spatial dimensions, is considered, and its approximation properties are studied. In [22] Maday and Quarteroni use the results of Section 2 to study the approximation properties of some projection operators in higher order Sobolev norms. Recently, an interesting method which lies inbetween finite elements and This content downloaded from 207.46.13.111 on Tue, 09 Aug 2016 06:29:39 UTC All use subject to http://about.jstor.org/terms ORTHOGONAL POLYNOMIALS IN SOBOLEV SPACES 69 spectral methods has been investigated from the theoretical point of view by Babuska, Szabo and Katz [3]. In particular they obtain approximation properties of polynomials in the norms of the usual Sobolev spaces. Acknowledgements. Some of the results of this paper were announced in [9]; we thank Professor J. L. Lions for the presentation to the C. R. Acad. Sci. of Paris. We also wish to express our gratitude to Professors F. Brezzi and P. A. Raviart for helpful suggestions and continuous encouragement. Notations. Throughout this paper we shall use the following notations: I will be an open bounded interval c R, whose variable is denoted by x; Q the product Id C Rd (d integer > 1) whose variable is denoted by x = (x(.')I_ d; for a multi-integer k E Zd, we set ikV = jd X I'12 and IkloK = m x 1, Dj = a/ax@). The symbol X'J=p (q eventually + oo) will denote the summation over all integral k such that p 0 in U. Set L2(Q) = ({: Q -C I 0 is measurable and ( 0, set Hs ( () = C E L(Q) I 1111ksI, < +?}, where /d 2 11I412I= kENd f DI L/)4 D w dx."
            },
            "slug": "Approximation-results-for-orthogonal-polynomials-in-Canuto-Quarteroni",
            "title": {
                "fragments": [],
                "text": "Approximation results for orthogonal polynomials in Sobolev spaces"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 122569569,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ae7beb7920485aca9c252ce3ecc3972c52eb3c37",
            "isKey": false,
            "numCitedBy": 1833,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "of the number of bits required to write down the observed data, has been reformulated to extend the classical maximum likelihood principle. The principle permits estimation of the number of the parameters in statistical models in addition to their values and even of the way the parameters appear in the models; i.e., of the model structures. The principle rests on a new way to interpret and construct a universal prior distribution for the integers, which makes sense even when the parameter is an individual object. Truncated realvalued parameters are converted to integers by dividing them by their precision, and their prior is determined from the universal prior for the integers by optimizing the precision. 1. Introduction. In this paper we study estimation based upon the principle of minimizing the total number of binary digits required to rewrite the observed data, when each observation is given with some precision. Instead of attempting at an absolutely shortest description, which would be futile, we look for the optimum relative to a class of parametrically given distributions. This Minimum Description Length (MDL) principle, which we introduced in a less comprehensive form in [25], turns out to degenerate to the more familiar Maximum Likelihood (ML) principle in case the number of parameters in the models is fixed, so that the description length of the parameters themselves can be ignored. In another extreme case, where the parameters determine the data, it similarly degenerates to Jaynes's principle of maximum entropy, [14]. But the main power of the new criterion is that it permits estimates of the entire model, its parameters, their number, and even the way the parameters appear in the model; i.e., the model structure. Hence, there will be no need to supplement the estimated parameters with a separate hypothesis test to decide whether a model is adequately parameterized or, perhaps, over parameterized."
            },
            "slug": "A-UNIVERSAL-PRIOR-FOR-INTEGERS-AND-ESTIMATION-BY-Rissanen",
            "title": {
                "fragments": [],
                "text": "A UNIVERSAL PRIOR FOR INTEGERS AND ESTIMATION BY MINIMUM DESCRIPTION LENGTH"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34923849"
                        ],
                        "name": "G. Lorentz",
                        "slug": "G.-Lorentz",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Lorentz",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lorentz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 121058713,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9744f03355bccb69a0563d1f47d59206e5f9e9ec",
            "isKey": false,
            "numCitedBy": 1067,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Possibility of Approximation: 1. Basic notions 2. Linear operators 3. Approximation theorems 4. The theorem of Stone 5. Notes Polynomials of Best Approximation: 1. Existence of polynomials of best approximation 2. Characterization of polynomials of best approximation 3. Applications of convexity 4. Chebyshev systems 5. Uniqueness of polynomials of best approximation 6. Chebyshev's theorem 7. Chebyshev polynomials 8. Approximation of some complex functions 9. Notes Properties of Polynomials and Moduli of Continuity: 1. Interpolation 2. Inequalities of Bernstein 3. The inequality of Markov 4. Growth of polynomials in the complex plane 5. Moduli of continuity 6. Moduli of smoothness 7. Classes of functions 8. Notes The Degree of Approximation by Trigonometric Polynomials: 1. Generalities 2. The theorem of Jackson 3. The degree of approximation of differentiable functions 4. Inverse theorems 5. Differentiable functions 6. Notes The Degree of Approximation by Algebraic Polynomials: 1. Preliminaries 2. The approximation theorems 3. Inequalities for the derivatives of polynomials 4. Inverse theorems 5. Approximation of analytic functions 6. Notes Approximation by Rational Functions. Functions of Several Variables: 1. Degree of rational approximation 2. Inverse theorems 3. Periodic functions of several variables 4. Approximation by algebraic polynomials 5. Notes Approximation by Linear Polynomial Operators: 1. Sums of de la Vallee-Poussin. Positive operators 2. The principle of uniform boundedness 3. Operators that preserve trigonometric polynomials 4. Trigonometric saturation classes 5. The saturation class of the Bernstein polynomials 6. Notes Approximation of Classes of Functions: 1. Introduction 2. Approximation in the space 3. The degree of approximation of the classes 4. Distance matrices 5. Approximation of the classes 6. Arbitrary moduli of continuity Approximation by operators 7. Analytic functions 8. Notes Widths: 1. Definitions and basic properties 2. Sets of continuous and differentiable functions 3. Widths of balls 4. Applications of theorem 2 5. Differential operators 6. Widths of the sets 7. Notes Entropy: 1. Entropy and capacity 2. Sets of continuous and differentiable functions 3. Entropy of classes of analytic functions 4. More general sets of analytic functions 5. Relations between entropy and widths 6. Notes Representation of Functions of Several Variables by Functions of One Variable: 1. The Theorem of Kolmogorov 2. The fundamental lemma 3. The completion of the proof 4. Functions not representable by superpositions 5. Notes Bibliography Index."
            },
            "slug": "Approximation-of-Functions-Lorentz",
            "title": {
                "fragments": [],
                "text": "Approximation of Functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1966
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60899176,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "249ce7a85b158c16ba108451070c07aa1156e7eb",
            "isKey": false,
            "numCitedBy": 1140,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Very rarely, a book is published which not only advances our knowledge of a particular topic, but fundamentally recasts our methods of investigating and thinking about large tracts of the map of learning. Linguists remember 1957 as the publication year of Noam Chomsky's Syntactic structures-a book whose ostensible subjects were the structure of English grammatical rules and the goals of grammatical description, but which can be seen with hindsight as the first shot in an intellectual revolution which ended by radically changing the texture of day-to-day research activity and discourse throughout almost all of linguistics, and in substantial parts of other cognition-related disciplines. In decades to come, perhaps 1986 will be remembered by academics as the year of publication of the pair of volumes reviewed here: they constitute the first large-scale public statement of an intellectual paradigm fully as revolutionary as the generative paradigm ever was (there have been scattered journal articles in the preceding four or five years). I would go further and suggest that, if the promises of this book can be redeemed, the contrast in linguistics and neighboring disciplines between the 1990's and the 1970's will be significantly greater than the contrast between the 1970's and the 1950's. (I need hardly add, of course, that it is one thing to fire an opening salvo, but another to achieve ultimate predominance.) The new paradigm is called Parallel Distributed Processing by the sixteen writers who contributed to this book, many of whom work either at the University of California, San Diego, or at Carnegie-Mellon University in Pittsburgh. Some other researchers (e.g. Feldman 1985) use the term 'connectionism' for the same concept. These two volumes comprise 26 chapters which, among them, (i) explain the over-all nature and aims of PDP/connectionist models, (ii) define a family of specific variants of the general paradigm, and (iii) exemplify it by describing experiments in which PDP models were used to simulate human performance in various cognitive domains. The experiments, inevitably, treat their respective domains in a simplified, schematic way by comparison with the endless complexity found in any real-life cognitive area; but simplification in this case does not mean trivialization. There are also auxiliary chapters on relevant related topics; thus Chap. 9, by M. I. JORDAN, is a tutorial on linear algebra, a branch of mathematics having special significance for the PDP paradigm. (Each chapter is attributed to a particular author or"
            },
            "slug": "Parallel-Distributed-Processing:-Explorations-in-of-Rumelhart-McClelland",
            "title": {
                "fragments": [],
                "text": "Parallel Distributed Processing: Explorations in the microstructures of cogni-"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "In decades to come, perhaps 1986 will be remembered by academics as the year of publication of the pair of volumes reviewed here: they constitute the first large-scale public statement of an intellectual paradigm fully as revolutionary as the generative paradigm ever was."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684300"
                        ],
                        "name": "W. Stuetzle",
                        "slug": "W.-Stuetzle",
                        "structuredName": {
                            "firstName": "Werner",
                            "lastName": "Stuetzle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Stuetzle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14183758,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "589b8659007e1124f765a5d1bd940b2bf4d79054",
            "isKey": false,
            "numCitedBy": 2178,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract A new method for nonparametric multiple regression is presented. The procedure models the regression surface as a sum of general smooth functions of linear combinations of the predictor variables in an iterative manner. It is more general than standard stepwise and stagewise regression procedures, does not require the definition of a metric in the predictor space, and lends itself to graphical interpretation."
            },
            "slug": "Projection-Pursuit-Regression-Friedman-Stuetzle",
            "title": {
                "fragments": [],
                "text": "Projection Pursuit Regression"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48627980"
                        ],
                        "name": "R. Shibata",
                        "slug": "R.-Shibata",
                        "structuredName": {
                            "firstName": "Ritei",
                            "lastName": "Shibata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Shibata"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Another approach is to use one of several related criteria (cross validation, Mallows Cp, Akaike's AIC, or the predicted squared error [4, Chapter 41) which have been shown to have certain asymptotic optimality properties for selection problems with nested linear models, see [ 12 ],[13]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122369511,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "242e18e7c5bbcd1bfc63c89d9e086a7340ab4177",
            "isKey": false,
            "numCitedBy": 543,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY An asymptotically optimal selection of regression variables is proposed. The key assumption is that the number of control variables is infinite or increases with the sample size. It is also shown that Mallows's Qp, Akaike's FPE -and AIC methods are all asymptotically equivalent to this method."
            },
            "slug": "An-optimal-selection-of-regression-variables-Shibata",
            "title": {
                "fragments": [],
                "text": "An optimal selection of regression variables"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2637505"
                        ],
                        "name": "D. Cox",
                        "slug": "D.-Cox",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "Cox",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cox"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 122141195,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1d9e82ab55414e9e86e46996800ffd4226c545e2",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "On presente une methode generale pour approximer le biais et la variance dans une echelle de normes hilbertiennes"
            },
            "slug": "Approximation-of-Least-Squares-Regression-on-Nested-Cox",
            "title": {
                "fragments": [],
                "text": "Approximation of Least Squares Regression on Nested Subspaces"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123402533"
                        ],
                        "name": "M. C. Jones",
                        "slug": "M.-C.-Jones",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Jones",
                            "middleNames": [
                                "Chris"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. C. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1859519"
                        ],
                        "name": "R. Eubank",
                        "slug": "R.-Eubank",
                        "structuredName": {
                            "firstName": "Randy",
                            "lastName": "Eubank",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Eubank"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 124078690,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "616989e3eeb160c42ecb10eba0e62a495c7d01b9",
            "isKey": false,
            "numCitedBy": 1081,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Spline-Smoothing-and-Nonparametric-Regression.-Jones-Eubank",
            "title": {
                "fragments": [],
                "text": "Spline Smoothing and Nonparametric Regression."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "F'rojection pursuit classification,"
            },
            "venue": {
                "fragments": [],
                "text": "Department of Statistics Tech. Rep., Stan ford University, Stan ford, California,"
            },
            "year": 1980
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Organizing Methodr in Modeling: GMDH Type Algorithms, New York: Marcel"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Projection pursuit (with discussion),"
            },
            "venue": {
                "fragments": [],
                "text": "Ann. Statist., vol.13,"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistical learning networks: a unifying view,"
            },
            "venue": {
                "fragments": [],
                "text": "Computing Science and Statirtics: Proc. 20th Symp. Interface.,"
            },
            "year": 1988
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "methodology": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 15,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Statistical-properties-of-artificial-neural-Barron/72d761afbe35634213849419ff63fad5bc9fabeb?sort=total-citations"
}