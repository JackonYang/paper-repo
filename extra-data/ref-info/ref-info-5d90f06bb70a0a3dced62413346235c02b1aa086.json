{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 180
                            }
                        ],
                        "text": "1 The dataset The tiny images dataset on which we based all of our experiments was collected by colleagues at MIT and NYU over the span of six months; it is described in detail in [14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7487588,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54d2b5c64a67f65c5dd812b89e07973f97699552",
            "isKey": false,
            "numCitedBy": 1868,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "With the advent of the Internet, billions of images are now freely available online and constitute a dense sampling of the visual world. Using a variety of non-parametric methods, we explore this world with the aid of a large dataset of 79,302,017 images collected from the Internet. Motivated by psychophysical results showing the remarkable tolerance of the human visual system to degradations in image resolution, the images in the dataset are stored as 32 x 32 color images. Each image is loosely labeled with one of the 75,062 non-abstract nouns in English, as listed in the Wordnet lexical database. Hence the image database gives a comprehensive coverage of all object categories and scenes. The semantic information from Wordnet can be used in conjunction with nearest-neighbor methods to perform object classification over a range of semantic levels minimizing the effects of labeling noise. For certain classes that are particularly prevalent in the dataset, such as people, we are able to demonstrate a recognition performance comparable to class-specific Viola-Jones style detectors."
            },
            "slug": "80-Million-Tiny-Images:-A-Large-Data-Set-for-Object-Torralba-Fergus",
            "title": {
                "fragments": [],
                "text": "80 Million Tiny Images: A Large Data Set for Nonparametric Object and Scene Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "For certain classes that are particularly prevalent in the dataset, such as people, this work is able to demonstrate a recognition performance comparable to class-specific Viola-Jones style detectors."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725303"
                        ],
                        "name": "Y. Teh",
                        "slug": "Y.-Teh",
                        "structuredName": {
                            "firstName": "Yee",
                            "lastName": "Teh",
                            "middleNames": [
                                "Whye"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Teh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 156
                            }
                        ],
                        "text": "RBMs and DBNs have been shown to be capable of extracting meaningful features when trained on other vision datasets, such as hand-written digits and faces ([6, 12])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2334304,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "73e93d0346e8eee6c2ab45e46c26eaafb66e12a8",
            "isKey": false,
            "numCitedBy": 154,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a neurally-inspired, unsupervised learning algorithm that builds a non-linear generative model for pairs of face images from the same individual. Individuals are then recognized by finding the highest relative probability pair among all pairs that consist of a test image and an image whose identity is known. Our method compares favorably with other methods in the literature. The generative model consists of a single layer of rate-coded, non-linear feature detectors and it has the property that, given a data vector, the true posterior probability distribution over the feature detector activities can be inferred rapidly without iteration or approximation. The weights of the feature detectors are learned by comparing the correlations of pixel intensities and feature activations in two phases: When the network is observing real data and when it is observing reconstructions of real data generated from the feature activations."
            },
            "slug": "Rate-coded-Restricted-Boltzmann-Machines-for-Face-Teh-Hinton",
            "title": {
                "fragments": [],
                "text": "Rate-coded Restricted Boltzmann Machines for Face Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "A neurally-inspired, unsupervised learning algorithm that builds a non-linear generative model for pairs of face images from the same individual and individuals are then recognized by finding the highest relative probability pair among all pairs that consist of a test image and an image whose identity is known."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1981539"
                        ],
                        "name": "Thomas Serre",
                        "slug": "Thomas-Serre",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Serre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Serre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145128145"
                        ],
                        "name": "Lior Wolf",
                        "slug": "Lior-Wolf",
                        "structuredName": {
                            "firstName": "Lior",
                            "lastName": "Wolf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lior Wolf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747918"
                        ],
                        "name": "S. Bileschi",
                        "slug": "S.-Bileschi",
                        "structuredName": {
                            "firstName": "Stanley",
                            "lastName": "Bileschi",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bileschi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1996960"
                        ],
                        "name": "M. Riesenhuber",
                        "slug": "M.-Riesenhuber",
                        "structuredName": {
                            "firstName": "Maximilian",
                            "lastName": "Riesenhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Riesenhuber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "This result is in line with that found in [10]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 2179592,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71e3d9fc53ba14c2feeb7390f0dc99076553b05a",
            "isKey": false,
            "numCitedBy": 1714,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new general framework for the recognition of complex visual scenes, which is motivated by biology: We describe a hierarchical system that closely follows the organization of visual cortex and builds an increasingly complex and invariant feature representation by alternating between a template matching and a maximum pooling operation. We demonstrate the strength of the approach on a range of recognition tasks: From invariant single object recognition in clutter to multiclass categorization problems and complex scene understanding tasks that rely on the recognition of both shape-based as well as texture-based objects. Given the biological constraints that the system had to satisfy, the approach performs surprisingly well: It has the capability of learning from only a few training examples and competes with state-of-the-art systems. We also discuss the existence of a universal, redundant dictionary of features that could handle the recognition of most object categories. In addition to its relevance for computer vision, the success of this approach suggests a plausibility proof for a class of feedforward models of object recognition in cortex"
            },
            "slug": "Robust-Object-Recognition-with-Cortex-Like-Serre-Wolf",
            "title": {
                "fragments": [],
                "text": "Robust Object Recognition with Cortex-Like Mechanisms"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A hierarchical system that closely follows the organization of visual cortex and builds an increasingly complex and invariant feature representation by alternating between a template matching and a maximum pooling operation is described."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187218"
                        ],
                        "name": "A. J. Bell",
                        "slug": "A.-J.-Bell",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Bell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6219133,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ca1d23be869380ac9e900578c601c2d1febcc0c9",
            "isKey": false,
            "numCitedBy": 2372,
            "numCiting": 90,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-\u201cindependent-components\u201d-of-natural-scenes-are-Bell-Sejnowski",
            "title": {
                "fragments": [],
                "text": "The \u201cindependent components\u201d of natural scenes are edge filters"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3087941"
                        ],
                        "name": "Pascal Lamblin",
                        "slug": "Pascal-Lamblin",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Lamblin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Lamblin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32384143"
                        ],
                        "name": "D. Popovici",
                        "slug": "D.-Popovici",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Popovici",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Popovici"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777528"
                        ],
                        "name": "H. Larochelle",
                        "slug": "H.-Larochelle",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Larochelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Larochelle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 34
                            }
                        ],
                        "text": "This type of model is explored in [6, 2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14201947,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "355d44f53428b1ac4fb2ab468d593c720640e5bd",
            "isKey": false,
            "numCitedBy": 3427,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization."
            },
            "slug": "Greedy-Layer-Wise-Training-of-Deep-Networks-Bengio-Lamblin",
            "title": {
                "fragments": [],
                "text": "Greedy Layer-Wise Training of Deep Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "These experiments confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13456135,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "939d584316be99e2db3fec3fbf7d71f22a477f67",
            "isKey": false,
            "numCitedBy": 340,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a distribution model for binary vectors, called the influence combination model and show how this model can be used as the basis for unsupervised learning algorithms for feature selection. The model can be represented by a particular type of Boltzmann machine with a bipartite graph structure that we call the combination machine. This machine is closely related to the Harmonium model defined by Smolensky. In the first part of the paper we analyze properties of this distribution representation scheme. We show that arbitrary distributions of binary vectors can be approximated by the combination model. We show how the weight vectors in the model can be interpreted as high order correlation patterns among the input bits, and how the combination machine can be used as a mechanism for detecting these patterns. We compare the combination model with the mixture model and with principle component analysis. In the second part of the paper we present two algorithms for learning the combination model from examples. The first learning algorithm is the standard gradient ascent heuristic for computing maximum likelihood estimates for the parameters of the model. Here we give a closed form for this gradient that is significantly easier to compute than the corresponding gradient for the general Boltzmann machine. The second learning algorithm is a greedy method that creates the hidden units and computes their weights one at a time. This method is a variant of projection pursuit density estimation. In the third part of the paper we give experimental results for these learning methods on synthetic data and on natural data of handwritten digit images."
            },
            "slug": "Unsupervised-Learning-of-Distributions-of-Binary-Freund-Haussler",
            "title": {
                "fragments": [],
                "text": "Unsupervised Learning of Distributions of Binary Vectors Using 2-Layer Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "It is shown that arbitrary distributions of binary vectors can be approximated by the combination model and shown how the weight vectors in the model can be interpreted as high order correlation patterns among the input bits, and how the combination machine can be used as a mechanism for detecting these patterns."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145797336"
                        ],
                        "name": "Iain Murray",
                        "slug": "Iain-Murray",
                        "structuredName": {
                            "firstName": "Iain",
                            "lastName": "Murray",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Iain Murray"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 331,
                                "start": 328
                            }
                        ],
                        "text": "If one makes the size of the second hidden layer the same as the size of the rst hidden layer and initializes the weights of the second from the weights of the rst, it can be proven that training the second hidden layer while keeping the rst hidden layer's weights constant improves the log likelihood of the dataunder the model[9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 458722,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "08d0ea90b53aba0008d25811268fe46562cfb38c",
            "isKey": false,
            "numCitedBy": 459,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep Belief Networks (DBN's) are generative models that contain many layers of hidden variables. Efficient greedy algorithms for learning and approximate inference have allowed these models to be applied successfully in many application domains. The main building block of a DBN is a bipartite undirected graphical model called a restricted Boltzmann machine (RBM). Due to the presence of the partition function, model selection, complexity control, and exact maximum likelihood learning in RBM's are intractable. We show that Annealed Importance Sampling (AIS) can be used to efficiently estimate the partition function of an RBM, and we present a novel AIS scheme for comparing RBM's with different architectures. We further show how an AIS estimator, along with approximate inference, can be used to estimate a lower bound on the log-probability that a DBN model with multiple hidden layers assigns to the test data. This is, to our knowledge, the first step towards obtaining quantitative results that would allow us to directly assess the performance of Deep Belief Networks as generative models of data."
            },
            "slug": "On-the-quantitative-analysis-of-deep-belief-Salakhutdinov-Murray",
            "title": {
                "fragments": [],
                "text": "On the quantitative analysis of deep belief networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that Annealed Importance Sampling (AIS) can be used to efficiently estimate the partition function of an RBM, and a novel AIS scheme for comparing RBM's with different architectures is presented."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 88
                            }
                        ],
                        "text": "(2)The model we use is also di erent from the product of uni-Gauss experts presented in [5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 30
                            }
                        ],
                        "text": "However, it has been shown in [5] that this expectation can be approximated well in nite time by a procedure known as Contrastive Divergence (CD)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207596505,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9360e5ce9c98166bb179ad479a9d2919ff13d022",
            "isKey": false,
            "numCitedBy": 4567,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual expert models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called contrastive divergence whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data."
            },
            "slug": "Training-Products-of-Experts-by-Minimizing-Hinton",
            "title": {
                "fragments": [],
                "text": "Training Products of Experts by Minimizing Contrastive Divergence"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A product of experts (PoE) is an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary because it is hard even to approximate the derivatives of the renormalization term in the combination rule."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 156
                            }
                        ],
                        "text": "RBMs and DBNs have been shown to be capable of extracting meaningful features when trained on other vision datasets, such as hand-written digits and faces ([6, 12])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 146
                            }
                        ],
                        "text": "One can also use the features learned by an RBM to initialize a neural network, and this gave us our best results (this approach was presented in [6])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 34
                            }
                        ],
                        "text": "This type of model is explored in [6, 2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1658773,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e",
            "isKey": false,
            "numCitedBy": 14635,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such \u201cautoencoder\u201d networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data."
            },
            "slug": "Reducing-the-Dimensionality-of-Data-with-Neural-Hinton-Salakhutdinov",
            "title": {
                "fragments": [],
                "text": "Reducing the Dimensionality of Data with Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work describes an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748557"
                        ],
                        "name": "P. Smolensky",
                        "slug": "P.-Smolensky",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Smolensky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Smolensky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 533055,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4f7476037408ac3d993f5088544aab427bc319c1",
            "isKey": false,
            "numCitedBy": 1947,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : At this early stage in the development of cognitive science, methodological issues are both open and central. There may have been times when developments in neuroscience, artificial intelligence, or cognitive psychology seduced researchers into believing that their discipline was on the verge of discovering the secret of intelligence. But a humbling history of hopes disappointed has produced the realization that understanding the mind will challenge the power of all these methodologies combined. The work reported in this chapter rests on the conviction that a methodology that has a crucial role to play in the development of cognitive science is mathematical analysis. The success of cognitive science, like that of many other sciences, will, I believe, depend upon the construction of a solid body of theoretical results: results that express in a mathematical language the conceptual insights of the field; results that squeeze all possible implications out of those insights by exploiting powerful mathematical techniques. This body of results, which I will call the theory of information processing, exists because information is a concept that lends itself to mathematical formalization. One part of the theory of information processing is already well-developed. The classical theory of computation provides powerful and elegant results about the notion of effective procedure, including languages for precisely expressing them and theoretical machines for realizing them."
            },
            "slug": "Information-processing-in-dynamical-systems:-of-Smolensky",
            "title": {
                "fragments": [],
                "text": "Information processing in dynamical systems: foundations of harmony theory"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The work reported in this chapter rests on the conviction that a methodology that has a crucial role to play in the development of cognitive science is mathematical analysis."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2957517"
                        ],
                        "name": "T. Tieleman",
                        "slug": "T.-Tieleman",
                        "structuredName": {
                            "firstName": "Tijmen",
                            "lastName": "Tieleman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Tieleman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 160
                            }
                        ],
                        "text": "The speci c algorithm we will describe is a distributed version of CD-1, but the principle remains the same for any variant of CD, including Persistent CD (PCD)[13]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7330145,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "73d6a26f407db77506959fdf3f7b853e44f3844a",
            "isKey": false,
            "numCitedBy": 901,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "A new algorithm for training Restricted Boltzmann Machines is introduced. The algorithm, named Persistent Contrastive Divergence, is different from the standard Contrastive Divergence algorithms in that it aims to draw samples from almost exactly the model distribution. It is compared to some standard Contrastive Divergence and Pseudo-Likelihood algorithms on the tasks of modeling and classifying various types of data. The Persistent Contrastive Divergence algorithm outperforms the other algorithms, and is equally fast and simple."
            },
            "slug": "Training-restricted-Boltzmann-machines-using-to-the-Tieleman",
            "title": {
                "fragments": [],
                "text": "Training restricted Boltzmann machines using approximations to the likelihood gradient"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A new algorithm for training Restricted Boltzmann Machines is introduced, which is compared to some standard Contrastive Divergence and Pseudo-Likelihood algorithms on the tasks of modeling and classifying various types of data."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144096985"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Miller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 116
                            }
                        ],
                        "text": "They assembled it by searching the web for images of every non-abstract English noun in the lexical database WordNet[15, 8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1671874,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "68c03788224000794d5491ab459be0b2a2c38677",
            "isKey": false,
            "numCitedBy": 13886,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4]."
            },
            "slug": "WordNet:-A-Lexical-Database-for-English-Miller",
            "title": {
                "fragments": [],
                "text": "WordNet: A Lexical Database for English"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "WordNet1 provides a more effective combination of traditional lexicographic information and modern computing, and is an online lexical database designed for use under program control."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1392098997"
                        ],
                        "name": "Peter Van Coutren",
                        "slug": "Peter-Van-Coutren",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Van Coutren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Van Coutren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2742282"
                        ],
                        "name": "A. Mezza",
                        "slug": "A.-Mezza",
                        "structuredName": {
                            "firstName": "A",
                            "lastName": "Mezza",
                            "middleNames": [
                                "Di"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mezza"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 246189889,
            "fieldsOfStudy": [],
            "id": "f617117ce62947267b6aaec14e9c83815b0fddfa",
            "isKey": false,
            "numCitedBy": 241,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Bibliography-Coutren-Mezza",
            "title": {
                "fragments": [],
                "text": "Bibliography"
            },
            "venue": {
                "fragments": [],
                "text": "More Than They Promised"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "If a category has two meanings (like mouse), only include the main meaning. If there is doubt about what this is, then ask"
            },
            "venue": {
                "fragments": [],
                "text": "If a category has two meanings (like mouse), only include the main meaning. If there is doubt about what this is, then ask"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "WordNet can be found at http://wordnet.princeton"
            },
            "venue": {
                "fragments": [],
                "text": "WordNet can be found at http://wordnet.princeton"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 5,
            "methodology": 4,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 15,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Learning-Multiple-Layers-of-Features-from-Tiny-Krizhevsky/5d90f06bb70a0a3dced62413346235c02b1aa086?sort=total-citations"
}