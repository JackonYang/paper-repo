{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39671808"
                        ],
                        "name": "Fu-Sheng Tsung",
                        "slug": "Fu-Sheng-Tsung",
                        "structuredName": {
                            "firstName": "Fu-Sheng",
                            "lastName": "Tsung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fu-Sheng Tsung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48524582"
                        ],
                        "name": "G. Cottrell",
                        "slug": "G.-Cottrell",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Cottrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Cottrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734225"
                        ],
                        "name": "A. Selverston",
                        "slug": "A.-Selverston",
                        "structuredName": {
                            "firstName": "Allen",
                            "lastName": "Selverston",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Selverston"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 179
                            }
                        ],
                        "text": "Supervised learning in recurrent neural networks has been extensively applied to speech recognition, language processing [2, 5, 6], and the modeling of biological neural networks [1, 11, 16, 18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 46607341,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "03ac166407838478d33a4603f6f27e047a513d64",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors focus on limit cycle experiments with neural nets, showing that it is possible for standard sigmoidal unit networks to learn stable, collective oscillations involving tens of units. The authors also model a biological network oscillator, showing that recurrent networks can help gain useful insights into the biological system. The R. Williams and D. Zipser (1989) learning algorithm was used with the teacher-forcing technique during the learning phase"
            },
            "slug": "Some-experiments-on-learning-stable-network-Tsung-Cottrell",
            "title": {
                "fragments": [],
                "text": "Some experiments on learning stable network oscillations"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "It is shown that it is possible for standard sigmoidal unit networks to learn stable, collective oscillations involving tens of units and a biological network oscillator is model, showing that recurrent networks can help gain useful insights into the biological system."
            },
            "venue": {
                "fragments": [],
                "text": "1990 IJCNN International Joint Conference on Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700974"
                        ],
                        "name": "Barak A. Pearlmutter",
                        "slug": "Barak-A.-Pearlmutter",
                        "structuredName": {
                            "firstName": "Barak",
                            "lastName": "Pearlmutter",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barak A. Pearlmutter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 178
                            }
                        ],
                        "text": "@ < E >\n@w\nkl\n=\n1\nT\nZ\nT\n0\nn\nX\ni=1\ni\n(t)p\nkl i (t)dt\n=\n1\nT\nZ\nT\n0\nn\nX\ni=1\nq\ni\n(t)\nkl i (t)dt =\n1\nT\nZ\nT\n0\nq\nk\n(t)y\nl\n(t)dt: (7)\nThis method is called back-propagation through time (BPTT) [12, 13, 14]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 59
                            }
                        ],
                        "text": "This method is called back-propagation through time (BPTT) [12, 13, 14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16813485,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "34468c0aa95a7aea212d8738ab899a69b2fc14c6",
            "isKey": false,
            "numCitedBy": 744,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Many neural network learning procedures compute gradients of the errors on the output layer of units after they have settled to their final values. We describe a procedure for finding E/wij, where E is an error functional of the temporal trajectory of the states of a continuous recurrent network and wij are the weights of that network. Computing these quantities allows one to perform gradient descent in the weights to minimize E. Simulations in which networks are taught to move through limit cycles are shown. This type of recurrent network seems particularly suited for temporally continuous domains, such as signal processing, control, and speech."
            },
            "slug": "Learning-State-Space-Trajectories-in-Recurrent-Pearlmutter",
            "title": {
                "fragments": [],
                "text": "Learning State Space Trajectories in Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A procedure for finding E/wij, where E is an error functional of the temporal trajectory of the states of a continuous recurrent network and wij are the weights of that network, which seems particularly suited for temporally continuous domains."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714997"
                        ],
                        "name": "K. Doya",
                        "slug": "K.-Doya",
                        "structuredName": {
                            "firstName": "Kenji",
                            "lastName": "Doya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Doya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143921528"
                        ],
                        "name": "S. Yoshizawa",
                        "slug": "S.-Yoshizawa",
                        "structuredName": {
                            "firstName": "Shuji",
                            "lastName": "Yoshizawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Yoshizawa"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 92
                            }
                        ],
                        "text": "It has been shown that teacher forcing is essential in the learning of oscillatory patterns [3, 17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 196
                            }
                        ],
                        "text": "@ < E >\n@w\nkl\n=\n1\nT\nZ\nT\n0\no\nX\ni=1\n@E(t)\n@x\ni\n(t)\n@x\ni\n(t)\n@w\nkl\ndt\n=\n1\nT\nZ\nT\n0\no\nX\ni=1\n(y\ni\n(t) d(t))g\n0\n(x\ni\n(t))p\nkl i (t)dt: (5)\nThis computation method is called real-time recurrent learning (RTRL) [3, 17]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 46
                            }
                        ],
                        "text": "They need only O(on 2 ) computations, whereas RTRL requires O(n 4 ) computations."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 70
                            }
                        ],
                        "text": "This computation method is called real-time recurrent learning (RTRL) [3, 17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 95
                            }
                        ],
                        "text": "Feedforward approximations of recurrent dynamics were successfully used in sequence generation [3, 9] and sequence prediction tasks [2, 5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 148
                            }
                        ],
                        "text": "Although the stability of the non-forced solution is not theoretically guaranteed, they were found to be stable in most of the computer simulations [3, 13, 17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 27248882,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fd0da2f1d2b95e5b62221a00ff132219d0c853b7",
            "isKey": false,
            "numCitedBy": 147,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Adaptive-neural-oscillator-using-continuous-time-Doya-Yoshizawa",
            "title": {
                "fragments": [],
                "text": "Adaptive neural oscillator using continuous-time back-propagation learning"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714997"
                        ],
                        "name": "K. Doya",
                        "slug": "K.-Doya",
                        "structuredName": {
                            "firstName": "Kenji",
                            "lastName": "Doya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Doya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143921528"
                        ],
                        "name": "S. Yoshizawa",
                        "slug": "S.-Yoshizawa",
                        "structuredName": {
                            "firstName": "Shuji",
                            "lastName": "Yoshizawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Yoshizawa"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 183
                            }
                        ],
                        "text": "For example, the learning of multiple limit cycle attractors, each with speci c waveforms, was successfully carried out by preprogramming the network to have multiple attractor basins[4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16166113,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9b6c5f9cd78725a3534132eb01a3b3adb8d6bb39",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "An inverse problem relating to associative memory, namely, that of finding a weight matrix such that a network has periodic attractors with the given output waveforms, is investigated. One solution to this problem is given by adaptive neural oscillator (ANO) learning. The ANO is a recurrent network of continuous-time, continuous-output model neurons. Modified back-propagation learning is performed so as to make the output waveform as similar as possible to the external input waveform. If the output waveform sufficiently resembles the input waveform, by using the output feedback waveform instead of that of the external input, the network continues an autonomous oscillation with a waveform similar to the previously given external input one. By combining ANO learning with the scheme of the associative memory network, multiple oscillatory waveforms can be stored in one neural network and can be selectively regenerated with the initial state of the network.<<ETX>>"
            },
            "slug": "Memorizing-oscillatory-patterns-in-the-analog-Doya-Yoshizawa",
            "title": {
                "fragments": [],
                "text": "Memorizing oscillatory patterns in the analog neuron network"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "By combining adaptive neural oscillator (ANO) learning with the scheme of the associative memory network, multiple oscillatory waveforms can be stored in one neural network and can be selectively regenerated with the initial state of the network."
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686829"
                        ],
                        "name": "Masa-aki Sato",
                        "slug": "Masa-aki-Sato",
                        "structuredName": {
                            "firstName": "Masa-aki",
                            "lastName": "Sato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Masa-aki Sato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144796189"
                        ],
                        "name": "K. Joe",
                        "slug": "K.-Joe",
                        "structuredName": {
                            "firstName": "Kazuki",
                            "lastName": "Joe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Joe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20922453"
                        ],
                        "name": "T. Hirahara",
                        "slug": "T.-Hirahara",
                        "structuredName": {
                            "firstName": "Tatsuya",
                            "lastName": "Hirahara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hirahara"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 29015068,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fc031e20ba23db17aebd1f463272c26ab7cb7fda",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent neural networks with arbitrary feedback connections are highly nonlinear dynamical systems exhibiting variegated complex dynamical behavior. The applications of this temporal behavior hold possibilities for information processing. Supervised learning for recurrent networks is studied with emphasis on learning aperiodic motions. APOLONN (adaptive nonlinear pair oscillators with local connections) is used for speech synthesis. The naturalness of a human's voice seems to come from fluctuations in voice source waveforms. The authors trained APOLONN to learn the voice source waveforms, including fluctuations of amplitudes and periodicities. After the learning, APOLONN was able to generate the waveforms with fluctuations. APOLONN can also generate waveforms with modulated amplitudes and frequencies by a simple scaling of the parameters. The results encourage further applications of recurrent networks"
            },
            "slug": "APOLONN-brings-us-to-the-real-world:-learning-and-Sato-Joe",
            "title": {
                "fragments": [],
                "text": "APOLONN brings us to the real world: learning nonlinear dynamics and fluctuations in nature"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The authors trained APOLONN (adaptive nonlinear pair oscillators with local connections) to learn the voice source waveforms, including fluctuations of amplitudes and periodicities, and trained it to generate waveforms with fluctuations."
            },
            "venue": {
                "fragments": [],
                "text": "1990 IJCNN International Joint Conference on Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895771"
                        ],
                        "name": "D. Zipser",
                        "slug": "D.-Zipser",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zipser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zipser"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 92
                            }
                        ],
                        "text": "It has been shown that teacher forcing is essential in the learning of oscillatory patterns [3, 17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 196
                            }
                        ],
                        "text": "@ < E >\n@w\nkl\n=\n1\nT\nZ\nT\n0\no\nX\ni=1\n@E(t)\n@x\ni\n(t)\n@x\ni\n(t)\n@w\nkl\ndt\n=\n1\nT\nZ\nT\n0\no\nX\ni=1\n(y\ni\n(t) d(t))g\n0\n(x\ni\n(t))p\nkl i (t)dt: (5)\nThis computation method is called real-time recurrent learning (RTRL) [3, 17]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 46
                            }
                        ],
                        "text": "They need only O(on 2 ) computations, whereas RTRL requires O(n 4 ) computations."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 70
                            }
                        ],
                        "text": "This computation method is called real-time recurrent learning (RTRL) [3, 17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 161
                            }
                        ],
                        "text": "In spite of the possible problems, there are many examples where gradient learning successfully trained a recurrent network to model complex dynamical behaviors [6, 15, 17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 148
                            }
                        ],
                        "text": "Although the stability of the non-forced solution is not theoretically guaranteed, they were found to be stable in most of the computer simulations [3, 13, 17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60666828,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "424710825d726e10b016204ed2bc979e2a342d10",
            "isKey": true,
            "numCitedBy": 336,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract The real-time recurrent learning algorithm is a gradient-following learning algorithm for completely recurrent networks running in continually sampled time. Here we use a series of simulation experiments to investigate the power and properties of this algorithm. In the recurrent networks studied here, any unit can be connected to any other, and any unit can receive external input. These networks run continually in the sense that they sample their inputs on every update cycle, and any unit can have a training target on any cycle. The storage required and computation time on each step are independent of time and are completely determined by the size of the network, so no prior knowledge of the temporal structure of the task being learned is required. The algorithm is nonlocal in the sense that each unit must have knowledge of the complete recurrent weight matrix and error vector. The algorithm is computationally intensive in sequential computers, requiring a storage capacity of the order of the thi..."
            },
            "slug": "Experimental-Analysis-of-the-Real-time-Recurrent-Williams-Zipser",
            "title": {
                "fragments": [],
                "text": "Experimental Analysis of the Real-time Recurrent Learning Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A series of simulation experiments are used to investigate the power and properties of the real-time recurrent learning algorithm, a gradient-following learning algorithm for completely recurrent networks running in continually sampled time."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30320952"
                        ],
                        "name": "F. Pineda",
                        "slug": "F.-Pineda",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pineda",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Pineda"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 103
                            }
                        ],
                        "text": "forcing was required to avoid degeneration of the network trajectories with di erent training patterns [13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 178
                            }
                        ],
                        "text": "@ < E >\n@w\nkl\n=\n1\nT\nZ\nT\n0\nn\nX\ni=1\ni\n(t)p\nkl i (t)dt\n=\n1\nT\nZ\nT\n0\nn\nX\ni=1\nq\ni\n(t)\nkl i (t)dt =\n1\nT\nZ\nT\n0\nq\nk\n(t)y\nl\n(t)dt: (7)\nThis method is called back-propagation through time (BPTT) [12, 13, 14]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 166
                            }
                        ],
                        "text": "In contrast, the output of a recurrent network can change drastically with an in nitesimal change in the network parameter when it passes through a bifurcation point [7, 13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 148
                            }
                        ],
                        "text": "Although the stability of the non-forced solution is not theoretically guaranteed, they were found to be stable in most of the computer simulations [3, 13, 17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 59
                            }
                        ],
                        "text": "This method is called back-propagation through time (BPTT) [12, 13, 14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 27867182,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5146d7902132bcb0b2e6fe5f607358768fc47323",
            "isKey": true,
            "numCitedBy": 207,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Dynamics-and-architecture-for-neural-computation-Pineda",
            "title": {
                "fragments": [],
                "text": "Dynamics and architecture for neural computation"
            },
            "venue": {
                "fragments": [],
                "text": "J. Complex."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895771"
                        ],
                        "name": "D. Zipser",
                        "slug": "D.-Zipser",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zipser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zipser"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 148
                            }
                        ],
                        "text": "A recurrent network with a single or a few attractor basins can be used as a non-linear adaptive lter [1, 11] or as the source of short term memory [18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 179
                            }
                        ],
                        "text": "Supervised learning in recurrent neural networks has been extensively applied to speech recognition, language processing [2, 5, 6], and the modeling of biological neural networks [1, 11, 16, 18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 35804400,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "603474c95a5f1e509211bcdb92c38ac630d44b1a",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Two decades of single unit recording in monkeys performing short-term memory tasks has established that information can be stored as sustained neural activity. The mechanism of this information storage is unknown. The learning-based model described here demonstrates that a mechanism using only the dynamic activity in recurrent networks is sufficient to account for the observed phenomena. The temporal activity patterns of neurons in the model match those of real memory-associated neurons, while the model's gating properties and attractor dynamics provide explanations for puzzling aspects of the experimental data."
            },
            "slug": "Recurrent-Network-Model-of-the-Neural-Mechanism-of-Zipser",
            "title": {
                "fragments": [],
                "text": "Recurrent Network Model of the Neural Mechanism of Short-Term Active Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The learning-based model described here demonstrates that a mechanism using only the dynamic activity in recurrent networks is sufficient to account for the observed phenomena."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 205001834,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "052b1d8ce63b07fec3de9dbb583772d860b7c769",
            "isKey": false,
            "numCitedBy": 20327,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1."
            },
            "slug": "Learning-representations-by-back-propagating-errors-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning representations by back-propagating errors"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "Back-propagation repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector, which helps to represent important features of the task domain."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2537431"
                        ],
                        "name": "Axel Cleeremans",
                        "slug": "Axel-Cleeremans",
                        "structuredName": {
                            "firstName": "Axel",
                            "lastName": "Cleeremans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Axel Cleeremans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403615640"
                        ],
                        "name": "D. Servan-Schreiber",
                        "slug": "D.-Servan-Schreiber",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Servan-Schreiber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Servan-Schreiber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 132
                            }
                        ],
                        "text": "Feedforward approximations of recurrent dynamics were successfully used in sequence generation [3, 9] and sequence prediction tasks [2, 5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 121
                            }
                        ],
                        "text": "Supervised learning in recurrent neural networks has been extensively applied to speech recognition, language processing [2, 5, 6], and the modeling of biological neural networks [1, 11, 16, 18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7741931,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd46c1b5948abe04e565a8bae6454da63a1b021e",
            "isKey": false,
            "numCitedBy": 513,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We explore a network architecture introduced by Elman (1988) for predicting successive elements of a sequence. The network uses the pattern of activation over a set of hidden units from time-step t1, together with element t, to predict element t 1. When the network is trained with strings from a particular finite-state grammar, it can learn to be a perfect finite-state recognizer for the grammar. When the network has a minimal number of hidden units, patterns on the hidden units come to correspond to the nodes of the grammar, although this correspondence is not necessary for the network to act as a perfect finite-state recognizer. We explore the conditions under which the network can carry information about distant sequential contingencies across intervening elements. Such information is maintained with relative ease if it is relevant at each intermediate step; it tends to be lost when intervening elements do not depend on it. At first glance this may suggest that such networks are not relevant to natural language, in which dependencies may span indefinite distances. However, embeddings in natural language are not completely independent of earlier information. The final simulation shows that long distance sequential contingencies can be encoded by the network even if only subtle statistical properties of embedded strings depend on the early information."
            },
            "slug": "Finite-State-Automata-and-Simple-Recurrent-Networks-Cleeremans-Servan-Schreiber",
            "title": {
                "fragments": [],
                "text": "Finite State Automata and Simple Recurrent Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A network architecture introduced by Elman (1988) for predicting successive elements of a sequence and shows that long distance sequential contingencies can be encoded by the network even if only subtle statistical properties of embedded strings depend on the early information."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3286081"
                        ],
                        "name": "S. Lockery",
                        "slug": "S.-Lockery",
                        "structuredName": {
                            "firstName": "Shawn",
                            "lastName": "Lockery",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lockery"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1840458369"
                        ],
                        "name": "Yan Fang",
                        "slug": "Yan-Fang",
                        "structuredName": {
                            "firstName": "Yan",
                            "lastName": "Fang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yan Fang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13096984,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "f8c5359e841a71480eb7436b897f51610486fde5",
            "isKey": false,
            "numCitedBy": 88,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Interneurons in leech ganglia receive multiple sensory inputs and make synaptic contacts with many motor neurons. These hidden units coordinate several different behaviors. We used physiological and anatomical constraints to construct a model of the local bending reflex. Dynamic networks were trained on experimentally derived input-output patterns using recurrent backpropagation. Units in the model were modified to include electrical synapses and multiple synaptic time constants. The properties of the hidden units that emerged in the simulations matched those in the leech. The model and data support distributed rather than localist representations in the local bending reflex. These results also explain counterintuitive aspects of the local bending circuitry."
            },
            "slug": "A-Dynamic-Neural-Network-Model-of-Sensorimotor-in-Lockery-Fang",
            "title": {
                "fragments": [],
                "text": "A Dynamic Neural Network Model of Sensorimotor Transformations in the Leech"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A model of the local bending reflex was constructed using physiological and anatomical constraints to construct a model of interneurons in leech ganglia and the properties of the hidden units that emerged in the simulations matched those in the leech."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157784"
                        ],
                        "name": "C. Lee Giles",
                        "slug": "C.-Lee-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee Giles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153170210"
                        ],
                        "name": "Clifford B. Miller",
                        "slug": "Clifford-B.-Miller",
                        "structuredName": {
                            "firstName": "Clifford",
                            "lastName": "Miller",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Clifford B. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2158193072"
                        ],
                        "name": "Dong Chen",
                        "slug": "Dong-Chen",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115401300"
                        ],
                        "name": "Hsing-Hen Chen",
                        "slug": "Hsing-Hen-Chen",
                        "structuredName": {
                            "firstName": "Hsing-Hen",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hsing-Hen Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34922532"
                        ],
                        "name": "Guo-Zheng Sun",
                        "slug": "Guo-Zheng-Sun",
                        "structuredName": {
                            "firstName": "Guo-Zheng",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guo-Zheng Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2552960"
                        ],
                        "name": "Yee-Chun Lee",
                        "slug": "Yee-Chun-Lee",
                        "structuredName": {
                            "firstName": "Yee-Chun",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yee-Chun Lee"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 19666035,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "872cdc269f3cb59f8a227818f35041415091545f",
            "isKey": false,
            "numCitedBy": 489,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We show that a recurrent, second-order neural network using a real-time, forward training algorithm readily learns to infer small regular grammars from positive and negative string training samples. We present simulations that show the effect of initial conditions, training set size and order, and neural network architecture. All simulations were performed with random initial weight strengths and usually converge after approximately a hundred epochs of training. We discuss a quantization algorithm for dynamically extracting finite state automata during and after training. For a well-trained neural net, the extracted automata constitute an equivalence class of state machines that are reducible to the minimal machine of the inferred grammar. We then show through simulations that many of the neural net state machines are dynamically stable, that is, they correctly classify many long unseen strings. In addition, some of these extracted automata actually outperform the trained neural network for classification of unseen strings."
            },
            "slug": "Learning-and-Extracting-Finite-State-Automata-with-Giles-Miller",
            "title": {
                "fragments": [],
                "text": "Learning and Extracting Finite State Automata with Second-Order Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "It is shown that a recurrent, second-order neural network using a real-time, forward training algorithm readily learns to infer small regular grammars from positive and negative string training samples, and many of the neural net state machines are dynamically stable, that is, they correctly classify many long unseen strings."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51074932"
                        ],
                        "name": "H. Kalmus",
                        "slug": "H.-Kalmus",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Kalmus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kalmus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4177611,
            "fieldsOfStudy": [
                "Political Science"
            ],
            "id": "ec1d7e0479233c22dcfbeb9ee7ff0f157b2b94f2",
            "isKey": false,
            "numCitedBy": 322,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Information and Control in the Living Organism. By Bernhard Hassenstein. Pp. viii + 159. (Chapman and Hall: London, December 1971.) \u00a31.30."
            },
            "slug": "Biological-Cybernetics-Kalmus",
            "title": {
                "fragments": [],
                "text": "Biological Cybernetics"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2884373"
                        ],
                        "name": "J. Elman",
                        "slug": "J.-Elman",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Elman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Elman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 132
                            }
                        ],
                        "text": "Feedforward approximations of recurrent dynamics were successfully used in sequence generation [3, 9] and sequence prediction tasks [2, 5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 121
                            }
                        ],
                        "text": "Supervised learning in recurrent neural networks has been extensively applied to speech recognition, language processing [2, 5, 6], and the modeling of biological neural networks [1, 11, 16, 18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2763403,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "668087f0ae7ce1de6e0bd0965dbb480c08103260",
            "isKey": false,
            "numCitedBy": 9858,
            "numCiting": 111,
            "paperAbstract": {
                "fragments": [],
                "text": "Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction."
            },
            "slug": "Finding-Structure-in-Time-Elman",
            "title": {
                "fragments": [],
                "text": "Finding Structure in Time"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory and suggests a method for representing lexical categories and the type/token distinction is developed."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "118969901"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 122
                            }
                        ],
                        "text": "The continuous-time model allows simple mathematical derivations and the results thus obtained can easily be transferred to a discrete-time model by setting\ni\n= 1\nand substituting x(t + 1) x(t) for dx(t)=dt."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59859558,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d453386011ef21285fa81fb4f87fdf811c6ad7a",
            "isKey": false,
            "numCitedBy": 522,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-internal-representations-by-errors-Rumelhart",
            "title": {
                "fragments": [],
                "text": "Learning internal representations by back-propagating errors"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 161
                            }
                        ],
                        "text": "\u2026x\ni\n(t) +\nj=1\nw\nij\ny\nj\n(t) +\nj=1\nb\nij\nz\nj\n(t); (1)\ny\ni\n(t) = g(x\ni\n(t));\nwhere x\ni\n(t) and y\ni\n(t) (i = 1; : : : ; n) represent the inter-\nnal state and the output of the units, g( ) is a sigmoid function, and z\nj\n(t) (j = 1; : : : ;m) represent the exter-\nnal inputs to the network."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ordinary Dierential Equations"
            },
            "venue": {
                "fragments": [],
                "text": "Ordinary Dierential Equations"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 102
                            }
                        ],
                        "text": "A recurrent network with a single or a few attractor basins can be used as a non-linear adaptive lter [1, 11] or as the source of short term memory [18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 179
                            }
                        ],
                        "text": "Supervised learning in recurrent neural networks has been extensively applied to speech recognition, language processing [2, 5, 6], and the modeling of biological neural networks [1, 11, 16, 18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A dynamic neural network model of sensorimotor transformation in the leech"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 64
                            }
                        ],
                        "text": "Since the solutions of (4) and (6) satis es the Green's formula [8], the error gradient can be computed as follows."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 243
                            }
                        ],
                        "text": "It means that the asymptotic stability of the learning equation is not guaranteed Furthermore, if an equilibrium changes into a limit cycle through a Hopf bifurcation, the linear system (4) with periodic matrix has a characteristic root i = 1 [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ordinary Di erential Equations, Birkh\u007fauser"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 82
                            }
                        ],
                        "text": "The convergence of gradient descent learning has been proved under this condition [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Some convergence results for learning in recurrent neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "UCSD Department of Economics Discussion"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 196
                            }
                        ],
                        "text": "However, the global structure of the state space, which is more important than speci c trajectories, has discontinuities at some points in the parameter space, which are called bifurcation points [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 166
                            }
                        ],
                        "text": "In contrast, the output of a recurrent network can change drastically with an in nitesimal change in the network parameter when it passes through a bifurcation point [7, 13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nonlinear oscillation, dynamical systems, and bifurcations of vector elds"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 102
                            }
                        ],
                        "text": "A recurrent network with a single or a few attractor basins can be used as a non-linear adaptive lter [1, 11] or as the source of short term memory [18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 179
                            }
                        ],
                        "text": "Supervised learning in recurrent neural networks has been extensively applied to speech recognition, language processing [2, 5, 6], and the modeling of biological neural networks [1, 11, 16, 18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural network models of velocity storage in the horizontal vestibulo-ocular re ex"
            },
            "venue": {
                "fragments": [],
                "text": "Biological Cybernetics,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 178
                            }
                        ],
                        "text": "@ < E >\n@w\nkl\n=\n1\nT\nZ\nT\n0\nn\nX\ni=1\ni\n(t)p\nkl i (t)dt\n=\n1\nT\nZ\nT\n0\nn\nX\ni=1\nq\ni\n(t)\nkl i (t)dt =\n1\nT\nZ\nT\n0\nq\nk\n(t)y\nl\n(t)dt: (7)\nThis method is called back-propagation through time (BPTT) [12, 13, 14]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 134
                            }
                        ],
                        "text": "In multi-layer feedforward networks, the existence of local minima in the weight space has been supposed to be practically no problem [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 59
                            }
                        ],
                        "text": "This method is called back-propagation through time (BPTT) [12, 13, 14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning internal representations by backpropagating"
            },
            "venue": {
                "fragments": [],
                "text": "errors. Nature,"
            },
            "year": 1986
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 14,
            "methodology": 8
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 22,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Bifurcations-in-the-learning-of-recurrent-neural-Doya/f55e5107f756e29f1e6ac6109dc44d698d6301fb?sort=total-citations"
}