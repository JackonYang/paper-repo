{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778174"
                        ],
                        "name": "Sam S. Tsai",
                        "slug": "Sam-S.-Tsai",
                        "structuredName": {
                            "firstName": "Sam",
                            "lastName": "Tsai",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sam S. Tsai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12600623"
                        ],
                        "name": "David M. Chen",
                        "slug": "David-M.-Chen",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Chen",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David M. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802086"
                        ],
                        "name": "V. Chandrasekhar",
                        "slug": "V.-Chandrasekhar",
                        "structuredName": {
                            "firstName": "Vijay",
                            "lastName": "Chandrasekhar",
                            "middleNames": [
                                "Ramaseshan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Chandrasekhar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1773602"
                        ],
                        "name": "G. Takacs",
                        "slug": "G.-Takacs",
                        "structuredName": {
                            "firstName": "Gabriel",
                            "lastName": "Takacs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Takacs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143770929"
                        ],
                        "name": "Ngai-Man Cheung",
                        "slug": "Ngai-Man-Cheung",
                        "structuredName": {
                            "firstName": "Ngai-Man",
                            "lastName": "Cheung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ngai-Man Cheung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2201723"
                        ],
                        "name": "Ramakrishna Vedantham",
                        "slug": "Ramakrishna-Vedantham",
                        "structuredName": {
                            "firstName": "Ramakrishna",
                            "lastName": "Vedantham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ramakrishna Vedantham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2026212"
                        ],
                        "name": "R. Grzeszczuk",
                        "slug": "R.-Grzeszczuk",
                        "structuredName": {
                            "firstName": "Radek",
                            "lastName": "Grzeszczuk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Grzeszczuk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739786"
                        ],
                        "name": "B. Girod",
                        "slug": "B.-Girod",
                        "structuredName": {
                            "firstName": "Bernd",
                            "lastName": "Girod",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Girod"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 114
                            }
                        ],
                        "text": "INTRODUCTION\nVisual search has gained interest as hand-held devices become equipped with powerful computing resources and high resolution cameras."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6306874,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "534bc9af67d772fd3eafcab19a7fff823dd28858",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a mobile product recognition system for the camera-phone. By snapping a picture of a product with a camera-phone, the user can retrieve online information of the product. The product is recognized by an image-based retrieval system located on a remote server. Our database currently comprises more than one million entries, primarily products packaged in rigid boxes with printed labels, such as CDs, DVDs, and books. We extract low bit-rate descriptors from the query image and compress the location of the descriptors using location histogram coding on the camera-phone. We transmit the compressed query features, instead of a query image, to reduce the transmission delay. We use inverted index compression and fast geometric re-ranking on our database to provide a low delay image recognition response for large scale databases. Experimental timing results on different parts of the mobile product recognition system is reported in this work."
            },
            "slug": "Mobile-product-recognition-Tsai-Chen",
            "title": {
                "fragments": [],
                "text": "Mobile product recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work uses inverted index compression and fast geometric re-ranking on their database to provide a low delay image recognition response for large scale databases."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Multimedia"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794500"
                        ],
                        "name": "Qiong Liu",
                        "slug": "Qiong-Liu",
                        "structuredName": {
                            "firstName": "Qiong",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiong Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067545012"
                        ],
                        "name": "Hironori Yano",
                        "slug": "Hironori-Yano",
                        "structuredName": {
                            "firstName": "Hironori",
                            "lastName": "Yano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hironori Yano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145698468"
                        ],
                        "name": "Don Kimber",
                        "slug": "Don-Kimber",
                        "structuredName": {
                            "firstName": "Don",
                            "lastName": "Kimber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Don Kimber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2686363"
                        ],
                        "name": "Chunyuan Liao",
                        "slug": "Chunyuan-Liao",
                        "structuredName": {
                            "firstName": "Chunyuan",
                            "lastName": "Liao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunyuan Liao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144115507"
                        ],
                        "name": "L. Wilcox",
                        "slug": "L.-Wilcox",
                        "structuredName": {
                            "firstName": "Lynn",
                            "lastName": "Wilcox",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Wilcox"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "3(d) where the angle corresponding to the maximum count bin is the dominant angle."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "Applications such as a location recognition [1, 2], product recognition [3, 4], and document retrieval [5, 6] are many but not the only ones being developed."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9369849,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "841820bb6ae61dea52b42d65968d970c610eca55",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a tool and a novel Fast Invariant Transform (FIT) algorithm for language independent e-documents access. The tool enables a person to access an e-document through an informal camera capture of a document hardcopy. It can save people from remembering/exploring numerous directories and file names, or even going through many pages/paragraphs in one document. It can also facilitate people's manipulation of a document or people's interactions through documents. Additionally, the algorithm is useful for binding multimedia data to language independent paper documents. Our document recognition algorithm is inspired by the widely known SIFT descriptor [4] but can be computed much more efficiently for both descriptor construction and search. It also uses much less storage space than the SIFT approach. By testing our algorithm with randomly scaled and rotated document pages, we can achieve a 99.73% page recognition rate on the 2188-page ICME06 proceedings and 99.9% page recognition rate on a 504-page Japanese math book [2]."
            },
            "slug": "High-accuracy-and-language-independent-document-a-Liu-Yano",
            "title": {
                "fragments": [],
                "text": "High accuracy and language independent document retrieval with a Fast Invariant Transform"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "A tool and a novel Fast Invariant Transform (FIT) algorithm for language independent e-documents access that enables a person to access an e-document through an informal camera capture of a document hardcopy and uses much less storage space than the SIFT approach."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE International Conference on Multimedia and Expo"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778174"
                        ],
                        "name": "Sam S. Tsai",
                        "slug": "Sam-S.-Tsai",
                        "structuredName": {
                            "firstName": "Sam",
                            "lastName": "Tsai",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sam S. Tsai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12600623"
                        ],
                        "name": "David M. Chen",
                        "slug": "David-M.-Chen",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Chen",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David M. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1773602"
                        ],
                        "name": "G. Takacs",
                        "slug": "G.-Takacs",
                        "structuredName": {
                            "firstName": "Gabriel",
                            "lastName": "Takacs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Takacs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802086"
                        ],
                        "name": "V. Chandrasekhar",
                        "slug": "V.-Chandrasekhar",
                        "structuredName": {
                            "firstName": "Vijay",
                            "lastName": "Chandrasekhar",
                            "middleNames": [
                                "Ramaseshan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Chandrasekhar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34272181"
                        ],
                        "name": "J. Singh",
                        "slug": "J.-Singh",
                        "structuredName": {
                            "firstName": "Jatinder",
                            "lastName": "Singh",
                            "middleNames": [
                                "Pal"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739786"
                        ],
                        "name": "B. Girod",
                        "slug": "B.-Girod",
                        "structuredName": {
                            "firstName": "Bernd",
                            "lastName": "Girod",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Girod"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4830130,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f11459c32da1e1b3fdbd34ceba47b5de0c617a9",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "For mobile image retrieval, efficient data transmission can be achieved by sending only the query features. Each query feature is composed of a descriptor and a location in the image. The former is used to find candidate matching images using a \"bag-of-words\" approach while the latter is used in a geometric consistency check to map features in the query image to corresponding features in the database image. \n \nWe investigate how to compress the location information and how lossy compression affects the geometric consistency check. The location information is converted into a location histogram and a context-based arithmetic coding with location refinement method is then proposed to code the histogram. The effects of lossily compressing the location information are evaluated empirically in terms of the errors in corresponding features and the error of the estimated geometric transformation model. From our experiments, rates at ~5.1 bits per feature can achieve errors comparable to lossless coding. The proposed scheme achieves a 12.5x rate reduction compared to the floating point representation, and 2.8x rate reduction compared to a fixed point representation."
            },
            "slug": "Location-coding-for-mobile-image-retrieval-Tsai-Chen",
            "title": {
                "fragments": [],
                "text": "Location coding for mobile image retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work investigates how to compress the location information and how lossy compression affects the geometric consistency check and proposes a context-based arithmetic coding with location refinement method to code the location histogram."
            },
            "venue": {
                "fragments": [],
                "text": "MobiMedia"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46772671"
                        ],
                        "name": "Xiangrong Chen",
                        "slug": "Xiangrong-Chen",
                        "structuredName": {
                            "firstName": "Xiangrong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiangrong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Since our text detection algorithm uses MSERs, they can also act as the interest points for CHoG to further reduce computation time."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61234963,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "37ba7b9a823e8a400046bd149b7756adf5d698da",
            "isKey": false,
            "numCitedBy": 512,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper gives an algorithm for detecting and reading text in natural images. The algorithm is intended for use by blind and visually impaired subjects walking through city scenes. We first obtain a dataset of city images taken by blind and normally sighted subjects. From this dataset, we manually label and extract the text regions. Next we perform statistical analysis of the text regions to determine which image features are reliable indicators of text and have low entropy (i.e. feature response is similar for all text images). We obtain weak classifiers by using joint probabilities for feature responses on and off text. These weak classifiers are used as input to an AdaBoost machine learning algorithm to train a strong classifier. In practice, we trained a cascade with 4 strong classifiers containing 79 features. An adaptive binarization and extension algorithm is applied to those regions selected by the cascade classifier. Commercial OCR software is used to read the text or reject it as a non-text region. The overall algorithm has a success rate of over 90% (evaluated by complete detection and reading of the text) on the test set and the unread text is typically small and distant from the viewer."
            },
            "slug": "Detecting-and-reading-text-in-natural-scenes-Chen-Yuille",
            "title": {
                "fragments": [],
                "text": "Detecting and reading text in natural scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The overall algorithm has a success rate of over 90% (evaluated by complete detection and reading of the text) on the test set and the unread text is typically small and distant from the viewer."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2004"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685739"
                        ],
                        "name": "B. Erol",
                        "slug": "B.-Erol",
                        "structuredName": {
                            "firstName": "Berna",
                            "lastName": "Erol",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Erol"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37254421"
                        ],
                        "name": "Emilio R. Ant\u00fanez",
                        "slug": "Emilio-R.-Ant\u00fanez",
                        "structuredName": {
                            "firstName": "Emilio",
                            "lastName": "Ant\u00fanez",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Emilio R. Ant\u00fanez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694191"
                        ],
                        "name": "J. Hull",
                        "slug": "J.-Hull",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Hull",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hull"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "3(d) where the angle corresponding to the maximum count bin is the dominant angle."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "Most of these systems rely on image-based features [7, 8, 9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1093455,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "faafa3927d06f60cb6f4c8b1dbb7e8e84fe48618",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The popularity of camera phones enables many exciting multimedia applications. In this paper, we present a novel technology and several applications that allow users to interact with paper documents, books, and magazines. This interaction is in the form of reading and writing electronic information, such as images, web urls, video, and audio, to the paper medium by pointing a camera phone at a patch of text on a document. Our application does not require any special markings, barcodes, or watermarks on the paper document. Instead, we propose a document recognition algorithm that automatically determines the location of a patch of text in a large collection of document images given a small document image. This is very challenging because the majority of phone cameras lack autofocus and macro capabilities and they produce low quality images and video. We developed a novel algorithm, Brick Wall Coding (BWC), that performs image-based document recognition using the mobile phone video frames. Given a document patch image, BWC utilizes the layout, i.e. relative locations, of word boxes in order to determine the original file, page, and the location on the page. BWC runs real-time (4 frames per second) on a Treo 700w smartphone with a 312 MHz processor and 64MB RAM. Using our method we can recognize blurry document patch frames that contain as little as 4-5 lines of text and a video resolution as low as 176x144. We performed experiments by indexing 4397 document pages and querying this database with 533 document patches. Besides describing the basic algorithm, this paper also describes several applications that are enabled by mobile phone-paper interaction, such as inserting electronic annotations to paper, using paper as a tangible interface to collect and communicate multimedia data, and collaborative homework."
            },
            "slug": "HOTPAPER:-multimedia-interaction-with-paper-using-Erol-Ant\u00fanez",
            "title": {
                "fragments": [],
                "text": "HOTPAPER: multimedia interaction with paper using mobile phones"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A novel algorithm, Brick Wall Coding (BWC), that performs image-based document recognition using the mobile phone video frames and can recognize blurry document patch frames that contain as little as 4-5 lines of text and a video resolution as low as 176x144."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Multimedia"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144752865"
                        ],
                        "name": "Jian Liang",
                        "slug": "Jian-Liang",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Liang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115196978"
                        ],
                        "name": "Huiping Li",
                        "slug": "Huiping-Li",
                        "structuredName": {
                            "firstName": "Huiping",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huiping Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "In parallel with the title text recognition, we extract local image features from the query image."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5053740,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82b6f95e805a92887f8efccf5a0dc8d5783676f5",
            "isKey": false,
            "numCitedBy": 457,
            "numCiting": 131,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract.The increasing availability of high-performance, low-priced, portable digital imaging devices has created a tremendous opportunity for supplementing traditional scanning for document image acquisition. Digital cameras attached to cellular phones, PDAs, or wearable computers, and standalone image or video devices are highly mobile and easy to use; they can capture images of thick books, historical manuscripts too fragile to touch, and text in scenes, making them much more versatile than desktop scanners. Should robust solutions to the analysis of documents captured with such devices become available, there will clearly be a demand in many domains. Traditional scanner-based document analysis techniques provide us with a good reference and starting point, but they cannot be used directly on camera-captured images. Camera-captured images can suffer from low resolution, blur, and perspective distortion, as well as complex layout and interaction of the content and background. In this paper we present a survey of application domains, technical challenges, and solutions for the analysis of documents captured by digital cameras. We begin by describing typical imaging devices and the imaging process. We discuss document analysis from a single camera-captured image as well as multiple frames and highlight some sample applications under development and feasible ideas for future development."
            },
            "slug": "Camera-based-analysis-of-text-and-documents:-a-Liang-Doermann",
            "title": {
                "fragments": [],
                "text": "Camera-based analysis of text and documents: a survey"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A survey of application domains, technical challenges, and solutions for the analysis of documents captured by digital cameras, and some sample applications under development and feasible ideas for future development is presented."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Document Analysis and Recognition (IJDAR)"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49177577"
                        ],
                        "name": "Huizhong Chen",
                        "slug": "Huizhong-Chen",
                        "structuredName": {
                            "firstName": "Huizhong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huizhong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778174"
                        ],
                        "name": "Sam S. Tsai",
                        "slug": "Sam-S.-Tsai",
                        "structuredName": {
                            "firstName": "Sam",
                            "lastName": "Tsai",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sam S. Tsai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701294"
                        ],
                        "name": "Georg Schroth",
                        "slug": "Georg-Schroth",
                        "structuredName": {
                            "firstName": "Georg",
                            "lastName": "Schroth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Georg Schroth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12600623"
                        ],
                        "name": "David M. Chen",
                        "slug": "David-M.-Chen",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Chen",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David M. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2026212"
                        ],
                        "name": "R. Grzeszczuk",
                        "slug": "R.-Grzeszczuk",
                        "structuredName": {
                            "firstName": "Radek",
                            "lastName": "Grzeszczuk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Grzeszczuk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739786"
                        ],
                        "name": "B. Girod",
                        "slug": "B.-Girod",
                        "structuredName": {
                            "firstName": "Bernd",
                            "lastName": "Girod",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Girod"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": "We further remove keywords that are not found in the dictionary."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 104
                            }
                        ],
                        "text": "Since our text detection algorithm uses MSERs, they can also act as the interest points for CHoG to further reduce computation time."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 155
                            }
                        ],
                        "text": "(3)\nThen, a response of an angle is calculated using the following:\nHM (\u03b1) = \u2211\n(u,v)\u2208M Gr(u, v) \u00b7 I\n( \u2016G\u03b8(u, v)\u2212 \u03b1\u2016   \u03b4\n2\n) , (4)\nwhere \u03b4 is a tolerance factor, M is the region between the x-line and base-line, and I is the indicator function."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11311196,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6cb3153e5773053916a27bf3ab4530705a6bcf80",
            "isKey": false,
            "numCitedBy": 444,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Detecting text in natural images is an important prerequisite. In this paper, we propose a novel text detection algorithm, which employs edge-enhanced Maximally Stable Extremal Regions as basic letter candidates. These candidates are then filtered using geometric and stroke width information to exclude non-text objects. Letters are paired to identify text lines, which are subsequently separated into words. We evaluate our system using the ICDAR competition dataset and our mobile document database. The experimental results demonstrate the excellent performance of the proposed method."
            },
            "slug": "Robust-text-detection-in-natural-images-with-Stable-Chen-Tsai",
            "title": {
                "fragments": [],
                "text": "Robust text detection in natural images with edge-enhanced Maximally Stable Extremal Regions"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A novel text detection algorithm is proposed, which employs edge-enhanced Maximally Stable Extremal Regions as basic letter candidates and Letters are paired to identify text lines, which are subsequently separated into words."
            },
            "venue": {
                "fragments": [],
                "text": "2011 18th IEEE International Conference on Image Processing"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3103069"
                        ],
                        "name": "T. Nakai",
                        "slug": "T.-Nakai",
                        "structuredName": {
                            "firstName": "Tomohiro",
                            "lastName": "Nakai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Nakai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3277321"
                        ],
                        "name": "K. Kise",
                        "slug": "K.-Kise",
                        "structuredName": {
                            "firstName": "Koichi",
                            "lastName": "Kise",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kise"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35613969"
                        ],
                        "name": "M. Iwamura",
                        "slug": "M.-Iwamura",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Iwamura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Iwamura"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "3(d) where the angle corresponding to the maximum count bin is the dominant angle."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "Most of these systems rely on image-based features [7, 8, 9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18197812,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1260c060c54f4c2fc106f4809905730dc305c2cf",
            "isKey": false,
            "numCitedBy": 122,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Camera-based document image retrieval is a task of searching document images from the database based on query images captured using digital cameras. For this task, it is required to solve the problem of \u201cperspective distortion\u201d of images,as well as to establish a way of matching document images efficiently. To solve these problems we have proposed a method called Locally Likely Arrangement Hashing (LLAH) which is characterized by both the use of a perspective invariant to cope with the distortion and the efficiency: LLAH only requires O(N) time where N is the number of feature points that describe the query image. In this paper, we introduce into LLAH an affine invariant instead of the perspective invariant so as to improve its adjustability. Experimental results show that the use of the affine invariant enables us to improve either the accuracy from 96.2% to 97.8%, or the retrieval time from 112 msec./query to 75 msec./query by selecting parameters of processing."
            },
            "slug": "Use-of-Affine-Invariants-in-Locally-Likely-Hashing-Nakai-Kise",
            "title": {
                "fragments": [],
                "text": "Use of Affine Invariants in Locally Likely Arrangement Hashing for Camera-Based Document Image Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper introduces into LLAH an affine invariant instead of the perspective invariant so as to improve its adjustability and experimental results show that the use of the affines enables us to improve either the accuracy from 96.2% to 97.8%, or the retrieval time from 112 msec./query to 75 msec./ query by selecting parameters of processing."
            },
            "venue": {
                "fragments": [],
                "text": "Document Analysis Systems"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2176024"
                        ],
                        "name": "J. Moraleda",
                        "slug": "J.-Moraleda",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Moraleda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Moraleda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694191"
                        ],
                        "name": "J. Hull",
                        "slug": "J.-Hull",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Hull",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hull"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "3(d) where the angle corresponding to the maximum count bin is the dominant angle."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "INTRODUCTION\nVisual search has gained interest as hand-held devices become equipped with powerful computing resources and high resolution cameras."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16910521,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a35bde835b0d345e2719fd9ad5bb4ab7dfe72575",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "A method for image matching from partial blurry images is presented that leverages existing text retrieval algorithms to provide a solution that scales to hundreds of thousands of images. As an initial application, we present a document image matching system in which the user supplies a query image of a small patch of a paper document taken with a cell phone camera, and the system returns a label identifying the original electronic document if found in a previously indexed collection. Experimental results show that a retrieval rate of over 70% is achieved on a collection of nearly 500,000 document pages."
            },
            "slug": "Toward-Massive-Scalability-in-Image-Matching-Moraleda-Hull",
            "title": {
                "fragments": [],
                "text": "Toward Massive Scalability in Image Matching"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A method for image matching from partial blurry images is presented that leverages existing text retrieval algorithms to provide a solution that scales to hundreds of thousands of images."
            },
            "venue": {
                "fragments": [],
                "text": "2010 20th International Conference on Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1773602"
                        ],
                        "name": "G. Takacs",
                        "slug": "G.-Takacs",
                        "structuredName": {
                            "firstName": "Gabriel",
                            "lastName": "Takacs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Takacs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802086"
                        ],
                        "name": "V. Chandrasekhar",
                        "slug": "V.-Chandrasekhar",
                        "structuredName": {
                            "firstName": "Vijay",
                            "lastName": "Chandrasekhar",
                            "middleNames": [
                                "Ramaseshan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Chandrasekhar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683095"
                        ],
                        "name": "N. Gelfand",
                        "slug": "N.-Gelfand",
                        "structuredName": {
                            "firstName": "Natasha",
                            "lastName": "Gelfand",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Gelfand"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37040517"
                        ],
                        "name": "Yingen Xiong",
                        "slug": "Yingen-Xiong",
                        "structuredName": {
                            "firstName": "Yingen",
                            "lastName": "Xiong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yingen Xiong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715853"
                        ],
                        "name": "Wei-Chao Chen",
                        "slug": "Wei-Chao-Chen",
                        "structuredName": {
                            "firstName": "Wei-Chao",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei-Chao Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3089195"
                        ],
                        "name": "Thanos Bismpigiannis",
                        "slug": "Thanos-Bismpigiannis",
                        "structuredName": {
                            "firstName": "Thanos",
                            "lastName": "Bismpigiannis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thanos Bismpigiannis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2026212"
                        ],
                        "name": "R. Grzeszczuk",
                        "slug": "R.-Grzeszczuk",
                        "structuredName": {
                            "firstName": "Radek",
                            "lastName": "Grzeszczuk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Grzeszczuk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704409"
                        ],
                        "name": "K. Pulli",
                        "slug": "K.-Pulli",
                        "structuredName": {
                            "firstName": "Kari",
                            "lastName": "Pulli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Pulli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739786"
                        ],
                        "name": "B. Girod",
                        "slug": "B.-Girod",
                        "structuredName": {
                            "firstName": "Bernd",
                            "lastName": "Girod",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Girod"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 85
                            }
                        ],
                        "text": "INTRODUCTION\nVisual search has gained interest as hand-held devices become equipped with powerful computing resources and high resolution cameras."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15715636,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "37c7ea6c4de77114b729a2dc65b2536bee46ff4b",
            "isKey": false,
            "numCitedBy": 334,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We have built an outdoors augmented reality system for mobile phones that matches camera-phone images against a large database of location-tagged images using a robust image retrieval algorithm. We avoid network latency by implementing the algorithm on the phone and deliver excellent performance by adapting a state-of-the-art image retrieval algorithm based on robust local descriptors. Matching is performed against a database of highly relevant features, which is continuously updated to reflect changes in the environment. We achieve fast updates and scalability by pruning of irrelevant features based on proximity to the user. By compressing and incrementally updating the features stored on the phone we make the system amenable to low-bandwidth wireless connections. We demonstrate system robustness on a dataset of location-tagged images and show a smart-phone implementation that achieves a high image matching rate while operating in near real-time."
            },
            "slug": "Outdoors-augmented-reality-on-mobile-phone-using-Takacs-Chandrasekhar",
            "title": {
                "fragments": [],
                "text": "Outdoors augmented reality on mobile phone using loxel-based visual feature organization"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "An outdoors augmented reality system for mobile phones that matches camera-phone images against a large database of location-tagged images using a robust image retrieval algorithm and shows a smart-phone implementation that achieves a high image matching rate while operating in near real-time."
            },
            "venue": {
                "fragments": [],
                "text": "MIR '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31693932"
                        ],
                        "name": "G. Myers",
                        "slug": "G.-Myers",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Myers",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Myers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764443"
                        ],
                        "name": "R. Bolles",
                        "slug": "R.-Bolles",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Bolles",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bolles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2624076"
                        ],
                        "name": "Q. Luong",
                        "slug": "Q.-Luong",
                        "structuredName": {
                            "firstName": "Quang-Tuan",
                            "lastName": "Luong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Q. Luong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48804780"
                        ],
                        "name": "James A. Herson",
                        "slug": "James-A.-Herson",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Herson",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James A. Herson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3312922"
                        ],
                        "name": "H. Aradhye",
                        "slug": "H.-Aradhye",
                        "structuredName": {
                            "firstName": "Hrishikesh",
                            "lastName": "Aradhye",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Aradhye"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Since our text detection algorithm uses MSERs, they can also act as the interest points for CHoG to further reduce computation time."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 29394851,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4599b80a96821ed9276476edd17c6d70380f150c",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract.Real-world text on street signs, nameplates, etc. often lies in an oblique plane and hence cannot be recognized by traditional OCR systems due to perspective distortion. Furthermore, such text often comprises only one or two lines, preventing the use of existing perspective rectification methods that were primarily designed for images of document pages. We propose an approach that reliably rectifies and subsequently recognizes individual lines of text. Our system, which includes novel algorithms for extraction of text from real-world scenery, perspective rectification, and binarization, has been rigorously tested on still imagery as well as on MPEG-2 video clips in real time."
            },
            "slug": "Rectification-and-recognition-of-text-in-3-D-scenes-Myers-Bolles",
            "title": {
                "fragments": [],
                "text": "Rectification and recognition of text in 3-D scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes an approach that reliably rectifies and subsequently recognizes individual lines of text in real-world text that has been rigorously tested on still imagery as well as on MPEG-2 video clips in real time."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Document Analysis and Recognition (IJDAR)"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34854285"
                        ],
                        "name": "Yuanping Zhu",
                        "slug": "Yuanping-Zhu",
                        "structuredName": {
                            "firstName": "Yuanping",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuanping Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145841729"
                        ],
                        "name": "Ruwei Dai",
                        "slug": "Ruwei-Dai",
                        "structuredName": {
                            "firstName": "Ruwei",
                            "lastName": "Dai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ruwei Dai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2658590"
                        ],
                        "name": "Baihua Xiao",
                        "slug": "Baihua-Xiao",
                        "structuredName": {
                            "firstName": "Baihua",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Baihua Xiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683416"
                        ],
                        "name": "Chunheng Wang",
                        "slug": "Chunheng-Wang",
                        "structuredName": {
                            "firstName": "Chunheng",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunheng Wang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Since our text detection algorithm uses MSERs, they can also act as the interest points for CHoG to further reduce computation time."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10318200,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1eb58923627d3c74589dd875567e218b8c8193bc",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of perspective distortion rectification of camera-based document images. A perspective rectification method using local linear structure is proposed. In this work, only local structure information such as characters, text lines, and strokes are involved in perspective parallel line detection. Horizontal lines are detected from the baselines of words or text lines, and vertical lines are detected from vertical strokes. An iterative method is used to locate vanishing points from perspective line bundles. Finally, the perspective image is rectified by a quadrilateral-rectangle pair constructed by horizontal and vertical vanishing point. Not using global information such as document boundary, paragraph format, this method has good generalization and is able to rectify partial documents or sparse documents."
            },
            "slug": "Perspective-rectification-of-camera-based-document-Zhu-Dai",
            "title": {
                "fragments": [],
                "text": "Perspective rectification of camera-based document images using local linear structure"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "This paper addresses the problem of perspective distortion rectification of camera-based document images by proposing a perspective rectification method using local linear structure that has good generalization and is able to rectify partial documents or sparse documents."
            },
            "venue": {
                "fragments": [],
                "text": "SAC '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069641275"
                        ],
                        "name": "Paul Clark",
                        "slug": "Paul-Clark",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728108"
                        ],
                        "name": "M. Mirmehdi",
                        "slug": "M.-Mirmehdi",
                        "structuredName": {
                            "firstName": "Majid",
                            "lastName": "Mirmehdi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mirmehdi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Since our text detection algorithm uses MSERs, they can also act as the interest points for CHoG to further reduce computation time."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 29494888,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7cc0d51a1901fdf1d1fe80f4cc6f3748d0ec6346",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Rectifying-perspective-views-of-text-in-3D-scenes-Clark-Mirmehdi",
            "title": {
                "fragments": [],
                "text": "Rectifying perspective views of text in 3D scenes using vanishing points"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068255843"
                        ],
                        "name": "Christopher Hunt",
                        "slug": "Christopher-Hunt",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Hunt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher Hunt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 110
                            }
                        ],
                        "text": "Applications such as a location recognition [1, 2], product recognition [3, 4], and document retrieval [5, 6] are many but not the only ones being developed."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 161878,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c7cf406a47048730c1a08d46cb0166b16566524",
            "isKey": false,
            "numCitedBy": 6212,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "In this document, the SURF detector-descriptor scheme used in the OpenSURF library is discussed in detail. First the algorithm is analysed from a theoretical standpoint to provide a detailed overview of how and why it works. Next the design and development choices for the implementation of the library are discussed and justified. During the implementation of the library, it was found that some of the finer details of the algorithm had been omitted or overlooked, so Section 1.5 serves to make clear the concepts which are not explicitly defined in the SURF paper [1]."
            },
            "slug": "SURF:-Speeded-Up-Robust-Features-Hunt",
            "title": {
                "fragments": [],
                "text": "SURF: Speeded-Up Robust Features"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "In this document, the SURF detector-descriptor scheme used in the OpenSURF library is discussed in detail and the algorithm is analysed from a theoretical standpoint to provide a detailed overview of how and why it works."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1644050191"
                        ],
                        "name": "G. LoweDavid",
                        "slug": "G.-LoweDavid",
                        "structuredName": {
                            "firstName": "G",
                            "lastName": "LoweDavid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. LoweDavid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 106
                            }
                        ],
                        "text": "Applications such as a location recognition [1, 2], product recognition [3, 4], and document retrieval [5, 6] are many but not the only ones being developed."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 174065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4cab9c4b571761203ed4c3a4c5a07dd615f57a91",
            "isKey": false,
            "numCitedBy": 25497,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are ..."
            },
            "slug": "Distinctive-Image-Features-from-Scale-Invariant-LoweDavid",
            "title": {
                "fragments": [],
                "text": "Distinctive Image Features from Scale-Invariant Keypoints"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741782"
                        ],
                        "name": "Camille Monnier",
                        "slug": "Camille-Monnier",
                        "structuredName": {
                            "firstName": "Camille",
                            "lastName": "Monnier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Camille Monnier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "93844619"
                        ],
                        "name": "Steven Holden",
                        "slug": "Steven-Holden",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Holden",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven Holden"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2789970"
                        ],
                        "name": "M. Snorrason",
                        "slug": "M.-Snorrason",
                        "structuredName": {
                            "firstName": "Magn\u00fas",
                            "lastName": "Snorrason",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Snorrason"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1852308"
                        ],
                        "name": "Vitaly Ablavsky",
                        "slug": "Vitaly-Ablavsky",
                        "structuredName": {
                            "firstName": "Vitaly",
                            "lastName": "Ablavsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vitaly Ablavsky"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Since our text detection algorithm uses MSERs, they can also act as the interest points for CHoG to further reduce computation time."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13239806,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e648c157ca71f80bf94c4b2fb3aabfa8f43e02e8",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Documents captured with hand-held devices, such as digital cameras often exhibit perspective warp artifacts. These artifacts pose problems for OCR systems which at best can only handle in-plane rotation. We propose a method for recovering the planar appearance of an input document image by examining the vertical rate of change in scale of features in the document. Our method makes fewer assumptions about the document structure than do previously published algorithms."
            },
            "slug": "Sequential-correction-of-perspective-warp-in-Monnier-Holden",
            "title": {
                "fragments": [],
                "text": "Sequential correction of perspective warp in camera-based documents"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A method for recovering the planar appearance of an input document image by examining the vertical rate of change in scale of features in the document by making fewer assumptions about the document structure than do previously published algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "Eighth International Conference on Document Analysis and Recognition (ICDAR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1925316"
                        ],
                        "name": "M. Pilu",
                        "slug": "M.-Pilu",
                        "structuredName": {
                            "firstName": "Maurizio",
                            "lastName": "Pilu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pilu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Since our text detection algorithm uses MSERs, they can also act as the interest points for CHoG to further reduce computation time."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9009321,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "18221f843f531627f1a808f8ce0592894717e85e",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The article deals with the recovery of illusory linear clues from perspectively skewed documents, with the purpose of using them for rectification. The computational approach proposed implements the perceptual organization principles implicitly used in textual layouts. The numerous examples provided show that the method is robust and viewpoint and scale invariant."
            },
            "slug": "Extraction-of-illusory-linear-clues-in-skewed-Pilu",
            "title": {
                "fragments": [],
                "text": "Extraction of illusory linear clues in perspectively skewed documents"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "The article deals with the recovery of illusory linear clues from perspectively skewed documents, with the purpose of using them for rectification in perceptual organization principles implicitly used in textual layouts."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61683649,
            "fieldsOfStudy": [],
            "id": "906e09889cf96ece2652d338e4382bece2f155d1",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "CHoG: Compressed histogram of gradients A low bit-rate feature descriptor"
            },
            "venue": {
                "fragments": [],
                "text": "CVPR"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Google goggles"
            },
            "venue": {
                "fragments": [],
                "text": "Google goggles"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Quantization schemes for CHoG"
            },
            "venue": {
                "fragments": [],
                "text": "International Workshop on Mobile Vision"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 114
                            }
                        ],
                        "text": "Applications such as a location recognition [1, 2], product recognition [3, 4], and document retrieval [5, 6] are many but not the only ones being developed."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "CHoG: Compressed Histogram of Gradients"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 118
                            }
                        ],
                        "text": "INTRODUCTION\nVisual search has gained interest as hand-held devices become equipped with powerful computing resources and high resolution cameras."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Snaptell"
            },
            "venue": {
                "fragments": [],
                "text": "Snaptell"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 10,
            "methodology": 8
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 22,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Mobile-visual-search-on-printed-documents-using-and-Tsai-Chen/4521532bacee79303a17706fd05c34196013ee63?sort=total-citations"
}