{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781574"
                        ],
                        "name": "Chin-Yew Lin",
                        "slug": "Chin-Yew-Lin",
                        "structuredName": {
                            "firstName": "Chin-Yew",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chin-Yew Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 55156862,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cfad182f567b6a6d54a4dd517e0122692afca9fb",
            "isKey": false,
            "numCitedBy": 89,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper discusses the validity of the evaluation method used in the Document Understanding Conference (DUC) and evaluates five different ROUGE metrics: ROUGE-N, ROUGE-L, ROUGE-W, ROUGE-S, and ROUGE-SU included in the ROUGE summarization evaluation package using data provided by DUC. A comprehensive study of the effects of using single or multiple references and various sample sizes on the stability of the results is also presented."
            },
            "slug": "Looking-for-a-Few-Good-Metrics:-ROUGE-and-its-Lin",
            "title": {
                "fragments": [],
                "text": "Looking for a Few Good Metrics: ROUGE and its Evaluation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The validity of the evaluation method used in the Document Understanding Conference (DUC) is discussed and five different ROUGE metrics are evaluated included in the RouGE summarization evaluation package using data provided by DUC."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781574"
                        ],
                        "name": "Chin-Yew Lin",
                        "slug": "Chin-Yew-Lin",
                        "structuredName": {
                            "firstName": "Chin-Yew",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chin-Yew Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144547315"
                        ],
                        "name": "E. Hovy",
                        "slug": "E.-Hovy",
                        "structuredName": {
                            "firstName": "Eduard",
                            "lastName": "Hovy",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Hovy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 143
                            }
                        ],
                        "text": "Following the successful application of automatic evaluation methods, such as BLEU (Papineni et al., 2001), in machine translation evaluation, Lin and Hovy (2003) showed that methods similar to BLEU,\ni.e. n-gram co-occurrence statistics, could be applied to evaluate summaries."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16292125,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c63bb976dc0d3a897f3b0920170a4c573ef904c6",
            "isKey": false,
            "numCitedBy": 1628,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Following the recent adoption by the machine translation community of automatic evaluation using the BLEU/NIST scoring process, we conduct an in-depth study of a similar idea for evaluating summaries. The results show that automatic evaluation using unigram co-occurrences between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; while direct application of the BLEU evaluation procedure does not always give good results."
            },
            "slug": "Automatic-Evaluation-of-Summaries-Using-N-gram-Lin-Hovy",
            "title": {
                "fragments": [],
                "text": "Automatic Evaluation of Summaries Using N-gram Co-occurrence Statistics"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The results show that automatic evaluation using unigram co-occurrences between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; while direct application of the BLEU evaluation procedure does not always give good results."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781574"
                        ],
                        "name": "Chin-Yew Lin",
                        "slug": "Chin-Yew-Lin",
                        "structuredName": {
                            "firstName": "Chin-Yew",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chin-Yew Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2002316"
                        ],
                        "name": "F. Och",
                        "slug": "F.-Och",
                        "structuredName": {
                            "firstName": "Franz",
                            "lastName": "Och",
                            "middleNames": [
                                "Josef"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Och"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1586456,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9ca86842aad16797d0fe0323358f3beb1ac6a5c6",
            "isKey": false,
            "numCitedBy": 519,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we describe two new objective automatic evaluation methods for machine translation. The first method is based on longest common subsequence between a candidate translation and a set of reference translations. Longest common subsequence takes into account sentence level structure similarity naturally and identifies longest co-occurring in-sequence n-grams automatically. The second method relaxes strict n-gram matching to skip-bigram matching. Skip-bigram is any pair of words in their sentence order. Skip-bigram cooccurrence statistics measure the overlap of skip-bigrams between a candidate translation and a set of reference translations. The empirical results show that both methods correlate with human judgments very well in both adequacy and fluency."
            },
            "slug": "Automatic-Evaluation-of-Machine-Translation-Quality-Lin-Och",
            "title": {
                "fragments": [],
                "text": "Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statistics"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "Two new objective automatic evaluation methods for machine translation based on longest common subsequence between a candidate translation and a set of reference translations and relaxes strict n-gram matching to skip-bigram matching are described."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144599392"
                        ],
                        "name": "I. D. Melamed",
                        "slug": "I.-D.-Melamed",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Melamed",
                            "middleNames": [
                                "Dan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. D. Melamed"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1842,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "42fd4d469c53e4eedd7eb76e7859e3270367f795",
            "isKey": false,
            "numCitedBy": 168,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper shows how to induce an N-best translation lexicon from a bilingual text corpus using statistical properties of the corpus together with four external knowledge sources. The knowledge sources are cast as filters, so that any subset of them can be cascaded in a uniform framework. A new objective evaluation measure is used to compare the quality of lexicons induced with different filter cascades. The best filter cascades improve lexicon quality by up to 137% over the plain vanilla statistical method, and approach human performance. Drastically reducing the size of the training corpus has a much smaller impact on lexicon quality when these knowledge sources are used. This makes it practical to train on small hand-built corpora for language pairs where large bilingual corpora are unavailable. Moreover, three of the four filters prove useful even when used with large training corpora."
            },
            "slug": "Automatic-Evaluation-and-Uniform-Filter-Cascades-Melamed",
            "title": {
                "fragments": [],
                "text": "Automatic Evaluation and Uniform Filter Cascades for Inducing N-Best Translation Lexicons"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This paper shows how to induce an N-best translation lexicon from a bilingual text corpus using statistical properties of the corpus together with four external knowledge sources, which improve lexicon quality by up to 137% over the plain vanilla statistical method, and approach human performance."
            },
            "venue": {
                "fragments": [],
                "text": "VLC@ACL"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729172"
                        ],
                        "name": "I. Mani",
                        "slug": "I.-Mani",
                        "structuredName": {
                            "firstName": "Inderjeet",
                            "lastName": "Mani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Mani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701063"
                        ],
                        "name": "M. Maybury",
                        "slug": "M.-Maybury",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Maybury",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Maybury"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 176
                            }
                        ],
                        "text": "Traditionally evaluation of summarization involves human judgments of different quality metrics, for example, coherence, conciseness, grammaticality, readability, and content (Mani, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5393989,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e53dbd9f57937931a124aed93d744fe50e031af0",
            "isKey": false,
            "numCitedBy": 1015,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes an automatic speech summarization technique for English. In our proposed method, a set of words maximizing a summarization score indicating appropriateness of summarization is extracted from automatically transcribed speech and concatenated to create a summary. The extraction process is performed using a Dynamic Programming (DP) technique according to a target compression ratio. In this paper, English broadcast news speech transcribed using a speech recognizer is automatically summarized. In order to apply our method, originally proposed for Japanese, to English, the model of estimating word concatenation probabilities based on a dependency structure in the original speech given by a Stochastic Dependency Context Free Grammar (SDCFG) is modified. A summarization method for multiple utterances using twolevel DP technique is also proposed. The automatically summarized sentences are evaluated by a summarization accuracy based on the comparison with the manual summarization of correctly transcribed speech by human subjects. Experimental results show that our proposed method effectively extracts relatively important information and remove redundant and irrelevant information from English news speech."
            },
            "slug": "Automatic-Summarization-Mani-Maybury",
            "title": {
                "fragments": [],
                "text": "Automatic Summarization"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "Experimental results show that the proposed automatic speech summarization technique for English effectively extracts relatively important information and remove redundant and irrelevant information from English news speech."
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3323275"
                        ],
                        "name": "Kishore Papineni",
                        "slug": "Kishore-Papineni",
                        "structuredName": {
                            "firstName": "Kishore",
                            "lastName": "Papineni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kishore Papineni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781292"
                        ],
                        "name": "S. Roukos",
                        "slug": "S.-Roukos",
                        "structuredName": {
                            "firstName": "Salim",
                            "lastName": "Roukos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roukos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144582029"
                        ],
                        "name": "T. Ward",
                        "slug": "T.-Ward",
                        "structuredName": {
                            "firstName": "Todd",
                            "lastName": "Ward",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ward"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2587983"
                        ],
                        "name": "Wei-Jing Zhu",
                        "slug": "Wei-Jing-Zhu",
                        "structuredName": {
                            "firstName": "Wei-Jing",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei-Jing Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "This result is more intuitive than using BLEU-2 and ROUGE-L."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 133
                            }
                        ],
                        "text": "Melamed et al. (2003) used unigram F-measure to estimate machine translation quality and showed that unigram Fmeasure was as good as BLEU."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 93
                            }
                        ],
                        "text": "Note that the number of n-grams in the denominator of the ROUGE-N formula increases as we add more references."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 33
                            }
                        ],
                        "text": "One advantage of skip-bigram vs. BLEU is that it does not require consecutive matches but is still sensitive to word order."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 84
                            }
                        ],
                        "text": "Following the successful application of automatic evaluation methods, such as BLEU (Papineni et al., 2001), in machine translation evaluation, Lin and Hovy (2003) showed that methods similar to BLEU,\ni.e. n-gram co-occurrence statistics, could be applied to evaluate summaries."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "BLEU measures how well a candidate translation matches a set of reference translations by counting the percentage of n-grams in the candidate translation overlapping with the references."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 11
                            }
                        ],
                        "text": "Please see Papineni et al. (2001) for details about BLEU."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "A closely related measure, BLEU, used in automatic evaluation of machine translation, is a precision-based measure."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11080756,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d7da009f457917aa381619facfa5ffae9329a6e9",
            "isKey": true,
            "numCitedBy": 16616,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations."
            },
            "slug": "Bleu:-a-Method-for-Automatic-Evaluation-of-Machine-Papineni-Roukos",
            "title": {
                "fragments": [],
                "text": "Bleu: a Method for Automatic Evaluation of Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work proposes a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3014320"
                        ],
                        "name": "Horacio Saggion",
                        "slug": "Horacio-Saggion",
                        "structuredName": {
                            "firstName": "Horacio",
                            "lastName": "Saggion",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Horacio Saggion"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9215251"
                        ],
                        "name": "Dragomir R. Radev",
                        "slug": "Dragomir-R.-Radev",
                        "structuredName": {
                            "firstName": "Dragomir",
                            "lastName": "Radev",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dragomir R. Radev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2480901"
                        ],
                        "name": "Simone Teufel",
                        "slug": "Simone-Teufel",
                        "structuredName": {
                            "firstName": "Simone",
                            "lastName": "Teufel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simone Teufel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144594306"
                        ],
                        "name": "Wai Lam",
                        "slug": "Wai-Lam",
                        "structuredName": {
                            "firstName": "Wai",
                            "lastName": "Lam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wai Lam"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 13
                            }
                        ],
                        "text": "For example, Saggion et al. (2002) proposed three content-based evaluation methods that measure similarity between summaries."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Saggion et al. (2002) used normalized pairwise LCS to compare simila rity between two texts in automatic summarization evaluation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 19265207,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "836173541b0cbf1c14dcd4f5b46cc67e469c392e",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a framework for the evaluation of summaries in English and Chinese using similarity measures. The framework can be used to evaluate extractive, non-extractive, single and multi-document summarization. We focus on the resources developed that are made available for the research community."
            },
            "slug": "Meta-evaluation-of-Summaries-in-a-Cross-lingual-Saggion-Radev",
            "title": {
                "fragments": [],
                "text": "Meta-evaluation of Summaries in a Cross-lingual Environment using Content-based Metrics"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A framework for the evaluation of summaries in English and Chinese using similarity measures that can be used to evaluate extractive, non-extractive, single and multi-document summarization is described."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144599392"
                        ],
                        "name": "I. D. Melamed",
                        "slug": "I.-D.-Melamed",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Melamed",
                            "middleNames": [
                                "Dan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. D. Melamed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2072879827"
                        ],
                        "name": "Ryan Green",
                        "slug": "Ryan-Green",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Green",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ryan Green"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153160559"
                        ],
                        "name": "Joseph P. Turian",
                        "slug": "Joseph-P.-Turian",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Turian",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph P. Turian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Melamed et al. (2003) used unigram F-measure to estimate machine translation quality and showed that unigram Fmeasure was as good as BLEU."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 31245542,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0cf7771a02921f9d4725f973a01c240d1a20634",
            "isKey": false,
            "numCitedBy": 267,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Machine translation can be evaluated using precision, recall, and the F-measure. These standard measures have significantly higher correlation with human judgments than recently proposed alternatives. More importantly, the standard measures have an intuitive interpretation, which can facilitate insights into how MT systems might be improved. The relevant software is publicly available."
            },
            "slug": "Precision-and-Recall-of-Machine-Translation-Melamed-Green",
            "title": {
                "fragments": [],
                "text": "Precision and Recall of Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "Machine translation can be evaluated using precision, recall, and the F-measure, which have significantly higher correlation with human judgments than recently proposed alternatives."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48477893"
                        ],
                        "name": "Debashis Kushary",
                        "slug": "Debashis-Kushary",
                        "structuredName": {
                            "firstName": "Debashis",
                            "lastName": "Kushary",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Debashis Kushary"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 86
                            }
                        ],
                        "text": "To assess the significance of the results, we applied bootstrap resampling technique (Davison and Hinkley, 1997) to estimate 95% confidence intervals for every correlation computation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 40651554,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "309886b7268a16a7d2bfd047e80d54ce381efc66",
            "isKey": false,
            "numCitedBy": 2116,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "sive models further, It gives additional illustrations of the performance of selection criteria in small samples (e.g., with respect to several varying characteristics that include sample size and parameter structure), in large samples (e.g., with respect to the degree of overfitting allowed), and on real data. This book looks at many selection criteria for a wide range of models, from traditional models to more recently developed classes of models. Currently, however, there appear to be few conclusive rules that give the best performing selection criteria for a given setting for this subject. As a result, the greatest value of this book appears to be in its derivation/description of the various selection criteria and the presentation of approaches for evaluating selection-criterion performance. The book also contains several instances in which key terms are not defined explicitly and no proofs or arguments are given for key claims. One message that summarizes several results given in this book is that certain selection criteria developed for traditional regression and time series models (e.g., Al&), when naively applied to certain nonnormal settings, appear to perform at least as well as selection criteria specifically designed for those settings."
            },
            "slug": "Bootstrap-Methods-and-Their-Application-Kushary",
            "title": {
                "fragments": [],
                "text": "Bootstrap Methods and Their Application"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Certain selection criteria developed for traditional regression and time series models, when naively applied to certain nonnormal settings, appear to perform at least as well as selection criteria specifically designed for those settings."
            },
            "venue": {
                "fragments": [],
                "text": "Technometrics"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 2
                            }
                        ],
                        "text": "\u2022 Lin and Hovy (2003), Lin (2004) \u2013 ROUGE, an automatic evaluation method used in summarization (DUC 2004) and MT (Lin and Och, ACL, COLING 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 393,
                                "start": 9
                            }
                        ],
                        "text": "Chin-Yew Lin, Workshop on Text Summarization Branches Out, Barcelona, Spain, July 25 - 26, 2004 Document Understanding Conference (DUC) \u2022 Part of US DARPA TIDES Project DUC 01 - 04 (http://duc.nist.gov) \u2013 Tasks \u2022 Single-doc summarization (DUC 01 and 02: 30 topics) \u2022 Single-doc headline generation (DUC 03: 30 topics, 04: 50 topics) \u2022 Multi-doc summarization \u2013 Generic 10, 50, 100, 200 (2002) , and 400 (2001) words summaries \u2013 Short summaries of about 100 words in three different tasks in 2003 \u00bb focused by an event (30 TDT clusters) \u00bb focused by a viewpoint (30 TREC clusters) \u00bb in response to a question (30 TREC Novelty track clusters) \u2013 Short summaries of about 665 bytes in three different tasks in 2004 \u00bb focused by an event (50 TDT clusters) \u00bb focused by an event but documents were translated into English from Arabic (24 topics) \u00bb in response to a \u201cwho is X?\u201d question (50 persons) \u2013 Participants \u2022 15 systems in DUC 2001, 17 in DUC 2002, 21 in DUC 2003, and 25 in DUC 2004 \u2022 A new 3-year roadmap will be released during the summer."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 96
                            }
                        ],
                        "text": "The stability and reliability of ROUGE at different sample sizes was reported by the author in (Lin, 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Workshop on Text Summarization Branches"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143645506"
                        ],
                        "name": "P. Over",
                        "slug": "P.-Over",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Over",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Over"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 59835348,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23996d66d95bbd5dd85cf7c53ecae976c924008d",
            "isKey": false,
            "numCitedBy": 135,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Intrinsic-Evaluation-of-Generic-News-Text-Systems-Over",
            "title": {
                "fragments": [],
                "text": "Intrinsic Evaluation of Generic News Text Summarization Systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Proceedings of Workshop on Text Summarization Branches Out, Post-Conference Workshop of ACL 2004"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Workshop on Text Summarization Branches Out, Post-Conference Workshop of ACL 2004"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 285,
                                "start": 265
                            }
                        ],
                        "text": "Notice that ROUGE-L is 1 when X = Y; while ROUGE-L is zero when LCS(X,Y) = 0, i.e. there is nothing in common between X and Y. Fmeasure or its equivalents has been shown to have met several theoretical criteria in measuring accuracy involving more than one factor (Van Rijsbergen, 1979)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 56
                            }
                        ],
                        "text": "Melamed et al. (2003) used unigram F-measure to estimate machine translation quality and showed that unigram Fmeasure was as good as BLEU."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information Retrieval. Butterworths"
            },
            "venue": {
                "fragments": [],
                "text": "Information Retrieval. Butterworths"
            },
            "year": 1979
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 0
                            }
                        ],
                        "text": "Melamed (1995) used the ratio (LCSR) between the length of the LCS of two words and the length of the longer word of the two words to measure the cognateness between them. He used LCS as an approximate string matching algorithm. Saggion et al. (2002) used normalized pairwise LCS to compare simila rity between two texts in automatic summarization evaluation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Melamed et al. (2003) used unigram F-measure to estimate machine translation quality and showed that unigram Fmeasure was as good as BLEU."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Melamed (1995) used the ratio (LCSR) between the length of the LCS of two words and the length of the longer word of the two words to measure the cognateness between them."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic evaluation and uni"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 181
                            }
                        ],
                        "text": "However, even simple manual evaluation of summaries on a large scale over a few linguistic quality questions and content coverage as in the Document Understanding Conference (DUC) (Over and Yen, 2003) would require over 3,000 hours of human efforts."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 491,
                                "start": 181
                            }
                        ],
                        "text": "However, even simple manual evaluation of summaries on a large scale over a few linguistic quality questions and content coverage as in the Document Understanding Conference (DUC) (Over and Yen, 2003) would require over 3,000 hours of human efforts. This is very expensive and difficult to conduct in a frequent basis. Therefore, how to evaluate summaries automatically has drawn a lot of attention in the summarization research community in recent years. For example, Saggion et al. (2002) proposed three content-based evaluation methods that measure similarity between summaries."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 965,
                                "start": 181
                            }
                        ],
                        "text": "However, even simple manual evaluation of summaries on a large scale over a few linguistic quality questions and content coverage as in the Document Understanding Conference (DUC) (Over and Yen, 2003) would require over 3,000 hours of human efforts. This is very expensive and difficult to conduct in a frequent basis. Therefore, how to evaluate summaries automatically has drawn a lot of attention in the summarization research community in recent years. For example, Saggion et al. (2002) proposed three content-based evaluation methods that measure similarity between summaries. These methods are: cosine similarity, unit overlap (i.e. unigram or bigram), and longest common subsequence. However, they did not show how the results of these automatic evaluation methods correlate to human judgments. Following the successful application of automatic evaluation methods, such as BLEU (Papineni et al., 2001), in machine translation evaluation, Lin and Hovy (2003) showed that methods similar to BLEU, i."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An introduction to DUC"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 181
                            }
                        ],
                        "text": "However, even simple manual evaluation of summaries on a large scale over a few linguistic quality questions and content coverage as in the Document Understanding Conference (DUC) (Over and Yen, 2003) would require over 3,000 hours of human efforts."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An introduction to DUC 2003 \u2013 Intrinsic evaluation of generic news text summarization s ystems"
            },
            "venue": {
                "fragments": [],
                "text": "An introduction to DUC 2003 \u2013 Intrinsic evaluation of generic news text summarization s ystems"
            },
            "year": 2003
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 7,
            "methodology": 8,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 16,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/ROUGE:-A-Package-for-Automatic-Evaluation-of-Lin/60b05f32c32519a809f21642ef1eb3eaf3848008?sort=total-citations"
}