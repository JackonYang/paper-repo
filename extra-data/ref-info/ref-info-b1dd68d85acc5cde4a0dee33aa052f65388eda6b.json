{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2802529"
                        ],
                        "name": "Maksim Lapin",
                        "slug": "Maksim-Lapin",
                        "structuredName": {
                            "firstName": "Maksim",
                            "lastName": "Lapin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Maksim Lapin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143610806"
                        ],
                        "name": "Matthias Hein",
                        "slug": "Matthias-Hein",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Hein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthias Hein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10168801,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c571699cb01cc04f40a43da71d09d23595e85587",
            "isKey": false,
            "numCitedBy": 107,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-using-privileged-information:-SV-M+-and-Lapin-Hein",
            "title": {
                "fragments": [],
                "text": "Learning using privileged information: SV M+ and weighted SVM"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723883"
                        ],
                        "name": "F. Perronnin",
                        "slug": "F.-Perronnin",
                        "structuredName": {
                            "firstName": "Florent",
                            "lastName": "Perronnin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Perronnin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143995438"
                        ],
                        "name": "Jorge S\u00e1nchez",
                        "slug": "Jorge-S\u00e1nchez",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "S\u00e1nchez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jorge S\u00e1nchez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722052"
                        ],
                        "name": "Thomas Mensink",
                        "slug": "Thomas-Mensink",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Mensink",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Mensink"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10402702,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "39f3b1804b8df5be645a1dcb4a876e128385d9be",
            "isKey": false,
            "numCitedBy": 2662,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "The Fisher kernel (FK) is a generic framework which combines the benefits of generative and discriminative approaches. In the context of image classification the FK was shown to extend the popular bag-of-visual-words (BOV) by going beyond count statistics. However, in practice, this enriched representation has not yet shown its superiority over the BOV. In the first part we show that with several well-motivated modifications over the original framework we can boost the accuracy of the FK. On PASCAL VOC 2007 we increase the Average Precision (AP) from 47.9% to 58.3%. Similarly, we demonstrate state-of-the-art accuracy on CalTech 256. A major advantage is that these results are obtained using only SIFT descriptors and costless linear classifiers. Equipped with this representation, we can now explore image classification on a larger scale. In the second part, as an application, we compare two abundant resources of labeled images to learn classifiers: ImageNet and Flickr groups. In an evaluation involving hundreds of thousands of training images we show that classifiers learned on Flickr groups perform surprisingly well (although they were not intended for this purpose) and that they can complement classifiers learned on more carefully annotated datasets."
            },
            "slug": "Improving-the-Fisher-Kernel-for-Large-Scale-Image-Perronnin-S\u00e1nchez",
            "title": {
                "fragments": [],
                "text": "Improving the Fisher Kernel for Large-Scale Image Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "In an evaluation involving hundreds of thousands of training images, it is shown that classifiers learned on Flickr groups perform surprisingly well and that they can complement classifier learned on more carefully annotated datasets."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787591"
                        ],
                        "name": "Christoph H. Lampert",
                        "slug": "Christoph-H.-Lampert",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Lampert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christoph H. Lampert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748758"
                        ],
                        "name": "H. Nickisch",
                        "slug": "H.-Nickisch",
                        "structuredName": {
                            "firstName": "Hannes",
                            "lastName": "Nickisch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Nickisch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734990"
                        ],
                        "name": "S. Harmeling",
                        "slug": "S.-Harmeling",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Harmeling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Harmeling"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 148
                            }
                        ],
                        "text": "Feature vectors made out of semantic attributes have been used to improve object categorization when very few or no training examples are available [14, 27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "The values of the predicted attributes are obtained from DAP model [14] and correspond to probability estimates of the binary attributes in the images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 49
                            }
                        ],
                        "text": "We use the Animals with Attributes (AwA) dataset [14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7016601,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9bc0295460089592d04e754a5fd427060b7bfa8c",
            "isKey": false,
            "numCitedBy": 1212,
            "numCiting": 69,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the problem of object recognition for categories for which we have no training examples, a task also called zero--data or zero-shot learning. This situation has hardly been studied in computer vision research, even though it occurs frequently; the world contains tens of thousands of different object classes, and image collections have been formed and suitably annotated for only a few of them. To tackle the problem, we introduce attribute-based classification: Objects are identified based on a high-level description that is phrased in terms of semantic attributes, such as the object's color or shape. Because the identification of each such property transcends the specific learning task at hand, the attribute classifiers can be prelearned independently, for example, from existing image data sets unrelated to the current task. Afterward, new classes can be detected based on their attribute representation, without the need for a new training phase. In this paper, we also introduce a new data set, Animals with Attributes, of over 30,000 images of 50 animal classes, annotated with 85 semantic attributes. Extensive experiments on this and two more data sets show that attribute-based classification indeed is able to categorize images without access to any training images of the target classes."
            },
            "slug": "Attribute-Based-Classification-for-Zero-Shot-Visual-Lampert-Nickisch",
            "title": {
                "fragments": [],
                "text": "Attribute-Based Classification for Zero-Shot Visual Object Categorization"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2871555"
                        ],
                        "name": "P. Gehler",
                        "slug": "P.-Gehler",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Gehler",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gehler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2388416"
                        ],
                        "name": "S. Nowozin",
                        "slug": "S.-Nowozin",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Nowozin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nowozin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 232
                            }
                        ],
                        "text": "Methods suggested here range from stacking, where one simply concatenates the feature vectors of all data modalities, to complex adaptive methods for early or late data fusions [23], including multiple kernel learning [26] and LP-\u03b2 [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15536375,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ba0548583a5ab3dca551f60e30f85ea42b2a4873",
            "isKey": false,
            "numCitedBy": 900,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "A key ingredient in the design of visual object classification systems is the identification of relevant class specific aspects while being robust to intra-class variations. While this is a necessity in order to generalize beyond a given set of training images, it is also a very difficult problem due to the high variability of visual appearance within each class. In the last years substantial performance gains on challenging benchmark datasets have been reported in the literature. This progress can be attributed to two developments: the design of highly discriminative and robust image features and the combination of multiple complementary features based on different aspects such as shape, color or texture. In this paper we study several models that aim at learning the correct weighting of different features from training data. These include multiple kernel learning as well as simple baseline methods. Furthermore we derive ensemble methods inspired by Boosting which are easily extendable to several multiclass setting. All methods are thoroughly evaluated on object classification datasets using a multitude of feature descriptors. The key results are that even very simple baseline methods, that are orders of magnitude faster than learning techniques are highly competitive with multiple kernel learning. Furthermore the Boosting type methods are found to produce consistently better results in all experiments. We provide insight of when combination methods can be expected to work and how the benefit of complementary features can be exploited most efficiently."
            },
            "slug": "On-feature-combination-for-multiclass-object-Gehler-Nowozin",
            "title": {
                "fragments": [],
                "text": "On feature combination for multiclass object classification"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Several models that aim at learning the correct weighting of different features from training data are studied, including multiple kernel learning as well as simple baseline methods and ensemble methods inspired by Boosting are derived."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE 12th International Conference on Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808423"
                        ],
                        "name": "G. Csurka",
                        "slug": "G.-Csurka",
                        "structuredName": {
                            "firstName": "Gabriela",
                            "lastName": "Csurka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Csurka"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In the following, we will think of the examples as images and of their representation as computed from the image content, for example bag-of-visual-words histograms [6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17606900,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b91180d8853d00e8f2df7ee3532e07d3d0cce2af",
            "isKey": false,
            "numCitedBy": 5008,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel method for generic visual categorization: the problem of identifying the object content of natural images while generalizing across variations inherent to the object class. This bag of keypoints method is based on vector quantization of affine invariant descriptors of image patches. We propose and compare two alternative implementations using different classifiers: Naive Bayes and SVM. The main advantages of the method are that it is simple, computationally efficient and intrinsically invariant. We present results for simultaneously classifying seven semantic visual categories. These results clearly demonstrate that the method is robust to background clutter and produces good categorization accuracy even without exploiting geometric information."
            },
            "slug": "Visual-categorization-with-bags-of-keypoints-Csurka",
            "title": {
                "fragments": [],
                "text": "Visual categorization with bags of keypoints"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This bag of keypoints method is based on vector quantization of affine invariant descriptors of image patches and shows that it is simple, computationally efficient and intrinsically invariant."
            },
            "venue": {
                "fragments": [],
                "text": "eccv 2004"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056091"
                        ],
                        "name": "M. Everingham",
                        "slug": "M.-Everingham",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Everingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Everingham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4246903,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82635fb63640ae95f90ee9bdc07832eb461ca881",
            "isKey": false,
            "numCitedBy": 11680,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension."
            },
            "slug": "The-Pascal-Visual-Object-Classes-(VOC)-Challenge-Everingham-Gool",
            "title": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes (VOC) Challenge"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The state-of-the-art in evaluated methods for both classification and detection are reviewed, whether the methods are statistically different, what they are learning from the images, and what the methods find easy or confuse."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2617872"
                        ],
                        "name": "Dmitry Pechyony",
                        "slug": "Dmitry-Pechyony",
                        "structuredName": {
                            "firstName": "Dmitry",
                            "lastName": "Pechyony",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dmitry Pechyony"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[17, 25]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 13217651,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d96dd9c9de89afa6828b88b7613590f50018585",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "In Learning Using Privileged Information (LUPI) paradigm, along with the standard training data in the decision space, a teacher supplies a learner with the privileged information in the correcting space. The goal of the learner is to find a classifier with a low generalization error in the decision space. We consider an empirical risk minimization algorithm, called Privileged ERM, that takes into account the privileged information in order to find a good function in the decision space. We outline the conditions on the correcting space that, if satisfied, allow Privileged ERM to have much faster learning rate in the decision space than the one of the regular empirical risk minimization."
            },
            "slug": "On-the-theory-of-learning-with-Privileged-Pechyony-Vapnik",
            "title": {
                "fragments": [],
                "text": "On the theory of learning with Privileged Information"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This work considers an empirical risk minimization algorithm, called Privileged ERM, that takes into account the privileged information in order to find a good function in the decision space, and outlines the conditions on the correcting space that, if satisfied, allow Priv privileged ERM to have much faster learning rate in the decided space."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 2010"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761495"
                        ],
                        "name": "C. M. Christoudias",
                        "slug": "C.-M.-Christoudias",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Christoudias",
                            "middleNames": [
                                "Mario"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. M. Christoudias"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2422559"
                        ],
                        "name": "R. Urtasun",
                        "slug": "R.-Urtasun",
                        "structuredName": {
                            "firstName": "Raquel",
                            "lastName": "Urtasun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Urtasun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 172
                            }
                        ],
                        "text": "Alternatively, one can use the training data to learn a mapping from the image to the privileged modality and use this predictor to fill in the values missing at test time [5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11745132,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "34884c64b6221b95501283f6bb856121a5515c24",
            "isKey": false,
            "numCitedBy": 123,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditional multi-view learning approaches suffer in the presence of view disagreement, i.e., when samples in each view do not belong to the same class due to view corruption, occlusion or other noise processes. In this paper we present a multi-view learning approach that uses a conditional entropy criterion to detect view disagreement. Once detected, samples with view disagreement are filtered and standard multi-view learning methods can be successfully applied to the remaining samples. Experimental evaluation on synthetic and audio-visual databases demonstrates that the detection and filtering of view disagreement considerably increases the performance of traditional multi-view learning approaches."
            },
            "slug": "Multi-View-Learning-in-the-Presence-of-View-Christoudias-Urtasun",
            "title": {
                "fragments": [],
                "text": "Multi-View Learning in the Presence of View Disagreement"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A multi-view learning approach that uses a conditional entropy criterion to detect view disagreement and once detected, samples with view disagreement are filtered and standard multi- view learning methods can be successfully applied to the remaining samples."
            },
            "venue": {
                "fragments": [],
                "text": "UAI 2008"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687524"
                        ],
                        "name": "A. Vedaldi",
                        "slug": "A.-Vedaldi",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Vedaldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vedaldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3236386"
                        ],
                        "name": "Varun Gulshan",
                        "slug": "Varun-Gulshan",
                        "structuredName": {
                            "firstName": "Varun",
                            "lastName": "Gulshan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Varun Gulshan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145859952"
                        ],
                        "name": "M. Varma",
                        "slug": "M.-Varma",
                        "structuredName": {
                            "firstName": "Manik",
                            "lastName": "Varma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Varma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 218
                            }
                        ],
                        "text": "Methods suggested here range from stacking, where one simply concatenates the feature vectors of all data modalities, to complex adaptive methods for early or late data fusions [23], including multiple kernel learning [26] and LP-\u03b2 [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206769604,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c9c7ab03bdb5fee15174d910d7fea14a16b086b7",
            "isKey": false,
            "numCitedBy": 883,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Our objective is to obtain a state-of-the art object category detector by employing a state-of-the-art image classifier to search for the object in all possible image sub-windows. We use multiple kernel learning of Varma and Ray (ICCV 2007) to learn an optimal combination of exponential \u03c72 kernels, each of which captures a different feature channel. Our features include the distribution of edges, dense and sparse visual words, and feature descriptors at different levels of spatial organization."
            },
            "slug": "Multiple-kernels-for-object-detection-Vedaldi-Gulshan",
            "title": {
                "fragments": [],
                "text": "Multiple kernels for object detection"
            },
            "tldr": {
                "abstractSimilarityScore": 35,
                "text": "This work uses multiple kernel learning of Varma and Ray (ICCV 2007) to learn an optimal combination of exponential \u03c72 kernels, each of which captures a different feature channel."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE 12th International Conference on Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7408951"
                        ],
                        "name": "Jeff Donahue",
                        "slug": "Jeff-Donahue",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Donahue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Donahue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794409"
                        ],
                        "name": "K. Grauman",
                        "slug": "K.-Grauman",
                        "structuredName": {
                            "firstName": "Kristen",
                            "lastName": "Grauman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Grauman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "In [7] it was shown that these can act as additional sources of information during training, as long as the rationales can be expressed in the same data representation as the original data (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 97
                            }
                        ],
                        "text": "Nevertheless we perform this experiment to position our result with respect to the original work [7]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 25
                            }
                        ],
                        "text": "Following the setting of [7], we use 108 and 104 images for female and male classes accordingly with +1(hot) and\u22121(not) label."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 39
                            }
                        ],
                        "text": "Rationale annotation was introduced in [7] as a way to capture additional information why an annotator makes the decision about the given image."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13193640,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d2b779ad15c6f73011673105dcaf02d243c6d2ae",
            "isKey": true,
            "numCitedBy": 74,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditional supervised visual learning simply asks annotators \u201cwhat\u201d label an image should have. We propose an approach for image classification problems requiring subjective judgment that also asks \u201cwhy\u201d, and uses that information to enrich the learned model. We develop two forms of visual annotator rationales: in the first, the annotator highlights the spatial region of interest he found most influential to the label selected, and in the second, he comments on the visual attributes that were most important. For either case, we show how to map the response to synthetic contrast examples, and then exploit an existing large-margin learning technique to refine the decision boundary accordingly. Results on multiple scene categorization and human attractiveness tasks show the promise of our approach, which can more accurately learn complex categories with the explanations behind the label choices."
            },
            "slug": "Annotator-rationales-for-visual-recognition-Donahue-Grauman",
            "title": {
                "fragments": [],
                "text": "Annotator rationales for visual recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Results on multiple scene categorization and human attractiveness tasks show the promise of the proposed approach, which can more accurately learn complex categories with the explanations behind the label choices."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704531"
                        ],
                        "name": "Novi Quadrianto",
                        "slug": "Novi-Quadrianto",
                        "structuredName": {
                            "firstName": "Novi",
                            "lastName": "Quadrianto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Novi Quadrianto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787591"
                        ],
                        "name": "Christoph H. Lampert",
                        "slug": "Christoph-H.-Lampert",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Lampert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christoph H. Lampert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "Similarly the shared representation is also used for cross-modal retrieval [20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1740642,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c0d584423cd4afc3c1b861b7c62bb5da48021d66",
            "isKey": false,
            "numCitedBy": 111,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of metric learning for multi-view data, namely the construction of embedding projections from data in different representations into a shared feature space, such that the Euclidean distance in this space provides a meaningful within-view as well as between-view similarity. Our motivation stems from the problem of cross-media retrieval tasks, where the availability of a joint Euclidean distance function is a prerequisite to allow fast, in particular hashing-based, nearest neighbor queries. \n \nWe formulate an objective function that expresses the intuitive concept that matching samples are mapped closely together in the output space, whereas non-matching samples are pushed apart, no matter in which view they are available. The resulting optimization problem is not convex, but it can be decomposed explicitly into a convex and a concave part, thereby allowing efficient optimization using the convex-concave procedure. Experiments on an image retrieval task show that nearest-neighbor based cross-view retrieval is indeed possible, and the proposed technique improves the retrieval accuracy over baseline techniques."
            },
            "slug": "Learning-Multi-View-Neighborhood-Preserving-Quadrianto-Lampert",
            "title": {
                "fragments": [],
                "text": "Learning Multi-View Neighborhood Preserving Projections"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This work addresses the problem of metric learning for multi-view data, namely the construction of embedding projections from data in different representations into a shared feature space, such that the Euclidean distance in this space provides a meaningful within-view as well as between-view similarity."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40462943"
                        ],
                        "name": "D. K\u00fcttel",
                        "slug": "D.-K\u00fcttel",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "K\u00fcttel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. K\u00fcttel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2737253"
                        ],
                        "name": "M. Guillaumin",
                        "slug": "M.-Guillaumin",
                        "structuredName": {
                            "firstName": "Matthieu",
                            "lastName": "Guillaumin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Guillaumin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143865718"
                        ],
                        "name": "V. Ferrari",
                        "slug": "V.-Ferrari",
                        "structuredName": {
                            "firstName": "Vittorio",
                            "lastName": "Ferrari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Ferrari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 100
                            }
                        ],
                        "text": ", when trying to learn an image segmentation system using only per-image or bounding box annotation [13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9346547,
            "fieldsOfStudy": [
                "Computer Science",
                "Environmental Science"
            ],
            "id": "50bf30469f4d95f28acf80cf7c4ba5ab2f1c422e",
            "isKey": false,
            "numCitedBy": 619,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "ImageNet is a large-scale hierarchical database of object classes. We propose to automatically populate it with pixelwise segmentations, by leveraging existing manual annotations in the form of class labels and bounding-boxes. The key idea is to recursively exploit images segmented so far to guide the segmentation of new images. At each stage this propagation process expands into the images which are easiest to segment at that point in time, e.g. by moving to the semantically most related classes to those segmented so far. The propagation of segmentation occurs both (a) at the image level, by transferring existing segmentations to estimate the probability of a pixel to be foreground, and (b) at the class level, by jointly segmenting images of the same class and by importing the appearance models of classes that are already segmented. Through an experiment on 577 classes and 500k images we show that our technique (i) annotates a wide range of classes with accurate segmentations; (ii) effectively exploits the hierarchical structure of ImageNet; (iii) scales efficiently; (iv) outperforms a baseline GrabCut [1] initialized on the image center, as well as our recent segmentation transfer technique [2] on which this paper is based. Moreover, our method also delivers state-of-the-art results on the recent iCoseg dataset for co-segmentation."
            },
            "slug": "Segmentation-Propagation-in-ImageNet-K\u00fcttel-Guillaumin",
            "title": {
                "fragments": [],
                "text": "Segmentation Propagation in ImageNet"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This paper proposes to automatically populate ImageNet with pixelwise segmentations, by leveraging existing manual annotations in the form of class labels and bounding-boxes, and effectively exploits the hierarchical structure of ImageNet."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46396571"
                        ],
                        "name": "Yang Wang",
                        "slug": "Yang-Wang",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yang Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10771328"
                        ],
                        "name": "Greg Mori",
                        "slug": "Greg-Mori",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Mori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg Mori"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 148
                            }
                        ],
                        "text": "Feature vectors made out of semantic attributes have been used to improve object categorization when very few or no training examples are available [14, 27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18882594,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fc23a386c2189f221b25dbd0bb34fcd26ccf60fa",
            "isKey": false,
            "numCitedBy": 253,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a discriminatively trained model for joint modelling of object class labels (e.g. \"person\", \"dog\", \"chair\", etc.) and their visual attributes (e.g. \"has head\", \"furry\", \"metal\", etc.). We treat attributes of an object as latent variables in our model and capture the correlations among attributes using an undirected graphical model built from training data. The advantage of our model is that it allows us to infer object class labels using the information of both the test image itself and its (latent) attributes. Our model unifies object class prediction and attribute prediction in a principled framework. It is also flexible enough to deal with different performance measurements. Our experimental results provide quantitative evidence that attributes can improve object naming."
            },
            "slug": "A-Discriminative-Latent-Model-of-Object-Classes-and-Wang-Mori",
            "title": {
                "fragments": [],
                "text": "A Discriminative Latent Model of Object Classes and Attributes"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This work presents a discriminatively trained model for joint modelling of object class labels and their visual attributes and captures the correlations among attributes using an undirected graphical model built from training data."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1988453"
                        ],
                        "name": "R. Bekkerman",
                        "slug": "R.-Bekkerman",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Bekkerman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bekkerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791802"
                        ],
                        "name": "J. Jeon",
                        "slug": "J.-Jeon",
                        "structuredName": {
                            "firstName": "Jiwoon",
                            "lastName": "Jeon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Jeon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 42
                            }
                        ],
                        "text": "We use IsraelImages dataset introduced in [2]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11425716,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1fd701d243c9a44b2c9ff8040f72a9b9b88cb037",
            "isKey": false,
            "numCitedBy": 92,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Most of the online multimedia collections, such as picture galleries or video archives, are categorized in a fully manual process, which is very expensive and may soon be infeasible with the rapid growth of multimedia repositories. In this paper, we present an effective method for automating this process within the unsupervised learning framework. We exploit the truly multi-modal nature of multimedia collections - they have multiple views, or modalities, each of which contributes its own perspective to the collection's organization. For example, in picture galleries, image captions are often provided that form a separate view on the collection. Color histograms (or any other set of global features) form another view. Additional views are blobs, interest points and other sets of local features. Our model, called Comraf* (pronounced Comraf-Star), efficiently incorporates various views in multi-modal clustering, by which it allows great modeling flexibility. Comraf* is a light-weight version of the recently introduced combinatorial Markov random field (Comraf). We show how to translate an arbitrary Comraf into a series of Comraf* models, and give an empirical evidence for comparable effectiveness of the two. Comraf* demonstrates excellent results on two real-world image galleries: it obtains 2.5-3 times higher accuracy compared with a uni-modal k-means."
            },
            "slug": "Multi-modal-Clustering-for-Multimedia-Collections-Bekkerman-Jeon",
            "title": {
                "fragments": [],
                "text": "Multi-modal Clustering for Multimedia Collections"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A light-weight version of the recently introduced combinatorial Markov random field (Comraf), Comraf* (pronounced Comraf-Star), efficiently incorporates various views in multi-modal clustering, by which it allows great modeling flexibility."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 61116019,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7550a05bf00f7b24aed9c1ac3ef000575388d21c",
            "isKey": false,
            "numCitedBy": 5454,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Training a support vector machine SVM leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples on the shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. SVM light is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for SVM light V 2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains."
            },
            "slug": "Making-large-scale-SVM-learning-practical-Joachims",
            "title": {
                "fragments": [],
                "text": "Making large scale SVM learning practical"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This chapter presents algorithmic and computational results developed for SVM light V 2.0, which make large-scale SVM training more practical and give guidelines for the application of SVMs to large domains."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784512"
                        ],
                        "name": "A. Vashist",
                        "slug": "A.-Vashist",
                        "structuredName": {
                            "firstName": "Akshay",
                            "lastName": "Vashist",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vashist"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12342641,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "58059409e131f2a854367052636138e835f14f60",
            "isKey": false,
            "numCitedBy": 590,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-new-learning-paradigm:-Learning-using-privileged-Vapnik-Vashist",
            "title": {
                "fragments": [],
                "text": "A new learning paradigm: Learning using privileged information"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207605508,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cfd4259d305a00f13d5f08841230389f61322422",
            "isKey": false,
            "numCitedBy": 4304,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an approach to automatically optimizing the retrieval quality of search engines using clickthrough data. Intuitively, a good information retrieval system should present relevant documents high in the ranking, with less relevant documents following below. While previous approaches to learning retrieval functions from examples exist, they typically require training data generated from relevance judgments by experts. This makes them difficult and expensive to apply. The goal of this paper is to develop a method that utilizes clickthrough data for training, namely the query-log of the search engine in connection with the log of links the users clicked on in the presented ranking. Such clickthrough data is available in abundance and can be recorded at very low cost. Taking a Support Vector Machine (SVM) approach, this paper presents a method for learning retrieval functions. From a theoretical perspective, this method is shown to be well-founded in a risk minimization framework. Furthermore, it is shown to be feasible even for large sets of queries and features. The theoretical results are verified in a controlled experiment. It shows that the method can effectively adapt the retrieval function of a meta-search engine to a particular group of users, outperforming Google in terms of retrieval quality after only a couple of hundred training examples."
            },
            "slug": "Optimizing-search-engines-using-clickthrough-data-Joachims",
            "title": {
                "fragments": [],
                "text": "Optimizing search engines using clickthrough data"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The goal of this paper is to develop a method that utilizes clickthrough data for training, namely the query-log of the search engine in connection with the log of links the users clicked on in the presented ranking."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2195306"
                        ],
                        "name": "H. Bay",
                        "slug": "H.-Bay",
                        "structuredName": {
                            "firstName": "Herbert",
                            "lastName": "Bay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2433494"
                        ],
                        "name": "Andreas Ess",
                        "slug": "Andreas-Ess",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Ess",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Ess"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704728"
                        ],
                        "name": "T. Tuytelaars",
                        "slug": "T.-Tuytelaars",
                        "structuredName": {
                            "firstName": "Tinne",
                            "lastName": "Tuytelaars",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Tuytelaars"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 55
                            }
                        ],
                        "text": "We use L1 normalized 2000 dimensional SURF descriptors [1] as original features, and 85 dimensional predicted attributes as the privileged information."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14777911,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cdbb606ae47c64049262dfbd3bb147d3f4ba8420",
            "isKey": false,
            "numCitedBy": 11651,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Speeded-Up-Robust-Features-(SURF)-Bay-Ess",
            "title": {
                "fragments": [],
                "text": "Speeded-Up Robust Features (SURF)"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1936277"
                        ],
                        "name": "Omar Zaidan",
                        "slug": "Omar-Zaidan",
                        "structuredName": {
                            "firstName": "Omar",
                            "lastName": "Zaidan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Omar Zaidan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145043214"
                        ],
                        "name": "Jason Eisner",
                        "slug": "Jason-Eisner",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Eisner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason Eisner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 30
                            }
                        ],
                        "text": "Recently annotator rationales [28] have been introduced to the computer vision community."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 635609,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14e2aec7e25d8880a851a547cf8d27a9721f8e6c",
            "isKey": false,
            "numCitedBy": 117,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "A human annotator can provide hints to a machine learner by highlighting contextual \"rationales\" for each of his or her annotations (Zaidan et al., 2007). How can one exploit this side information to better learn the desired parameters \u03b8? We present a generative model of how a given annotator, knowing the true \u03b8, stochastically chooses rationales. Thus, observing the rationales helps us infer the true \u03b8. We collect substring rationales for a sentiment classification task (Pang and Lee, 2004) and use them to obtain significant accuracy improvements for each annotator. Our new generative approach exploits the rationales more effectively than our previous \"masking SVM\" approach. It is also more principled, and could be adapted to help learn other kinds of probabilistic classifiers for quite different tasks."
            },
            "slug": "Modeling-Annotators:-A-Generative-Approach-to-from-Zaidan-Eisner",
            "title": {
                "fragments": [],
                "text": "Modeling Annotators: A Generative Approach to Learning from Annotator Rationales"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A generative model of how a given annotator, knowing the true \u03b8, stochastically chooses rationales is presented, and observing the rationales helps us infer thetrue \u03b8."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 143
                            }
                        ],
                        "text": "Furthermore, in contrast to SVM+, we can use standard SVM packages to solve them, including efficient methods working in primal representation [4], and solvers based on stochastic gradient descent [22]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12601634,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "835c1fa10bbe06730b55ccca95be239f9421e52c",
            "isKey": false,
            "numCitedBy": 818,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "Most literature on support vector machines (SVMs) concentrates on the dual optimization problem. In this letter, we point out that the primal problem can also be solved efficiently for both linear and nonlinear SVMs and that there is no reason for ignoring this possibility. On the contrary, from the primal point of view, new families of algorithms for large-scale SVM training can be investigated."
            },
            "slug": "Training-a-Support-Vector-Machine-in-the-Primal-Chapelle",
            "title": {
                "fragments": [],
                "text": "Training a Support Vector Machine in the Primal"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is pointed out that the primal problem can also be solved efficiently for both linear and nonlinear SVMs and that there is no reason for ignoring this possibility."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1849128"
                        ],
                        "name": "Rong-En Fan",
                        "slug": "Rong-En-Fan",
                        "structuredName": {
                            "firstName": "Rong-En",
                            "lastName": "Fan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rong-En Fan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2782886"
                        ],
                        "name": "Kai-Wei Chang",
                        "slug": "Kai-Wei-Chang",
                        "structuredName": {
                            "firstName": "Kai-Wei",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai-Wei Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793529"
                        ],
                        "name": "Cho-Jui Hsieh",
                        "slug": "Cho-Jui-Hsieh",
                        "structuredName": {
                            "firstName": "Cho-Jui",
                            "lastName": "Hsieh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cho-Jui Hsieh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144799660"
                        ],
                        "name": "Xiang-Rui Wang",
                        "slug": "Xiang-Rui-Wang",
                        "structuredName": {
                            "firstName": "Xiang-Rui",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiang-Rui Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711460"
                        ],
                        "name": "Chih-Jen Lin",
                        "slug": "Chih-Jen-Lin",
                        "structuredName": {
                            "firstName": "Chih-Jen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Jen Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 96
                            }
                        ],
                        "text": "Many existing SVM packages support such per-sample weights, in our experiments we use LIBLINEAR [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3116168,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "268a4f8da15a42f3e0e71691f760ff5edbf9cec8",
            "isKey": false,
            "numCitedBy": 7764,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "LIBLINEAR is an open source library for large-scale linear classification. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efficient on large sparse data sets."
            },
            "slug": "LIBLINEAR:-A-Library-for-Large-Linear-Fan-Chang",
            "title": {
                "fragments": [],
                "text": "LIBLINEAR: A Library for Large Linear Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "LIBLINEAR is an open source library for large-scale linear classification that supports logistic regression and linear support vector machines and provides easy-to-use command-line tools and library calls for users and developers."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389955537"
                        ],
                        "name": "S. Shalev-Shwartz",
                        "slug": "S.-Shalev-Shwartz",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Shalev-Shwartz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Shalev-Shwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706280"
                        ],
                        "name": "Nathan Srebro",
                        "slug": "Nathan-Srebro",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Srebro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nathan Srebro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145658292"
                        ],
                        "name": "Andrew Cotter",
                        "slug": "Andrew-Cotter",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Cotter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Cotter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 197
                            }
                        ],
                        "text": "Furthermore, in contrast to SVM+, we can use standard SVM packages to solve them, including efficient methods working in primal representation [4], and solvers based on stochastic gradient descent [22]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 53306004,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb4ffd271451a8fbed752582543bd34925fa4396",
            "isKey": false,
            "numCitedBy": 2003,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe and analyze a simple and effective stochastic sub-gradient descent algorithm for solving the optimization problem cast by Support Vector Machines (SVM). We prove that the number of iterations required to obtain a solution of accuracy $${\\epsilon}$$ is $${\\tilde{O}(1 / \\epsilon)}$$, where each iteration operates on a single training example. In contrast, previous analyses of stochastic gradient descent methods for SVMs require $${\\Omega(1 / \\epsilon^2)}$$ iterations. As in previously devised SVM solvers, the number of iterations also scales linearly with 1/\u03bb, where \u03bb is the regularization parameter of SVM. For a linear kernel, the total run-time of our method is $${\\tilde{O}(d/(\\lambda \\epsilon))}$$, where d is a bound on the number of non-zero features in each example. Since the run-time does not depend directly on the size of the training set, the resulting algorithm is especially suited for learning from large datasets. Our approach also extends to non-linear kernels while working solely on the primal objective function, though in this case the runtime does depend linearly on the training set size. Our algorithm is particularly well suited for large text classification problems, where we demonstrate an order-of-magnitude speedup over previous SVM learning methods."
            },
            "slug": "Pegasos:-primal-estimated-sub-gradient-solver-for-Shalev-Shwartz-Singer",
            "title": {
                "fragments": [],
                "text": "Pegasos: primal estimated sub-gradient solver for SVM"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A simple and effective stochastic sub-gradient descent algorithm for solving the optimization problem cast by Support Vector Machines, which is particularly well suited for large text classification problems, and demonstrates an order-of-magnitude speedup over previous SVM learning methods."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 69
                            }
                        ],
                        "text": "Kernelizing and dualizing them is possible using standard techniques [21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 29871328,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5051890e501117097eeffbd8ded87694f0d8063",
            "isKey": false,
            "numCitedBy": 6578,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "All rights reserved. No part of this book may be reproduced in any form by any electronic or mechanical means (including photocopying, recording, or information storage and retrieval) without permission in writing from the publisher."
            },
            "slug": "Learning-with-kernels-Smola",
            "title": {
                "fragments": [],
                "text": "Learning with kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "This book is intended to be a guide to the art of self-consistency and should not be used as a substitute for a comprehensive guide to self-confidence."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758219"
                        ],
                        "name": "Matthew B. Blaschko",
                        "slug": "Matthew-B.-Blaschko",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Blaschko",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew B. Blaschko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787591"
                        ],
                        "name": "Christoph H. Lampert",
                        "slug": "Christoph-H.-Lampert",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Lampert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christoph H. Lampert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 124
                            }
                        ],
                        "text": "ities, it has been proposed to use CCA to learn a shared representation that can be computed from either of representations [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1145503,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7fd700f4a010d765c506841de9884df394c1de1c",
            "isKey": false,
            "numCitedBy": 199,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new method for spectral clustering with paired data based on kernel canonical correlation analysis, called correlational spectral clustering. Paired data are common in real world data sources, such as images with text captions. Traditional spectral clustering algorithms either assume that data can be represented by a single similarity measure, or by co-occurrence matrices that are then used in biclustering. In contrast, the proposed method uses separate similarity measures for each data representation, and allows for projection of previously unseen data that are only observed in one representation (e.g. images but not text). We show that this algorithm generalizes traditional spectral clustering algorithms and show consistent empirical improvement over spectral clustering on a variety of datasets of images with associated text."
            },
            "slug": "Correlational-spectral-clustering-Blaschko-Lampert",
            "title": {
                "fragments": [],
                "text": "Correlational spectral clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The proposed method uses separate similarity measures for each data representation, and allows for projection of previously unseen data that are only observed in one representation (e.g. images but not text)."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "sifier [24]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7138354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8213dbed4db44e113af3ed17d6dad57471a0c048",
            "isKey": false,
            "numCitedBy": 38755,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?."
            },
            "slug": "The-Nature-of-Statistical-Learning-Theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics for Engineering and Information Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2617872"
                        ],
                        "name": "Dmitry Pechyony",
                        "slug": "Dmitry-Pechyony",
                        "structuredName": {
                            "firstName": "Dmitry",
                            "lastName": "Pechyony",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dmitry Pechyony"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We analyze two methods of learning using privileged information: our proposed Rank Transfer method for transferring the rank, and the SVM+ method [18] kindly provided by the authors."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In [18], suitable sequential minimal optimization (SMO) algorithms were derived, one of which we use for our experiments (Section 4)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14535949,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "530f6c71e9624939cbb3e7c03bd180137733ac3a",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "1."
            },
            "slug": "Fast-Optimization-Algorithms-for-Solving-SVM-Pechyony",
            "title": {
                "fragments": [],
                "text": "Fast Optimization Algorithms for Solving SVM"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40584063"
                        ],
                        "name": "Binoy Pinto",
                        "slug": "Binoy-Pinto",
                        "structuredName": {
                            "firstName": "Binoy",
                            "lastName": "Pinto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Binoy Pinto"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 124673168,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbc4f344af0057dd35bf69c5924bc71d2751a69d",
            "isKey": false,
            "numCitedBy": 706,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Speeded-Up-Robust-Features-Pinto",
            "title": {
                "fragments": [],
                "text": "Speeded Up Robust Features"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3331580"
                        ],
                        "name": "O. Maron",
                        "slug": "O.-Maron",
                        "structuredName": {
                            "firstName": "Oded",
                            "lastName": "Maron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Maron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2578809"
                        ],
                        "name": "A. L. Ratan",
                        "slug": "A.-L.-Ratan",
                        "structuredName": {
                            "firstName": "Aparna",
                            "lastName": "Ratan",
                            "middleNames": [
                                "Lakshmi"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. L. Ratan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 126
                            }
                        ],
                        "text": "In multiple instance learning, training labels are given not for individual examples, but collectively for groups of examples [16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 39240439,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "6e0616e65727c7ab9c185c92e15ccd405cfc2a0b",
            "isKey": false,
            "numCitedBy": 654,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multiple-Instance-Learning-for-Natural-Scene-Maron-Ratan",
            "title": {
                "fragments": [],
                "text": "Multiple-Instance Learning for Natural Scene Classification"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145404204"
                        ],
                        "name": "Cees G. M. Snoek",
                        "slug": "Cees-G.-M.-Snoek",
                        "structuredName": {
                            "firstName": "Cees",
                            "lastName": "Snoek",
                            "middleNames": [
                                "G.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cees G. M. Snoek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717056"
                        ],
                        "name": "M. Worring",
                        "slug": "M.-Worring",
                        "structuredName": {
                            "firstName": "Marcel",
                            "lastName": "Worring",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Worring"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144638781"
                        ],
                        "name": "A. Smeulders",
                        "slug": "A.-Smeulders",
                        "structuredName": {
                            "firstName": "Arnold",
                            "lastName": "Smeulders",
                            "middleNames": [
                                "W.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Smeulders"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 177
                            }
                        ],
                        "text": "Methods suggested here range from stacking, where one simply concatenates the feature vectors of all data modalities, to complex adaptive methods for early or late data fusions [23], including multiple kernel learning [26] and LP-\u03b2 [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2912873,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54035c00a70ce812ee7d23aa84368be8dc51cbaa",
            "isKey": false,
            "numCitedBy": 795,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Semantic analysis of multimodal video aims to index segments of interest at a conceptual level. In reaching this goal, it requires an analysis of several information streams. At some point in the analysis these streams need to be fused. In this paper, we consider two classes of fusion schemes, namely early fusion and late fusion. The former fuses modalities in feature space, the latter fuses modalities in semantic space. We show by experiment on 184 hours of broadcast video data and for 20 semantic concepts, that late fusion tends to give slightly better performance for most concepts. However, for those concepts where early fusion performs better the difference is more significant."
            },
            "slug": "Early-versus-late-fusion-in-semantic-video-analysis-Snoek-Worring",
            "title": {
                "fragments": [],
                "text": "Early versus late fusion in semantic video analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown by experiment on 184 hours of broadcast video data and for 20 semantic concepts, that late fusion tends to give slightly better performance for most concepts, however, for those concepts where early fusion performs better the difference is more significant."
            },
            "venue": {
                "fragments": [],
                "text": "MULTIMEDIA '05"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 49
                            }
                        ],
                        "text": "In this manuscript we rely on the intuition that the priv-\nileged data helps us to distinguish between easy and hard examples in the training set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Pascal VOC challenge. IJCV"
            },
            "venue": {
                "fragments": [],
                "text": "The Pascal VOC challenge. IJCV"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ringneck snake 73.33 \u00b1 0"
            },
            "venue": {
                "fragments": [],
                "text": "Ringneck snake 73.33 \u00b1 0"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 279,
                                "start": 276
                            }
                        ],
                        "text": "The inverse situation also occurs: for example in the PASCAL object recognition challenge, it has become a standard technique to incorporate strong annotation in the form of bounding boxes or per-pixel segmentations, even when the goal is just per-image object categorization [8]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Pascal VOC challenge"
            },
            "venue": {
                "fragments": [],
                "text": "IJCV"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Speeded-up robust features (SURF). CVIU"
            },
            "venue": {
                "fragments": [],
                "text": "Speeded-up robust features (SURF). CVIU"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "2009 Special Issue"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 9,
            "methodology": 12,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 34,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Learning-to-Rank-Using-Privileged-Information-Sharmanska-Quadrianto/b1dd68d85acc5cde4a0dee33aa052f65388eda6b?sort=total-citations"
}