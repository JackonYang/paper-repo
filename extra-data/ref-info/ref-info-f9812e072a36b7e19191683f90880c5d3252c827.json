{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064955826"
                        ],
                        "name": "Gopal Datt Joshi",
                        "slug": "Gopal-Datt-Joshi",
                        "structuredName": {
                            "firstName": "Gopal",
                            "lastName": "Joshi",
                            "middleNames": [
                                "Datt"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gopal Datt Joshi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144901372"
                        ],
                        "name": "S. Garg",
                        "slug": "S.-Garg",
                        "structuredName": {
                            "firstName": "Saurabh",
                            "lastName": "Garg",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Garg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790095"
                        ],
                        "name": "J. Sivaswamy",
                        "slug": "J.-Sivaswamy",
                        "structuredName": {
                            "firstName": "Jayanthi",
                            "lastName": "Sivaswamy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sivaswamy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 79904,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e9f7f93b1411035d787f5edeb4664b162a369bc3",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic identification of a script in a given document image facilitates many important applications such as automatic archiving of multilingual documents, searching online archives of document images and for the selection of script specific OCR in a multilingual environment. In this paper, we present a scheme to identify different Indian scripts from a document image. This scheme employs hierarchical classification which uses features consistent with human perception. Such features are extracted from the responses of a multi-channel log-Gabor filter bank, designed at an optimal scale and multiple orientations. In the first stage, the classifier groups the scripts into five major classes using global features. At the next stage, a sub-classification is performed based on script-specific features. All features are extracted globally from a given text block which does not require any complex and reliable segmentation of the document image into lines and characters. Thus the proposed scheme is efficient and can be used for many practical applications which require processing large volumes of data. The scheme has been tested on 10 Indian scripts and found to be robust to skew generated in the process of scanning and relatively insensitive to change in font size. This proposed system achieves an overall classification accuracy of 97.11% on a large testing data set. These results serve to establish the utility of global approach to classification of scripts."
            },
            "slug": "Script-Identification-from-Indian-Documents-Joshi-Garg",
            "title": {
                "fragments": [],
                "text": "Script Identification from Indian Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper presents a scheme to identify different Indian scripts from a document image which employs hierarchical classification which uses features consistent with human perception and achieves an overall classification accuracy of 97.11% on a large testing data set."
            },
            "venue": {
                "fragments": [],
                "text": "Document Analysis Systems"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144556482"
                        ],
                        "name": "Saurabh Singh",
                        "slug": "Saurabh-Singh",
                        "structuredName": {
                            "firstName": "Saurabh",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Saurabh Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726095131"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Gupta",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 165
                            }
                        ],
                        "text": "patterns, termed as discriminative patterns (the image patches containing the representative strokes or components) from script images via discriminative clustering [45]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "in [45], which is a discriminative clustering method for discovering patches that are both representative and discriminative."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 117
                            }
                        ],
                        "text": "The visual differences between these scripts can be observed only via a few local regions, or discriminative patches [45], which may correspond to special characters or special character components."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 119
                            }
                        ],
                        "text": "For each class c, a set of local descriptors X c is extracted from the feature hierarchies, taken as the discovery set [45]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14970392,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce854ea2c797bd10cbdf4563a558cd8652c4946e",
            "isKey": true,
            "numCitedBy": 575,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of this paper is to discover a set of discriminative patches which can serve as a fully unsupervised mid-level visual representation. The desired patches need to satisfy two requirements: 1) to be representative, they need to occur frequently enough in the visual world; 2) to be discriminative, they need to be different enough from the rest of the visual world. The patches could correspond to parts, objects, \"visual phrases\", etc. but are not restricted to be any one of them. We pose this as an unsupervised discriminative clustering problem on a huge dataset of image patches. We use an iterative procedure which alternates between clustering and training discriminative classifiers, while applying careful cross-validation at each step to prevent overfitting. The paper experimentally demonstrates the effectiveness of discriminative patches as an unsupervised mid-level visual representation, suggesting that it could be used in place of visual words for many tasks. Furthermore, discriminative patches can also be used in a supervised regime, such as scene classification, where they demonstrate state-of-the-art performance on the MIT Indoor-67 dataset."
            },
            "slug": "Unsupervised-Discovery-of-Mid-Level-Discriminative-Singh-Gupta",
            "title": {
                "fragments": [],
                "text": "Unsupervised Discovery of Mid-Level Discriminative Patches"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "The paper experimentally demonstrates the effectiveness of discriminative patches as an unsupervised mid-level visual representation, suggesting that it could be used in place of visual words for many tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "Recently, CNN has achieved great success in image classification tasks [27], due to its strong capacity and invariance to"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 70
                            }
                        ],
                        "text": "Typical image classification algorithms, such as the conventional CNN [27] and the Single-Layer Networks (SLN) [10], usually describe images in a holistic style without explicit emphasis on discriminative"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 195908774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "isKey": false,
            "numCitedBy": 80938,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry."
            },
            "slug": "ImageNet-classification-with-deep-convolutional-Krizhevsky-Sutskever",
            "title": {
                "fragments": [],
                "text": "ImageNet classification with deep convolutional neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A large, deep convolutional neural network was trained to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes and employed a recently developed regularization method called \"dropout\" that proved to be very effective."
            },
            "venue": {
                "fragments": [],
                "text": "Commun. ACM"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784196"
                        ],
                        "name": "Datong Chen",
                        "slug": "Datong-Chen",
                        "structuredName": {
                            "firstName": "Datong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Datong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719610"
                        ],
                        "name": "J. Odobez",
                        "slug": "J.-Odobez",
                        "structuredName": {
                            "firstName": "Jean-Marc",
                            "lastName": "Odobez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Odobez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733733"
                        ],
                        "name": "H. Bourlard",
                        "slug": "H.-Bourlard",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Bourlard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bourlard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11796155,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "42a8ff86566538103c6116f9047a4c3128e1542c",
            "isKey": false,
            "numCitedBy": 303,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Text-detection,-recognition-in-images-and-video-Chen-Odobez",
            "title": {
                "fragments": [],
                "text": "Text detection, recognition in images and video frames"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276155"
                        ],
                        "name": "Baoguang Shi",
                        "slug": "Baoguang-Shi",
                        "structuredName": {
                            "firstName": "Baoguang",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Baoguang Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146721"
                        ],
                        "name": "C. Yao",
                        "slug": "C.-Yao",
                        "structuredName": {
                            "firstName": "Cong",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1979323"
                        ],
                        "name": "Chengquan Zhang",
                        "slug": "Chengquan-Zhang",
                        "structuredName": {
                            "firstName": "Chengquan",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chengquan Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49932595"
                        ],
                        "name": "Xiaowei Guo",
                        "slug": "Xiaowei-Guo",
                        "structuredName": {
                            "firstName": "Xiaowei",
                            "lastName": "Guo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaowei Guo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1835006"
                        ],
                        "name": "Feiyue Huang",
                        "slug": "Feiyue-Huang",
                        "structuredName": {
                            "firstName": "Feiyue",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Feiyue Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145905113"
                        ],
                        "name": "X. Bai",
                        "slug": "X.-Bai",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Bai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Bai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "We use the same architecture as used in [43] for comparisons."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 64
                            }
                        ],
                        "text": "This paper is a continuation and extension of our previous work [43]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 14
                            }
                        ],
                        "text": "Compared with [43], this paper describes text images via discriminative mid-level representation, instead of the global horizontal pooling on convolutional feature maps."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 82
                            }
                        ],
                        "text": "Table 4 compares the proposed DiscNet with the MSPN proposed in our previous work [43]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [43] we have proposed Multi-stage Spatially sensitive Pooling Network (MSPN) and a 10-classes dataset called SIW-10 for the in-the-wild script identification task."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "MSPN: Multi-Stage Pooling Network is proposed in our previous work [43]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14824675,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e8bb98c7c01f21eed8a760fc433a9f3f4b549cd1",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "With the rapid increase of transnational communication and cooperation, people frequently encounter multilingual scenarios in various situations. In this paper, we are concerned with a relatively new problem: script identification at word or line levels in natural scenes. A large-scale dataset with a great quantity of natural images and 10 types of widely-used languages is constructed and released. In allusion to the challenges in script identification in real-world scenarios, a deep learning based algorithm is proposed. The experiments on the proposed dataset demonstrate that our algorithm achieves superior performance, compared with conventional image classification or script identification methods, including as the original CNN architecture, LLC and GLCM."
            },
            "slug": "Automatic-script-identification-in-the-wild-Shi-Yao",
            "title": {
                "fragments": [],
                "text": "Automatic script identification in the wild"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A large-scale dataset with a great quantity of natural images and 10 types of widely-used languages is constructed and a deep learning based algorithm is proposed for script identification at word or line levels in natural scenes."
            },
            "venue": {
                "fragments": [],
                "text": "2015 13th International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110818914"
                        ],
                        "name": "Danni Zhao",
                        "slug": "Danni-Zhao",
                        "structuredName": {
                            "firstName": "Danni",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Danni Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744575"
                        ],
                        "name": "P. Shivakumara",
                        "slug": "P.-Shivakumara",
                        "structuredName": {
                            "firstName": "Palaiahnakote",
                            "lastName": "Shivakumara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Shivakumara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771189"
                        ],
                        "name": "Shijian Lu",
                        "slug": "Shijian-Lu",
                        "structuredName": {
                            "firstName": "Shijian",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shijian Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679749"
                        ],
                        "name": "C. Tan",
                        "slug": "C.-Tan",
                        "structuredName": {
                            "firstName": "Chew",
                            "lastName": "Tan",
                            "middleNames": [
                                "Lim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 99
                            }
                        ],
                        "text": "Previous works on script identification mainly focus on texts in documents [48,20,7,24] and videos [40,58]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 112
                            }
                        ],
                        "text": "Different from the previous approaches which have been mainly designed for document images [48,19,20] or videos [40,58], this work focuses on identifying the language/script types of texts in natural images (in the wild), at word or text line level."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 67
                            }
                        ],
                        "text": "In the area of script identification, there exist several datasets [20,40,58]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17349812,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4eba2be8294ed31ed09153ad5c30126bc08f245",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present new features based on Spatial-Gradient-Features (SGF) at block level for identifying six video scripts namely, Arabic, Chinese, English, Japanese, Korean and Tamil. This works helps in enhancing the capability of the current OCR on video text recognition by choosing an appropriate OCR engine when video contains multi-script frames. The input for script identification is the text blocks obtained by our text frame classification method. For each text block, we obtain horizontal and vertical gradient information to enhance the contrast of the text pixels. We divide the horizontal gradient block into two equal parts as upper and lower at the centroid in the horizontal direction. Histogram on the horizontal gradient values of the upper and the lower part is performed to select dominant text pixels. In the same way, the method selects dominant pixels from the right and the left parts obtained by dividing the vertical gradient block vertically. The method combines the horizontal and the vertical dominant pixels to obtain text components. Skeleton concept is used to reduce pixel width to a single pixel to extract spatial features. We extract four features based on proximity between end points, junction points, intersection points and pixels. The method is evaluated on 770 frames of six scripts in terms of classification rate and is compared with an existing method. We have achieved 82.1% average classification rate."
            },
            "slug": "New-Spatial-Gradient-Features-for-Video-Script-Zhao-Shivakumara",
            "title": {
                "fragments": [],
                "text": "New Spatial-Gradient-Features for Video Script Identification"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "New features based on Spatial-Gradient-Features (SGF) at block level for identifying six video scripts namely, Arabic, Chinese, English, Japanese, Korean and Tamil are presented, which helps in enhancing the capability of the current OCR on video text recognition by choosing an appropriate OCR engine when video contains multi-script frames."
            },
            "venue": {
                "fragments": [],
                "text": "2012 10th IAPR International Workshop on Document Analysis Systems"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39719398"
                        ],
                        "name": "Anand Mishra",
                        "slug": "Anand-Mishra",
                        "structuredName": {
                            "firstName": "Anand",
                            "lastName": "Mishra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anand Mishra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72492981"
                        ],
                        "name": "Alahari Karteek",
                        "slug": "Alahari-Karteek",
                        "structuredName": {
                            "firstName": "Alahari",
                            "lastName": "Karteek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alahari Karteek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694502"
                        ],
                        "name": "C. Jawahar",
                        "slug": "C.-Jawahar",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Jawahar",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jawahar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 128
                            }
                        ],
                        "text": "There exist several public datasets that consist of texts in the wild, for instance, ICDAR 2011 [42], SVT [53] and IIIT 5K-Word [35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9695967,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb5b2df137a4d54c3a9145fa363e66531b491580",
            "isKey": false,
            "numCitedBy": 550,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of recognizing text in images taken in the wild has gained significant attention from the computer vision community in recent years. Contrary to recognition of printed documents, recognizing scene text is a challenging problem. We focus on the problem of recognizing text extracted from natural scene images and the web. Significant attempts have been made to address this problem in the recent past. However, many of these works benefit from the availability of strong context, which naturally limits their applicability. In this work we present a framework that uses a higher order prior computed from an English dictionary to recognize a word, which may or may not be a part of the dictionary. We show experimental results on publicly available datasets. Furthermore, we introduce a large challenging word dataset with five thousand words to evaluate various steps of our method exhaustively. The main contributions of this work are: (1) We present a framework, which incorporates higher order statistical language models to recognize words in an unconstrained manner (i.e. we overcome the need for restricted word lists, and instead use an English dictionary to compute the priors). (2) We achieve significant improvement (more than 20%) in word recognition accuracies without using a restricted word list. (3) We introduce a large word recognition dataset (atleast 5 times larger than other public datasets) with character level annotation and benchmark it."
            },
            "slug": "Scene-Text-Recognition-using-Higher-Order-Language-Mishra-Karteek",
            "title": {
                "fragments": [],
                "text": "Scene Text Recognition using Higher Order Language Priors"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A framework is presented that uses a higher order prior computed from an English dictionary to recognize a word, which may or may not be a part of the dictionary, and achieves significant improvement in word recognition accuracies without using a restricted word list."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144638694"
                        ],
                        "name": "Adam Coates",
                        "slug": "Adam-Coates",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Coates",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Coates"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697141"
                        ],
                        "name": "Honglak Lee",
                        "slug": "Honglak-Lee",
                        "structuredName": {
                            "firstName": "Honglak",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Honglak Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 53
                            }
                        ],
                        "text": "This indicates that the deep-like feature learned by SLN is superior than the hand-crafted HOG."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "For SLN, CNN and DiscCNN, the tendencies of the accuracy decreases are similar."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 26
                            }
                        ],
                        "text": "On the Logograhic subset, SLN and Basic-CNN achieve accuracies comparable with DiscCNN."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "conventional CNN, the SLN [10] and the LBP."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 58
                            }
                        ],
                        "text": "Without explicitly utilizing the special characters, LBP, SLN and Basic-CNN cannot well distinguish scripts that share characters, e.g. English and Greek."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "Single-Layer Networks (SLN): In [10] Coates et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 106
                            }
                        ],
                        "text": "Typical image classification algorithms, such as the conventional CNN [27] and the Single-Layer Networks (SLN) [10], usually describe images in a holistic style without explicit emphasis on discriminative patches that play an important role in distinguishing some script categories (e.g. English and Greek)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 82
                            }
                        ],
                        "text": "In BoW, local descriptors such as SIFT [33], HOG [12] or simply raw pixel patches [10] are extracted from images, and encoded by some coding methods such as the locality-constrained linear coding (LLC [52]) or the triangle activation [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 112
                            }
                        ],
                        "text": "For comparison, we test three other methods, namely the Local Binary Patterns (LBP), the Single-Layer Networks (SLN) and the conventional CNN (BasicCNN)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 5
                            }
                        ],
                        "text": "Both SLN and Basic-CNN perform better than LBP."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 23
                            }
                        ],
                        "text": "Single-Layer Networks (SLN): In [10] Coates et al. propose SLN, showing that, with simple unsupervised feature learning via KMeans clustering, one can achieve state-of-the-art performances at that time on image classification tasks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 287,
                                "start": 284
                            }
                        ],
                        "text": "In this section, we evaluate the performance of the proposed DiscCNN on three tasks, namely script identification in the wild, in videos and in documents, and compare it with other widely used image classification or script identification methods, including the conventional CNN, the SLN [10] and the LBP."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 39
                            }
                        ],
                        "text": "Script Alphabetic Logographic Full\nLBP SLN CNN Ours LBP SLN CNN Ours LBP SLN CNN Ours\nAra 0.80 0.91 0.94 0.96 \u2013 \u2013 \u2013 \u2013 0.64 0.87 0.90 0.94 Cam \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 0.46 0.76 0.83 0.88 Chi \u2013 \u2013 \u2013 \u2013 0.82 0.90 0.86 0.91 0.66 0.87 0.85 0.88 Eng 0.63 0.77 0.72 0.83 \u2013 \u2013 \u2013 \u2013 0.31 0.64 0.58 0.71 Gre 0.70 0.79 0.74 0.86 \u2013 \u2013 \u2013 \u2013 0.57 0.75 0.70 0.81 Heb \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 0.61 0.91 0.89 0.91 Jap \u2013 \u2013 \u2013 \u2013 0.85 0.93 0.88 0.93 0.58 0.88 0.75 0.90 Kan \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 0.56 0.88 0.82 0.91 Kor \u2013 \u2013 \u2013 \u2013 0.87 0.94 0.93 0.96 0.69 0.93 0.90 0.95 Mon 0.88 0.97 0.94 0.98 \u2013 \u2013 \u2013 \u2013 0.77 0.95 0.96 0.96 Rus 0.62 0.78 0.71 0.82 \u2013 \u2013 \u2013 \u2013 0.44 0.70 0.66 0.79 Tha \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 0.61 0.91 0.79 0.94 Tib \u2013 \u2013 \u2013 \u2013 0.96 0.97 0.99 0.98 0.88 0.97 0.97 0.97 Avg."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 114
                            }
                        ],
                        "text": "We have also tested the widely adopted method, Locality-constrained Linear Coding [52] (LLC), which is similar to SLN except that it uses hand-crafted HOG features."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 111
                            }
                        ],
                        "text": "Typical image classification algorithms, such as the conventional CNN [27] and the Single-Layer Networks (SLN) [10], usually describe images in a holistic style without explicit emphasis on discriminative"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 96
                            }
                        ],
                        "text": "On the three subsets, LLC achieves average accuracies 0.83, 0.91 and 0.85, generally lower than SLN."
                    },
                    "intents": []
                }
            ],
            "corpusId": 308212,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "be9a17321537d9289875fe475b71f4821457b435",
            "isKey": true,
            "numCitedBy": 2598,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "A great deal of research has focused on algorithms for learning features from unlabeled data. Indeed, much progress has been made on benchmark datasets like NORB and CIFAR by employing increasingly complex unsupervised learning algorithms and deep models. In this paper, however, we show that several simple factors, such as the number of hidden nodes in the model, may be more important to achieving high performance than the learning algorithm or the depth of the model. Specifically, we will apply several othe-shelf feature learning algorithms (sparse auto-encoders, sparse RBMs, K-means clustering, and Gaussian mixtures) to CIFAR, NORB, and STL datasets using only singlelayer networks. We then present a detailed analysis of the eect of changes in the model setup: the receptive field size, number of hidden nodes (features), the step-size (\u201cstride\u201d) between extracted features, and the eect of whitening. Our results show that large numbers of hidden nodes and dense feature extraction are critical to achieving high performance\u2014so critical, in fact, that when these parameters are pushed to their limits, we achieve state-of-the-art performance on both CIFAR-10 and NORB using only a single layer of features. More surprisingly, our best performance is based on K-means clustering, which is extremely fast, has no hyperparameters to tune beyond the model structure itself, and is very easy to implement. Despite the simplicity of our system, we achieve accuracy beyond all previously published results on the CIFAR-10 and NORB datasets (79.6% and 97.2% respectively)."
            },
            "slug": "An-Analysis-of-Single-Layer-Networks-in-Feature-Coates-Ng",
            "title": {
                "fragments": [],
                "text": "An Analysis of Single-Layer Networks in Unsupervised Feature Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The results show that large numbers of hidden nodes and dense feature extraction are critical to achieving high performance\u2014so critical, in fact, that when these parameters are pushed to their limits, they achieve state-of-the-art performance on both CIFAR-10 and NORB using only a single layer of features."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771551"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 49
                            }
                        ],
                        "text": "strategy inspired by the Spatial Pyramid Pooling [18,28], called horizontal pooling, is adopted in the mid-level representation process."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 138
                            }
                        ],
                        "text": "To describe the whole image from the encoding results, we adopt a horizontal pooling scheme, inspired by the spatial pyramid pooling (SPP [18])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 35
                            }
                        ],
                        "text": "A pooling strategy inspired by the Spatial Pyramid Pooling [18,28], called horizontal pooling, is adopted in the mid-level representation process."
                    },
                    "intents": []
                }
            ],
            "corpusId": 436933,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cbb19236820a96038d000dc629225d36e0b6294a",
            "isKey": false,
            "numCitedBy": 4774,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224<inline-formula><tex-math>$\\times$ </tex-math><alternatives><inline-graphic xlink:type=\"simple\" xlink:href=\"he-ieq1-2389824.gif\"/></alternatives></inline-formula>224) input image. This requirement is \u201cartificial\u201d and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, \u201cspatial pyramid pooling\u201d, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image representation and no fine-tuning. The power of SPP-net is also significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is 24-102 <inline-formula><tex-math>$\\times$</tex-math><alternatives><inline-graphic xlink:type=\"simple\" xlink:href=\"he-ieq2-2389824.gif\"/> </alternatives></inline-formula> faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007. In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank #2 in object detection and #3 in image classification among all 38 teams. This manuscript also introduces the improvement made for this competition."
            },
            "slug": "Spatial-Pyramid-Pooling-in-Deep-Convolutional-for-He-Zhang",
            "title": {
                "fragments": [],
                "text": "Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This work equips the networks with another pooling strategy, \u201cspatial pyramid pooling\u201d, to eliminate the above requirement, and develops a new network structure, called SPP-net, which can generate a fixed-length representation regardless of image size/scale."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146721"
                        ],
                        "name": "C. Yao",
                        "slug": "C.-Yao",
                        "structuredName": {
                            "firstName": "Cong",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145905113"
                        ],
                        "name": "X. Bai",
                        "slug": "X.-Bai",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Bai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Bai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743698"
                        ],
                        "name": "Wenyu Liu",
                        "slug": "Wenyu-Liu",
                        "structuredName": {
                            "firstName": "Wenyu",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenyu Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16258738,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1a853d5e85b152bd4edfa78b4fc65164ef5b6cba",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "High level semantics embodied in scene texts are both rich and clear and thus can serve as important cues for a wide range of vision applications, for instance, image understanding, image indexing, video search, geolocation and automatic navigation. In this paper, we present a unified framework for text detection and recognition in natural images. The contributions of this work are three-fold: (1) Text detection and recognition are accomplished concurrently using exactly the same features and classification scheme. (2) In contrast to methods in the literature, which mainly focus on horizontal or near-horizontal texts, the proposed system is capable of localizing and reading texts of varying orientations. (3) A new dictionary search method is proposed, to correct the recognition errors usually caused by confusions among similar yet different characters. As an additional contribution, a novel image database with texts of different scales, colors, fonts and orientations in diverse realworld scenarios, is generated and released. Extensive experiments on standard benchmarks as well as the proposed database demonstrate that the proposed system achieves highly competitive performance, especially on multi-oriented texts."
            },
            "slug": "A-Unified-Framework-for-Multi-Oriented-Text-and-Yao-Bai",
            "title": {
                "fragments": [],
                "text": "A Unified Framework for Multi-Oriented Text Detection and Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A unified framework for text detection and recognition in natural images using exactly the same features and classification scheme is presented and a novel image database with texts of different scales, colors, fonts and orientations in diverse realworld scenarios is generated and released."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146721"
                        ],
                        "name": "C. Yao",
                        "slug": "C.-Yao",
                        "structuredName": {
                            "firstName": "Cong",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145905113"
                        ],
                        "name": "X. Bai",
                        "slug": "X.-Bai",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Bai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Bai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743698"
                        ],
                        "name": "Wenyu Liu",
                        "slug": "Wenyu-Liu",
                        "structuredName": {
                            "firstName": "Wenyu",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenyu Liu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14457729,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2950be66e7b4c94dbb16e3319d8bece5da4e799f",
            "isKey": false,
            "numCitedBy": 220,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "High level semantics embodied in scene texts are both rich and clear and thus can serve as important cues for a wide range of vision applications, for instance, image understanding, image indexing, video search, geolocation, and automatic navigation. In this paper, we present a unified framework for text detection and recognition in natural images. The contributions of this paper are threefold: 1) text detection and recognition are accomplished concurrently using exactly the same features and classification scheme; 2) in contrast to methods in the literature, which mainly focus on horizontal or near-horizontal texts, the proposed system is capable of localizing and reading texts of varying orientations; and 3) a new dictionary search method is proposed, to correct the recognition errors usually caused by confusions among similar yet different characters. As an additional contribution, a novel image database with texts of different scales, colors, fonts, and orientations in diverse real-world scenarios, is generated and released. Extensive experiments on standard benchmarks as well as the proposed database demonstrate that the proposed system achieves highly competitive performance, especially on multioriented texts."
            },
            "slug": "A-Unified-Framework-for-Multioriented-Text-and-Yao-Bai",
            "title": {
                "fragments": [],
                "text": "A Unified Framework for Multioriented Text Detection and Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A unified framework for text detection and recognition in natural images using exactly the same features and classification scheme and a new dictionary search method is proposed, to correct the recognition errors usually caused by confusions among similar yet different characters."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064955826"
                        ],
                        "name": "Gopal Datt Joshi",
                        "slug": "Gopal-Datt-Joshi",
                        "structuredName": {
                            "firstName": "Gopal",
                            "lastName": "Joshi",
                            "middleNames": [
                                "Datt"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gopal Datt Joshi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144901372"
                        ],
                        "name": "S. Garg",
                        "slug": "S.-Garg",
                        "structuredName": {
                            "firstName": "Saurabh",
                            "lastName": "Garg",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Garg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790095"
                        ],
                        "name": "J. Sivaswamy",
                        "slug": "J.-Sivaswamy",
                        "structuredName": {
                            "firstName": "Jayanthi",
                            "lastName": "Sivaswamy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sivaswamy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[24] present a generalized framework to identify scripts at paragraph and text-block levels."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 75
                            }
                        ],
                        "text": "Previous works on script identification mainly focus on texts in documents [48,20,7,24] and videos [40,58]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15599217,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "caf92b93613d871ba9bf53fbbee3f65e77fb9cb4",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic identification of a script in a given document image facilitates many important applications such as automatic archiving of multilingual documents, searching online archives of document images and for the selection of script-specific OCR in a multi-lingual environment. In this paper, we model script identification as a texture classification problem and examine a global approach inspired by human visual perception. A generalised, hierarchical framework is proposed for script identification. A set of energy and intensity space features for this task is also presented. The framework serves to establish the utility of a global approach to the classification of scripts. The framework has been tested on two datasets: 10 Indian and 13 world scripts. The obtained accuracy of identification across the two datasets is above 94%. The results demonstrate that the framework can be used to develop solutions for script identification from document images across a large set of script classes."
            },
            "slug": "A-generalised-framework-for-script-identification-Joshi-Garg",
            "title": {
                "fragments": [],
                "text": "A generalised framework for script identification"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A generalised, hierarchical framework for script identification is proposed and a set of energy and intensity space features for this task are presented to establish the utility of a global approach to the classification of scripts."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Document Analysis and Recognition (IJDAR)"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1979323"
                        ],
                        "name": "Chengquan Zhang",
                        "slug": "Chengquan-Zhang",
                        "structuredName": {
                            "firstName": "Chengquan",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chengquan Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146721"
                        ],
                        "name": "C. Yao",
                        "slug": "C.-Yao",
                        "structuredName": {
                            "firstName": "Cong",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276155"
                        ],
                        "name": "Baoguang Shi",
                        "slug": "Baoguang-Shi",
                        "structuredName": {
                            "firstName": "Baoguang",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Baoguang Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145905113"
                        ],
                        "name": "X. Bai",
                        "slug": "X.-Bai",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Bai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Bai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 308,
                                "start": 272
                            }
                        ],
                        "text": "Due to the rapidly increasing amount of multimedia data, especially those captured and stored by mobile terminals, how to recognize text content in natural scenes has become an active and important task in the fields of pattern recognition, computer vision and multimedia [15,16,25,26,47,49,59,23,57,38,8,41]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16970739,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f5fc1638c4fab41be17b91042ddf58907293caec",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "With the rapid growth of image and video data, there comes an interesting yet challenging problem: How to organize and utilize such large volume of data? Textual content in images and videos is an important source of information, which can be of great usefulness and assistance. Therefore, we investigate in this paper the problem of text image discrimination, which aims at distinguishing natural images with text from those without text. To tackle this problem, we propose a method that combines three mature techniques in this area, namely: MSER, CNN and BoW. To better evaluate the proposed algorithm, we also construct a large benchmark for text image discrimination, which includes natural images in a variety of scenarios. This algorithm has proven to be both effective and efficient, thus it can serve as a tool for mining valuable textual information from huge amount of image and video data."
            },
            "slug": "Automatic-discrimination-of-text-and-non-text-Zhang-Yao",
            "title": {
                "fragments": [],
                "text": "Automatic discrimination of text and non-text natural images"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper investigates the problem of text image discrimination, which aims at distinguishing natural images with text from those without text, and proposes a method that combines three mature techniques in this area, namely: MSER, CNN and BoW."
            },
            "venue": {
                "fragments": [],
                "text": "2015 13th International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71563118"
                        ],
                        "name": "Jinjun Wang",
                        "slug": "Jinjun-Wang",
                        "structuredName": {
                            "firstName": "Jinjun",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jinjun Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706007"
                        ],
                        "name": "Jianchao Yang",
                        "slug": "Jianchao-Yang",
                        "structuredName": {
                            "firstName": "Jianchao",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianchao Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144782042"
                        ],
                        "name": "Kai Yu",
                        "slug": "Kai-Yu",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39157653"
                        ],
                        "name": "Fengjun Lv",
                        "slug": "Fengjun-Lv",
                        "structuredName": {
                            "firstName": "Fengjun",
                            "lastName": "Lv",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fengjun Lv"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652752"
                        ],
                        "name": "Thomas S. Huang",
                        "slug": "Thomas-S.-Huang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Huang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas S. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144768792"
                        ],
                        "name": "Yihong Gong",
                        "slug": "Yihong-Gong",
                        "structuredName": {
                            "firstName": "Yihong",
                            "lastName": "Gong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yihong Gong"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 82
                            }
                        ],
                        "text": "We have also tested the widely adopted method, Locality-constrained Linear Coding [52] (LLC), which is similar to SLN except that it uses hand-crafted HOG features."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 201
                            }
                        ],
                        "text": "In BoW, local descriptors such as SIFT [33], HOG [12] or simply raw pixel patches [10] are extracted from images, and encoded by some coding methods such as the locality-constrained linear coding (LLC [52]) or the triangle activation [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6718692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f7713dcc35e7c05becf3be5522f36c9546b0364",
            "isKey": false,
            "numCitedBy": 3240,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "The traditional SPM approach based on bag-of-features (BoF) requires nonlinear classifiers to achieve good image classification performance. This paper presents a simple but effective coding scheme called Locality-constrained Linear Coding (LLC) in place of the VQ coding in traditional SPM. LLC utilizes the locality constraints to project each descriptor into its local-coordinate system, and the projected coordinates are integrated by max pooling to generate the final representation. With linear classifier, the proposed approach performs remarkably better than the traditional nonlinear SPM, achieving state-of-the-art performance on several benchmarks. Compared with the sparse coding strategy [22], the objective function used by LLC has an analytical solution. In addition, the paper proposes a fast approximated LLC method by first performing a K-nearest-neighbor search and then solving a constrained least square fitting problem, bearing computational complexity of O(M + K2). Hence even with very large codebooks, our system can still process multiple frames per second. This efficiency significantly adds to the practical values of LLC for real applications."
            },
            "slug": "Locality-constrained-Linear-Coding-for-image-Wang-Yang",
            "title": {
                "fragments": [],
                "text": "Locality-constrained Linear Coding for image classification"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "This paper presents a simple but effective coding scheme called Locality-constrained Linear Coding (LLC) in place of the VQ coding in traditional SPM, using the locality constraints to project each descriptor into its local-coordinate system, and the projected coordinates are integrated by max pooling to generate the final representation."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2730090"
                        ],
                        "name": "L. G. I. Bigorda",
                        "slug": "L.-G.-I.-Bigorda",
                        "structuredName": {
                            "firstName": "Llu\u00eds",
                            "lastName": "Bigorda",
                            "middleNames": [
                                "G\u00f3mez",
                                "i"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. G. I. Bigorda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694974"
                        ],
                        "name": "Dimosthenis Karatzas",
                        "slug": "Dimosthenis-Karatzas",
                        "structuredName": {
                            "firstName": "Dimosthenis",
                            "lastName": "Karatzas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dimosthenis Karatzas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 57
                            }
                        ],
                        "text": "Bai), understanding system under multi-lingual scenarios [5,6,22], potentially useful in many applications such as scene understanding [32], product image search [17], mobile phone navigation, film caption recognition [11], and machine translation [4,50]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18726109,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "49eb053d79a823aea3994b329670f08d838a338c",
            "isKey": false,
            "numCitedBy": 99,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Scene text extraction methodologies are usually based in classification of individual regions or patches, using a priori knowledge for a given script or language. Human perception of text, on the other hand, is based on perceptual organisation through which text emerges as a perceptually significant group of atomic objects. Therefore humans are able to detect text even in languages and scripts never seen before. In this paper, we argue that the text extraction problem could be posed as the detection of meaningful groups of regions. We present a method built around a perceptual organisation framework that exploits collaboration of proximity and similarity laws to create text-group hypotheses. Experiments demonstrate that our algorithm is competitive with state of the art approaches on a standard dataset covering text in variable orientations and two languages."
            },
            "slug": "Multi-script-Text-Extraction-from-Natural-Scenes-Bigorda-Karatzas",
            "title": {
                "fragments": [],
                "text": "Multi-script Text Extraction from Natural Scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper presents a method built around a perceptual organisation framework that exploits collaboration of proximity and similarity laws to create text-group hypotheses and demonstrates that the algorithm is competitive with state of the art approaches on a standard dataset covering text in variable orientations and two languages."
            },
            "venue": {
                "fragments": [],
                "text": "2013 12th International Conference on Document Analysis and Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146721"
                        ],
                        "name": "C. Yao",
                        "slug": "C.-Yao",
                        "structuredName": {
                            "firstName": "Cong",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145905113"
                        ],
                        "name": "X. Bai",
                        "slug": "X.-Bai",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Bai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Bai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276155"
                        ],
                        "name": "Baoguang Shi",
                        "slug": "Baoguang-Shi",
                        "structuredName": {
                            "firstName": "Baoguang",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Baoguang Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743698"
                        ],
                        "name": "Wenyu Liu",
                        "slug": "Wenyu-Liu",
                        "structuredName": {
                            "firstName": "Wenyu",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenyu Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11341313,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ca0eb5d81484f62af7b10f18aa4ed65d7856c106",
            "isKey": false,
            "numCitedBy": 243,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Driven by the wide range of applications, scene text detection and recognition have become active research topics in computer vision. Though extensively studied, localizing and reading text in uncontrolled environments remain extremely challenging, due to various interference factors. In this paper, we propose a novel multi-scale representation for scene text recognition. This representation consists of a set of detectable primitives, termed as strokelets, which capture the essential substructures of characters at different granularities. Strokelets possess four distinctive advantages: (1) Usability: automatically learned from bounding box labels, (2) Robustness: insensitive to interference factors, (3) Generality: applicable to variant languages, and (4) Expressivity: effective at describing characters. Extensive experiments on standard benchmarks verify the advantages of strokelets and demonstrate the effectiveness of the proposed algorithm for text recognition."
            },
            "slug": "Strokelets:-A-Learned-Multi-scale-Representation-Yao-Bai",
            "title": {
                "fragments": [],
                "text": "Strokelets: A Learned Multi-scale Representation for Scene Text Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper proposes a novel multi-scale representation for scene text recognition that consists of a set of detectable primitives, termed as strokelets, which capture the essential substructures of characters at different granularities."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145532509"
                        ],
                        "name": "Luk\u00e1s Neumann",
                        "slug": "Luk\u00e1s-Neumann",
                        "structuredName": {
                            "firstName": "Luk\u00e1s",
                            "lastName": "Neumann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luk\u00e1s Neumann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145564537"
                        ],
                        "name": "Jiri Matas",
                        "slug": "Jiri-Matas",
                        "structuredName": {
                            "firstName": "Jiri",
                            "lastName": "Matas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiri Matas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 450338,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "da8154af82fd62944399fc7fad65e44d82ee9ee2",
            "isKey": false,
            "numCitedBy": 511,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "A general method for text localization and recognition in real-world images is presented. The proposed method is novel, as it (i) departs from a strict feed-forward pipeline and replaces it by a hypothesesverification framework simultaneously processing multiple text line hypotheses, (ii) uses synthetic fonts to train the algorithm eliminating the need for time-consuming acquisition and labeling of real-world training data and (iii) exploits Maximally Stable Extremal Regions (MSERs) which provides robustness to geometric and illumination conditions. \n \nThe performance of the method is evaluated on two standard datasets. On the Char74k dataset, a recognition rate of 72% is achieved, 18% higher than the state-of-the-art. The paper is first to report both text detection and recognition results on the standard and rather challenging ICDAR 2003 dataset. The text localization works for number of alphabets and the method is easily adapted to recognition of other scripts, e.g. cyrillics."
            },
            "slug": "A-Method-for-Text-Localization-and-Recognition-in-Neumann-Matas",
            "title": {
                "fragments": [],
                "text": "A Method for Text Localization and Recognition in Real-World Images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The paper is first to report both text detection and recognition results on the standard and rather challenging ICDAR 2003 dataset, and the text localization works for number of alphabets and the method is easily adapted to recognition of other scripts, e.g. cyrillics."
            },
            "venue": {
                "fragments": [],
                "text": "ACCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143874948"
                        ],
                        "name": "T. Tan",
                        "slug": "T.-Tan",
                        "structuredName": {
                            "firstName": "Tieniu",
                            "lastName": "Tan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Tan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 4
                            }
                        ],
                        "text": "Tan [48] proposes to extract rotation invariant texture features for identifying document scripts."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 75
                            }
                        ],
                        "text": "Previous works on script identification mainly focus on texts in documents [48,20,7,24] and videos [40,58]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 91
                            }
                        ],
                        "text": "Different from the previous approaches which have been mainly designed for document images [48,19,20] or videos [40,58], this work focuses on identifying the language/script types of texts in natural images (in the wild), at word or text line level."
                    },
                    "intents": []
                }
            ],
            "corpusId": 23824412,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7b1e339ecda8704ce28315706ace62fe39d66f29",
            "isKey": false,
            "numCitedBy": 299,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Concerns the extraction of rotation invariant texture features and the use of such features in script identification from document images. Rotation invariant texture features are computed based on an extension of the popular multi-channel Gabor filtering technique, and their effectiveness is tested with 300 randomly rotated samples of 15 Brodatz textures. These features are then used in an attempt to solve a practical but hitherto mostly overlooked problem in document image processing-the identification of the script of a machine printed document. Automatic script and language recognition is an essential front-end process for the efficient and correct use of OCR and language translation products in a multilingual environment. Six languages (Chinese, English, Greek, Russian, Persian, and Malayalam) are chosen to demonstrate the potential of such a texture-based approach in script identification."
            },
            "slug": "Rotation-Invariant-Texture-Features-and-Their-Use-Tan",
            "title": {
                "fragments": [],
                "text": "Rotation Invariant Texture Features and Their Use in Automatic Script Identification"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "Rotation invariant texture features are computed based on an extension of the popular multi-channel Gabor filtering technique, and their effectiveness is tested with 300 randomly rotated samples of 15 Brodatz textures to solve a practical but hitherto mostly overlooked problem in document image processing."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145165052"
                        ],
                        "name": "D. Ghosh",
                        "slug": "D.-Ghosh",
                        "structuredName": {
                            "firstName": "Debashis",
                            "lastName": "Ghosh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ghosh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059143961"
                        ],
                        "name": "T. Dube",
                        "slug": "T.-Dube",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Dube",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Dube"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144536030"
                        ],
                        "name": "A. Shivaprasad",
                        "slug": "A.-Shivaprasad",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Shivaprasad",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Shivaprasad"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6611803,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9793a0231441be1d5b02154090e83e06f050d0cf",
            "isKey": false,
            "numCitedBy": 233,
            "numCiting": 99,
            "paperAbstract": {
                "fragments": [],
                "text": "A variety of different scripts are used in writing languages throughout the world. In a multiscript, multilingual environment, it is essential to know the script used in writing a document before an appropriate character recognition and document analysis algorithm can be chosen. In view of this, several methods for automatic script identification have been developed so far. They mainly belong to two broad categories-structure-based and visual-appearance-based techniques. This survey report gives an overview of the different script identification methodologies under each of these categories. Methods for script identification in online data and video-texts are also presented. It is noted that the research in this field is relatively thin and still more research is to be done, particularly in the case of handwritten documents."
            },
            "slug": "Script-Recognition\u2014A-Review-Ghosh-Dube",
            "title": {
                "fragments": [],
                "text": "Script Recognition\u2014A Review"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "An overview of the different script identification methodologies under each of the two broad categories-structure-based and visual-appearance-based techniques is given."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40581114"
                        ],
                        "name": "P. Hiremath",
                        "slug": "P.-Hiremath",
                        "structuredName": {
                            "firstName": "Prakash",
                            "lastName": "Hiremath",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hiremath"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696936"
                        ],
                        "name": "S. Shivashankar",
                        "slug": "S.-Shivashankar",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Shivashankar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Shivashankar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 91
                            }
                        ],
                        "text": "Different from the previous approaches which have been mainly designed for document images [48,19,20] or videos [40,58], this work focuses on identifying the language/script types of texts in natural images (in the wild), at word or text line level."
                    },
                    "intents": []
                }
            ],
            "corpusId": 12723163,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14e2def6e98c78d0e3fece11e7066b6f59aa12ca",
            "isKey": false,
            "numCitedBy": 92,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Wavelet-based-co-occurrence-histogram-features-for-Hiremath-Shivashankar",
            "title": {
                "fragments": [],
                "text": "Wavelet based co-occurrence histogram features for texture classification with an application to script identification in a document image"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1959339"
                        ],
                        "name": "Cunzhao Shi",
                        "slug": "Cunzhao-Shi",
                        "structuredName": {
                            "firstName": "Cunzhao",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cunzhao Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683416"
                        ],
                        "name": "Chunheng Wang",
                        "slug": "Chunheng-Wang",
                        "structuredName": {
                            "firstName": "Chunheng",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunheng Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2658590"
                        ],
                        "name": "Baihua Xiao",
                        "slug": "Baihua-Xiao",
                        "structuredName": {
                            "firstName": "Baihua",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Baihua Xiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145955846"
                        ],
                        "name": "Yang Zhang",
                        "slug": "Yang-Zhang",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yang Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145284815"
                        ],
                        "name": "Song Gao",
                        "slug": "Song-Gao",
                        "structuredName": {
                            "firstName": "Song",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song Gao"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6801531,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "22064d411a128de1a91ad87f86055e254c9c5321",
            "isKey": false,
            "numCitedBy": 191,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Scene-text-detection-using-graph-model-built-upon-Shi-Wang",
            "title": {
                "fragments": [],
                "text": "Scene text detection using graph model built upon maximally stable extremal regions"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715066"
                        ],
                        "name": "T. Phan",
                        "slug": "T.-Phan",
                        "structuredName": {
                            "firstName": "Trung",
                            "lastName": "Phan",
                            "middleNames": [
                                "Quy"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Phan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744575"
                        ],
                        "name": "P. Shivakumara",
                        "slug": "P.-Shivakumara",
                        "structuredName": {
                            "firstName": "Palaiahnakote",
                            "lastName": "Shivakumara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Shivakumara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112355335"
                        ],
                        "name": "Zhang Ding",
                        "slug": "Zhang-Ding",
                        "structuredName": {
                            "firstName": "Zhang",
                            "lastName": "Ding",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhang Ding"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771189"
                        ],
                        "name": "Shijian Lu",
                        "slug": "Shijian-Lu",
                        "structuredName": {
                            "firstName": "Shijian",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shijian Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679749"
                        ],
                        "name": "C. Tan",
                        "slug": "C.-Tan",
                        "structuredName": {
                            "firstName": "Chew",
                            "lastName": "Tan",
                            "middleNames": [
                                "Lim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[40] propose to identify text-line level video scripts using edge-based features."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 99
                            }
                        ],
                        "text": "Previous works on script identification mainly focus on texts in documents [48,20,7,24] and videos [40,58]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 112
                            }
                        ],
                        "text": "Different from the previous approaches which have been mainly designed for document images [48,19,20] or videos [40,58], this work focuses on identifying the language/script types of texts in natural images (in the wild), at word or text line level."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 67
                            }
                        ],
                        "text": "In the area of script identification, there exist several datasets [20,40,58]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1808863,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a863421400c0ee810f53dbec1e9b2f688fa07533",
            "isKey": true,
            "numCitedBy": 38,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a new method for video script identification which is essential before choosing an appropriate OCR engine for identifying text lines when a video frame contains more than one language. The input for script identification is the text lines obtained by our text detection method. We extract upper and lower extreme points for each connected component of Canny edges of text lines. The extracted points are connected to study the behavior of upper and lower lines. The direction of each 10-pixel segment of the lines is determined using PCA. The average angle of the segments of the upper and lower lines is computed to study the smoothness and cursiveness of the lines. In addition, to discriminate the scripts accurately, the method divides a text line into five equal zones horizontally to study the smoothness and cursiveness of the upper and lower lines of each zone. We evaluate the method by conducting experiments on different combinations of languages such as English and Chinese, English and Tamil, Chinese and Tamil, and English, Chinese and Tamil."
            },
            "slug": "Video-Script-Identification-Based-on-Text-Lines-Phan-Shivakumara",
            "title": {
                "fragments": [],
                "text": "Video Script Identification Based on Text Lines"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A new method for video script identification which is essential before choosing an appropriate OCR engine for identifying text lines when a video frame contains more than one language is presented."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Document Analysis and Recognition"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721248"
                        ],
                        "name": "P. Haffner",
                        "slug": "P.-Haffner",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Haffner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Haffner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 163
                            }
                        ],
                        "text": "Recent research on image classification and other visual tasks has seen a leap forward, thanks to the wide application of deep convolutional neural networks (CNNs [29])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 126
                            }
                        ],
                        "text": "method, a deep feature hierarchy, which is a set of feature maps, is extracted from the input images through a pretrained CNN [29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14542261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "162d958ff885f1462aeda91cd72582323fd6a1f4",
            "isKey": false,
            "numCitedBy": 35254,
            "numCiting": 248,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day."
            },
            "slug": "Gradient-based-learning-applied-to-document-LeCun-Bottou",
            "title": {
                "fragments": [],
                "text": "Gradient-based learning applied to document recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task, and Convolutional neural networks are shown to outperform all other techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34077469"
                        ],
                        "name": "P. Pati",
                        "slug": "P.-Pati",
                        "structuredName": {
                            "firstName": "Peeta",
                            "lastName": "Pati",
                            "middleNames": [
                                "Basa"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pati"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145677714"
                        ],
                        "name": "A. Ramakrishnan",
                        "slug": "A.-Ramakrishnan",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ramakrishnan",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ramakrishnan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 49
                            }
                        ],
                        "text": "Compared to the Gabor or DCT features adopted in [39], our representation captures information of special characters/components of scripts."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "Our approach significantly outperforms [39] on all scripts, reaching saturated accuracies on some scripts."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [39], Pati and Ramakrishnan have proposed a large-scale word images dataset for the script identification task."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 98
                            }
                        ],
                        "text": "Table 6 Recognition accuracies on all 11 scripts included in the Multi-Script dataset proposed in [39]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29800201,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "05be1e4d5a63c827e7f49fba5bb480675f77af05",
            "isKey": true,
            "numCitedBy": 119,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Word-level-multi-script-identification-Pati-Ramakrishnan",
            "title": {
                "fragments": [],
                "text": "Word level multi-script identification"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060306474"
                        ],
                        "name": "J. Schenk",
                        "slug": "J.-Schenk",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Schenk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schenk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34239688"
                        ],
                        "name": "J. Lenz",
                        "slug": "J.-Lenz",
                        "structuredName": {
                            "firstName": "Johannes",
                            "lastName": "Lenz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lenz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145512909"
                        ],
                        "name": "G. Rigoll",
                        "slug": "G.-Rigoll",
                        "structuredName": {
                            "firstName": "Gerhard",
                            "lastName": "Rigoll",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Rigoll"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 308,
                                "start": 272
                            }
                        ],
                        "text": "Due to the rapidly increasing amount of multimedia data, especially those captured and stored by mobile terminals, how to recognize text content in natural scenes has become an active and important task in the fields of pattern recognition, computer vision and multimedia [15,16,25,26,47,49,59,23,57,38,8,41]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17765223,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a5d7fbfe2136b11e07d491cce6a01c8fecdd680",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Novel-script-line-identification-method-for-script-Schenk-Lenz",
            "title": {
                "fragments": [],
                "text": "Novel script line identification method for script normalization and feature extraction in on-line handwritten whiteboard note recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2807482"
                        ],
                        "name": "Timoth\u00e9e Cour",
                        "slug": "Timoth\u00e9e-Cour",
                        "structuredName": {
                            "firstName": "Timoth\u00e9e",
                            "lastName": "Cour",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timoth\u00e9e Cour"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053283925"
                        ],
                        "name": "Christopher T. Jordan",
                        "slug": "Christopher-T.-Jordan",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Jordan",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher T. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759302"
                        ],
                        "name": "Eleni Miltsakaki",
                        "slug": "Eleni-Miltsakaki",
                        "structuredName": {
                            "firstName": "Eleni",
                            "lastName": "Miltsakaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eleni Miltsakaki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685978"
                        ],
                        "name": "B. Taskar",
                        "slug": "B.-Taskar",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Taskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Taskar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 218
                            }
                        ],
                        "text": "Bai), understanding system under multi-lingual scenarios [5,6,22], potentially useful in many applications such as scene understanding [32], product image search [17], mobile phone navigation, film caption recognition [11], and machine translation [4,50]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14415655,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c980b058f98dc1904ad328c2341a47c31479d076",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Movies and TV are a rich source of diverse and complex video of people, objects, actions and locales \"in the wild\". Harvesting automatically labeled sequences of actions from video would enable creation of large-scale and highly-varied datasets. To enable such collection, we focus on the task of recovering scene structure in movies and TV series for object tracking and action retrieval. We present a weakly supervised algorithm that uses the screenplay and closed captions to parse a movie into a hierarchy of shots and scenes. Scene boundaries in the movie are aligned with screenplay scene labels and shots are reordered into a sequence of long continuous tracks or threads which allow for more accurate tracking of people, actions and objects. Scene segmentation, alignment, and shot threading are formulated as inference in a unified generative model and a novel hierarchical dynamic programming algorithm that can handle alignment and jump-limited reorderings in linear time is presented. We present quantitative and qualitative results on movie alignment and parsing, and use the recovered structure to improve character naming and retrieval of common actions in several episodes of popular TV series."
            },
            "slug": "Movie/Script:-Alignment-and-Parsing-of-Video-and-Cour-Jordan",
            "title": {
                "fragments": [],
                "text": "Movie/Script: Alignment and Parsing of Video and Text Transcription"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A weakly supervised algorithm is presented that uses the screenplay and closed captions to parse a movie into a hierarchy of shots and scenes, and the recovered structure is used to improve character naming and retrieval of common actions in several episodes of popular TV series."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7516411"
                        ],
                        "name": "Andrew Busch",
                        "slug": "Andrew-Busch",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Busch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Busch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2170172"
                        ],
                        "name": "W. Boles",
                        "slug": "W.-Boles",
                        "structuredName": {
                            "firstName": "Wageeh",
                            "lastName": "Boles",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Boles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729760"
                        ],
                        "name": "S. Sridharan",
                        "slug": "S.-Sridharan",
                        "structuredName": {
                            "firstName": "Sridha",
                            "lastName": "Sridharan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sridharan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "In [7], several texture features, including gray-level co-occurrence matrix features, Gabor energy features, and wavelet energy features, are tested."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 75
                            }
                        ],
                        "text": "Previous works on script identification mainly focus on texts in documents [48,20,7,24] and videos [40,58]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9448504,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ccdf47e3498b1eac815b50abc9336b608a66e4e",
            "isKey": false,
            "numCitedBy": 191,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of determining the script and language of a document image has a number of important applications in the field of document analysis, such as indexing and sorting of large collections of such images, or as a precursor to optical character recognition (OCR). In this paper, we investigate the use of texture as a tool for determining the script of a document image, based on the observation that text has a distinct visual texture. An experimental evaluation of a number of commonly used texture features is conducted on a newly created script database, providing a qualitative measure of which features are most appropriate for this task. Strategies for improving classification results in situations with limited training data and multiple font types are also proposed."
            },
            "slug": "Texture-for-script-identification-Busch-Boles",
            "title": {
                "fragments": [],
                "text": "Texture for script identification"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An experimental evaluation of a number of commonly used texture features is conducted on a newly created script database, providing a qualitative measure of which features are most appropriate for this task."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 33
                            }
                        ],
                        "text": "The Bag-of-Words (BoW) framework [31] is a technique that is widely adopted in image classification problems."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6387937,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a2252ccce2b65abc3759149b5c06587cc318e2f",
            "isKey": false,
            "numCitedBy": 3886,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel approach to learn and recognize natural scene categories. Unlike previous work, it does not require experts to annotate the training set. We represent the image of a scene by a collection of local regions, denoted as codewords obtained by unsupervised learning. Each region is represented as part of a \"theme\". In previous work, such themes were learnt from hand-annotations of experts, while our method learns the theme distributions as well as the codewords distribution over the themes without supervision. We report satisfactory categorization performances on a large set of 13 categories of complex scenes."
            },
            "slug": "A-Bayesian-hierarchical-model-for-learning-natural-Fei-Fei-Perona",
            "title": {
                "fragments": [],
                "text": "A Bayesian hierarchical model for learning natural scene categories"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "This work proposes a novel approach to learn and recognize natural scene categories by representing the image of a scene by a collection of local regions, denoted as codewords obtained by unsupervised learning."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2202522"
                        ],
                        "name": "S. Sinha",
                        "slug": "S.-Sinha",
                        "structuredName": {
                            "firstName": "Suranjit",
                            "lastName": "Sinha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sinha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144167309"
                        ],
                        "name": "U. Pal",
                        "slug": "U.-Pal",
                        "structuredName": {
                            "firstName": "Umapada",
                            "lastName": "Pal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Pal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759420"
                        ],
                        "name": "B. Chaudhuri",
                        "slug": "B.-Chaudhuri",
                        "structuredName": {
                            "firstName": "Bidyut.",
                            "lastName": "Chaudhuri",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Chaudhuri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [46], a method based on structure analysis is introduced."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 33208778,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f27bb09ce0e62e59a466dca092c64d399bec3142",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "In a country like India, a single text line of most of the official documents contains two different script words. Under two-language formula, the Indian documents are written in English and the state official language. For Optical Character Recognition (OCR) of such a document page, it is necessary to separate different script words before feeding them to the OCRs of individual scripts. In this paper a robust technique is proposed to extract word-wise script identification from Indian doublet form documents. Here, at first, the document is segmented into lines and then the lines are segmented into words. Using different topological and structural features (like number of loops, headline feature, water reservoir concept based features, profile features, etc.) individual script words are identified from the documents. The proposed scheme is tested on 24210 words of different doublets and we received more than 97% accuracy, on average."
            },
            "slug": "Word-Wise-Script-Identification-from-Indian-Sinha-Pal",
            "title": {
                "fragments": [],
                "text": "Word-Wise Script Identification from Indian Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A robust technique is proposed to extract word-wise script identification from Indian doublet form documents using different topological and structural features to separate different script words from such documents."
            },
            "venue": {
                "fragments": [],
                "text": "Document Analysis Systems"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2530942"
                        ],
                        "name": "H. Hase",
                        "slug": "H.-Hase",
                        "structuredName": {
                            "firstName": "Hiroyuki",
                            "lastName": "Hase",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hase"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3188516"
                        ],
                        "name": "T. Shinokawa",
                        "slug": "T.-Shinokawa",
                        "structuredName": {
                            "firstName": "Toshiyuki",
                            "lastName": "Shinokawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Shinokawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71114145"
                        ],
                        "name": "M. Yoneda",
                        "slug": "M.-Yoneda",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Yoneda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Yoneda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713795"
                        ],
                        "name": "C. Suen",
                        "slug": "C.-Suen",
                        "structuredName": {
                            "firstName": "Ching",
                            "lastName": "Suen",
                            "middleNames": [
                                "Yee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Suen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 308,
                                "start": 272
                            }
                        ],
                        "text": "Due to the rapidly increasing amount of multimedia data, especially those captured and stored by mobile terminals, how to recognize text content in natural scenes has become an active and important task in the fields of pattern recognition, computer vision and multimedia [15,16,25,26,47,49,59,23,57,38,8,41]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 40333068,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2b24ea59f58374750894ad050dbafe88c81ed54f",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Character-string-extraction-from-color-documents-Hase-Shinokawa",
            "title": {
                "fragments": [],
                "text": "Character string extraction from color documents"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121267347"
                        ],
                        "name": "K. Jung",
                        "slug": "K.-Jung",
                        "structuredName": {
                            "firstName": "Keechul",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144602022"
                        ],
                        "name": "K. Kim",
                        "slug": "K.-Kim",
                        "structuredName": {
                            "firstName": "Kwang",
                            "lastName": "Kim",
                            "middleNames": [
                                "In"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 308,
                                "start": 272
                            }
                        ],
                        "text": "Due to the rapidly increasing amount of multimedia data, especially those captured and stored by mobile terminals, how to recognize text content in natural scenes has become an active and important task in the fields of pattern recognition, computer vision and multimedia [15,16,25,26,47,49,59,23,57,38,8,41]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5999466,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cedf72be1fe814ef2ee9d65633dc3226f80f0785",
            "isKey": false,
            "numCitedBy": 936,
            "numCiting": 100,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Text-information-extraction-in-images-and-video:-a-Jung-Kim",
            "title": {
                "fragments": [],
                "text": "Text information extraction in images and video: a survey"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749609"
                        ],
                        "name": "S. Lazebnik",
                        "slug": "S.-Lazebnik",
                        "structuredName": {
                            "firstName": "Svetlana",
                            "lastName": "Lazebnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lazebnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189388"
                        ],
                        "name": "J. Ponce",
                        "slug": "J.-Ponce",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Ponce",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ponce"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 49
                            }
                        ],
                        "text": "strategy inspired by the Spatial Pyramid Pooling [18,28], called horizontal pooling, is adopted in the mid-level representation process."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 35
                            }
                        ],
                        "text": "A pooling strategy inspired by the Spatial Pyramid Pooling [18,28], called horizontal pooling, is adopted in the mid-level representation process."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2421251,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6dbaff29d3898cf60f63f5a34cb9610ebb75220c",
            "isKey": false,
            "numCitedBy": 8328,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a method for recognizing scene categories based on approximate global geometric correspondence. This technique works by partitioning the image into increasingly fine sub-regions and computing histograms of local features found inside each sub-region. The resulting \"spatial pyramid\" is a simple and computationally efficient extension of an orderless bag-of-features image representation, and it shows significantly improved performance on challenging scene categorization tasks. Specifically, our proposed method exceeds the state of the art on the Caltech-101 database and achieves high accuracy on a large database of fifteen natural scene categories. The spatial pyramid framework also offers insights into the success of several recently proposed image descriptions, including Torralba\u2019s \"gist\" and Lowe\u2019s SIFT descriptors."
            },
            "slug": "Beyond-Bags-of-Features:-Spatial-Pyramid-Matching-Lazebnik-Schmid",
            "title": {
                "fragments": [],
                "text": "Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This paper presents a method for recognizing scene categories based on approximate global geometric correspondence that exceeds the state of the art on the Caltech-101 database and achieves high accuracy on a large database of fifteen natural scene categories."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2662709"
                        ],
                        "name": "Xiaoxiao Niu",
                        "slug": "Xiaoxiao-Niu",
                        "structuredName": {
                            "firstName": "Xiaoxiao",
                            "lastName": "Niu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoxiao Niu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713795"
                        ],
                        "name": "C. Suen",
                        "slug": "C.-Suen",
                        "structuredName": {
                            "firstName": "Ching",
                            "lastName": "Suen",
                            "middleNames": [
                                "Yee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Suen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 308,
                                "start": 272
                            }
                        ],
                        "text": "Due to the rapidly increasing amount of multimedia data, especially those captured and stored by mobile terminals, how to recognize text content in natural scenes has become an active and important task in the fields of pattern recognition, computer vision and multimedia [15,16,25,26,47,49,59,23,57,38,8,41]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9329224,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c2b2ffb47c7bfaeb04e387a67ba2e3e4a1dc545d",
            "isKey": false,
            "numCitedBy": 508,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-novel-hybrid-CNN-SVM-classifier-for-recognizing-Niu-Suen",
            "title": {
                "fragments": [],
                "text": "A novel hybrid CNN-SVM classifier for recognizing handwritten digits"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148896777"
                        ],
                        "name": "Kai Wang",
                        "slug": "Kai-Wang",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 106
                            }
                        ],
                        "text": "There exist several public datasets that consist of texts in the wild, for instance, ICDAR 2011 [42], SVT [53] and IIIT 5K-Word [35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14911813,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d307221fa52e3939d46180cb5921ebbd92c8adb",
            "isKey": false,
            "numCitedBy": 425,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for spotting words in the wild, i.e., in real images taken in unconstrained environments. Text found in the wild has a surprising range of difficulty. At one end of the spectrum, Optical Character Recognition (OCR) applied to scanned pages of well formatted printed text is one of the most successful applications of computer vision to date. At the other extreme lie visual CAPTCHAs - text that is constructed explicitly to fool computer vision algorithms. Both tasks involve recognizing text, yet one is nearly solved while the other remains extremely challenging. In this work, we argue that the appearance of words in the wild spans this range of difficulties and propose a new word recognition approach based on state-of-the-art methods from generic object recognition, in which we consider object categories to be the words themselves. We compare performance of leading OCR engines - one open source and one proprietary - with our new approach on the ICDAR Robust Reading data set and a new word spotting data set we introduce in this paper: the Street View Text data set. We show improvements of up to 16% on the data sets, demonstrating the feasibility of a new approach to a seemingly old problem."
            },
            "slug": "Word-Spotting-in-the-Wild-Wang-Belongie",
            "title": {
                "fragments": [],
                "text": "Word Spotting in the Wild"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is argued that the appearance of words in the wild spans this range of difficulties and a new word recognition approach based on state-of-the-art methods from generic object recognition is proposed, in which object categories are considered to be the words themselves."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145096292"
                        ],
                        "name": "Subhadip Basu",
                        "slug": "Subhadip-Basu",
                        "structuredName": {
                            "firstName": "Subhadip",
                            "lastName": "Basu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Subhadip Basu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723341"
                        ],
                        "name": "N. Das",
                        "slug": "N.-Das",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Das",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Das"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35846271"
                        ],
                        "name": "R. Sarkar",
                        "slug": "R.-Sarkar",
                        "structuredName": {
                            "firstName": "Ram",
                            "lastName": "Sarkar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sarkar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143995288"
                        ],
                        "name": "M. Kundu",
                        "slug": "M.-Kundu",
                        "structuredName": {
                            "firstName": "Mahantapas",
                            "lastName": "Kundu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kundu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729425"
                        ],
                        "name": "M. Nasipuri",
                        "slug": "M.-Nasipuri",
                        "structuredName": {
                            "firstName": "Mita",
                            "lastName": "Nasipuri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Nasipuri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143701816"
                        ],
                        "name": "D. K. Basu",
                        "slug": "D.-K.-Basu",
                        "structuredName": {
                            "firstName": "Dipak",
                            "lastName": "Basu",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. K. Basu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 57
                            }
                        ],
                        "text": "Bai), understanding system under multi-lingual scenarios [5,6,22], potentially useful in many applications such as scene understanding [32], product image search [17], mobile phone navigation, film caption recognition [11], and machine translation [4,50]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11158012,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6731085509cebee9be14a42c91844860486facae",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-novel-framework-for-automatic-sorting-of-postal-Basu-Das",
            "title": {
                "fragments": [],
                "text": "A novel framework for automatic sorting of postal documents with multi-script address blocks"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3200914"
                        ],
                        "name": "Asif Shahab",
                        "slug": "Asif-Shahab",
                        "structuredName": {
                            "firstName": "Asif",
                            "lastName": "Shahab",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Asif Shahab"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688013"
                        ],
                        "name": "F. Shafait",
                        "slug": "F.-Shafait",
                        "structuredName": {
                            "firstName": "Faisal",
                            "lastName": "Shafait",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Shafait"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145279674"
                        ],
                        "name": "A. Dengel",
                        "slug": "A.-Dengel",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Dengel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dengel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 96
                            }
                        ],
                        "text": "There exist several public datasets that consist of texts in the wild, for instance, ICDAR 2011 [42], SVT [53] and IIIT 5K-Word [35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1468345,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f4edbb12d346e873ca1faeff959aa7d4809495f",
            "isKey": false,
            "numCitedBy": 431,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Recognition of text in natural scene images is becoming a prominent research area due to the widespread availablity of imaging devices in low-cost consumer products like mobile phones. To evaluate the performance of recent algorithms in detecting and recognizing text from complex images, the ICDAR 2011 Robust Reading Competition was organized. Challenge 2 of the competition dealt specifically with detecting/recognizing text in natural scene images. This paper presents an overview of the approaches that the participants used, the evaluation measure, and the dataset used in the Challenge 2 of the contest. We also report the performance of all participating methods for text localization and word recognition tasks and compare their results using standard methods of area precision/recall and edit distance."
            },
            "slug": "ICDAR-2011-Robust-Reading-Competition-Challenge-2:-Shahab-Shafait",
            "title": {
                "fragments": [],
                "text": "ICDAR 2011 Robust Reading Competition Challenge 2: Reading Text in Scene Images"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An overview of the approaches that the participants used, the evaluation measure, and the dataset used in the ICDAR 2011 Robust Reading Competition for detecting/recognizing text in natural scene images is presented."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Document Analysis and Recognition"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2565323"
                        ],
                        "name": "C. Strouthopoulos",
                        "slug": "C.-Strouthopoulos",
                        "structuredName": {
                            "firstName": "Charalambos",
                            "lastName": "Strouthopoulos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Strouthopoulos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144368634"
                        ],
                        "name": "N. Papamarkos",
                        "slug": "N.-Papamarkos",
                        "structuredName": {
                            "firstName": "Nikos",
                            "lastName": "Papamarkos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Papamarkos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2938539"
                        ],
                        "name": "A. Atsalakis",
                        "slug": "A.-Atsalakis",
                        "structuredName": {
                            "firstName": "Antonios",
                            "lastName": "Atsalakis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Atsalakis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 308,
                                "start": 272
                            }
                        ],
                        "text": "Due to the rapidly increasing amount of multimedia data, especially those captured and stored by mobile terminals, how to recognize text content in natural scenes has become an active and important task in the fields of pattern recognition, computer vision and multimedia [15,16,25,26,47,49,59,23,57,38,8,41]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15419767,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "89140621fa5771868e5a3e170769eb6089214aa4",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Text-extraction-in-complex-color-documents-Strouthopoulos-Papamarkos",
            "title": {
                "fragments": [],
                "text": "Text extraction in complex color documents"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48950628"
                        ],
                        "name": "N. Dalal",
                        "slug": "N.-Dalal",
                        "structuredName": {
                            "firstName": "Navneet",
                            "lastName": "Dalal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dalal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 91
                            }
                        ],
                        "text": "This indicates that the deep-like feature learned by SLN is superior than the hand-crafted HOG."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 151
                            }
                        ],
                        "text": "We have also tested the widely adopted method, Locality-constrained Linear Coding [52] (LLC), which is similar to SLN except that it uses hand-crafted HOG features."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 49
                            }
                        ],
                        "text": "In BoW, local descriptors such as SIFT [33], HOG [12] or simply raw pixel patches [10] are extracted from images, and encoded by some coding methods such as the locality-constrained linear coding (LLC [52]) or the triangle activation [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 166
                            }
                        ],
                        "text": "In addition, convolutional features are learned from data, thus domain-specific and potentially stronger than general hand-crafted features such as SIFT [33] and HOG [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206590483,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cec734d7097ab6b1e60d95228ffd64248eb89d66",
            "isKey": true,
            "numCitedBy": 29262,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds."
            },
            "slug": "Histograms-of-oriented-gradients-for-human-Dalal-Triggs",
            "title": {
                "fragments": [],
                "text": "Histograms of oriented gradients for human detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection, and the influence of each stage of the computation on performance is studied."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116415943"
                        ],
                        "name": "B. Yu",
                        "slug": "B.-Yu",
                        "structuredName": {
                            "firstName": "Bin",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 308,
                                "start": 272
                            }
                        ],
                        "text": "Due to the rapidly increasing amount of multimedia data, especially those captured and stored by mobile terminals, how to recognize text content in natural scenes has become an active and important task in the fields of pattern recognition, computer vision and multimedia [15,16,25,26,47,49,59,23,57,38,8,41]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5196787,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f565f502ad1acb81c5659b051c04683a34ed138f",
            "isKey": false,
            "numCitedBy": 576,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic text location (without character recognition capabilities) deals with extracting image regions that contain text only. The images of these regions can then be fed to an optical character recognition module or highlighted for users. This is very useful in a number of applications such as database indexing and converting paper documents to their electronic versions. The performance of our automatic text location algorithm is shown in several applications. Compared with some traditional text location methods, our method has the following advantages: 1) low computational cost; 2) robust to font size; and 3) high accuracy."
            },
            "slug": "Automatic-text-location-in-images-and-video-frames-Jain-Yu",
            "title": {
                "fragments": [],
                "text": "Automatic text location in images and video frames"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Compared with some traditional text location methods, this method has the following advantages: 1) low computational cost; 2) robust to font size; and 3) high accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48345000"
                        ],
                        "name": "T. Ahonen",
                        "slug": "T.-Ahonen",
                        "structuredName": {
                            "firstName": "Timo",
                            "lastName": "Ahonen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ahonen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144979251"
                        ],
                        "name": "A. Hadid",
                        "slug": "A.-Hadid",
                        "structuredName": {
                            "firstName": "Abdenour",
                            "lastName": "Hadid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hadid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145962204"
                        ],
                        "name": "M. Pietik\u00e4inen",
                        "slug": "M.-Pietik\u00e4inen",
                        "structuredName": {
                            "firstName": "Matti",
                            "lastName": "Pietik\u00e4inen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pietik\u00e4inen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "Local Binary Patterns (LBP): LBP [3] is a widely adopted texture"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 23
                            }
                        ],
                        "text": "Local Binary Patterns (LBP): LBP [3] is a widely adopted texture analysis technique."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "LBP seems to be more robust to imperfect crop."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 53
                            }
                        ],
                        "text": "Without explicitly utilizing the special characters, LBP, SLN and Basic-CNN cannot well distinguish scripts that share characters, e.g. English and Greek."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 30
                            }
                        ],
                        "text": "The texture analysis approach LBP performs well on Logographic scripts, but significantly worse on alphabetic scripts."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 79
                            }
                        ],
                        "text": "For comparison, we test three other methods, namely the Local Binary Patterns (LBP), the Single-Layer Networks (SLN) and the conventional CNN (BasicCNN)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 43
                            }
                        ],
                        "text": "Both SLN and Basic-CNN perform better than LBP."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 27
                            }
                        ],
                        "text": "One of the reasons is that LBP describes the texture, which is often insensitive to cropping."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 304,
                                "start": 301
                            }
                        ],
                        "text": "In this section, we evaluate the performance of the proposed DiscCNN on three tasks, namely script identification in the wild, in videos and in documents, and compare it with other widely used image classification or script identification methods, including the conventional CNN, the SLN [10] and the LBP."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 35
                            }
                        ],
                        "text": "Script Alphabetic Logographic Full\nLBP SLN CNN Ours LBP SLN CNN Ours LBP SLN CNN Ours\nAra 0.80 0.91 0.94 0.96 \u2013 \u2013 \u2013 \u2013 0.64 0.87 0.90 0.94 Cam \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 0.46 0.76 0.83 0.88 Chi \u2013 \u2013 \u2013 \u2013 0.82 0.90 0.86 0.91 0.66 0.87 0.85 0.88 Eng 0.63 0.77 0.72 0.83 \u2013 \u2013 \u2013 \u2013 0.31 0.64 0.58 0.71 Gre 0.70 0.79 0.74 0.86 \u2013 \u2013 \u2013 \u2013 0.57 0.75 0.70 0.81 Heb \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 0.61 0.91 0.89 0.91 Jap \u2013 \u2013 \u2013 \u2013 0.85 0.93 0.88 0.93 0.58 0.88 0.75 0.90 Kan \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 0.56 0.88 0.82 0.91 Kor \u2013 \u2013 \u2013 \u2013 0.87 0.94 0.93 0.96 0.69 0.93 0.90 0.95 Mon 0.88 0.97 0.94 0.98 \u2013 \u2013 \u2013 \u2013 0.77 0.95 0.96 0.96 Rus 0.62 0.78 0.71 0.82 \u2013 \u2013 \u2013 \u2013 0.44 0.70 0.66 0.79 Tha \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 0.61 0.91 0.79 0.94 Tib \u2013 \u2013 \u2013 \u2013 0.96 0.97 0.99 0.98 0.88 0.97 0.97 0.97 Avg."
                    },
                    "intents": []
                }
            ],
            "corpusId": 369876,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e7c4665ce36a53484f8a7b7dfa821a9f6273eab4",
            "isKey": true,
            "numCitedBy": 5461,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a novel and efficient facial image representation based on local binary pattern (LBP) texture features. The face image is divided into several regions from which the LBP feature distributions are extracted and concatenated into an enhanced feature vector to be used as a face descriptor. The performance of the proposed method is assessed in the face recognition problem under different challenges. Other applications and several extensions are also discussed"
            },
            "slug": "Face-Description-with-Local-Binary-Patterns:-to-Ahonen-Hadid",
            "title": {
                "fragments": [],
                "text": "Face Description with Local Binary Patterns: Application to Face Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "This paper presents a novel and efficient facial image representation based on local binary pattern (LBP) texture features that is assessed in the face recognition problem under different challenges."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38807427"
                        ],
                        "name": "Junfeng He",
                        "slug": "Junfeng-He",
                        "structuredName": {
                            "firstName": "Junfeng",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junfeng He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153285206"
                        ],
                        "name": "Jinyuan Feng",
                        "slug": "Jinyuan-Feng",
                        "structuredName": {
                            "firstName": "Jinyuan",
                            "lastName": "Feng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jinyuan Feng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6820648"
                        ],
                        "name": "Xianglong Liu",
                        "slug": "Xianglong-Liu",
                        "structuredName": {
                            "firstName": "Xianglong",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xianglong Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113571289"
                        ],
                        "name": "Tao Cheng",
                        "slug": "Tao-Cheng",
                        "structuredName": {
                            "firstName": "Tao",
                            "lastName": "Cheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tao Cheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109641232"
                        ],
                        "name": "Tai-Hsu Lin",
                        "slug": "Tai-Hsu-Lin",
                        "structuredName": {
                            "firstName": "Tai-Hsu",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tai-Hsu Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2520026"
                        ],
                        "name": "Hyunjin Chung",
                        "slug": "Hyunjin-Chung",
                        "structuredName": {
                            "firstName": "Hyunjin",
                            "lastName": "Chung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hyunjin Chung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9546964"
                        ],
                        "name": "Shih-Fu Chang",
                        "slug": "Shih-Fu-Chang",
                        "structuredName": {
                            "firstName": "Shih-Fu",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shih-Fu Chang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 162
                            }
                        ],
                        "text": "Bai), understanding system under multi-lingual scenarios [5,6,22], potentially useful in many applications such as scene understanding [32], product image search [17], mobile phone navigation, film caption recognition [11], and machine translation [4,50]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10276897,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a9c657fb60d826a28376674451b8ae0c14560fd5",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Rapidly growing applications on smartphones have provided an excellent platform for mobile visual search. Most of previous visual search systems adopt the framework of \u201dBag of Words\u201d, in which words indicate quantized codes of visual features. In this work, we propose a novel visual search system based on \u201dBag of Hash Bits\u201d (BoHB), in which each local feature is encoded to a very small number of hash bits, instead of quantized to visual words, and the whole image is represented as bag of hash bits. The proposed BoHB method offers unique benefits in solving the challenges associated with mobile visual search, e.g., low transmission cost, cheap memory and computation on the mobile side, etc. Moreover, our BoHB method leverages the distinct properties of hashing bits such as multi-table indexing, multiple bucket probing, bit reuse, and hamming distance based ranking to achieve efficient search over gigantic visual databases. The proposed method significantly outperforms state-of-the-art mobile visual search methods like CHoG, and other (conventional desktop) visual search approaches like bag of words via vocabulary tree, or product quantization. The proposed BoHB approach is easy to implement on mobile devices, and general in the sense that it can be applied to different types of local features, hashing algorithms and image databases. We also incorporate a boundary feature in the reranking step to describe the object shapes, complementing the local features that are usually used to characterize the local details. The boundary feature can further filter out noisy results and improve the search performance, especially at the coarse category level. Extensive experiments over large-scale data sets up to 400k product images demonstrate the effectiveness of our approach."
            },
            "slug": "Mobile-product-search-with-Bag-of-Hash-Bits-and-He-Feng",
            "title": {
                "fragments": [],
                "text": "Mobile product search with Bag of Hash Bits and boundary reranking"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work proposes a novel visual search system based on \u201dBag of Hash Bits\u201d (BoHB), in which each local feature is encoded to a very small number of hash bits, instead of quantized to visual words, and the whole image is represented as bag of hashbits."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056091"
                        ],
                        "name": "M. Everingham",
                        "slug": "M.-Everingham",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Everingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Everingham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "in PASCAL VOC 2007 [13], the recognition accuracies are, respec-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4246903,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82635fb63640ae95f90ee9bdc07832eb461ca881",
            "isKey": false,
            "numCitedBy": 11688,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension."
            },
            "slug": "The-Pascal-Visual-Object-Classes-(VOC)-Challenge-Everingham-Gool",
            "title": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes (VOC) Challenge"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The state-of-the-art in evaluated methods for both classification and detection are reviewed, whether the methods are statistically different, what they are learning from the images, and what the methods find easy or confuse."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145554412"
                        ],
                        "name": "M. Haji",
                        "slug": "M.-Haji",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Haji",
                            "middleNames": [
                                "Mehdi"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Haji"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144957197"
                        ],
                        "name": "T. Bui",
                        "slug": "T.-Bui",
                        "structuredName": {
                            "firstName": "Tien",
                            "lastName": "Bui",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Bui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713795"
                        ],
                        "name": "C. Suen",
                        "slug": "C.-Suen",
                        "structuredName": {
                            "firstName": "Ching",
                            "lastName": "Suen",
                            "middleNames": [
                                "Yee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Suen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 308,
                                "start": 272
                            }
                        ],
                        "text": "Due to the rapidly increasing amount of multimedia data, especially those captured and stored by mobile terminals, how to recognize text content in natural scenes has become an active and important task in the fields of pattern recognition, computer vision and multimedia [15,16,25,26,47,49,59,23,57,38,8,41]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14669938,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1145353ba209926c44cd416d7eb83927e29ff065",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Removal-of-noise-patterns-in-handwritten-images-and-Haji-Bui",
            "title": {
                "fragments": [],
                "text": "Removal of noise patterns in handwritten images using expectation maximization and fuzzy inference systems"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755187"
                        ],
                        "name": "P. Cavalin",
                        "slug": "P.-Cavalin",
                        "structuredName": {
                            "firstName": "Paulo",
                            "lastName": "Cavalin",
                            "middleNames": [
                                "Rodrigo"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Cavalin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744351"
                        ],
                        "name": "R. Sabourin",
                        "slug": "R.-Sabourin",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Sabourin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sabourin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713795"
                        ],
                        "name": "C. Suen",
                        "slug": "C.-Suen",
                        "structuredName": {
                            "firstName": "Ching",
                            "lastName": "Suen",
                            "middleNames": [
                                "Yee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Suen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143772141"
                        ],
                        "name": "A. Britto",
                        "slug": "A.-Britto",
                        "structuredName": {
                            "firstName": "Alceu",
                            "lastName": "Britto",
                            "middleNames": [
                                "de",
                                "Souza"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Britto"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15396952,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f8393414b13a80911b8a7434e44ca5e8a97ad5a",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Evaluation-of-incremental-learning-algorithms-for-Cavalin-Sabourin",
            "title": {
                "fragments": [],
                "text": "Evaluation of incremental learning algorithms for HMM in the recognition of alphanumeric characters"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2554435"
                        ],
                        "name": "Issam Bazzi",
                        "slug": "Issam-Bazzi",
                        "structuredName": {
                            "firstName": "Issam",
                            "lastName": "Bazzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Issam Bazzi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35442155"
                        ],
                        "name": "R. Schwartz",
                        "slug": "R.-Schwartz",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10080270"
                        ],
                        "name": "J. Makhoul",
                        "slug": "J.-Makhoul",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Makhoul",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Makhoul"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 57
                            }
                        ],
                        "text": "Bai), understanding system under multi-lingual scenarios [5,6,22], potentially useful in many applications such as scene understanding [32], product image search [17], mobile phone navigation, film caption recognition [11], and machine translation [4,50]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11815403,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c6272317b62292deb6f342f499f36e6ecf7b9c1b",
            "isKey": false,
            "numCitedBy": 240,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an omnifont, unlimited-vocabulary OCR system for English and Arabic. The system is based on hidden Markov models (HMM), an approach that has proven to be very successful in the area of automatic speech recognition. We focus on two aspects of the OCR system. First, we address the issue of how to perform OCR on omnifont and multi-style data, such as plain and italic, without the need to have a separate model for each style. The amount of training data from each style, which is used to train a single model, becomes an important issue in the face of the conditional independence assumption inherent in the use of HMMs. We demonstrate mathematically and empirically how to allocate training data among the different styles to alleviate this problem. Second, we show how to use a word-based HMM system to perform character recognition with unlimited vocabulary. The method includes the use of a trigram language model on character sequences. Using all these techniques, we have achieved character error rates of 1.1 percent on data from the University of Washington English Document Image Database and 3.3 percent on data from the DARPA Arabic OCR Corpus."
            },
            "slug": "An-Omnifont-Open-Vocabulary-OCR-System-for-English-Bazzi-Schwartz",
            "title": {
                "fragments": [],
                "text": "An Omnifont Open-Vocabulary OCR System for English and Arabic"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "An omnifont, unlimited-vocabulary OCR system for English and Arabic based on hidden Markov models (HMM), an approach that has proven to be very successful in the area of automatic speech recognition, is presented."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2776166"
                        ],
                        "name": "Muna Khayyat",
                        "slug": "Muna-Khayyat",
                        "structuredName": {
                            "firstName": "Muna",
                            "lastName": "Khayyat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Muna Khayyat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144905110"
                        ],
                        "name": "L. Lam",
                        "slug": "L.-Lam",
                        "structuredName": {
                            "firstName": "Louisa",
                            "lastName": "Lam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Lam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713795"
                        ],
                        "name": "C. Suen",
                        "slug": "C.-Suen",
                        "structuredName": {
                            "firstName": "Ching",
                            "lastName": "Suen",
                            "middleNames": [
                                "Yee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Suen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 308,
                                "start": 272
                            }
                        ],
                        "text": "Due to the rapidly increasing amount of multimedia data, especially those captured and stored by mobile terminals, how to recognize text content in natural scenes has become an active and important task in the fields of pattern recognition, computer vision and multimedia [15,16,25,26,47,49,59,23,57,38,8,41]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4814192,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2604629c54644bdfed9ba8f66022bdfb4510a932",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-based-word-spotting-system-for-Arabic-Khayyat-Lam",
            "title": {
                "fragments": [],
                "text": "Learning-based word spotting system for Arabic handwritten documents"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4132077"
                        ],
                        "name": "Vicente Alabau",
                        "slug": "Vicente-Alabau",
                        "structuredName": {
                            "firstName": "Vicente",
                            "lastName": "Alabau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vicente Alabau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794594"
                        ],
                        "name": "A. Sanch\u00eds",
                        "slug": "A.-Sanch\u00eds",
                        "structuredName": {
                            "firstName": "Alberto",
                            "lastName": "Sanch\u00eds",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sanch\u00eds"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696761"
                        ],
                        "name": "F. Casacuberta",
                        "slug": "F.-Casacuberta",
                        "structuredName": {
                            "firstName": "Francisco",
                            "lastName": "Casacuberta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Casacuberta"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 254,
                                "start": 248
                            }
                        ],
                        "text": "Bai), understanding system under multi-lingual scenarios [5,6,22], potentially useful in many applications such as scene understanding [32], product image search [17], mobile phone navigation, film caption recognition [11], and machine translation [4,50]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29819560,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14e10a11449e10faff24632a890a391447411873",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Improving-on-line-handwritten-recognition-in-Alabau-Sanch\u00eds",
            "title": {
                "fragments": [],
                "text": "Improving on-line handwritten recognition in interactive machine translation"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52151344"
                        ],
                        "name": "A. Toselli",
                        "slug": "A.-Toselli",
                        "structuredName": {
                            "firstName": "Alejandro",
                            "lastName": "Toselli",
                            "middleNames": [
                                "H\u00e9ctor"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Toselli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144340605"
                        ],
                        "name": "Ver\u00f3nica Romero",
                        "slug": "Ver\u00f3nica-Romero",
                        "structuredName": {
                            "firstName": "Ver\u00f3nica",
                            "lastName": "Romero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ver\u00f3nica Romero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143614719"
                        ],
                        "name": "Mois\u00e9s Pastor",
                        "slug": "Mois\u00e9s-Pastor",
                        "structuredName": {
                            "firstName": "Mois\u00e9s",
                            "lastName": "Pastor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mois\u00e9s Pastor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143783065"
                        ],
                        "name": "E. Vidal",
                        "slug": "E.-Vidal",
                        "structuredName": {
                            "firstName": "Enrique",
                            "lastName": "Vidal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Vidal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 847254,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb632cef60c32041c660673c0b89bd4e0858ff58",
            "isKey": false,
            "numCitedBy": 89,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multimodal-interactive-transcription-of-text-images-Toselli-Romero",
            "title": {
                "fragments": [],
                "text": "Multimodal interactive transcription of text images"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144530706"
                        ],
                        "name": "J. Hochberg",
                        "slug": "J.-Hochberg",
                        "structuredName": {
                            "firstName": "Judith",
                            "lastName": "Hochberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hochberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35206821"
                        ],
                        "name": "P. Kelly",
                        "slug": "P.-Kelly",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Kelly",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Kelly"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068996812"
                        ],
                        "name": "Timothy Thomas",
                        "slug": "Timothy-Thomas",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Thomas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timothy Thomas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145607454"
                        ],
                        "name": "L. Kerns",
                        "slug": "L.-Kerns",
                        "structuredName": {
                            "firstName": "Lila",
                            "lastName": "Kerns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Kerns"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 75
                            }
                        ],
                        "text": "Previous works on script identification mainly focus on texts in documents [48,20,7,24] and videos [40,58]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[20] discover a set of templates by clustering \u201ctextual symbols\u201d, which are connected components extracted from training scripts."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 91
                            }
                        ],
                        "text": "Different from the previous approaches which have been mainly designed for document images [48,19,20] or videos [40,58], this work focuses on identifying the language/script types of texts in natural images (in the wild), at word or text line level."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 67
                            }
                        ],
                        "text": "In the area of script identification, there exist several datasets [20,40,58]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 37476217,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ea8ef4bda4b7c9f659b7cb8ea378ed758015dcde",
            "isKey": true,
            "numCitedBy": 210,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an automated script identification system for typeset document images. Templates for each script are created by clustering textual symbols from a training set. Symbols from new images are compared to the templates to find the best script. Our current system processes thirteen scripts with minimal preprocessing and high accuracy."
            },
            "slug": "Automatic-Script-Identification-From-Document-Using-Hochberg-Kelly",
            "title": {
                "fragments": [],
                "text": "Automatic Script Identification From Document Images Using Cluster-Based Templates"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "An automated script identification system for typeset document images that processes thirteen scripts with minimal preprocessing and high accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39702442"
                        ],
                        "name": "Y. Tang",
                        "slug": "Y.-Tang",
                        "structuredName": {
                            "firstName": "Yuan",
                            "lastName": "Tang",
                            "middleNames": [
                                "Yan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50112753"
                        ],
                        "name": "Seong-Whan Lee",
                        "slug": "Seong-Whan-Lee",
                        "structuredName": {
                            "firstName": "Seong-Whan",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seong-Whan Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713795"
                        ],
                        "name": "C. Suen",
                        "slug": "C.-Suen",
                        "structuredName": {
                            "firstName": "Ching",
                            "lastName": "Suen",
                            "middleNames": [
                                "Yee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Suen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 308,
                                "start": 272
                            }
                        ],
                        "text": "Due to the rapidly increasing amount of multimedia data, especially those captured and stored by mobile terminals, how to recognize text content in natural scenes has become an active and important task in the fields of pattern recognition, computer vision and multimedia [15,16,25,26,47,49,59,23,57,38,8,41]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29737518,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "92df66c37211e55faafdf879f95185fa86f196a6",
            "isKey": false,
            "numCitedBy": 140,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automatic-document-processing:-A-survey-Tang-Lee",
            "title": {
                "fragments": [],
                "text": "Automatic document processing: A survey"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48764203"
                        ],
                        "name": "Victor Wu",
                        "slug": "Victor-Wu",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Victor Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758550"
                        ],
                        "name": "R. Manmatha",
                        "slug": "R.-Manmatha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Manmatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manmatha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31338632"
                        ],
                        "name": "E. Riseman",
                        "slug": "E.-Riseman",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Riseman",
                            "middleNames": [
                                "M."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Riseman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1830124,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e5c342ba0edbebadc7c95c7e59d1bef87d7e4add",
            "isKey": false,
            "numCitedBy": 451,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "A robust system is proposed to automatically detect and extract text in images from different sources, including video, newspapers, advertisements, stock certificates, photographs, and checks. Text is first detected using multiscale texture segmentation and spatial cohesion constraints, then cleaned up and extracted using a histogram-based binarization algorithm. An automatic performance evaluation scheme is also proposed."
            },
            "slug": "TextFinder:-An-Automatic-System-to-Detect-and-Text-Wu-Manmatha",
            "title": {
                "fragments": [],
                "text": "TextFinder: An Automatic System to Detect and Recognize Text In Images"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A robust system is proposed to automatically detect and extract text in images from different sources, including video, newspapers, advertisements, stock certificates, photographs, and checks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1644050191"
                        ],
                        "name": "G. LoweDavid",
                        "slug": "G.-LoweDavid",
                        "structuredName": {
                            "firstName": "G",
                            "lastName": "LoweDavid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. LoweDavid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "In BoW, local descriptors such as SIFT [33], HOG [12] or simply raw pixel patches [10] are extracted from images, and encoded by some coding methods such as the locality-constrained linear coding (LLC [52]) or the triangle activation [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 153
                            }
                        ],
                        "text": "In addition, convolutional features are learned from data, thus domain-specific and potentially stronger than general hand-crafted features such as SIFT [33] and HOG [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 174065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4cab9c4b571761203ed4c3a4c5a07dd615f57a91",
            "isKey": false,
            "numCitedBy": 25501,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are ..."
            },
            "slug": "Distinctive-Image-Features-from-Scale-Invariant-LoweDavid",
            "title": {
                "fragments": [],
                "text": "Distinctive Image Features from Scale-Invariant Keypoints"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073603971"
                        ],
                        "name": "Vinod Nair",
                        "slug": "Vinod-Nair",
                        "structuredName": {
                            "firstName": "Vinod",
                            "lastName": "Nair",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vinod Nair"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 65
                            }
                        ],
                        "text": "Then, a threshold function max\u00f00; x\u00de is applied, equivalent to the ReLU nonlinearity."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 103
                            }
                        ],
                        "text": "In our implementation, we use the element-wise thresholding function max\u00f00; x\u00de, also known as the ReLU [36]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 96
                            }
                        ],
                        "text": "In our implementation, we use the element-wise thresholding function max\u00f00; x\u00de, also known as the ReLU [36]. mp is the max-pooling function, which downsamples feature maps by taking the maximum values on downsampling subregions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 133
                            }
                        ],
                        "text": "Therefore, we model the layer as the sequential combination of a linear layer that is parameterized by codebook weights W, b, and an ReLU layer."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15539264,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a538b05ebb01a40323997629e171c91aa28b8e2f",
            "isKey": true,
            "numCitedBy": 12806,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these \"Stepped Sigmoid Units\" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors."
            },
            "slug": "Rectified-Linear-Units-Improve-Restricted-Boltzmann-Nair-Hinton",
            "title": {
                "fragments": [],
                "text": "Rectified Linear Units Improve Restricted Boltzmann Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "Restricted Boltzmann machines were developed using binary stochastic hidden units that learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37900058"
                        ],
                        "name": "R. Manthalkar",
                        "slug": "R.-Manthalkar",
                        "structuredName": {
                            "firstName": "Ramchandra",
                            "lastName": "Manthalkar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manthalkar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758797"
                        ],
                        "name": "P. Biswas",
                        "slug": "P.-Biswas",
                        "structuredName": {
                            "firstName": "Prabir",
                            "lastName": "Biswas",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Biswas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799356"
                        ],
                        "name": "B. N. Chatterji",
                        "slug": "B.-N.-Chatterji",
                        "structuredName": {
                            "firstName": "Biswanath",
                            "lastName": "Chatterji",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. N. Chatterji"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[34] propose rotation and scale invariant texture features, using discrete wavelet packet transform."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 20865439,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "74c273a74ebe0aaecae4c8660378921bc7d50fbf",
            "isKey": false,
            "numCitedBy": 109,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Rotation-and-scale-invariant-texture-features-using-Manthalkar-Biswas",
            "title": {
                "fragments": [],
                "text": "Rotation and scale invariant texture features using discrete wavelet packet transform"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687524"
                        ],
                        "name": "A. Vedaldi",
                        "slug": "A.-Vedaldi",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Vedaldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vedaldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2487006"
                        ],
                        "name": "B. Fulkerson",
                        "slug": "B.-Fulkerson",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Fulkerson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Fulkerson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1458265,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d720a95e1501922ea17ee31f299f43b2db5e15ef",
            "isKey": false,
            "numCitedBy": 3386,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "VLFeat is an open and portable library of computer vision algorithms. It aims at facilitating fast prototyping and reproducible research for computer vision scientists and students. It includes rigorous implementations of common building blocks such as feature detectors, feature extractors, (hierarchical) k-means clustering, randomized kd-tree matching, and super-pixelization. The source code and interfaces are fully documented. The library integrates directly with MATLAB, a popular language for computer vision research."
            },
            "slug": "Vlfeat:-an-open-and-portable-library-of-computer-Vedaldi-Fulkerson",
            "title": {
                "fragments": [],
                "text": "Vlfeat: an open and portable library of computer vision algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "VLFeat is an open and portable library of computer vision algorithms that includes rigorous implementations of common building blocks such as feature detectors, feature extractors, (hierarchical) k-means clustering, randomized kd-tree matching, and super-pixelization."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Multimedia"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2334226"
                        ],
                        "name": "D. Hubel",
                        "slug": "D.-Hubel",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hubel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hubel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2629471"
                        ],
                        "name": "T. Wiesel",
                        "slug": "T.-Wiesel",
                        "structuredName": {
                            "firstName": "Torsten",
                            "lastName": "Wiesel",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Wiesel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17055992,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "6b4fe4aa4d66fecc7b2869569002714d91d0b3f7",
            "isKey": false,
            "numCitedBy": 12431,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "What chiefly distinguishes cerebral cortex from other parts of the central nervous system is the great diversity of its cell types and interconnexions. It would be astonishing if such a structure did not profoundly modify the response patterns of fibres coming into it. In the cat's visual cortex, the receptive field arrangements of single cells suggest that there is indeed a degree of complexity far exceeding anything yet seen at lower levels in the visual system. In a previous paper we described receptive fields of single cortical cells, observing responses to spots of light shone on one or both retinas (Hubel & Wiesel, 1959). In the present work this method is used to examine receptive fields of a more complex type (Part I) and to make additional observations on binocular interaction (Part II). This approach is necessary in order to understand the behaviour of individual cells, but it fails to deal with the problem of the relationship of one cell to its neighbours. In the past, the technique of recording evoked slow waves has been used with great success in studies of functional anatomy. It was employed by Talbot & Marshall (1941) and by Thompson, Woolsey & Talbot (1950) for mapping out the visual cortex in the rabbit, cat, and monkey. Daniel & Whitteiidge (1959) have recently extended this work in the primate. Most of our present knowledge of retinotopic projections, binocular overlap, and the second visual area is based on these investigations. Yet the method of evoked potentials is valuable mainly for detecting behaviour common to large populations of neighbouring cells; it cannot differentiate functionally between areas of cortex smaller than about 1 mm2. To overcome this difficulty a method has in recent years been developed for studying cells separately or in small groups during long micro-electrode penetrations through nervous tissue. Responses are correlated with cell location by reconstructing the electrode tracks from histological material. These techniques have been applied to"
            },
            "slug": "Receptive-fields,-binocular-interaction-and-in-the-Hubel-Wiesel",
            "title": {
                "fragments": [],
                "text": "Receptive fields, binocular interaction and functional architecture in the cat's visual cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This method is used to examine receptive fields of a more complex type and to make additional observations on binocular interaction and this approach is necessary in order to understand the behaviour of individual cells, but it fails to deal with the problem of the relationship of one cell to its neighbours."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of physiology"
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4433870"
                        ],
                        "name": "P. Lennie",
                        "slug": "P.-Lennie",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Lennie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Lennie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 196
                            }
                        ],
                        "text": "A pixel on the feature map hk1\u20442i; j , where i and j are the row and column indices, respectively, is determined by a corresponding subregion on the input image, also known as the receptive field [21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17950742,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "dc7d03d05a437aaf0cc02a4ed36339cc259d2583",
            "isKey": false,
            "numCitedBy": 306,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Receptive-fields-Lennie",
            "title": {
                "fragments": [],
                "text": "Receptive fields"
            },
            "venue": {
                "fragments": [],
                "text": "Current Biology"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34782608"
                        ],
                        "name": "G. Orr",
                        "slug": "G.-Orr",
                        "structuredName": {
                            "firstName": "Genevieve",
                            "lastName": "Orr",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Orr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 130
                            }
                        ],
                        "text": "It learns the feature representation from raw pixels, and can be trained in an end-to-end manner by the backpropagation algorithm [30]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 153
                            }
                        ],
                        "text": "Usually fine-tuning is carried out in a neural network structure, where gradients on layer parameters are calculated with the back-propagation algorithm [30]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 155
                            }
                        ],
                        "text": "discriminative encoder for extracting the mid-level representations, into a single deep network for joint optimization with the back-propagation algorithm [30]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 20158889,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b87274e6d9aa4e6ba5148898aa92941617d2b6ed",
            "isKey": true,
            "numCitedBy": 2630,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Efficient-BackProp-LeCun-Bottou",
            "title": {
                "fragments": [],
                "text": "Efficient BackProp"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks: Tricks of the Trade"
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62540156,
            "fieldsOfStudy": [],
            "id": "51966f024c49544669450819a595774a99de7369",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Towards total scene understanding: Classification, annotation and segmentation in an automatic framework"
            },
            "venue": {
                "fragments": [],
                "text": "CVPR"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 35696139,
            "fieldsOfStudy": [],
            "id": "d68b95534860e2bddd17d17ef7f362d16c550bde",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Locating text in complex color images"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 308,
                                "start": 272
                            }
                        ],
                        "text": "Due to the rapidly increasing amount of multimedia data, especially those captured and stored by mobile terminals, how to recognize text content in natural scenes has become an active and important task in the fields of pattern recognition, computer vision and multimedia [15,16,25,26,47,49,59,23,57,38,8,41]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A"
            },
            "venue": {
                "fragments": [],
                "text": "de Souza Britto Jr., Evaluation of incremental learning algorithms for HMM in the recognition of alphanumeric characters, Pattern Recognit. 42 (12) "
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Text detection"
            },
            "venue": {
                "fragments": [],
                "text": "recognition in images and video frames, Pattern Recognit. 37 (3) "
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 254,
                                "start": 248
                            }
                        ],
                        "text": "Bai), understanding system under multi-lingual scenarios [5,6,22], potentially useful in many applications such as scene understanding [32], product image search [17], mobile phone navigation, film caption recognition [11], and machine translation [4,50]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "V"
            },
            "venue": {
                "fragments": [],
                "text": "Romero, M.P.i. Gadea, E. Vidal, Multimodal interactive transcription of text images, Pattern Recognit. 43 (5) "
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 135
                            }
                        ],
                        "text": "Bai), understanding system under multi-lingual scenarios [5,6,22], potentially useful in many applications such as scene understanding [32], product image search [17], mobile phone navigation, film caption recognition [11], and machine translation [4,50]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Towards total scene understanding: classification"
            },
            "venue": {
                "fragments": [],
                "text": "annotation and segmentation in an automatic framework, in: Proceedings of CVPR"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Receptive fi elds , binocular interaction and functional architecture in the cat ' s visual cortex"
            },
            "venue": {
                "fragments": [],
                "text": "J . Physiol ."
            },
            "year": 1962
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 38,
            "methodology": 15,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 65,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/Script-identification-in-the-wild-via-convolutional-Shi-Bai/f9812e072a36b7e19191683f90880c5d3252c827?sort=total-citations"
}