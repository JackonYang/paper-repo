{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2397742"
                        ],
                        "name": "H. Moon",
                        "slug": "H.-Moon",
                        "structuredName": {
                            "firstName": "Hankyu",
                            "lastName": "Moon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Moon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107493667"
                        ],
                        "name": "M. L. Miller",
                        "slug": "M.-L.-Miller",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Miller",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. L. Miller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 163
                            }
                        ],
                        "text": "Training: Our training set consisted of 52, 850, 32x32-pixel faces from natural images collected at NEC Labs and hand annotated with appropriate facial poses (see [9] for a description of how the annotation was done)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3196842,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "391d95c724d33549f46a28c3959dc9bea2c13a58",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an approach to estimate the poses off human heads in natural scenes. The essential features for estimating the head pose are the positions of the prominent facial features relative to the position of the head. We have developed a high-dimensional, randomly sparse representation of a human face using a simplified facial feature model. The representation transforms a raw face image into a vector, representing how well the image matches a large number of randomly-posed and shaped head models. This transformation is designed to collect salient features of the face image that are useful to estimate the pose, while suppressing any irrelevant variations of face appearance. The relation between the sparse representation and the pose is learned using SVR (support vector regression). The sparse representation combined with SVR is shown to estimate the pose more quickly and accurately than SVR applied to raw images."
            },
            "slug": "Estimating-facial-pose-from-a-sparse-representation-Moon-Miller",
            "title": {
                "fragments": [],
                "text": "Estimating facial pose from a sparse representation [face recognition applications]"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A high-dimensional, randomly sparse representation of a human face using a simplified facial feature model is developed, designed to collect salient features of the face image that are useful to estimate the pose, while suppressing any irrelevant variations of face appearance."
            },
            "venue": {
                "fragments": [],
                "text": "2004 International Conference on Image Processing, 2004. ICIP '04."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715634"
                        ],
                        "name": "Ming-Hsuan Yang",
                        "slug": "Ming-Hsuan-Yang",
                        "structuredName": {
                            "firstName": "Ming-Hsuan",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Hsuan Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765887"
                        ],
                        "name": "D. Kriegman",
                        "slug": "D.-Kriegman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Kriegman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kriegman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145237406"
                        ],
                        "name": "N. Ahuja",
                        "slug": "N.-Ahuja",
                        "structuredName": {
                            "firstName": "Narendra",
                            "lastName": "Ahuja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ahuja"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 35
                            }
                        ],
                        "text": "To exploit the synergy between these two tasks, we train a convolutional network to map face images to points on af ce manifold, and non-face images to points far away from that manifold."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9045232,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ebb34b75982f628f9ce5995821fff81fd967dc2d",
            "isKey": false,
            "numCitedBy": 3968,
            "numCiting": 251,
            "paperAbstract": {
                "fragments": [],
                "text": "Images containing faces are essential to intelligent vision-based human-computer interaction, and research efforts in face processing include face recognition, face tracking, pose estimation and expression recognition. However, many reported methods assume that the faces in an image or an image sequence have been identified and localized. To build fully automated systems that analyze the information contained in face images, robust and efficient face detection algorithms are required. Given a single image, the goal of face detection is to identify all image regions which contain a face, regardless of its 3D position, orientation and lighting conditions. Such a problem is challenging because faces are non-rigid and have a high degree of variability in size, shape, color and texture. Numerous techniques have been developed to detect faces in a single image, and the purpose of this paper is to categorize and evaluate these algorithms. We also discuss relevant issues such as data collection, evaluation metrics and benchmarking. After analyzing these algorithms and identifying their limitations, we conclude with several promising directions for future research."
            },
            "slug": "Detecting-Faces-in-Images:-A-Survey-Yang-Kriegman",
            "title": {
                "fragments": [],
                "text": "Detecting Faces in Images: A Survey"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144723337"
                        ],
                        "name": "Christophe Garcia",
                        "slug": "Christophe-Garcia",
                        "structuredName": {
                            "firstName": "Christophe",
                            "lastName": "Garcia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christophe Garcia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1812064"
                        ],
                        "name": "M. Delakis",
                        "slug": "M.-Delakis",
                        "structuredName": {
                            "firstName": "Manolis",
                            "lastName": "Delakis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Delakis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 148
                            }
                        ],
                        "text": "Previous Work: Learning-based approaches to face detection abound, including real-time methods [16], and approaches based on convolutional networks [15, 3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 32706640,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ffda2eefb2458bb50f1c71d6fab6f3d6470fac9a",
            "isKey": false,
            "numCitedBy": 99,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a connectionist approach for detecting and precisely localizing semi-frontal human faces in complex images, making no assumption about the content or the lighting conditions of the scene, or about the size or the appearance of the faces. We propose a convolutional neural network architecture designed to recognize strongly variable face patterns directly from pixel images with no preprocessing, by automatically synthesizing its own set of feature extractors from a large training set of faces. We present in details the optimized design of our architecture, our learning strategy and the resulting process of face detection. We also provide experimental results to demonstrate the robustness of our approach and its capability to precisely detect extremely variable faces in uncontrolled environments."
            },
            "slug": "A-neural-architecture-for-fast-and-robust-face-Garcia-Delakis",
            "title": {
                "fragments": [],
                "text": "A neural architecture for fast and robust face detection"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A convolutional neural network architecture designed to recognize strongly variable face patterns directly from pixel images with no preprocessing, by automatically synthesizing its own set of feature extractors from a large training set of faces."
            },
            "venue": {
                "fragments": [],
                "text": "Object recognition supported by user interaction for service robots"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38817267"
                        ],
                        "name": "K. Sung",
                        "slug": "K.-Sung",
                        "structuredName": {
                            "firstName": "Kah",
                            "lastName": "Sung",
                            "middleNames": [
                                "Kay"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 57
                            }
                        ],
                        "text": "The details of these sets are described below: \u2022 MIT+CMU [14, 11] \u2013 130 images for testing frontal face detectors."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 151
                            }
                        ],
                        "text": "Most multi-view systems take a view-basedapproach, which involves building separate detectors for different views and either applying them in parallel [10, 14, 13, 7] or using a pose estimator to select a detector [5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7164794,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "088eb2d102c6bb486f5270d0b2adff76961994cf",
            "isKey": false,
            "numCitedBy": 2061,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an example-based learning approach for locating vertical frontal views of human faces in complex scenes. The technique models the distribution of human face patterns by means of a few view-based \"face\" and \"nonface\" model clusters. At each image location, a difference feature vector is computed between the local image pattern and the distribution-based model. A trained classifier determines, based on the difference feature vector measurements, whether or not a human face exists at the current image location. We show empirically that the distance metric we adopt for computing difference feature vectors, and the \"nonface\" clusters we include in our distribution-based model, are both critical for the success of our system."
            },
            "slug": "Example-Based-Learning-for-View-Based-Human-Face-Sung-Poggio",
            "title": {
                "fragments": [],
                "text": "Example-Based Learning for View-Based Human Face Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "An example-based learning approach for locating vertical frontal views of human faces in complex scenes and shows empirically that the distance metric adopted for computing difference feature vectors, and the \"nonface\" clusters included in the distribution-based model, are both critical for the success of the system."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50024179"
                        ],
                        "name": "Yongmin Li",
                        "slug": "Yongmin-Li",
                        "structuredName": {
                            "firstName": "Yongmin",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yongmin Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144784813"
                        ],
                        "name": "S. Gong",
                        "slug": "S.-Gong",
                        "structuredName": {
                            "firstName": "Shaogang",
                            "lastName": "Gong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2934996"
                        ],
                        "name": "H. Liddell",
                        "slug": "H.-Liddell",
                        "structuredName": {
                            "firstName": "Heather",
                            "lastName": "Liddell",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Liddell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 34
                            }
                        ],
                        "text": "Closer to our approach is that of [8], in which a number of Support Vector Regressors are trained to approximate smooth functions, each of which has a maximum for a face at a particular pose."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1889567,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cf0a5ddc4e80359418e8a66f6f6c068efd12c1d3",
            "isKey": false,
            "numCitedBy": 276,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "A support vector machine-based multi-view face detection and recognition framework is described. Face detection is carried out by constructing several detectors, each of them in charge of one specific view. The symmetrical property of face images is employed to simplify the complexity of the modelling. The estimation of head pose, which is achieved by using the support vector regression technique, provides crucial information for choosing the appropriate face detector. This helps to improve the accuracy and reduce the computation in multi-view face detection compared to other methods. For video sequences, further computational reduction can be achieved by using a pose change smoothing strategy. When face detectors find a face in frontal view, a support vector machine-based multi-class classifier is activated for face recognition. All the above issues are integrated under a support vector machine framework. Test results on four video sequences are presented, among them the detection rate is above 95%, recognition accuracy is above 90%, average pose estimation error is around 10/spl deg/, and the full detection and recognition speed is up to 4 frames/second on a Pentium II 300 PC."
            },
            "slug": "Support-vector-regression-and-classification-based-Li-Gong",
            "title": {
                "fragments": [],
                "text": "Support vector regression and classification based multi-view face detection and recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The estimation of head pose, which is achieved by using the support vector regression technique, provides crucial information for choosing the appropriate face detector, which helps to improve the accuracy and reduce the computation in multi-view face detection compared to other methods."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145319478"
                        ],
                        "name": "Michael J. Jones",
                        "slug": "Michael-J.-Jones",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jones",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731948"
                        ],
                        "name": "Paul A. Viola",
                        "slug": "Paul-A.-Viola",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Viola",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul A. Viola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 104
                            }
                        ],
                        "text": "The table shows that our results on the TILTED and PROFILE sets are similar to those of the two Jones & Viola detectors, and even approach those of the Rowleyet al and Schneiderman & Kanade nonreal-time detectors."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 214
                            }
                        ],
                        "text": "Most multi-view systems take a view-basedapproach, which involves building separate detectors for different views and either applying them in parallel [10, 14, 13, 7] or using a pose estimator to select a detector [5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 54
                            }
                        ],
                        "text": "28 Our detector 90% 97% 67% 83% 83% 88% Jones & Viola [5] (tilted) 90% 95% x x Jones & Viola [5] (profile) x 70% 83% x Rowley et al [11] 89% 96% x x Schneiderman& Kanade [13] x 86% 93% x"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 153
                            }
                        ],
                        "text": "There seems to be some disagreement about the number of faces in the standard set of annotations: [13] reports using 347 faces of the 462 that we found, [5] reports using 355, and we found 353 annotations."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15810545,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "548ab5770b89650158f99caee1bed3a986b0dcdf",
            "isKey": true,
            "numCitedBy": 724,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract This paper extends the face detection framework proposedby Viola and Jones 2001 to handle pro\ufb01le views and rotatedfaces. As in the work of Rowley et al 1998. and Schneider-man et al. 2000, we build different detectors for differentviews of the face. A decision tree is then trained to deter-mine the viewpoint class (such as right pro\ufb01le or rotated60 degrees) for a given window of the image being exam-ined. This is similar to the approach of Rowley et al. 1998.The appropriate detector for that viewpoint can then be runinstead of running all detectors on all windows. This tech-niqueyields goodresults and maintainsthe speed advantageof the Viola-Jones detector. 1. Introduction There are a number of techniques that can successfullydetect frontal upright faces in a wide variety of images[11, 7, 10, 12, 3, 6]. While the de\ufb01nition of \u201cfrontal\u201d and\u201cupright\u201dmayvaryfromsystem to system, the reality is thatmany natural images contain rotated or pro\ufb01le faces thatare not reliably detected. There are a small number of sys-tems which explicitly address non-frontal, or non-uprightface detection [8, 10, 2]. This paper describes progress to-ward a system which can detect faces regardless of posereliably and in real-time.This paperextendsthe frameworkproposedby Viola andJones [12]. This approach is selected because of its compu-tational ef\ufb01ciency and simplicity.One observation which is shared among all previous re-lated work is that a multi-view detector must be carefullyconstructed by combining a collection of detectors eachtrained for a single viewpoint. It appears that a monolithicapproach, where a single classi\ufb01er is trained to detect allposes of a face, is unlearnable with existing classi\ufb01ers. Ourinformal experiments lend support to this conclusion, sincea classi\ufb01er trained on all poses appears to be hopelessly in-accurate.This paper addresses two types of pose variation: non-frontal faces, which are rotated out of the image plane, andnon-upright faces, which are rotated in the image plane.In both cases the multi-view detector presented in this pa-per is a combination of Viola-Jones detectors, each detectortrained on face data taken from a single viewpoint.Reliable non-upright face detection was \ufb01rst presentedin a paper by Rowley, Baluja and Kanade [8]. They traintwo neural network classi\ufb01ers. The \ufb01rst estimates the poseof a face in the detection window. The second is a conven-tional face detector. Faces are detected in three steps: foreach image window the pose of \u201cface\u201d is \ufb01rst estimated; thepose estimate is then used to de-rotate the image window;the window is then classi\ufb01ed by the second detector. Fornon-face windows, the poses estimate must be consideredrandom. Nevertheless, a rotated non-faceshouldbe rejectedby the conventional detector. One potential \ufb02aw of such asystem is that the \ufb01nal detection rate is roughly the productof the correct classi\ufb01cation rates of the two classi\ufb01ers (sincethe errors of the two classi\ufb01ers are somewhat independent).One could adopt the Rowley et al. three step approachwhile replacingthe classi\ufb01ers with those of Viola andJones.The \ufb01nal system would be more ef\ufb01cient, but not signi\ufb01-cantly. Classi\ufb01cation by the Viola-Jones system is so ef\ufb01-cient, that derotation would dominate the computational ex-pense. In principle derotation is not strictly necessary sinceit should be possible to construct a detector for rotated facesdirectly. Detection becomes a two stage process. First thepose of the window is estimated and then one ofrotationspeci\ufb01c detectors is called upon to classify the window.In this paper detection of non-upright faces is handledusing the two stage approach. In the \ufb01rst stage the pose ofeach window is estimated using a decision tree constructedusing features like those described by Viola and Jones. Inthe second stage one ofpose speci\ufb01c Viola-Jones dete-tectors are used to classify the window.Oncepose speci\ufb01c detectors are trained and available,an alternative detection process can be tested as well. In thiscase alldetectors are evaluated and the union of their de-"
            },
            "slug": "Fast-Multi-view-Face-Detection-Jones-Viola",
            "title": {
                "fragments": [],
                "text": "Fast Multi-view Face Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A multi-view detector presented in this pa-per is a combination of Viola-Jones detectors, each detectortrained on face data taken from a single viewpoint, which appears that a monolithic approach to face detection is unlearnable with existing classi\ufb01er trained on all poses."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39682833"
                        ],
                        "name": "H. Rowley",
                        "slug": "H.-Rowley",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Rowley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Rowley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767244"
                        ],
                        "name": "S. Baluja",
                        "slug": "S.-Baluja",
                        "structuredName": {
                            "firstName": "Shumeet",
                            "lastName": "Baluja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Baluja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 40
                            }
                        ],
                        "text": "The table shows that our results on the TILTED and PROFILE sets are similar to those of the two Jones & Viola detectors, and even approach those of the Rowleyet al and Schneiderman & Kanade nonreal-time detectors."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 1
                            }
                        ],
                        "text": "\u2022 TILTED [12] \u2013 50 images of frontal faces with in-plane rotations."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 111
                            }
                        ],
                        "text": "Another approach is to estimate and correct in-plane rotations before applying a single pose-specific detector [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 11
                            }
                        ],
                        "text": ") \u2022 TILTED [12] \u2013 50 images of frontal faces with in-plane rotations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61175113,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ab9d1a4ac312267c4ef2f552471e138bacf68df",
            "isKey": true,
            "numCitedBy": 503,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "The system (see Figure 2) uses a neural network, called a \u201crouter\u201d, to analyze each window of the input before it is processed by a \u201cdetector\u201d network. If the window contains a face, the router returns the angle of the face. The window can then be \u201cderotated\u201d to make the face upright. The derotated window is then passed to the detection network, which decides whether a face is present. If a non-face image is encountered, the router will return a meaningless rotation. Since a rotation of a non-face image will yield another nonface image, the detector network will still not detect a face. A rotated face, which would not have been detected by an upright face detector, will be rotated to an upright position, and subsequently detected as a face. Because the detector network is only applied once at each image location, this approach is significantly faster than exhaustively trying all orientations, and will yield fewer false detections [ 1,3]. To speed up the above algorithm for demonstration purposes, we used several techniques. First, we use a change detection algorithm to restrict the search area. Second, we use a model of skin color (acquired online as faces are detected) to further restrict the search. Finally, we use a candidate detection network to quickly rule out some portions of the input image, before examining them more carefully (and slowly) with the detection network. With these techniques, it takes about 6 seconds to process a 160x 120 pixel image on an SGI 02 workstation with a 174 MHz RIO000 processor."
            },
            "slug": "Rotation-Invariant-Neural-Network-Based-Face-Rowley-Baluja",
            "title": {
                "fragments": [],
                "text": "Rotation Invariant Neural Network-Based Face Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "A neural network is used to analyze each window of the input before it is processed by a \u201cdetector\u201d network, which decides whether a face is present, which is significantly faster than exhaustively trying all orientations, and will yield fewer false detections."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1998 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.98CB36231)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39682833"
                        ],
                        "name": "H. Rowley",
                        "slug": "H.-Rowley",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Rowley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Rowley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767244"
                        ],
                        "name": "S. Baluja",
                        "slug": "S.-Baluja",
                        "structuredName": {
                            "firstName": "Shumeet",
                            "lastName": "Baluja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Baluja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 57
                            }
                        ],
                        "text": "The details of these sets are described below: \u2022 MIT+CMU [14, 11] \u2013 130 images for testing frontal face detectors."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 132
                            }
                        ],
                        "text": "28 Our detector 90% 97% 67% 83% 83% 88% Jones & Viola [5] (tilted) 90% 95% x x Jones & Viola [5] (profile) x 70% 83% x Rowley et al [11] 89% 96% x x Schneiderman& Kanade [13] x 86% 93% x"
                    },
                    "intents": []
                }
            ],
            "corpusId": 40120983,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d76ef8e61395a6e9c32627f1f108772d084e2e9",
            "isKey": false,
            "numCitedBy": 4156,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a neural network-based face detection system. A retinally connected neural network examines small windows of an image and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We use a bootstrap algorithm for training the networks, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting non-face training examples, which must be chosen to span the entire space of non-face images. Comparisons with other state-of-the-art face detection systems are presented; our system has better performance in terms of detection and false-positive rates."
            },
            "slug": "Neural-network-based-face-detection-Rowley-Baluja",
            "title": {
                "fragments": [],
                "text": "Neural Network-Based Face Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A neural network-based face detection system that arbitrates between multiple networks to improve performance over a single network using a bootstrap algorithm, which eliminates the difficult task of manually selecting non-face training examples."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2721983"
                        ],
                        "name": "F. Fleuret",
                        "slug": "F.-Fleuret",
                        "structuredName": {
                            "firstName": "Fran\u00e7ois",
                            "lastName": "Fleuret",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Fleuret"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707642"
                        ],
                        "name": "D. Geman",
                        "slug": "D.-Geman",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 57
                            }
                        ],
                        "text": "The training system was implemented in the Lush language (Bottou and LeCun, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6754141,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9b535f4edc4cbf8d4fb6182ec6b5c54db3c1cccb",
            "isKey": false,
            "numCitedBy": 339,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We study visual selection: Detect and roughly localize all instances of a generic object class, such as a face, in a greyscale scene, measuring performance in terms of computation and false alarms. Our approach is sequential testing which is coarse-to-fine in both in the exploration of poses and the representation of objects. All the tests are binary and indicate the presence or absence of loose spatial arrangements of oriented edge fragments. Starting from training examples, we recursively find larger and larger arrangements which are \u201cdecomposable,\u201d which implies the probability of an arrangement appearing on an object decays slowly with its size. Detection means finding a sufficient number of arrangements of each size along a decreasing sequence of pose cells. At the beginning, the tests are simple and universal, accommodating many poses simultaneously, but the false alarm rate is relatively high. Eventually, the tests are more discriminating, but also more complex and dedicated to specific poses. As a result, the spatial distribution of processing is highly skewed and detection is rapid, but at the expense of (isolated) false alarms which, presumably, could be eliminated with localized, more intensive, processing."
            },
            "slug": "Coarse-to-Fine-Face-Detection-Fleuret-Geman",
            "title": {
                "fragments": [],
                "text": "Coarse-to-Fine Face Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The approach is sequential testing which is coarse-to-fine in both in the exploration of poses and the representation of objects, and the spatial distribution of processing is highly skewed and detection is rapid, but at the expense of (isolated) false alarms which could be eliminated with localized, more intensive, processing."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13919023"
                        ],
                        "name": "F. Huang",
                        "slug": "F.-Huang",
                        "structuredName": {
                            "firstName": "Fu",
                            "lastName": "Huang",
                            "middleNames": [
                                "Jie"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 712708,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f354310098e09c1e1dc88758fca36767fd9d084d",
            "isKey": false,
            "numCitedBy": 1306,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We assess the applicability of several popular learning methods for the problem of recognizing generic visual categories with invariance to pose, lighting, and surrounding clutter. A large dataset comprising stereo image pairs of 50 uniform-colored toys under 36 azimuths, 9 elevations, and 6 lighting conditions was collected (for a total of 194,400 individual images). The objects were 10 instances of 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. Five instances of each category were used for training, and the other five for testing. Low-resolution grayscale images of the objects with various amounts of variability and surrounding clutter were used for training and testing. Nearest neighbor methods, support vector machines, and convolutional networks, operating on raw pixels or on PCA-derived features were tested. Test error rates for unseen object instances placed on uniform backgrounds were around 13% for SVM and 7% for convolutional nets. On a segmentation/recognition task with highly cluttered images, SVM proved impractical, while convolutional nets yielded 16/7% error. A real-time version of the system was implemented that can detect and classify objects in natural scenes at around 10 frames per second."
            },
            "slug": "Learning-methods-for-generic-object-recognition-to-LeCun-Huang",
            "title": {
                "fragments": [],
                "text": "Learning methods for generic object recognition with invariance to pose and lighting"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A real-time version of the system was implemented that can detect and classify objects in natural scenes at around 10 frames per second and proved impractical, while convolutional nets yielded 16/7% error."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34679741"
                        ],
                        "name": "S. Li",
                        "slug": "S.-Li",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144850567"
                        ],
                        "name": "Long Zhu",
                        "slug": "Long-Zhu",
                        "structuredName": {
                            "firstName": "Long",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Long Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2651359"
                        ],
                        "name": "ZhenQiu Zhang",
                        "slug": "ZhenQiu-Zhang",
                        "structuredName": {
                            "firstName": "ZhenQiu",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "ZhenQiu Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145162067"
                        ],
                        "name": "A. Blake",
                        "slug": "A.-Blake",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Blake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blake"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108698841"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144154486"
                        ],
                        "name": "H. Shum",
                        "slug": "H.-Shum",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Shum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Shum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 151
                            }
                        ],
                        "text": "Most multi-view systems take a view-basedapproach, which involves building separate detectors for different views and either applying them in parallel [10, 14, 13, 7] or using a pose estimator to select a detector [5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 32107239,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3c4fdffc12589f9f312a44802b8e2fe8311aa13e",
            "isKey": false,
            "numCitedBy": 462,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "A new boosting algorithm, called FloatBoost, is proposed to overcome the monotonicity problem of the sequential AdaBoost learning. AdaBoost [1, 2] is a sequential forward search procedure using the greedy selection strategy. The premise oyered by the sequential procedure can be broken-down when the monotonicity assumption, i.e. that when adding a new feature to the current set, the value of the performance criterion does not decrease, is violated. FloatBoost incorporates the idea of Floating Search [3] into AdaBoost to solve the non-monotonicity problem encountered in the sequential search of AdaBoost.We then present a system which learns to detect multi-view faces using FloatBoost. The system uses a coarse-to-fine, simple-to-complex architecture called detector-pyramid. FloatBoost learns the component detectors in the pyramid and yields similar or higher classification accuracy than AdaBoost with a smaller number of weak classifiers. This work leads to the first real-time multi-view face detection system in the world. It runs at 200 ms per image of size 320x240 pixels on a Pentium-III CPU of 700 MHz. A live demo will be shown at the conference."
            },
            "slug": "Statistical-Learning-of-Multi-view-Face-Detection-Li-Zhu",
            "title": {
                "fragments": [],
                "text": "Statistical Learning of Multi-view Face Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "FloatBoost incorporates the idea of Floating Search into AdaBoost to solve the non-monotonicity problem encountered in the sequential search of AdaBoost and leads to the first real-time multi-view face detection system in the world."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144528373"
                        ],
                        "name": "Chang Huang",
                        "slug": "Chang-Huang",
                        "structuredName": {
                            "firstName": "Chang",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chang Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115595183"
                        ],
                        "name": "Bo Wu",
                        "slug": "Bo-Wu",
                        "structuredName": {
                            "firstName": "Bo",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bo Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679380"
                        ],
                        "name": "H. Ai",
                        "slug": "H.-Ai",
                        "structuredName": {
                            "firstName": "Haizhou",
                            "lastName": "Ai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710195"
                        ],
                        "name": "S. Lao",
                        "slug": "S.-Lao",
                        "structuredName": {
                            "firstName": "Shihong",
                            "lastName": "Lao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 135
                            }
                        ],
                        "text": "In this paper we describe a novel method that not only detects faces independently of their poses, but simultaneously estimates those poses."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10296973,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "69ecec2be51066470ad789d1fd88da102b574375",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an omni-directional face detection method based on the confidence-rated AdaBoost algorithm, called real AdaBoost, proposed by R.E. Schapire and Y. Singer (see Machine Learning, vol.37, p.297-336, 1999). To use real AdaBoost, we configure the confidence-rated look-up-table (LUT) weak classifiers based on Haar-type features. A nesting-structured framework is developed to combine a series of boosted classifiers into an efficient object detector. For omni-directional face detection, our method has achieved a rather high performance and the processing speed can reach 217 ms per 320/spl times/240 image. Experiment results on the CMU+MIT frontal and the CMU profile face test sets are reported to show its effectiveness."
            },
            "slug": "Omni-directional-face-detection-based-on-real-Huang-Wu",
            "title": {
                "fragments": [],
                "text": "Omni-directional face detection based on real AdaBoost"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "An omni-directional face detection method based on the confidence-rated AdaBoost algorithm, called real AdaBoost, and a nesting-structured framework is developed to combine a series of boosted classifiers into an efficient object detector."
            },
            "venue": {
                "fragments": [],
                "text": "2004 International Conference on Image Processing, 2004. ICIP '04."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780935"
                        ],
                        "name": "B. Moghaddam",
                        "slug": "B.-Moghaddam",
                        "structuredName": {
                            "firstName": "Baback",
                            "lastName": "Moghaddam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Moghaddam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738894"
                        ],
                        "name": "T. Starner",
                        "slug": "T.-Starner",
                        "structuredName": {
                            "firstName": "Thad",
                            "lastName": "Starner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Starner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 151
                            }
                        ],
                        "text": "Most multi-view systems take a view-basedapproach, which involves building separate detectors for different views and either applying them in parallel [10, 14, 13, 7] or using a pose estimator to select a detector [5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 136280,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0bf5d558220d39698ce96d59ee5772e8e1a0663",
            "isKey": false,
            "numCitedBy": 2234,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe experiments with eigenfaces for recognition and interactive search in a large-scale face database. Accurate visual recognition is demonstrated using a database of O(10/sup 3/) faces. The problem of recognition under general viewing orientation is also examined. A view-based multiple-observer eigenspace technique is proposed for use in face recognition under variable pose. In addition, a modular eigenspace description technique is used which incorporates salient features such as the eyes, nose and mouth, in an eigenfeature layer. This modular representation yields higher recognition rates as well as a more robust framework for face recognition. An automatic feature extraction technique using feature eigentemplates is also demonstrated.<<ETX>>"
            },
            "slug": "View-based-and-modular-eigenspaces-for-face-Pentland-Moghaddam",
            "title": {
                "fragments": [],
                "text": "View-based and modular eigenspaces for face recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A modular eigenspace description technique is used which incorporates salient features such as the eyes, nose and mouth, in an eigenfeature layer, which yields higher recognition rates as well as a more robust framework for face recognition."
            },
            "venue": {
                "fragments": [],
                "text": "1994 Proceedings of IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "147984118"
                        ],
                        "name": "R. Vaillant",
                        "slug": "R.-Vaillant",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Vaillant",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Vaillant"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3208918"
                        ],
                        "name": "C. Monrocq",
                        "slug": "C.-Monrocq",
                        "structuredName": {
                            "firstName": "Christophe",
                            "lastName": "Monrocq",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Monrocq"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9326933"
                        ],
                        "name": "Y. L. Cun",
                        "slug": "Y.-L.-Cun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "Cun",
                            "middleNames": [
                                "le"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. L. Cun"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16690291,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cc7fa2cf9d7d2b3aca4fa22271412831e9a61e22",
            "isKey": false,
            "numCitedBy": 92,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Presents an algorithm for the detection of faces in images using shared-weight replicated neural networks. A neural net forms rough hypotheses about the position of faces. These hypotheses are then verified using a second neural network. The algorithm applies to images where the size of the faces is unknown a priori. The computational time which is necessary for the complete processing of an image is reasonable. With a classical workstation an image of size 512*512 is treated in 50 seconds including smoothing and normalization of the image. This algorithm can be easily installed on a more specialized machine as the major part of the operation is based on convolutions with kernels of size 5*5 or 8*8. In this paper, the authors assume that the face are well oriented in the image. It is possible to eliminate this assumption by following an approach similar to the one used for the scale problem. A net is trained to be insensitive to the precise orientation of the face. This kind of segmentation algorithm can be applied to other problems where the objects to be detected cannot be characterized easily by its outline or by classical primitives in image processing."
            },
            "slug": "An-original-approach-for-the-localization-of-in-Vaillant-Monrocq",
            "title": {
                "fragments": [],
                "text": "An original approach for the localization of objects in images"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "An algorithm for the detection of faces in images using shared-weight replicated neural networks, trained to be insensitive to the precise orientation of the face by following an approach similar to the one used for the scale problem."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802785"
                        ],
                        "name": "S. Nowlan",
                        "slug": "S.-Nowlan",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Nowlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nowlan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189092"
                        ],
                        "name": "John C. Platt",
                        "slug": "John-C.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Platt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9066905,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2b234385356cb10d448908cef49584bece15d94b",
            "isKey": false,
            "numCitedBy": 117,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a system that can track a hand in a sequence of video frames and recognize hand gestures in a user-independent manner. The system locates the hand in each video frame and determines if the hand is open or closed. The tracking system is able to track the hand to within \u00b110 pixels of its correct location in 99.7% of the frames from a test set containing video sequences from 18 different individuals captured in 18 different room environments. The gesture recognition network correctly determines if the hand being tracked is open or closed in 99.1% of the frames in this test set. The system has been designed to operate in real time with existing hardware."
            },
            "slug": "A-Convolutional-Neural-Network-Hand-Tracker-Nowlan-Platt",
            "title": {
                "fragments": [],
                "text": "A Convolutional Neural Network Hand Tracker"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A system that can track a hand in a sequence of video frames and recognize hand gestures in a user-independent manner and is designed to operate in real time with existing hardware is described."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2085375589"
                        ],
                        "name": "H. Schneiderman",
                        "slug": "H.-Schneiderman",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Schneiderman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Schneiderman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12209481,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3565c5a65842f26091578b9d71d496cc1561239d",
            "isKey": false,
            "numCitedBy": 1292,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we describe a statistical method for 3D object detection. We represent the statistics of both object appearance and \"non-object\" appearance using a product of histograms. Each histogram represents the joint statistics of a subset of wavelet coefficients and their position on the object. Our approach is to use many such histograms representing a wide variety of visual attributes. Using this method, we have developed the first algorithm that can reliably detect human faces with out-of-plane rotation and the first algorithm that can reliably detect passenger cars over a wide range of viewpoints."
            },
            "slug": "A-statistical-method-for-3D-object-detection-to-and-Schneiderman-Kanade",
            "title": {
                "fragments": [],
                "text": "A statistical method for 3D object detection applied to faces and cars"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Using this method, this work has developed the first algorithm that can reliably detect human faces with out-of-plane rotation and the first algorithms thatCan reliably detect passenger cars over a wide range of viewpoints."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No.PR00662)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731948"
                        ],
                        "name": "Paul A. Viola",
                        "slug": "Paul-A.-Viola",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Viola",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul A. Viola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145319478"
                        ],
                        "name": "Michael J. Jones",
                        "slug": "Michael-J.-Jones",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jones",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 95
                            }
                        ],
                        "text": "Previous Work: Learning-based approaches to face detection abound, including real-time methods [16], and approaches based on convolutional networks [15, 3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2715202,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc6ea0e30e46163b706f2f8bdc9c67ca87f83d63",
            "isKey": false,
            "numCitedBy": 17884,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the \"integral image\" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a \"cascade\" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection."
            },
            "slug": "Rapid-object-detection-using-a-boosted-cascade-of-Viola-Jones",
            "title": {
                "fragments": [],
                "text": "Rapid object detection using a boosted cascade of simple features"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates and the introduction of a new image representation called the \"integral image\" which allows the features used by the detector to be computed very quickly."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13919023"
                        ],
                        "name": "F. Huang",
                        "slug": "F.-Huang",
                        "structuredName": {
                            "firstName": "Fu",
                            "lastName": "Huang",
                            "middleNames": [
                                "Jie"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Huang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 97
                            }
                        ],
                        "text": "1 Energy Minimization Framework We propose the following configuration of the Energy Based Model (LeCun and Huang, 2005; LeCun et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 237,
                                "start": 194
                            }
                        ],
                        "text": "As a learning machine we use the recently proposed Energy-Based Models (EBM) that provide a description and the inference process and the learning process in a single, well-principled framework (LeCun and Huang, 2005; LeCun et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18320130,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fae82787fc4268f579823696bf8f54b22e253711",
            "isKey": false,
            "numCitedBy": 134,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Probabilistic graphical models associate a probability to each configuration of the relevant variables. Energy-based models (EBM) associate an energy to those configurations, eliminating the need for proper normalization of probability distributions. Making a decision (an inference) with an EBM consists in comparing the energies associated with various configurations of the variable to be predicted, and choosing the one with the smallest energy. Such systems must be trained discriminatively to associate low energies to the desired configurations and higher energies to undesired configurations. A wide variety of loss function can be used for this purpose. We give sufficient conditions that a loss function should satisfy so that its minimization will cause the system to approach to desired behavior. We give many specific examples of suitable loss functions, and show an application to object recognition in images."
            },
            "slug": "Loss-Functions-for-Discriminative-Training-of-LeCun-Huang",
            "title": {
                "fragments": [],
                "text": "Loss Functions for Discriminative Training of Energy-Based Models"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "This work gives sufficient conditions that a loss function should satisfy so that its minimization will cause the system to approach to desired behavior, and gives many specific examples of suitable loss functions."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47953439"
                        ],
                        "name": "R\u00e9gis Vaillant",
                        "slug": "R\u00e9gis-Vaillant",
                        "structuredName": {
                            "firstName": "R\u00e9gis",
                            "lastName": "Vaillant",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R\u00e9gis Vaillant"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3208918"
                        ],
                        "name": "C. Monrocq",
                        "slug": "C.-Monrocq",
                        "structuredName": {
                            "firstName": "Christophe",
                            "lastName": "Monrocq",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Monrocq"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9326933"
                        ],
                        "name": "Y. L. Cun",
                        "slug": "Y.-L.-Cun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "Cun",
                            "middleNames": [
                                "le"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. L. Cun"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62763570,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "09ebd9ad4fa21c0d56433ac57a4cd69e94c72281",
            "isKey": false,
            "numCitedBy": 286,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "An original approach is presented for the localisation of objects in an image which approach is neuronal and has two steps. In the first step, a rough localisation is performed by presenting each pixel with its neighbourhood to a neural net which is able to indicate whether this pixel and its neighbourhood are the image of the search object. This first filter does not discriminate for position. From its result, areas which might contain an image of the object can be selected. In the second step, these areas are presented to another neural net which can determine the exact position of the object in each area. This algorithm is applied to the problem of localising faces in images."
            },
            "slug": "Original-approach-for-the-localisation-of-objects-Vaillant-Monrocq",
            "title": {
                "fragments": [],
                "text": "Original approach for the localisation of objects in images"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "An original approach is presented for the localisation of objects in an image which approach is neuronal and has two steps and is applied to the problem of localising faces in images."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3295092"
                        ],
                        "name": "S. Chopra",
                        "slug": "S.-Chopra",
                        "structuredName": {
                            "firstName": "Sumit",
                            "lastName": "Chopra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chopra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2315504"
                        ],
                        "name": "R. Hadsell",
                        "slug": "R.-Hadsell",
                        "structuredName": {
                            "firstName": "Raia",
                            "lastName": "Hadsell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hadsell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70478759"
                        ],
                        "name": "Aurelio Ranzato",
                        "slug": "Aurelio-Ranzato",
                        "structuredName": {
                            "firstName": "Aurelio",
                            "lastName": "Ranzato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aurelio Ranzato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13919023"
                        ],
                        "name": "F. Huang",
                        "slug": "F.-Huang",
                        "structuredName": {
                            "firstName": "Fu",
                            "lastName": "Huang",
                            "middleNames": [
                                "Jie"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Huang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 97
                            }
                        ],
                        "text": "1 Energy Minimization Framework We propose the following configuration of the Energy Based Model (LeCun and Huang, 2005; LeCun et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 237,
                                "start": 194
                            }
                        ],
                        "text": "As a learning machine we use the recently proposed Energy-Based Models (EBM) that provide a description and the inference process and the learning process in a single, well-principled framework (LeCun and Huang, 2005; LeCun et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8531544,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7fc604e1a3e45cd2d2742f96d62741930a363efa",
            "isKey": false,
            "numCitedBy": 793,
            "numCiting": 107,
            "paperAbstract": {
                "fragments": [],
                "text": "Energy-Based Models (EBMs) capture dependencies between variables by associating a scalar energy to each configuration of the variab les. Inference consists in clamping the value of observed variables and finding config urations of the remaining variables that minimize the energy. Learning consists in finding an energy function in which observed configurations of the variables a re given lower energies than unobserved ones. The EBM approach provides a common theoretical framework for many learning models, including traditional discr iminative and generative approaches, as well as graph-transformer networks, co nditional random fields, maximum margin Markov networks, and several manifold learning methods. Probabilistic models must be properly normalized, which sometimes requires evaluating intractable integrals over the space of all poss ible variable configurations. Since EBMs have no requirement for proper normalization, this problem is naturally circumvented. EBMs can be viewed as a form of non-probabilistic factor graphs, and they provide considerably more flexibility in th e design of architectures and training criteria than probabilistic approaches ."
            },
            "slug": "A-Tutorial-on-Energy-Based-Learning-LeCun-Chopra",
            "title": {
                "fragments": [],
                "text": "A Tutorial on Energy-Based Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The EBM approach provides a common theoretical framework for many learning models, including traditional discr iminative and generative approaches, as well as graph-transformer networks, co nditional random fields, maximum margin Markov networks, and several manifold learning methods."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721248"
                        ],
                        "name": "P. Haffner",
                        "slug": "P.-Haffner",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Haffner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Haffner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14542261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "162d958ff885f1462aeda91cd72582323fd6a1f4",
            "isKey": false,
            "numCitedBy": 35256,
            "numCiting": 248,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day."
            },
            "slug": "Gradient-based-learning-applied-to-document-LeCun-Bottou",
            "title": {
                "fragments": [],
                "text": "Gradient-based learning applied to document recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task, and Convolutional neural networks are shown to outperform all other techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37274089"
                        ],
                        "name": "D. Henderson",
                        "slug": "D.-Henderson",
                        "structuredName": {
                            "firstName": "Donnie",
                            "lastName": "Henderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Henderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2799635"
                        ],
                        "name": "R. Howard",
                        "slug": "R.-Howard",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Howard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Howard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34859193"
                        ],
                        "name": "W. Hubbard",
                        "slug": "W.-Hubbard",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "Hubbard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Hubbard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041866"
                        ],
                        "name": "L. Jackel",
                        "slug": "L.-Jackel",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Jackel",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jackel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 41312633,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8e8f3c8d4418c8d62e306538c9c1292635e9d27",
            "isKey": false,
            "numCitedBy": 7830,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification."
            },
            "slug": "Backpropagation-Applied-to-Handwritten-Zip-Code-LeCun-Boser",
            "title": {
                "fragments": [],
                "text": "Backpropagation Applied to Handwritten Zip Code Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This paper demonstrates how constraints from the task domain can be integrated into a backpropagation network through the architecture of the network, successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 164
                            }
                        ],
                        "text": "We suspect that, when trained together, each task can serve as an inductive bias for the other, yielding better generalization or requiring fewer training examples [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 155100635,
            "fieldsOfStudy": [],
            "id": "4d031e39474f2b622e87316314cb6c33eeda0786",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multitask Learning"
            },
            "venue": {
                "fragments": [],
                "text": "Learning to Learn"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 115
                            }
                        ],
                        "text": "To build a learning machine for the proposed approach we refer to the Minimum Energy Machineframework described in [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 125
                            }
                        ],
                        "text": "Conceptually, we can view the pose parameter as a latent variable that can be inferred through an energy-minimization process[4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Loss functions for discriminative training of energy-based graphical models"
            },
            "venue": {
                "fragments": [],
                "text": "Technical report, Courant Institute of Mathematical Science,"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Geman . Coarsetofine face detection"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 34
                            }
                        ],
                        "text": "Training was performed using LUSH [1], and the total training time was about 26 hours on a 2Ghz Pentium 4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "LeCun.The Lush Manual"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Figure 10: More examples of face detections"
            },
            "venue": {
                "fragments": [],
                "text": "Figure 10: More examples of face detections"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 34
                            }
                        ],
                        "text": "Training was performed using LUSH [1], and the total training time was about 26 hours on a 2Ghz Pentium 4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Lush Manual"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 11,
            "methodology": 9
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 28,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Synergistic-Face-Detection-and-Pose-Estimation-with-Osadchy-LeCun/6b728a7442ca158f895d07c11c77d302269a832d?sort=total-citations"
}