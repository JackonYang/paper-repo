{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35400286"
                        ],
                        "name": "Z. Harris",
                        "slug": "Z.-Harris",
                        "structuredName": {
                            "firstName": "Zellig",
                            "lastName": "Harris",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Harris"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 121
                            }
                        ],
                        "text": "The distributional hypothesis in linguistics is that words that occur in similar contexts tend to have similar meanings (Harris, 1954)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "on 2.5. 4. See http://www.natcorp.ox.ac.uk/. 5. See http://wordnet.princeton.edu/. 142 FromFrequency to Meaning words that occur in similar contexts tend to have similar meanings (Wittgenstein, 1953; Harris, 1954; Weaver, 1955; Firth, 1957; Deerwester, Dumais, Landauer, Furnas, &amp; Harshman, 1990). E\ufb00orts to apply this abstract hypothesis to concrete algorithms for measuring the similarity of meaning often "
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 75
                            }
                        ],
                        "text": "Deerwester et al. (1990) showed how the intuitions of Wittgenstein (1953), Harris (1954), Weaver, and Firth could be used in a practical algorithm."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "get word (the word that we want to disambiguate). Firth (1957, p. 11) said, \u201cYou shall know a word by the company it keeps.\u201d Deerwester et al. (1990) showed how the intuitions of Wittgenstein (1953), Harris (1954), Weaver, and Firth could be used in a practical algorithm. 2.3 Similarity of Relations: The Pair\u2013Pattern Matrix In a pair\u2013pattern matrix, row vectors correspond to pairs of words, such as mason:stone"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 123
                            }
                        ],
                        "text": "See http://wordnet.princeton.edu/.\nwords that occur in similar contexts tend to have similar meanings (Wittgenstein, 1953; Harris, 1954; Weaver, 1955; Firth, 1957; Deerwester, Dumais, Landauer, Furnas, & Harshman, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "ts, or more exotic possibilities, such as sequences of characters or patterns. The distributional hypothesis in linguistics is that words that occur in similar contexts tend to have similar meanings (Harris, 1954). This hypothesis is the justi\ufb01cation for applying the VSM to measuring word similarity. A word may be represented by a vector in which the elements are derived from the occurrences of the word in var"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "queries) have similar column vectors in a term\u2013document matrix, then they tend to have similar meanings. Distributional hypothesis: Words that occur in similar contexts tend to have similar meanings (Harris, 1954; Firth, 1957; Deerwester et al., 1990). \u2013 If words have similar row vectors in a word\u2013context matrix, then they tend to have similar meanings. Extended distributional hypothesis: Patterns that co-occ"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 95
                            }
                        ],
                        "text": "Distributional hypothesis: Words that occur in similar contexts tend to have similar meanings (Harris, 1954; Firth, 1957; Deerwester et al., 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 86680084,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "decd9bc0385612bdf936928206d83730718e737e",
            "isKey": true,
            "numCitedBy": 2644,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "For the purposes of the present discussion, the term structure will be used in the following non-rigorous sense: A set of phonemes or a set of data is structured in respect to some feature, to the extent that we can form in terms of that feature some organized system of statements which describes the members of the set and their interrelations (at least up to some limit of complexity). In this sense, language can be structured in respect to various independent features. And whether it is structured (to more than a trivial extent) in respect to, say, regular historical change, social intercourse, meaning, or distribution - or to what extent it is structured in any of these respects - is a matter decidable by investigation. Here we will discuss how each language can be described in terms of a distributional structure, i.e. in terms of the occurrence of parts (ultimately sounds) relative to other parts, and how this description is complete without intrusion of other features such as history or meaning. It goes without saying that other studies of language - historical, psychological, etc.-are also possible, both in relation to distributional structure and independently of it."
            },
            "slug": "Distributional-Structure-Harris",
            "title": {
                "fragments": [],
                "text": "Distributional Structure"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This discussion will discuss how each language can be described in terms of a distributional structure, i.e. in Terms of the occurrence of parts relative to other parts, and how this description is complete without intrusion of other features such as history or meaning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144514944"
                        ],
                        "name": "W. Lowe",
                        "slug": "W.-Lowe",
                        "structuredName": {
                            "firstName": "Will",
                            "lastName": "Lowe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Lowe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "t to smooth the matrix, to reduce the amount of random noise and to \ufb01ll in some of the zero elements in a sparse matrix. Fourth, there are many di\ufb00erent ways to measure the similarity of two vectors. Lowe (2001) gives a good summary of mathematical processing for word\u2013context VSMs. He decomposes VSM construction into a similar four-step process: calculate the frequencies, transform the raw frequency counts, "
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 0
                            }
                        ],
                        "text": "Lowe (2001) gives a good summary of mathematical processing for word\u2013context VSMs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15089423,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "12afb4e89aefd688f8c73d52b44cebca49221f3e",
            "isKey": false,
            "numCitedBy": 152,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Towards a Theory of Semantic Space Will Lowe (wlowe02@tufts.edu) Center for Cognitive Studies Tufts University; MA 21015 USA Abstract This paper adds some theory to the growing literature of semantic space models. We motivate semantic space models from the perspective of distributional linguistics and show how an explicit mathematical formulation can provide a better understanding of existing models and suggest changes and improvements. In addition to pro- viding a theoretical framework for current models, we consider the implications of statistical aspects of language data that have not been addressed in the psychological modeling literature. Statistical approaches to language must deal principally with count data, and this data will typically have a highly skewed frequency distribution due to Zipf\u2019s law. We consider the consequences of these facts for the construction of semantic space models, and present methods for removing frequency biases from se- mantic space models. Introduction There is a growing literature on the empirical adequacy of semantic space models across a wide range of sub- ject domains (Burgess et al., 1998; Landauer et al., 1998; Foltz et al., 1998; McDonald and Lowe, 1998; Lowe and McDonald, 2000). However, semantic space mod- els are typically structured and parameterized differently by each researcher. Levy and Bullinaria (2000) have ex- plored the implications of parameter changes empirically by running multiple simulations, but there has up until now been no work that places semantic space models in an overarching theoretical framework; consequently there there are few statements of how semantic spaces ought to be structured in the light of their intended pur- pose. In this paper we attempt to develop a theoretical framework for semantic space models by synthesizing theoretical analyses from vector space information re- trieval and categorical data analysis with new basic re- search. The structure of the paper is as follows. The next sec- tion brie\u00a4y motivates semantic space models using ideas from distributional linguistics. We then review Zipf\u2019s law and its consequences the distributional character of linguistic data. The \u00a3nal section presents a formal de\u00a3- nition of semantic space models and considers what ef- fects different choices of component have on the result- ing models. Motivating Semantic Space Firth (1968) observed that \u201cyou shall know a word by the company it keeps\u201d. If we interpret company as lex- ical company, the words that occur near to it in text or speech, then two related claims are possible. The \u00a3rst is unexceptional: we come to know about the syntactic character of a word by examining the other words that may and may not occur around it in text. Syntactic theory then postulates latent variables e.g. parts of speech and branching structure, that control the distributional prop- erties of words and restrictions on their contexts of occur- rence. The second claim is that we come to know about the semantic character of a word by examining the other words that may and may not occur around it in text. The intuition for this distributional characterization of semantics is that whatever makes words similar or dis- similar in meaning, it must show up distributionally, in the lexical company of the word. Otherwise the suppos- edly semantic difference is not available to hearers and it is not easy to see how it may be learned. If words are similar to the extent that they occur in the similar contexts then we may de\u00a3ne a statistical re- placement test (Finch, 1993) which tests the meaning- fulness of the result of switching one word for another in a sentence. When a corpus of meaningful sentences is available the test may be reversed (Lowe, 2000a), and un- der a suitable representation of lexical context, we may hold each word constant and estimate its typical sur- rounding context. A semantic space model is a way of representing similarity of typical context in a Euclidean space with axes determined by local word co-occurrence counts. Counting the co-occurrence of a target word with a \u00a3xed set of D other words makes it possible to position the target in a space of dimension D. A target\u2019s position with respect to other words then expresses similarity of lexical context. Since the basic notion from distributional linguistics is \u2018intersubstitutability in context\u2019, a semantic space model is effective to the extent it realizes this idea accurately. Zipf\u2019s Law The frequency of a word is (approximately) proportional to the reciprocal of its rank in a frequency list (Zipf, 1949; Mandelbrot, 1954). This is Zipf\u2019s Law. Zipf\u2019s law ensures dramatically skewed distributions for almost"
            },
            "slug": "Towards-a-Theory-of-Semantic-Space-Lowe",
            "title": {
                "fragments": [],
                "text": "Towards a Theory of Semantic Space"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A theoretical framework for semantic space models is developed by synthesizing theoretical analyses from vector space information re- trieval and categorical data analysis with new basic re- search."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3199842"
                        ],
                        "name": "Barbara Rosario",
                        "slug": "Barbara-Rosario",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Rosario",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barbara Rosario"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716902"
                        ],
                        "name": "Marti A. Hearst",
                        "slug": "Marti-A.-Hearst",
                        "structuredName": {
                            "firstName": "Marti",
                            "lastName": "Hearst",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marti A. Hearst"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2912454"
                        ],
                        "name": "C. Fillmore",
                        "slug": "C.-Fillmore",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Fillmore",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Fillmore"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 348331,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "94d2efafe5fd6f47b62e6165e361888ded570e0a",
            "isKey": false,
            "numCitedBy": 91,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "In many types of technical texts, meaning is embedded in noun compounds. A language understanding program needs to be able to interpret these in order to ascertain sentence meaning. We explore the possibility of using an existing lexical hierarchy for the purpose of placing words from a noun compound into categories, and then using this category membership to determine the relation that holds between the nouns. In this paper we present the results of an analysis of this method on two-word noun compounds from the biomedical domain, obtaining classification accuracy of approximately 90%. Since lexical hierarchies are not necessarily ideally suited for this task, we also pose the question: how far down the hierarchy must the algorithm descend before all the terms within the subhierarchy behave uniformly with respect to the semantic relation in question? We find that the topmost levels of the hierarchy yield an accurate classification, thus providing an economic way of assigning relations to noun compounds."
            },
            "slug": "The-Descent-of-Hierarchy,-and-Selection-in-Rosario-Hearst",
            "title": {
                "fragments": [],
                "text": "The Descent of Hierarchy, and Selection in Relational Semantics"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper explores the possibility of using an existing lexical hierarchy for the purpose of placing words from a noun compound into categories, and then using this category membership to determine the relation that holds between the nouns."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708114"
                        ],
                        "name": "Katrin Erk",
                        "slug": "Katrin-Erk",
                        "structuredName": {
                            "firstName": "Katrin",
                            "lastName": "Erk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katrin Erk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708581"
                        ],
                        "name": "Sebastian Pad\u00f3",
                        "slug": "Sebastian-Pad\u00f3",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Pad\u00f3",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Pad\u00f3"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Deerwester et al. (1990) showed how the intuitions of Wittgenstein (1953), Harris (1954), Weaver, and Firth could be used in a practical algorithm."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 199
                            }
                        ],
                        "text": "\u2026of words (Lund & Burgess, 1996), grammatical dependencies (Lin, 1998; Pado\u0301 & Lapata, 2007), and richer contexts consisting of dependency links and selectional preferences on the argument positions (Erk & Pado\u0301, 2008); see Sahlgren\u2019s (2006) thesis for a comprehensive study of various contexts."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1588782,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cb9cc883bdd08d58feee5c7da01acff6fdb4ad78",
            "isKey": false,
            "numCitedBy": 387,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the task of computing vector space representations for the meaning of word occurrences, which can vary widely according to context. This task is a crucial step towards a robust, vector-based compositional account of sentence meaning. We argue that existing models for this task do not take syntactic structure sufficiently into account. \n \nWe present a novel structured vector space model that addresses these issues by incorporating the selectional preferences for words' argument positions. This makes it possible to integrate syntax into the computation of word meaning in context. In addition, the model performs at and above the state of the art for modeling the contextual adequacy of paraphrases."
            },
            "slug": "A-Structured-Vector-Space-Model-for-Word-Meaning-in-Erk-Pad\u00f3",
            "title": {
                "fragments": [],
                "text": "A Structured Vector Space Model for Word Meaning in Context"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel structured vector space model is presented that makes it possible to integrate syntax into the computation of word meaning in context and performs at and above the state of the art for modeling the contextual adequacy of paraphrases."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689647"
                        ],
                        "name": "Peter D. Turney",
                        "slug": "Peter-D.-Turney",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Turney",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter D. Turney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144885169"
                        ],
                        "name": "M. Littman",
                        "slug": "M.-Littman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Littman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Littman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9322367,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e02837ac075543fe1b04f3003133a6015564d443",
            "isKey": false,
            "numCitedBy": 197,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an algorithm for learning from unlabeled text, based on the Vector Space Model (VSM) of information retrieval, that can solve verbal analogy questions of the kind found in the SAT college entrance exam. A verbal analogy has the form A:B::C:D, meaning \u201cA is to B as C is to D\u201d; for example, mason:stone::carpenter:wood. SAT analogy questions provide a word pair, A:B, and the problem is to select the most analogous word pair, C:D, from a set of five choices. The VSM algorithm correctly answers 47% of a collection of 374 college-level analogy questions (random guessing would yield 20% correct; the average college-bound senior high school student answers about 57% correctly). We motivate this research by applying it to a difficult problem in natural language processing, determining semantic relations in noun-modifier pairs. The problem is to classify a noun-modifier pair, such as \u201claser printer\u201d, according to the semantic relation between the noun (printer) and the modifier (laser). We use a supervised nearest-neighbour algorithm that assigns a class to a given noun-modifier pair by finding the most analogous noun-modifier pair in the training data. With 30 classes of semantic relations, on a collection of 600 labeled noun-modifier pairs, the learning algorithm attains an F value of 26.5% (random guessing: 3.3%). With 5 classes of semantic relations, the F value is 43.2% (random: 20%). The performance is state-of-the-art for both verbal analogies and noun-modifier relations."
            },
            "slug": "Corpus-based-Learning-of-Analogies-and-Semantic-Turney-Littman",
            "title": {
                "fragments": [],
                "text": "Corpus-based Learning of Analogies and Semantic Relations"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "An algorithm for learning from unlabeled text that can solve verbal analogy questions of the kind found in the SAT college entrance exam and is state-of-the-art for both verbal analogies and noun-modifier relations is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145036961"
                        ],
                        "name": "Graeme Hirst",
                        "slug": "Graeme-Hirst",
                        "structuredName": {
                            "firstName": "Graeme",
                            "lastName": "Hirst",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Graeme Hirst"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399394776"
                        ],
                        "name": "D. St-Onge",
                        "slug": "D.-St-Onge",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "St-Onge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. St-Onge"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 168
                            }
                        ],
                        "text": "\u2026(2003) VSM system for measuring word similarity is the British National Corpus (BNC),4 whereas the main resource used in non-VSM systems for measuring word similarity (Hirst & St-Onge, 1998; Leacock & Chodrow, 1998; Jarmasz & Szpakowicz, 2003) is a lexicon, such as WordNet5 or Roget\u2019s Thesaurus."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14394781,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "be3f6b91b0eae23e37b3fb877b6cc7fc7dfcef5a",
            "isKey": false,
            "numCitedBy": 791,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Natural language utterances are, in general, highlyambiguous, and a unique interpretationcan usuallybe determined only by taking into account the constraining in\ufb02uence of the context in which theutterance occurred. Much of the research in natural language understanding in the last twenty yearscan be thought of as attempts to characterize and represent context and then derive interpretationsthat\ufb01t best with that context. Typically, this research was heavy with AI, taking context to be nothing lessthan a complete conceptual understanding of the preceding utterances. This was reasonable, as suchan understanding of a text was often the main task anyway. However, there are many text-processingtasksthatrequireonlya partialunderstandingofthetext, andhencea \u2018lighter\u2019representationofcontextis suf\ufb01cient. In this paper, we examine the idea oflexical chains as such a representation. We showhow they can be constructed by means of WordNet, and how they can be applied in one particularlinguistic task: the detection and correction of malapropisms.A malapropism is the confounding of an intended word with another word of similar sound orsimilar spelling that has a quite different and malapropos meaning, e.g., an ingenuous [for ingenious]machine forpeelingoranges. In thisexample, there isaone-letterdifference betweenthe malapropismand the correct word. Ignorance, or a simple typing mistake, might cause such errors. However, sinceingenuous is a correctly spelled word, traditional spelling checkers cannot detect this kind of mistake.In section 4, we will propose an algorithm for detecting and correcting malapropisms that is based onthe construction of lexical chains."
            },
            "slug": "Lexical-chains-as-representations-of-context-for-of-Hirst-St-Onge",
            "title": {
                "fragments": [],
                "text": "Lexical chains as representations of context for the detection and correction of malapropisms"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "How lexical chains can be constructed by means of WordNet, and how they can be applied in one particularlinguistic task: the detection and correction of malapropisms is shown."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418438"
                        ],
                        "name": "Hinrich Sch\u00fctze",
                        "slug": "Hinrich-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Hinrich",
                            "lastName": "Sch\u00fctze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hinrich Sch\u00fctze"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 0
                            }
                        ],
                        "text": "Schu\u0308tze and Pedersen (1993) defined two ways that words can be distributed in a corpus of text: If two words tend to be neighbours of each other, then they are syntagmatic associates."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1528142,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e44937535d0849412e8042f4fddca5c9dde7da3e",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces context digests, high-dimensional real-valued representations for the typical left and right contexts of a word. Initial entries for the context digests are formed from the word\u2019s close left and right neighbors. A singular value decomposition reduces the dimensionality of the space to enable subsequent efficient processing. In contrast to similar techniques, no preprocessor such as a parser is required. Context digests summarize both syntagmatic and paradigmatic relations between words: how typical they are as neighbors and how well they are substitutable for each other. We apply context digests to identifying collocations, to assessing the similarity of the arguments of different verbs, and to clustering occurrences of adjectives and verbs according to the words they modify in context."
            },
            "slug": "A-Vector-Model-for-Syntagmatic-and-Paradigmatic-Sch\u00fctze",
            "title": {
                "fragments": [],
                "text": "A Vector Model for Syntagmatic and Paradigmatic Relatedness"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "This paper introduces context digests, high-dimensional real-valued representations for the typical left and right contexts of a word that apply to identifying collocations, assessing the similarity of the arguments of different verbs, and to clustering occurrences of adjectives and verbs according to the words they modify in context."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40591835"
                        ],
                        "name": "K. Lund",
                        "slug": "K.-Lund",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Lund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Lund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50611682"
                        ],
                        "name": "C. Burgess",
                        "slug": "C.-Burgess",
                        "structuredName": {
                            "firstName": "Curt",
                            "lastName": "Burgess",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burgess"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 148
                            }
                        ],
                        "text": "\u2026Semantic Analysis (LSA) (Deerwester et al., 1990; Landauer & Dumais, 1997), Hyperspace Analogue to Language (HAL) (Lund, Burgess, & Atchley, 1995; Lund & Burgess, 1996), and related research (Landauer, McNamara, Dennis, & Kintsch, 2007) is entirely within the scope of VSMs, as defined above,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 157
                            }
                        ],
                        "text": "For the purposes of this survey, we take it as a defining property of VSMs that the values of the elements in a VSM must be derived from event frequencies, such as the number of times that a given word appears in a given context (see Section 2.6)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 146
                            }
                        ],
                        "text": "\u2026may be represented by a vector in which the elements are derived from the occurrences of the word in various contexts, such as windows of words (Lund & Burgess, 1996), grammatical dependencies (Lin, 1998; Pado\u0301 & Lapata, 2007), and richer contexts consisting of dependency links and selectional\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61090106,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c7093913b4daa0f34d9d58a41ceb0475cc3cc9f4",
            "isKey": true,
            "numCitedBy": 1721,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "A procedure that processes a corpus of text and produces numeric vectors containing information about its meanings for each word is presented. This procedure is applied to a large corpus of natural language text taken from Usenet, and the resulting vectors are examined to determine what information is contained within them. These vectors provide the coordinates in a high-dimensional space in which word relationships can be analyzed. Analyses of both vector similarity and multidimensional scaling demonstrate that there is significant semantic information carried in the vectors. A comparison of vector similarity with human reaction times in a single-word priming experiment is presented. These vectors provide the basis for a representational model of semantic memory, hyperspace analogue to language (HAL)."
            },
            "slug": "Producing-high-dimensional-semantic-spaces-from-Lund-Burgess",
            "title": {
                "fragments": [],
                "text": "Producing high-dimensional semantic spaces from lexical co-occurrence"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A procedure that processes a corpus of text and produces numeric vectors containing information about its meanings for each word, which provide the basis for a representational model of semantic memory, hyperspace analogue to language (HAL)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145948254"
                        ],
                        "name": "J. Bullinaria",
                        "slug": "J.-Bullinaria",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Bullinaria",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bullinaria"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144053576"
                        ],
                        "name": "J. Levy",
                        "slug": "J.-Levy",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Levy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Levy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Bullinaria and Levy (2007) compared these five distance measures and the cosine similarity measure on four different tasks involving word similarity."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Bullinaria and Levy (2007) demonstrated that PPMI performs better than a wide variety of other weighting approaches when measuring semantic similarity with word\u2013 context matrices."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1025306,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "f76807536e7f6542a72609929a9630de802a597f",
            "isKey": false,
            "numCitedBy": 673,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "The idea that at least some aspects of word meaning can be induced from patterns of word co-occurrence is becoming increasingly popular. However, there is less agreement about the precise computations involved, and the appropriate tests to distinguish between the various possibilities. It is important that the effect of the relevant design choices and parameter values are understood if psychological models using these methods are to be reliably evaluated and compared. In this article, we present a systematic exploration of the principal computational possibilities for formulating and validating representations of word meanings from word co-occurrence statistics. We find that, once we have identified the best procedures, a very simple approach is surprisingly successful and robust over a range of psychologically relevant evaluation measures."
            },
            "slug": "Extracting-semantic-representations-from-word-A-Bullinaria-Levy",
            "title": {
                "fragments": [],
                "text": "Extracting semantic representations from word co-occurrence statistics: A computational study"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This article presents a systematic exploration of the principal computational possibilities for formulating and validating representations of word meanings from word co-occurrence statistics and finds that, once the best procedures are identified, a very simple approach is surprisingly successful and robust over a range of psychologically relevant evaluation measures."
            },
            "venue": {
                "fragments": [],
                "text": "Behavior research methods"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145984521"
                        ],
                        "name": "Siddharth Patwardhan",
                        "slug": "Siddharth-Patwardhan",
                        "structuredName": {
                            "firstName": "Siddharth",
                            "lastName": "Patwardhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Siddharth Patwardhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2001885"
                        ],
                        "name": "Ted Pedersen",
                        "slug": "Ted-Pedersen",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Pedersen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ted Pedersen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 285,
                                "start": 101
                            }
                        ],
                        "text": "Word sense disambiguation: A typical Word Sense Disambiguation (WSD) system (Agirre & Edmonds, 2006; Pedersen, 2006) uses a feature vector representation in which each vector corresponds to a token of a word, not a type (see Section 2.6). However, Leacock, Towell, and Voorhees (1993) used a word\u2013context frequency matrix for WSD, in which each vector corresponds to a type annotated with a sense tag."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 426,
                                "start": 101
                            }
                        ],
                        "text": "Word sense disambiguation: A typical Word Sense Disambiguation (WSD) system (Agirre & Edmonds, 2006; Pedersen, 2006) uses a feature vector representation in which each vector corresponds to a token of a word, not a type (see Section 2.6). However, Leacock, Towell, and Voorhees (1993) used a word\u2013context frequency matrix for WSD, in which each vector corresponds to a type annotated with a sense tag. Yuret and Yatbaz (2009) applied a word\u2013context frequency matrix to unsupervised WSD, achieving results comparable to the performance of supervised WSD systems."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10089399,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0f10c6f57e7b640a2e89e94b5ca7d2b0d46d3925",
            "isKey": true,
            "numCitedBy": 439,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we introduce a WordNetbased measure of semantic relatedness by combining the structure and content of WordNet with co\u2013occurrence information derived from raw text. We use the co\u2013occurrence information along with the WordNet definitions to build gloss vectors corresponding to each concept in WordNet. Numeric scores of relatedness are assigned to a pair of concepts by measuring the cosine of the angle between their respective gloss vectors. We show that this measure compares favorably to other measures with respect to human judgments of semantic relatedness, and that it performs well when used in a word sense disambiguation algorithm that relies on semantic relatedness. This measure is flexible in that it can make comparisons between any two concepts without regard to their part of speech. In addition, it can be adapted to different domains, since any plain text corpus can be used to derive the co\u2013occurrence information."
            },
            "slug": "Using-WordNet-based-Context-Vectors-to-Estimate-the-Patwardhan-Pedersen",
            "title": {
                "fragments": [],
                "text": "Using WordNet-based Context Vectors to Estimate the Semantic Relatedness of Concepts"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A WordNetbased measure of semantic relatedness is introduced by combining the structure and content of WordNet with co\u2013occurrence information derived from raw text that can make comparisons between any two concepts without regard to their part of speech."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3226331"
                        ],
                        "name": "C. Leacock",
                        "slug": "C.-Leacock",
                        "structuredName": {
                            "firstName": "Claudia",
                            "lastName": "Leacock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Leacock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804836"
                        ],
                        "name": "G. Towell",
                        "slug": "G.-Towell",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Towell",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Towell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746656"
                        ],
                        "name": "E. Voorhees",
                        "slug": "E.-Voorhees",
                        "structuredName": {
                            "firstName": "Ellen",
                            "lastName": "Voorhees",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Voorhees"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2946526,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b53b6b7ffd1435c2c6a1b6684f9975b73648d131",
            "isKey": false,
            "numCitedBy": 190,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The three corpus-based statistical sense resolution methods studied here attempt to infer the correct sense of a polysemous word by using knowledge about patterns of word cooccurrences. The techniques were based on Bayesian decision theory, neural, networks, and content vectors as used in information retrieval. To understand these methods better, we posed a very specific problem: given a set of contexts, each containing the noun line in a known sense, construct a classifier that selects the correct sense of line for new contexts. To see how the degree of polysemy affects performance, results from three- and six-sense tasks are compared.The results demonstrate that each of the techniques is able to distinguish six senses of line with an accuracy greater than 70%. Furthermore, the response patterns of the classifiers are, for the most part, statistically indistinguishable from one another. Comparison of the two tasks suggests that the degree of difficulty involved in resolving individual senses is a greater performance factor than the degree of polysemy."
            },
            "slug": "Corpus-Based-Statistical-Sense-Resolution-Leacock-Towell",
            "title": {
                "fragments": [],
                "text": "Corpus-Based Statistical Sense Resolution"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "Three corpus-based statistical sense resolution methods studied here attempt to infer the correct sense of a polysemous word by using knowledge about patterns of word cooccurrences, based on Bayesian decision theory, neural, networks, and content vectors."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708581"
                        ],
                        "name": "Sebastian Pad\u00f3",
                        "slug": "Sebastian-Pad\u00f3",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Pad\u00f3",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Pad\u00f3"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747893"
                        ],
                        "name": "Mirella Lapata",
                        "slug": "Mirella-Lapata",
                        "structuredName": {
                            "firstName": "Mirella",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirella Lapata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Deerwester et al. (1990) showed how the intuitions of Wittgenstein (1953), Harris (1954), Weaver, and Firth could be used in a practical algorithm."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 149
                            }
                        ],
                        "text": "\u2026derived from the occurrences of the word in various contexts, such as windows of words (Lund & Burgess, 1996), grammatical dependencies (Lin, 1998; Pado\u0301 & Lapata, 2007), and richer contexts consisting of dependency links and selectional preferences on the argument positions (Erk & Pado\u0301, 2008);\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7747235,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "7441116c5b5a745708a9d7c5aa0ecf04e0c76c93",
            "isKey": false,
            "numCitedBy": 695,
            "numCiting": 103,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditionally, vector-based semantic space models use word co-occurrence counts from large corpora to represent lexical meaning. In this article we present a novel framework for constructing semantic spaces that takes syntactic relations into account. We introduce a formalization for this class of models, which allows linguistic knowledge to guide the construction process. We evaluate our framework on a range of tasks relevant for cognitive science and natural language processing: semantic priming, synonymy detection, and word sense disambiguation. In all cases, our framework obtains results that are comparable or superior to the state of the art."
            },
            "slug": "Dependency-Based-Construction-of-Semantic-Space-Pad\u00f3-Lapata",
            "title": {
                "fragments": [],
                "text": "Dependency-Based Construction of Semantic Space Models"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This article presents a novel framework for constructing semantic spaces that takes syntactic relations into account, and introduces a formalization for this class of models, which allows linguistic knowledge to guide the construction process."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836606"
                        ],
                        "name": "T. Landauer",
                        "slug": "T.-Landauer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Landauer",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Landauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728602"
                        ],
                        "name": "S. Dumais",
                        "slug": "S.-Dumais",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Dumais",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dumais"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 61
                            }
                        ],
                        "text": "Regarding the average score of 64.5% on the TOEFL questions, Landauer and Dumais (1997) note that, \u201cAlthough we do not know how such a performance would compare, for example, with U.S. school children of a particular age, we have been told that the average score is adequate for admission to many\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Landauer and Dumais (1997) applied truncated SVD to word similarity, achieving human-level scores on multiple-choice synonym questions from the Test of English as a Foreign Language (TOEFL)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 26
                            }
                        ],
                        "text": "High-order co-occurrence: Landauer and Dumais (1997) also describe truncated SVD as a method for discovering high-order co-occurrence."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 79
                            }
                        ],
                        "text": "In cognitive science, Latent Semantic Analysis (LSA) (Deerwester et al., 1990; Landauer & Dumais, 1997), Hyperspace Analogue to Language (HAL) (Lund, Burgess, & Atchley, 1995; Lund & Burgess, 1996), and related research (Landauer, McNamara, Dennis, & Kintsch, 2007) is entirely within the scope of\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 45
                            }
                        ],
                        "text": "Latent meaning: Deerwester et al. (1990) and Landauer and Dumais (1997) describe truncated SVD as a method for discovering latent meaning."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1144461,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68dd4b89ce1407372a29d05ca9e4e1a2e0513617",
            "isKey": true,
            "numCitedBy": 5788,
            "numCiting": 210,
            "paperAbstract": {
                "fragments": [],
                "text": "How do people know as much as they do with as little information as they get? The problem takes many forms; learning vocabulary from text is an especially dramatic and convenient case for research. A new general theory of acquired similarity and knowledge representation, latent semantic analysis (LSA), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena. By inducing global knowledge indirectly from local co-occurrence data in a large body of representative text, LSA acquired knowledge about the full vocabulary of English at a comparable rate to schoolchildren. LSA uses no prior linguistic or perceptual similarity knowledge; it is based solely on a general mathematical learning method that achieves powerful inductive effects by extracting the right number of dimensions (e.g., 300) to represent objects and contexts. Relations to other theories, phenomena, and problems are sketched."
            },
            "slug": "A-Solution-to-Plato's-Problem:-The-Latent-Semantic-Landauer-Dumais",
            "title": {
                "fragments": [],
                "text": "A Solution to Plato's Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge."
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A new general theory of acquired similarity and knowledge representation, latent semantic analysis (LSA), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7465342"
                        ],
                        "name": "Ido Dagan",
                        "slug": "Ido-Dagan",
                        "structuredName": {
                            "firstName": "Ido",
                            "lastName": "Dagan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ido Dagan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145810617"
                        ],
                        "name": "Lillian Lee",
                        "slug": "Lillian-Lee",
                        "structuredName": {
                            "firstName": "Lillian",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lillian Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1473,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d37dff2d8e65764e7293750051d519359d8835d",
            "isKey": false,
            "numCitedBy": 405,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "In many applications of natural language processing (NLP) it is necessary to determine the likelihood of a given word combination. For example, a speech recognizer may need to determine which of the two word combinations \u201ceat a peach\u201d and \u201deat a beach\u201d is more likely. Statistical NLP methods determine the likelihood of a word combination from its frequency in a training corpus. However, the nature of language is such that many word combinations are infrequent and do not occur in any given corpus. In this work we propose a method for estimating the probability of such previously unseen word combinations using available information on \u201cmost similar\u201d words.We describe probabilistic word association models based on distributional word similarity, and apply them to two tasks, language modeling and pseudo-word disambiguation. In the language modeling task, a similarity-based model is used to improve probability estimates for unseen bigrams in a back-off language model. The similarity-based method yields a 20% perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech-recognition error.We also compare four similarity-based estimation methods against back-off and maximum-likelihood estimation methods on a pseudo-word sense disambiguation task in which we controlled for both unigram and bigram frequency to avoid giving too much weight to easy-to-disambiguate high-frequency configurations. The similarity-based methods perform up to 40% better on this particular task."
            },
            "slug": "Similarity-Based-Models-of-Word-Cooccurrence-Dagan-Lee",
            "title": {
                "fragments": [],
                "text": "Similarity-Based Models of Word Cooccurrence Probabilities"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes a method for estimating the probability of such previously unseen word combinations using available information on \u201cmost similar\u201d words, and describes probabilistic word association models based on distributional word similarity, and applies them to two tasks, language modeling and pseudo-word disambiguation."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777660"
                        ],
                        "name": "Naftali Tishby",
                        "slug": "Naftali-Tishby",
                        "structuredName": {
                            "firstName": "Naftali",
                            "lastName": "Tishby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naftali Tishby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145810617"
                        ],
                        "name": "Lillian Lee",
                        "slug": "Lillian-Lee",
                        "structuredName": {
                            "firstName": "Lillian",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lillian Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6713452,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5eb328cf7e94995199e4c82a1f4d0696430a80b5",
            "isKey": false,
            "numCitedBy": 1193,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts. Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering. Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership. In many cases, the clusters can be thought of as encoding coarse sense distinctions. Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical \"soft\" clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data."
            },
            "slug": "Distributional-Clustering-of-English-Words-Pereira-Tishby",
            "title": {
                "fragments": [],
                "text": "Distributional Clustering of English Words"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Deterministic annealing is used to find lowest distortion sets of clusters: as the annealed parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical \"soft\" clustering of the data."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836606"
                        ],
                        "name": "T. Landauer",
                        "slug": "T.-Landauer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Landauer",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Landauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801516"
                        ],
                        "name": "D. McNamara",
                        "slug": "D.-McNamara",
                        "structuredName": {
                            "firstName": "Danielle",
                            "lastName": "McNamara",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. McNamara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50582151"
                        ],
                        "name": "S. Dennis",
                        "slug": "S.-Dennis",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Dennis",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dennis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3025325"
                        ],
                        "name": "W. Kintsch",
                        "slug": "W.-Kintsch",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Kintsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Kintsch"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 140
                            }
                        ],
                        "text": "This emphasis on event frequencies brings unity to the variety of VSMs and explicitly connects them to the distributional hypothesis; furthermore, it avoids triviality by excluding many possible matrix representations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 46
                            }
                        ],
                        "text": "Existing summaries omit pair\u2013pattern matrices (Landauer et al., 2007)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 141
                            }
                        ],
                        "text": "Section 2 explains our framework for organizing the literature on VSMs according to the type of matrix involved: term\u2013document, word\u2013context, and pair\u2013pattern."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 185
                            }
                        ],
                        "text": "Cognitive scientists have argued that there are empirical and theoretical reasons for believing that VSMs, such as LSA and HAL, are plausible models of some aspects of human cognition (Landauer et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 50
                            }
                        ],
                        "text": "Existing overviews focus on cognitive psychology (Landauer et al., 2007)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 158
                            }
                        ],
                        "text": "Our goal in writing this paper has been to survey the state of the art in vector space models of semantics, to introduce the topic to those who are new to the area, and to give a new perspective to those who are already familiar with the area."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 58530979,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e181167dc005da9e19f7ff16b4707c828d043dfb",
            "isKey": true,
            "numCitedBy": 1171,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Contents: Part I: Introduction to LSA: Theory and Methods. T.K. Landauer, LSA as a Theory of Meaning. D. Martin, M. Berry, Mathematical Foundations Behind Latent Semantic Analysis. S. Dennis, How to Use the LSA Website. J. Quesada, Creating Your Own LSA Spaces. Part II: LSA in Cognitive Theory. W. Kintsch, Meaning in Context. M. Louwerse, Symbolic or Embodied Representations: A Case for Symbol Interdependency. M.W. Howard, K. Addis, B. Jing, M.K. Kahana, Semantic Structure and Episodic Memory. G. DenhiSre, B. Lemaire, C. Bellissens, S. Jhean-Larose, A Semantic Space for Modeling Children's Semantic Memory. P. Foltz, Discourse Coherence and LSA. J. Quesada, Spaces for Problem Solving. Part III: LSA in Educational Applications. K. Millis, J. Magliano, K. Wiemer-Hastings, S. Todaro, D.S. McNamara, Assessing and Improving Comprehension With Latent Semantic Analysis. D.S. McNamara, C. Boonthum, I. Levinstein, K. Millis, Evaluating Self-Explanations in iSTART: Comparing Word-Based and LSA Algorithms. A. Graesser, P. Penumatsa, M. Ventura, Z. Cai, X. Hu, Using LSA in AutoTutor: Learning Through Mixed-Initiative Dialog in Natural Language. E. Kintsch, D. Caccamise, M. Franzke, N. Johnson, S. Dooley, Summary Streetr: Computer-Guided Summary Writing. L. Streeter, K. Lochbaum, N. LaVoie, J.E. Psotka, Automated Tools for Collaborative Learning Environments. Part IV: Information Retrieval and HCI Applications of LSA. S.T. Dumais, LSA and Information Retrieval: Getting Back to Basics. P.K. Foltz, T.K. Landauer, Helping People Find and Learn From Documents: Exploiting Synergies Between Human and Computer Retrieval With SuperManual. M.H. Blackmon, M. Kitajima, D.R. Mandalia, P.G. Polson, Automating Usability Evaluation Cognitive Walkthrough for the Web Puts LSA to Work on Real-World HCI Design Problems. Part V: Extensions to LSA. D.S. McNamara, Z. Cai, M.M. Louwerse, Optimizing LSA Measures of Cohesion. X. Hu, Z. Cai, P. Wiemer-Hastings, A.C. Graesser, D.S. McNamara, Strength, Weakness, and Extensions of LSA. M. Steyvers, T. Griffiths, Probabilistic Topic Models. S. Dennis, Introducing Word Order: Within the LSA Framework. Part VI: Conclusion. W. Kintsch, D.S. McNamara, S. Dennis, T.K. Landauer, LSA and Meaning: In Theory and Application."
            },
            "slug": "Handbook-of-latent-semantic-analysis-Landauer-McNamara",
            "title": {
                "fragments": [],
                "text": "Handbook of latent semantic analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This book discusses Latent Semantic Analysis as a Theory of Meaning, its application in Cognitive Theory, and its applications in Educational Applications."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689109"
                        ],
                        "name": "Magnus Sahlgren",
                        "slug": "Magnus-Sahlgren",
                        "structuredName": {
                            "firstName": "Magnus",
                            "lastName": "Sahlgren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Magnus Sahlgren"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 132
                            }
                        ],
                        "text": "The idea of random projection is to take high-dimensional vectors and randomly project them into a relatively low-dimensional space (Sahlgren, 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 16
                            }
                        ],
                        "text": "Random indexing (Sahlgren, 2005) or incremental SVD (Brand, 2006) may help to address these scaling issues."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17228581,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3ef1b0469b43acc8ede2e56d8f001ad090b04826",
            "isKey": false,
            "numCitedBy": 500,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Word space models enjoy considerable attention in current research on semantic indexing. Most notably, Latent Semantic Analysis/Indexing (LSA/LSI; Deerwester et al., 1990, Landauer & Dumais, 1997) has become a household name in information access research, and deservedly so; LSA has proven its mettle in numerous applications, and has more or less spawned an entire research field since its introduction around 1990. Today, there is a rich flora of word space models available, and there are numerous publications that report exceptional results in many different applications, including information retrieval (Dumais et al., 1988), word sense disambiguation (Schutze, 1993), various semantic knowledge tests (Lund et al., 1995, Karlgren & Sahlgren, 2001), and text categorization (Sahlgren & Karlgren, 2004). This paper introduces the Random Indexing word space approach, which presents an efficient, scalable and incremental alternative to standard word space methods. The paper is organized as follows: in the next section, we review the basic word space methodology. We then look at some of the problems that are inherent in the basic methodology, and also review some of the solutions that have been proposed in the literature. In the final section, we introduce the Random Indexing word space approach, and briefly review some of the experimental results that have been achieved with Random Indexing."
            },
            "slug": "An-Introduction-to-Random-Indexing-Sahlgren",
            "title": {
                "fragments": [],
                "text": "An Introduction to Random Indexing"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The Random Indexing word space approach is introduced, which presents an efficient, scalable and incremental alternative to standard word space methods."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32780943"
                        ],
                        "name": "M. Jarmasz",
                        "slug": "M.-Jarmasz",
                        "structuredName": {
                            "firstName": "Mario",
                            "lastName": "Jarmasz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Jarmasz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66114341"
                        ],
                        "name": "Stanialaw Szpakowicz",
                        "slug": "Stanialaw-Szpakowicz",
                        "structuredName": {
                            "firstName": "Stanialaw",
                            "lastName": "Szpakowicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stanialaw Szpakowicz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 242,
                                "start": 216
                            }
                        ],
                        "text": "\u2026(2003) VSM system for measuring word similarity is the British National Corpus (BNC),4 whereas the main resource used in non-VSM systems for measuring word similarity (Hirst & St-Onge, 1998; Leacock & Chodrow, 1998; Jarmasz & Szpakowicz, 2003) is a lexicon, such as WordNet5 or Roget\u2019s Thesaurus."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1189582,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a67f083830790586ed41823d45a7b330d0a2fd95",
            "isKey": false,
            "numCitedBy": 315,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We have implemented a system that measures semantic similarity using a computerized 1987 Roget's Thesaurus, and evaluated it by performing a few typical tests. We compare the results of these tests with those produced by WordNet-based similarity measures. One of the benchmarks is Miller and Charles' list of 30 noun pairs to which human judges had assigned similarity measures. We correlate these measures with those computed by several NLP systems. The 30 pairs can be traced back to Rubenstein and Goodenough's 65 pairs, which we have also studied. Our Roget's-based system gets correlations of .878 for the smaller and .818 for the larger list of noun pairs; this is quite close to the .885 that Resnik obtained when he employed humans to replicate the Miller and Charles experiment. We further evaluate our measure by using Roget's and WordNet to answer 80 TOEFL, 50 ESL and 300 Reader's Digest questions: the correct synonym must be selected amongst a group of four words. Our system gets 78.75%, 82.00% and 74.33% of the questions respectively."
            },
            "slug": "Roget's-thesaurus-and-semantic-similarity-Jarmasz-Szpakowicz",
            "title": {
                "fragments": [],
                "text": "Roget's thesaurus and semantic similarity"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "A system that measures semantic similarity using a computerized 1987 Roget's Thesaurus, and evaluated it by performing a few typical tests, comparing the results with those produced by WordNet-based similarity measures."
            },
            "venue": {
                "fragments": [],
                "text": "RANLP"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689647"
                        ],
                        "name": "Peter D. Turney",
                        "slug": "Peter-D.-Turney",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Turney",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter D. Turney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 34
                            }
                        ],
                        "text": "The idea of the VSM is to represent each document in a collection as a point in a space (a vector in a vector space)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Turney (2006, 2008a) presented experimental evidence that relational similarity does not reduce to attributional similarity."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 174
                            }
                        ],
                        "text": "For example, dog and wolf have a relatively high degree of attributional similarity, whereas dog : bark and cat :meow have a relatively high degree of relational similarity (Turney, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 109
                            }
                        ],
                        "text": "The leading algorithms for measuring the similarity of semantic relations also use VSMs (Lin & Pantel, 2001; Turney, 2006; Nakov & Hearst, 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 147
                            }
                        ],
                        "text": "\u2026of 92.5% on multiple-choice synonym questions from the Test of English as a Foreign Language (TOEFL), whereas the average human score was 64.5%.1 Turney (2006) used a vector-based representation of semantic relations to attain a score of 56% on multiple-choice analogy questions from the SAT\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 178
                            }
                        ],
                        "text": "Two words are semantically related if they have any kind of semantic relation (Budanitsky & Hirst, 2001); they are semantically related to the degree that they share attributes (Turney, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2468783,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "212d2715aee9fbefe140685b088b789d6c8277b0",
            "isKey": false,
            "numCitedBy": 416,
            "numCiting": 121,
            "paperAbstract": {
                "fragments": [],
                "text": "There are at least two kinds of similarity. Relational similarity is correspondence between relations, in contrast with attributional similarity, which is correspondence between attributes. When two words have a high degree of attributional similarity, we call them synonyms. When two pairs of words have a high degree of relational similarity, we say that their relations are analogous. For example, the word pair mason:stone is analogous to the pair carpenter:wood. This article introduces Latent Relational Analysis (LRA), a method for measuring relational similarity. LRA has potential applications in many areas, including information extraction, word sense disambiguation, and information retrieval. Recently the Vector Space Model (VSM) of information retrieval has been adapted to measuring relational similarity, achieving a score of 47% on a collection of 374 college-level multiple-choice word analogy questions. In the VSM approach, the relation between a pair of words is characterized by a vector of frequencies of predefined patterns in a large corpus. LRA extends the VSM approach in three ways: (1) The patterns are derived automatically from the corpus, (2) the Singular Value Decomposition (SVD) is used to smooth the frequency data, and (3) automatically generated synonyms are used to explore variations of the word pairs. LRA achieves 56% on the 374 analogy questions, statistically equivalent to the average human score of 57%. On the related problem of classifying semantic relations, LRA achieves similar gains over the VSM."
            },
            "slug": "Similarity-of-Semantic-Relations-Turney",
            "title": {
                "fragments": [],
                "text": "Similarity of Semantic Relations"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "LRA extends the VSM approach in three ways: the patterns are derived automatically from the corpus, the Singular Value Decomposition (SVD) is used to smooth the frequency data, and automatically generated synonyms are used to explore variations of the word pairs."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153584694"
                        ],
                        "name": "R. Rapp",
                        "slug": "R.-Rapp",
                        "structuredName": {
                            "firstName": "Reinhard",
                            "lastName": "Rapp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rapp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 17
                            }
                        ],
                        "text": "Noise reduction: Rapp (2003) describes truncated SVD as a noise reduction technique."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 89
                            }
                        ],
                        "text": "The leading algorithms for measuring semantic relatedness use VSMs (Pantel & Lin, 2002a; Rapp, 2003; Turney, Littman, Bigham, & Shnayder, 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 14
                            }
                        ],
                        "text": "For instance, Rapp (2003) used a vector-based representation of word meaning to achieve a score of 92.5% on multiple-choice synonym questions from the Test of English as a Foreign Language (TOEFL), whereas the average human score was 64.5%.1 Turney (2006) used a vector-based representation of\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 130
                            }
                        ],
                        "text": "The VSM was developed for the SMART information retrieval system (Salton, 1971) by Gerard Salton and his colleagues (Salton, Wong, & Yang, 1975)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1171753,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b62fce4fc3476e36545acff0a4d0627326959a53",
            "isKey": true,
            "numCitedBy": 141,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "In machine translation, information on word ambiguities is usually provided by the lexicographers who construct the lexicon. In this paper we propose an automatic method for word sense induction, i.e. for the discovery of a set of sense descriptors to a given ambiguous word. The approach is based on the statistics of the distributional similarity between the words in a corpus. Our algorithm works as follows: The 20 strongest first-order associations to the ambiguous word are considered as sense descriptor candidates. All pairs of these candidates are ranked according to the following two criteria: First, the two words in a pair should be as dissimilar as possible. Second, although being dissimilar their co-occurrence vectors should add up to the co-occurrence vector of the ambiguous word scaled by two. Both conditions together have the effect that preference is given to pairs whose co-occurring words are complementary. For best results, our implementation uses singular value decomposition, entropy-based weights, and second-order similarity metrics."
            },
            "slug": "Word-sense-discovery-based-on-sense-descriptor-Rapp",
            "title": {
                "fragments": [],
                "text": "Word sense discovery based on sense descriptor dissimilarity"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper proposes an automatic method for word sense induction, i.e. for the discovery of a set of sense descriptors to a given ambiguous word, based on the statistics of the distributional similarity between the words in a corpus."
            },
            "venue": {
                "fragments": [],
                "text": "MTSUMMIT"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689647"
                        ],
                        "name": "Peter D. Turney",
                        "slug": "Peter-D.-Turney",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Turney",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter D. Turney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Turney (2006, 2008a) presented experimental evidence that relational similarity does not reduce to attributional similarity."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 129
                            }
                        ],
                        "text": "The latent relation hypothesis is that pairs of words that co-occur in similar patterns tend to have similar semantic relations (Turney, 2008a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 68
                            }
                        ],
                        "text": "For example, a :b :c could be decomposed into a :b, a :c, and b :c (Turney, 2008a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Turney (2008a) applied PPMI to pair\u2013pattern matrices."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7898033,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4b88329392a75287942d85f42012f47f356a2714",
            "isKey": false,
            "numCitedBy": 214,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Recognizing analogies, synonyms, antonyms, and associations appear to be four distinct tasks, requiring distinct NLP algorithms. In the past, the four tasks have been treated independently, using a wide variety of algorithms. These four semantic classes, however, are a tiny sample of the full range of semantic phenomena, and we cannot afford to create ad hoc algorithms for each semantic phenomenon; we need to seek a unified approach. We propose to subsume a broad range of phenomena under analogies. To limit the scope of this paper, we restrict our attention to the subsumption of synonyms, antonyms, and associations. We introduce a supervised corpus-based machine learning algorithm for classifying analogous word pairs, and we show that it can solve multiple-choice SAT analogy questions, TOEFL synonym questions, ESL synonym-antonym questions, and similar-associated-both questions from cognitive psychology."
            },
            "slug": "A-Uniform-Approach-to-Analogies,-Synonyms,-and-Turney",
            "title": {
                "fragments": [],
                "text": "A Uniform Approach to Analogies, Synonyms, Antonyms, and Associations"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A supervised corpus-based machine learning algorithm is introduced for classifying analogous word pairs and it is shown that it can solve multiple-choice SAT analogy questions, TOEFL synonyms questions, ESL synonym-antonym questions, and similar-associated-both questions from cognitive psychology."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683562"
                        ],
                        "name": "Preslav Nakov",
                        "slug": "Preslav-Nakov",
                        "structuredName": {
                            "firstName": "Preslav",
                            "lastName": "Nakov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Preslav Nakov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716902"
                        ],
                        "name": "Marti A. Hearst",
                        "slug": "Marti-A.-Hearst",
                        "structuredName": {
                            "firstName": "Marti",
                            "lastName": "Hearst",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marti A. Hearst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 48
                            }
                        ],
                        "text": "The idea of the VSM is to represent each document in a collection as a point in a space (a vector in a vector space)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 123
                            }
                        ],
                        "text": "The leading algorithms for measuring the similarity of semantic relations also use VSMs (Lin & Pantel, 2001; Turney, 2006; Nakov & Hearst, 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 682154,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "0303c9d91954673f653c517aa516de88baf402d7",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a simple linguistically-motivated method for characterizing the semantic relations that hold between two nouns. The approach leverages the vast size of the Web in order to build lexically-specific features. The main idea is to look for verbs, prepositions, and coordinating conjunctions that can help make explicit the hidden relations between the target nouns. Using these features in instance-based classifiers, we demonstrate state-of-the-art results on various relational similarity problems, including mapping noun-modifier pairs to abstract relations like TIME, LOCATION and CONTAINER, characterizing noun-noun compounds in terms of abstract linguistic predicates like CAUSE, USE, and FROM, classifying the relations between nominals in context, and solving SAT verbal analogy problems. In essence, the approach puts together some existing ideas, showing that they apply generally to various semantic tasks, finding that verbs are especially useful features."
            },
            "slug": "Solving-Relational-Similarity-Problems-Using-the-as-Nakov-Hearst",
            "title": {
                "fragments": [],
                "text": "Solving Relational Similarity Problems Using the Web as a Corpus"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The main idea is to look for verbs, prepositions, and coordinating conjunctions that can help make explicit the hidden relations between the target nouns."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733049"
                        ],
                        "name": "Eneko Agirre",
                        "slug": "Eneko-Agirre",
                        "structuredName": {
                            "firstName": "Eneko",
                            "lastName": "Agirre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eneko Agirre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145660941"
                        ],
                        "name": "P. Edmonds",
                        "slug": "P.-Edmonds",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Edmonds",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Edmonds"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 38
                            }
                        ],
                        "text": "Extended distributional hypothesis: Patterns that co-occur with similar pairs tend to have similar meanings (Lin & Pantel, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 108
                            }
                        ],
                        "text": "In applications dealing with polysemy, one approach uses vectors that represent word tokens (Schu\u0308tze, 1998; Agirre & Edmonds, 2006) and another uses vectors that represent word types (Pantel & Lin, 2002a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 203662857,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3a1e5d8f9f8b31d673ba96c17526e8296b957ba2",
            "isKey": false,
            "numCitedBy": 488,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "This is the first book to cover the entire topic of word sense disambiguation (WSD) including: all the major algorithms, techniques, performance measures, results, philosophical issues, and applications. Leading researchers in the field have contributed chapters that synthesize and provide an overview of past and state-of-the-art research across the field. The editors have carefully organized the chapters into sub-topics. Researchers and lecturers will learn about the full range of what has been done and where the field is headed. Developers will learn which technique(s) will apply to their particular application, how to build and evaluate systems, and what performance to expect. An accompanying Website provides links to resources for WSD and a searchable index of the book."
            },
            "slug": "Word-Sense-Disambiguation:-Algorithms-and-Agirre-Edmonds",
            "title": {
                "fragments": [],
                "text": "Word Sense Disambiguation: Algorithms and Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "This is the first book to cover the entire topic of word sense disambiguation including: all the major algorithms, techniques, performance measures, results, philosophical issues, and applications."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695574"
                        ],
                        "name": "T. Veale",
                        "slug": "T.-Veale",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Veale",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Veale"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 40
                            }
                        ],
                        "text": "The best non-VSM algorithm achieves 43% (Veale, 2004)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 29271823,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e46c547dcbf65d26e85ff723bae34067cceb3477",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "One can measure the extent to which a knowledge-base enables intelligent or creative behavior by determining how useful such a knowledge-base is to the solution of standard psychometric or scholastic tests. In this paper we consider the utility of WordNet, a comprehensive lexical knowledge-base of English word meanings, to the solution of S.A.T. analogies. We propose that these analogies test a student's ability to recognize and estimate a measure of pairwise analogical similarity, and describe an algorithmic formulation of this measure using the structure of WordNet. We report that the knowledge-based approach yields a precision at least equal to that of statistical machine-learning approaches."
            },
            "slug": "WordNet-Sits-the-S.A.T.-A-Knowledge-Based-Approach-Veale",
            "title": {
                "fragments": [],
                "text": "WordNet Sits the S.A.T. - A Knowledge-Based Approach to Lexical Analogy"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper considers the utility of WordNet, a comprehensive lexical knowledge-base of English word meanings, to the solution of S.A.T. analogies, and proposes that these analogies test a student's ability to recognize and estimate a measure of pairwise analogical similarity."
            },
            "venue": {
                "fragments": [],
                "text": "ECAI"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689647"
                        ],
                        "name": "Peter D. Turney",
                        "slug": "Peter-D.-Turney",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Turney",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter D. Turney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Turney (2006, 2008a) presented experimental evidence that relational similarity does not reduce to attributional similarity."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 129
                            }
                        ],
                        "text": "The latent relation hypothesis is that pairs of words that co-occur in similar patterns tend to have similar semantic relations (Turney, 2008a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 68
                            }
                        ],
                        "text": "For example, a :b :c could be decomposed into a :b, a :c, and b :c (Turney, 2008a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Turney (2008a) applied PPMI to pair\u2013pattern matrices."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7112602,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f11b6506cb9b9c24cbdf3fdd0122ed12c6c72f67",
            "isKey": false,
            "numCitedBy": 114,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "Many AI researchers and cognitive scientists have argued that analogy is the core of cognition. The most influential work on computational modeling of analogy-making is Structure Mapping Theory (SMT) and its implementation in the Structure Mapping Engine (SME). A limitation of SME is the requirement for complex hand-coded representations. We introduce the Latent Relation Mapping Engine (LRME), which combines ideas from SME and Latent Relational Analysis (LRA) in order to remove the requirement for hand-coded representations. LRME builds analogical mappings between lists of words, using a large corpus of raw text to automatically discover the semantic relations among the words. We evaluate LRME on a set of twenty analogical mapping problems, ten based on scientific analogies and ten based on common metaphors. LRME achieves human-level performance on the twenty problems. We compare LRME with a variety of alternative approaches and find that they are not able to reach the same level of performance."
            },
            "slug": "The-Latent-Relation-Mapping-Engine:-Algorithm-and-Turney",
            "title": {
                "fragments": [],
                "text": "The Latent Relation Mapping Engine: Algorithm and Experiments"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The Latent Relation Mapping Engine (LRME), which combines ideas from SME andLatent Relational Analysis (LRA) in order to remove the requirement for hand-coded representations, achieves human-level performance on a set of twenty analogical mapping problems."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144523372"
                        ],
                        "name": "S. Clark",
                        "slug": "S.-Clark",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50419262"
                        ],
                        "name": "S. Pulman",
                        "slug": "S.-Pulman",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pulman",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Pulman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Clark and Pulman (2007) assigned distributional meaning to sentences using the Hilbert space tensor product."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 585,
                                "start": 0
                            }
                        ],
                        "text": "Clark and Pulman (2007) assigned distributional meaning to sentences using the Hilbert space tensor product. Widdows and Ferraro (2008), inspired by quantum mechanics, explores several operators for modeling composition of meaning. Pair\u2013pattern matrices are sensitive to the order of the words in a pair (Turney, 2006). Thus there are several ways to handle word order in VSMs. This raises the question, what are the limits of VSMs for semantics? Can all semantics be represented with VSMs? There is much that we do not yet know how to represent with VSMs. For example, Widdows (2004) and van Rijsbergen (2004) show how disjunction, conjunction, and negation can be represented with vectors, but we do not yet know how to represent arbitrary statements in first-order predicate calculus."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 0
                            }
                        ],
                        "text": "Clark and Pulman (2007) assigned distributional meaning to sentences using the Hilbert space tensor product. Widdows and Ferraro (2008), inspired by quantum mechanics, explores several operators for modeling composition of meaning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2280191,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "73e897104540642698321c106cc9c35af369fe12",
            "isKey": true,
            "numCitedBy": 144,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The are two main approaches to the representation of meaning in Computational Linguistics: a symbolic approach and a distributional approach. This paper considers the fundamental question of how these approaches might be combined. The proposal is to adapt a method from the Cognitive Science literature, in which symbolic and connectionist representations are combined using tensor products. Possible applications of this method for language processing are described. Finally, a potentially fruitful link between Quantum Mechanics, Computational Linguistics, and other related areas such as Information Retrieval and Machine Learning, is proposed."
            },
            "slug": "Combining-Symbolic-and-Distributional-Models-of-Clark-Pulman",
            "title": {
                "fragments": [],
                "text": "Combining Symbolic and Distributional Models of Meaning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A method is to be adapted from the Cognitive Science literature, in which symbolic and connectionist representations are combined using tensor products, to adapt a method for language processing."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI Spring Symposium: Quantum Interaction"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721801"
                        ],
                        "name": "C. Fellbaum",
                        "slug": "C.-Fellbaum",
                        "structuredName": {
                            "firstName": "Christiane",
                            "lastName": "Fellbaum",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Fellbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 102
                            }
                        ],
                        "text": "Automatic thesaurus generation: WordNet is a popular tool for research in natural language processing (Fellbaum, 1998), but creating and maintaing such lexical resources"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5958691,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d87ceda3042f781c341ac17109d1e94a717f5f60",
            "isKey": false,
            "numCitedBy": 13574,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Part 1 The lexical database: nouns in WordNet, George A. Miller modifiers in WordNet, Katherine J. Miller a semantic network of English verbs, Christiane Fellbaum design and implementation of the WordNet lexical database and searching software, Randee I. Tengi. Part 2: automated discovery of WordNet relations, Marti A. Hearst representing verb alterations in WordNet, Karen T. Kohl et al the formalization of WordNet by methods of relational concept analysis, Uta E. Priss. Part 3 Applications of WordNet: building semantic concordances, Shari Landes et al performance and confidence in a semantic annotation task, Christiane Fellbaum et al WordNet and class-based probabilities, Philip Resnik combining local context and WordNet similarity for word sense identification, Claudia Leacock and Martin Chodorow using WordNet for text retrieval, Ellen M. Voorhees lexical chains as representations of context for the detection and correction of malapropisms, Graeme Hirst and David St-Onge temporal indexing through lexical chaining, Reem Al-Halimi and Rick Kazman COLOR-X - using knowledge from WordNet for conceptual modelling, J.F.M. Burg and R.P. van de Riet knowledge processing on an extended WordNet, Sanda M. Harabagiu and Dan I Moldovan appendix - obtaining and using WordNet."
            },
            "slug": "WordNet-:-an-electronic-lexical-database-Fellbaum",
            "title": {
                "fragments": [],
                "text": "WordNet : an electronic lexical database"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "The lexical database: nouns in WordNet, Katherine J. Miller a semantic network of English verbs, and applications of WordNet: building semantic concordances are presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689647"
                        ],
                        "name": "Peter D. Turney",
                        "slug": "Peter-D.-Turney",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Turney",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter D. Turney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "verbs into semantic classes. For example, taint:poison is classi\ufb01ed as strength (poisoning is stronger than tainting) and assess:review is classi\ufb01ed as enablement (assessing is enabled by reviewing). Turney (2005) used a pair\u2013pattern matrix to classify noun compounds into semantic classes. For example, \ufb02u virus is classi\ufb01ed as cause (the virus causes the \ufb02u), home town is classi\ufb01ed as location (the home is loc"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5104622,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0565c6903f86e2c649c326ee7607ecc5ccef3a0f",
            "isKey": false,
            "numCitedBy": 185,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces Latent Relational Analysis (LRA), a method for measuring semantic similarity. LRA measures similarity in the semantic relations between two pairs of words. When two pairs have a high degree of relational similarity, they are analogous. For example, the pair cat:meow is analogous to the pair dog:bark. There is evidence from cognitive science that relational similarity is fundamental to many cognitive and linguistic tasks (e.g., analogical reasoning). In the Vector Space Model (VSM) approach to measuring relational similarity, the similarity between two pairs is calculated by the cosine of the angle between the vectors that represent the two pairs. The elements in the vectors are based on the frequencies of manually constructed patterns in a large corpus. LRA extends the VSM approach in three ways: (1) patterns are derived automatically from the corpus, (2) Singular Value Decomposition is used to smooth the frequency data, and (3) synonyms are used to reformulate word pairs. This paper describes the LRA algorithm and experimentally compares LRA to VSM on two tasks, answering college-level multiple-choice word analogy questions and classifying semantic relations in noun-modifier expressions. LRA achieves state-of-the-art results, reaching human-level performance on the analogy questions and significantly exceeding VSM performance on both tasks."
            },
            "slug": "Measuring-Semantic-Similarity-by-Latent-Relational-Turney",
            "title": {
                "fragments": [],
                "text": "Measuring Semantic Similarity by Latent Relational Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The LRA algorithm is described and state-of-the-art results are achieved, reaching human-level performance on the analogy questions and significantly exceeding VSM performance on both tasks."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36768914"
                        ],
                        "name": "C. Barr",
                        "slug": "C.-Barr",
                        "structuredName": {
                            "firstName": "Cory",
                            "lastName": "Barr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Barr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144494993"
                        ],
                        "name": "R. Jones",
                        "slug": "R.-Jones",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2370843"
                        ],
                        "name": "M. Regelson",
                        "slug": "M.-Regelson",
                        "structuredName": {
                            "firstName": "Moira",
                            "lastName": "Regelson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Regelson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15917407,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db797990da35f778dae2e364da3c765ea55ad215",
            "isKey": false,
            "numCitedBy": 123,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Web-search queries are known to be short, but little else is known about their structure. In this paper we investigate the applicability of part-of-speech tagging to typical English-language web search-engine queries and the potential value of these tags for improving search results. We begin by identifying a set of part-of-speech tags suitable for search queries and quantifying their occurrence. We find that proper-nouns constitute 40% of query terms, and proper nouns and nouns together constitute over 70% of query terms. We also show that the majority of queries are noun-phrases, not unstructured collections of terms. We then use a set of queries manually labeled with these tags to train a Brill tagger and evaluate its performance. In addition, we investigate classification of search queries into grammatical classes based on the syntax of part-of-speech tag sequences. We also conduct preliminary investigative experiments into the practical applicability of leveraging query-trained part-of-speech taggers for information-retrieval tasks. In particular, we show that part-of-speech information can be a significant feature in machine-learned search-result relevance. These experiments also include the potential use of the tagger in selecting words for omission or substitution in query reformulation, actions which can improve recall. We conclude that training a part-of-speech tagger on labeled corpora of queries significantly outperforms taggers based on traditional corpora, and leveraging the unique linguistic structure of web-search queries can improve search experience."
            },
            "slug": "The-Linguistic-Structure-of-English-Web-Search-Barr-Jones",
            "title": {
                "fragments": [],
                "text": "The Linguistic Structure of English Web-Search Queries"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is concluded that training a part-of-speech tagger on labeled corpora of queries significantly outperforms taggers based on traditional corpora, and leveraging the unique linguistic structure of web-search queries can improve search experience."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143880621"
                        ],
                        "name": "Saif M. Mohammad",
                        "slug": "Saif-M.-Mohammad",
                        "structuredName": {
                            "firstName": "Saif",
                            "lastName": "Mohammad",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Saif M. Mohammad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145036961"
                        ],
                        "name": "Graeme Hirst",
                        "slug": "Graeme-Hirst",
                        "structuredName": {
                            "firstName": "Graeme",
                            "lastName": "Hirst",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Graeme Hirst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2103596,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "437d1a445185bdd8287c9431b188e9caabd80a24",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a framework to derive the distance between concepts from distributional measures of word co-occurrences. We use the categories in a published thesaurus as coarse-grained concepts, allowing all possible distance values to be stored in a concept--concept matrix roughly .01% the size of that created by existing measures. We show that the newly proposed concept-distance measures outperform traditional distributional word-distance measures in the tasks of (1) ranking word pairs in order of semantic distance, and (2) correcting real-word spelling errors. In the latter task, of all the WordNet-based measures, only that proposed by Jiang and Conrath outperforms the best distributional concept-distance measures."
            },
            "slug": "Distributional-measures-of-concept-distance:-A-Mohammad-Hirst",
            "title": {
                "fragments": [],
                "text": "Distributional measures of concept-distance: A task-oriented evaluation"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "This work proposes a framework to derive the distance between concepts from distributional measures of word co-occurrences, using the categories in a published thesaurus as coarse-grained concepts, and shows that the newly proposed concept-distance measures outperform traditional distributional word- distance measures in the tasks of ranking word pairs in order of semantic distance and correcting real-word spelling errors."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708425"
                        ],
                        "name": "Jay J. Jiang",
                        "slug": "Jay-J.-Jiang",
                        "structuredName": {
                            "firstName": "Jay",
                            "lastName": "Jiang",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jay J. Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075147"
                        ],
                        "name": "D. Conrath",
                        "slug": "D.-Conrath",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Conrath",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Conrath"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1359050,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b64e068a8face2540fc436af40dbcd2b0912bbf",
            "isKey": false,
            "numCitedBy": 3339,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new approach for measuring semantic similarity/distance between words and concepts. It combines a lexical taxonomy structure with corpus statistical information so that the semantic distance between nodes in the semantic space constructed by the taxonomy can be better quantified with the computational evidence derived from a distributional analysis of corpus data. Specifically, the proposed measure is a combined approach that inherits the edge-based approach of the edge counting scheme, which is then enhanced by the node-based approach of the information content calculation. When tested on a common data set of word pair similarity ratings, the proposed approach outperforms other computational models. It gives the highest correlation value (r = 0.828) with a benchmark based on human similarity judgements, whereas an upper bound (r = 0.885) is observed when human subjects replicate the same task."
            },
            "slug": "Semantic-Similarity-Based-on-Corpus-Statistics-and-Jiang-Conrath",
            "title": {
                "fragments": [],
                "text": "Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "This paper presents a new approach for measuring semantic similarity/distance between words and concepts that combines a lexical taxonomy structure with corpus statistical information so that the semantic distance between nodes in the semantic space constructed by the taxonomy can be better quantified with the computational evidence derived from a distributional analysis of corpus data."
            },
            "venue": {
                "fragments": [],
                "text": "ROCLING/IJCLCLP"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144621026"
                        ],
                        "name": "R. Snow",
                        "slug": "R.-Snow",
                        "structuredName": {
                            "firstName": "Rion",
                            "lastName": "Snow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Snow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746807"
                        ],
                        "name": "Dan Jurafsky",
                        "slug": "Dan-Jurafsky",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Jurafsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan Jurafsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14680675,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93bb6228776eafa606965e21f229d548de1998eb",
            "isKey": false,
            "numCitedBy": 540,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel algorithm for inducing semantic taxonomies. Previous algorithms for taxonomy induction have typically focused on independent classifiers for discovering new single relationships based on hand-constructed or automatically discovered textual patterns. By contrast, our algorithm flexibly incorporates evidence from multiple classifiers over heterogenous relationships to optimize the entire structure of the taxonomy, using knowledge of a word's coordinate terms to help in determining its hypernyms, and vice versa. We apply our algorithm on the problem of sense-disambiguated noun hyponym acquisition, where we combine the predictions of hypernym and coordinate term classifiers with the knowledge in a preexisting semantic taxonomy (WordNet 2.1). We add 10,000 novel synsets to WordNet 2.1 at 84% precision, a relative error reduction of 70% over a non-joint algorithm using the same component classifiers. Finally, we show that a taxonomy built using our algorithm shows a 23% relative F-score improvement over WordNet 2.1 on an independent testset of hypernym pairs."
            },
            "slug": "Semantic-Taxonomy-Induction-from-Heterogenous-Snow-Jurafsky",
            "title": {
                "fragments": [],
                "text": "Semantic Taxonomy Induction from Heterogenous Evidence"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This work proposes a novel algorithm for inducing semantic taxonomies that flexibly incorporates evidence from multiple classifiers over heterogenous relationships to optimize the entire structure of the taxonomy, using knowledge of a word's coordinate terms to help in determining its hypernyms, and vice versa."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689647"
                        ],
                        "name": "Peter D. Turney",
                        "slug": "Peter-D.-Turney",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Turney",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter D. Turney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 15
                            }
                        ],
                        "text": "A variation of PMI is Positive PMI (PPMI), in which all PMI values that are less than zero are replaced with zero (Niwa & Nitta, 1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 24
                            }
                        ],
                        "text": "Turney (2008a) applied PPMI to pair\u2013pattern matrices."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 51
                            }
                        ],
                        "text": "By keeping only the context-word dimensions with a PMI above a conservative threshold and setting the others to zero, Lin (1998) showed that the number of comparisons needed to compare vectors greatly decreases while losing little precision in the similarity score between the top-200 most similar words of every word."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 24
                            }
                        ],
                        "text": "A well-known problem of PMI is that it is biased towards infrequent events."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 60
                            }
                        ],
                        "text": "For a detailed description of the LRA algorithm, we suggest Turney\u2019s (2006) paper."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 46
                            }
                        ],
                        "text": "Bullinaria and Levy (2007) demonstrated that PPMI performs better than a wide variety of other weighting approaches when measuring semantic similarity with word\u2013 context matrices."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 39
                            }
                        ],
                        "text": "We will give the formal definition of PPMI here, as an example of an effective weighting function."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 43
                            }
                        ],
                        "text": "Thus Laplace smoothing reduces the bias of PMI towards infrequent events."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 1
                            }
                        ],
                        "text": "PPMI is designed to give a high value to xij when there is an interesting semantic relation between\nwi and cj ; otherwise, xij should have a value of zero, indicating that the occurrence of wi in cj is uninformative."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 85
                            }
                        ],
                        "text": "In our experience, pmiij appears to be approximately Gaussian, which may explain why PMI works well\nwith truncated SVD, but then PPMI is puzzling, because it is less Gaussian than PMI, yet it apparently yields better semantic models than PMI.\nx = \u3008x1, x2, . . . , xn\u3009 (8) y = \u3008y1, y2, . . . , yn\u3009 (9)\nThe cosine of the angle \u03b8 between x and y can be calculated as follows:\ncos(x,y) =\n\u2211n i=1 xi \u00b7 yi\n\u221a\n\u2211n i=1 x 2 i \u00b7 \u2211n i=1 y 2 i\n(10)\n= x \u00b7 y\u221a\nx \u00b7 x \u00b7 \u221ay \u00b7 y (11)\n= x \u2016x\u2016 \u00b7 y \u2016y\u2016 (12)\nIn other words, the cosine of the angle between two vectors is the inner product of the vectors, after they have been normalized to unit length."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "Hence (4) becomes log (1/pi\u2217) and PMI increases as the probability of word wi decreases."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 1
                            }
                        ],
                        "text": "PPMI weighting does not yield negative elements, but truncated SVD can generate negative elements, even when the input matrix has no negative values."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 86
                            }
                        ],
                        "text": "An alternative to tf-idf is Pointwise Mutual Information (PMI) (Church & Hanks, 1989; Turney, 2001), which works well for both word\u2013context matrices (Pantel & Lin, 2002a) and term\u2013document matrices (Pantel & Lin, 2002b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 39
                            }
                        ],
                        "text": "Let X be the matrix that results when PPMI is applied to F."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5509836,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e517e1645708e7b050787bb4734002ea194a1958",
            "isKey": false,
            "numCitedBy": 1474,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a simple unsupervised learning algorithm for recognizing synonyms, based on statistical data acquired by querying a Web search engine. The algorithm, called PMI-IR, uses Pointwise Mutual Information (PMI) and Information Retrieval (IR) to measure the similarity of pairs of words. PMI-IR is empirically evaluated using 80 synonym test questions from the Test of English as a Foreign Language (TOEFL) and 50 synonym test questions from a collection of tests for students of English as a Second Language (ESL). On both tests, the algorithm obtains a score of 74%. PMI-IR is contrasted with Latent Semantic Analysis (LSA), which achieves a score of 64% on the same 80 TOEFL questions. The paper discusses potential applications of the new unsupervised learning algorithm and some implications of the results for LSA and LSI (Latent Semantic Indexing)."
            },
            "slug": "Mining-the-Web-for-Synonyms:-PMI-IR-versus-LSA-on-Turney",
            "title": {
                "fragments": [],
                "text": "Mining the Web for Synonyms: PMI-IR versus LSA on TOEFL"
            },
            "venue": {
                "fragments": [],
                "text": "ECML"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66109618"
                        ],
                        "name": "J. B. Lovins",
                        "slug": "J.-B.-Lovins",
                        "structuredName": {
                            "firstName": "Julie",
                            "lastName": "Lovins",
                            "middleNames": [
                                "Beth"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. B. Lovins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 162
                            }
                        ],
                        "text": "In English, affixes are simpler and more regular than in many other languages, and stemming algorithms based on heuristics (rules of thumb) work relatively well (Lovins, 1968; Porter, 1980; Minnen, Carroll, & Pearce, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16628689,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b3853f08c482fe1bfbe39d656d50a8c73976f3c",
            "isKey": false,
            "numCitedBy": 1209,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "A stemming algorithm, a procedure to reduce all words with the same stem to a common form, is useful in many areas of computational linguistics and information-retrieval work. While the form of the algorithm varies with its application, certain linguistic problems are common to any stemming procedure. As a basis for evaluation of previous attempts to deal with these problems, this paper first discusses the theoretical and practical attributes of stemming algorithms. Then a new version of a context-sensitive, longest-match stemming algorithm for English is proposed; though developed for use in a library information transfer system, it is of general application. A major linguistic problem in stemming, variation in spelling of stems, is discussed in some detail and several feasible programmed solutions are outlined, along with sample results of one of these methods."
            },
            "slug": "Development-of-a-stemming-algorithm-Lovins",
            "title": {
                "fragments": [],
                "text": "Development of a stemming algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new version of a context-sensitive, longest-match stemming algorithm for English is proposed; though developed for use in a library information transfer system, it is of general application."
            },
            "venue": {
                "fragments": [],
                "text": "Mech. Transl. Comput. Linguistics"
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39952106"
                        ],
                        "name": "Michael B. W. Wolfe",
                        "slug": "Michael-B.-W.-Wolfe",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Wolfe",
                            "middleNames": [
                                "B.",
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael B. W. Wolfe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144029655"
                        ],
                        "name": "M. E. Schreiner",
                        "slug": "M.-E.-Schreiner",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Schreiner",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. E. Schreiner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2197285"
                        ],
                        "name": "B. Rehder",
                        "slug": "B.-Rehder",
                        "structuredName": {
                            "firstName": "Bob",
                            "lastName": "Rehder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Rehder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2444937"
                        ],
                        "name": "Darrell Laham",
                        "slug": "Darrell-Laham",
                        "structuredName": {
                            "firstName": "Darrell",
                            "lastName": "Laham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Darrell Laham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2036336"
                        ],
                        "name": "P. Foltz",
                        "slug": "P.-Foltz",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Foltz",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Foltz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3025325"
                        ],
                        "name": "W. Kintsch",
                        "slug": "W.-Kintsch",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Kintsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Kintsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836606"
                        ],
                        "name": "T. Landauer",
                        "slug": "T.-Landauer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Landauer",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Landauer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62526953,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0cb6eb8776d372f76be62c95252162086b46eff",
            "isKey": false,
            "numCitedBy": 249,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "This study examines the hypothesis that the ability of a reader to learn from text depends on the match between the background knowledge of the reader and the difficulty of the text information. Latent Semantic Analysis (LSA), a statistical technique that represents the content of a document as a vector in high\u2010dimensional semantic space based on a large text corpus, is used to predict how much readers will learn from texts based on the estimated conceptual match between their topic knowledge and the text information. Participants completed tests to assess their knowledge of the human heart and circulatory system, then read one of four texts that ranged in difficulty from elementary to medical school level, then completed the tests again. Results show a nonmonotonic relation in which learning was greatest for texts that were neither too easy nor too difficult. LSA proved as effective at predicting learning from these texts as traditional knowledge assessment measures. For these texts, optimal assignment o..."
            },
            "slug": "Learning-from-text:-Matching-readers-and-texts-by-Wolfe-Schreiner",
            "title": {
                "fragments": [],
                "text": "Learning from text: Matching readers and texts by latent semantic analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Results show a nonmonotonic relation in which learning was greatest for texts that were neither too easy nor too difficult, and LSA proved as effective at predicting learning from these texts as traditional knowledge assessment measures."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2808366"
                        ],
                        "name": "Deniz Yuret",
                        "slug": "Deniz-Yuret",
                        "structuredName": {
                            "firstName": "Deniz",
                            "lastName": "Yuret",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Deniz Yuret"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308772"
                        ],
                        "name": "Mehmet Ali Yatbaz",
                        "slug": "Mehmet-Ali-Yatbaz",
                        "structuredName": {
                            "firstName": "Mehmet",
                            "lastName": "Yatbaz",
                            "middleNames": [
                                "Ali"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mehmet Ali Yatbaz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3143876,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8856db6abd1c472911b8ac31158cedf4815fac70",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract We introduce a generative probabilistic model, the noisy channel model, for unsupervised word sense disambiguation. In our model, each context C is modeled as a distinct channel through which the speaker intends to transmit a particular meaning S using a possibly ambiguous word W. To reconstruct the intended meaning the hearer uses the distribution of possible meanings in the given context P(S|C) and possible words that can express each meaning P(W|S). We assume P(W|S) is independent of the context and estimate it using WordNet sense frequencies. The main problem of unsupervised WSD is estimating context-dependent P(S|C) without access to any sense-tagged text. We show one way to solve this problem using a statistical language model based on large amounts of untagged text. Our model uses coarse-grained semantic classes for S internally and we explore the effect of using different levels of granularity on WSD performance. The system outputs fine-grained senses for evaluation, and its performance on noun disambiguation is better than most previously reported unsupervised systems and close to the best supervised systems."
            },
            "slug": "The-Noisy-Channel-Model-for-Unsupervised-Word-Sense-Yuret-Yatbaz",
            "title": {
                "fragments": [],
                "text": "The Noisy Channel Model for Unsupervised Word Sense Disambiguation"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This model uses coarse-grained semantic classes for S internally and the effect of using different levels of granularity on WSD performance is explored, and its performance on noun disambiguation is better than most previously reported unsupervised systems and close to the best supervised systems."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1990190"
                        ],
                        "name": "P. Pantel",
                        "slug": "P.-Pantel",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Pantel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pantel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 184
                            }
                        ],
                        "text": "In applications dealing with polysemy, one approach uses vectors that represent word tokens (Schu\u0308tze, 1998; Agirre & Edmonds, 2006) and another uses vectors that represent word types (Pantel & Lin, 2002a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 13
                            }
                        ],
                        "text": "For example, Pantel and Lin (2002a) presented an algorithm that can discover word senses by clustering row vectors in a word\u2013context matrix, using contextual information derived from parsing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 20
                            }
                        ],
                        "text": "An example follows (Pantel & Lin, 2002a):\n\u03b4ij = fij fij + 1 \u00b7 min (\n\u2211nr k=1 fkj, \u2211nc k=1 fik)\nmin ( \u2211nr k=1 fkj, \u2211nc k=1 fik) + 1 (6)\nnewpmiij = \u03b4ij \u00b7 pmiij (7)\nAnother way to deal with infrequent events is Laplace smoothing of the probability estimates, pij , pi\u2217, and p\u2217j (Turney & Littman, 2003)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 109
                            }
                        ],
                        "text": "The VSM was developed for the SMART information retrieval system (Salton, 1971) by Gerard Salton and his colleagues (Salton, Wong, & Yang, 1975)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 149
                            }
                        ],
                        "text": "An alternative to tf-idf is Pointwise Mutual Information (PMI) (Church & Hanks, 1989; Turney, 2001), which works well for both word\u2013context matrices (Pantel & Lin, 2002a) and term\u2013document matrices (Pantel & Lin, 2002b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 68
                            }
                        ],
                        "text": "The leading algorithms for measuring semantic relatedness use VSMs (Pantel & Lin, 2002a; Rapp, 2003; Turney, Littman, Bigham, & Shnayder, 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1529624,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3317f2788b2b07d9ba4cb4335e29316fcf8a971a",
            "isKey": false,
            "numCitedBy": 736,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Inventories of manually compiled dictionaries usually serve as a source for word senses. However, they often include many rare senses while missing corpus/domain-specific senses. We present a clustering algorithm called CBC (Clustering By Committee) that automatically discovers word senses from text. It initially discovers a set of tight clusters called committees that are well scattered in the similarity space. The centroid of the members of a committee is used as the feature vector of the cluster. We proceed by assigning words to their most similar clusters. After assigning an element to a cluster, we remove their overlapping features from the element. This allows CBC to discover the less frequent senses of a word and to avoid discovering duplicate senses. Each cluster that a word belongs to represents one of its senses. We also present an evaluation methodology for automatically measuring the precision and recall of discovered senses."
            },
            "slug": "Discovering-word-senses-from-text-Pantel-Lin",
            "title": {
                "fragments": [],
                "text": "Discovering word senses from text"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A clustering algorithm called CBC (Clustering By Committee) that automatically discovers word senses from text that initially discovers a set of tight clusters called committees that are well scattered in the similarity space."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145165877"
                        ],
                        "name": "P. Hanks",
                        "slug": "P.-Hanks",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Hanks",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hanks"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 15
                            }
                        ],
                        "text": "A variation of PMI is Positive PMI (PPMI), in which all PMI values that are less than zero are replaced with zero (Niwa & Nitta, 1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 24
                            }
                        ],
                        "text": "Turney (2008a) applied PPMI to pair\u2013pattern matrices."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 51
                            }
                        ],
                        "text": "By keeping only the context-word dimensions with a PMI above a conservative threshold and setting the others to zero, Lin (1998) showed that the number of comparisons needed to compare vectors greatly decreases while losing little precision in the similarity score between the top-200 most similar words of every word."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 24
                            }
                        ],
                        "text": "A well-known problem of PMI is that it is biased towards infrequent events."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 46
                            }
                        ],
                        "text": "Bullinaria and Levy (2007) demonstrated that PPMI performs better than a wide variety of other weighting approaches when measuring semantic similarity with word\u2013 context matrices."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 39
                            }
                        ],
                        "text": "We will give the formal definition of PPMI here, as an example of an effective weighting function."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 43
                            }
                        ],
                        "text": "Thus Laplace smoothing reduces the bias of PMI towards infrequent events."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 1
                            }
                        ],
                        "text": "PPMI is designed to give a high value to xij when there is an interesting semantic relation between\nwi and cj ; otherwise, xij should have a value of zero, indicating that the occurrence of wi in cj is uninformative."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 85
                            }
                        ],
                        "text": "In our experience, pmiij appears to be approximately Gaussian, which may explain why PMI works well\nwith truncated SVD, but then PPMI is puzzling, because it is less Gaussian than PMI, yet it apparently yields better semantic models than PMI.\nx = \u3008x1, x2, . . . , xn\u3009 (8) y = \u3008y1, y2, . . . , yn\u3009 (9)\nThe cosine of the angle \u03b8 between x and y can be calculated as follows:\ncos(x,y) =\n\u2211n i=1 xi \u00b7 yi\n\u221a\n\u2211n i=1 x 2 i \u00b7 \u2211n i=1 y 2 i\n(10)\n= x \u00b7 y\u221a\nx \u00b7 x \u00b7 \u221ay \u00b7 y (11)\n= x \u2016x\u2016 \u00b7 y \u2016y\u2016 (12)\nIn other words, the cosine of the angle between two vectors is the inner product of the vectors, after they have been normalized to unit length."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "Hence (4) becomes log (1/pi\u2217) and PMI increases as the probability of word wi decreases."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 1
                            }
                        ],
                        "text": "PPMI weighting does not yield negative elements, but truncated SVD can generate negative elements, even when the input matrix has no negative values."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 64
                            }
                        ],
                        "text": "An alternative to tf-idf is Pointwise Mutual Information (PMI) (Church & Hanks, 1989; Turney, 2001), which works well for both word\u2013context matrices (Pantel & Lin, 2002a) and term\u2013document matrices (Pantel & Lin, 2002b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 39
                            }
                        ],
                        "text": "Let X be the matrix that results when PPMI is applied to F."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9558665,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "9e2caa39ac534744a180972a30a320ad0ae41ea3",
            "isKey": true,
            "numCitedBy": 4363,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "The term word association is used in a very particular sense in the psycholinguistic literature. (Generally speaking, subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor. ) We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntactic co-occurrence constraints between verbs and prepositions (content word/function word). This paper will propose an objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora. (The standard method of obtaining word association norms, testing a few thousand subjects on a few hundred words, is both costly and unreliable.) The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, making it possible to estimate norms for tens of thousands of words."
            },
            "slug": "Word-Association-Norms,-Mutual-Information-and-Church-Hanks",
            "title": {
                "fragments": [],
                "text": "Word Association Norms, Mutual Information and Lexicography"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, making it possible to estimate norms for tens of thousands of words."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418438"
                        ],
                        "name": "Hinrich Sch\u00fctze",
                        "slug": "Hinrich-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Hinrich",
                            "lastName": "Sch\u00fctze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hinrich Sch\u00fctze"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8754851,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3cd9fd8a36c8feb74bb20ae25817edb9c6a0518c",
            "isKey": false,
            "numCitedBy": 1401,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents context-group discrimination, a disambiguation algorithm based on clustering. Senses are interpreted as groups (or clusters) of similar contexts of the ambiguous word. Words, contexts, and senses are represented in Word Space, a high-dimensional, real-valued space in which closeness corresponds to semantic similarity. Similarity in Word Space is based on second-order co-occurrence: two tokens (or contexts) of the ambiguous word are assigned to the same sense cluster if the words they co-occur with in turn occur with similar words in a training corpus. The algorithm is automatic and unsupervised in both training and application: senses are induced from a corpus without labeled training instances or other external knowledge sources. The paper demonstrates good performance of context-group discrimination for a sample of natural and artificial ambiguous words."
            },
            "slug": "Automatic-Word-Sense-Discrimination-Sch\u00fctze",
            "title": {
                "fragments": [],
                "text": "Automatic Word Sense Discrimination"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This paper presents context-group discrimination, a disambiguation algorithm based on clustering that demonstrates good performance of context- group discrimination for a sample of natural and artificial ambiguous words."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33406957"
                        ],
                        "name": "D. Davidov",
                        "slug": "D.-Davidov",
                        "structuredName": {
                            "firstName": "Dmitry",
                            "lastName": "Davidov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Davidov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145009917"
                        ],
                        "name": "A. Rappoport",
                        "slug": "A.-Rappoport",
                        "structuredName": {
                            "firstName": "Ari",
                            "lastName": "Rappoport",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Rappoport"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8078270,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "49fae1d85dc435e45dc118247cfa5d75eb197b30",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel framework for the discovery and representation of general semantic relationships that hold between lexical items. We propose that each such relationship can be identified with a cluster of patterns that captures this relationship. We give a fully unsupervised algorithm for pattern cluster discovery, which searches, clusters and merges highfrequency words-based patterns around randomly selected hook words. Pattern clusters can be used to extract instances of the corresponding relationships. To assess the quality of discovered relationships, we use the pattern clusters to automatically generate SAT analogy questions. We also compare to a set of known relationships, achieving very good results in both methods. The evaluation (done in both English and Russian) substantiates the premise that our pattern clusters indeed reflect relationships perceived by humans."
            },
            "slug": "Unsupervised-Discovery-of-Generic-Relationships-and-Davidov-Rappoport",
            "title": {
                "fragments": [],
                "text": "Unsupervised Discovery of Generic Relationships Using Pattern Clusters and its Evaluation by Automatically Generated SAT Analogy Questions"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A fully unsupervised algorithm for pattern cluster discovery, which searches, clusters and merges highfrequency words-based patterns around randomly selected hook words, substantiates the premise that the pattern clusters indeed reflect relationships perceived by humans."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689647"
                        ],
                        "name": "Peter D. Turney",
                        "slug": "Peter-D.-Turney",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Turney",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter D. Turney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144885169"
                        ],
                        "name": "M. Littman",
                        "slug": "M.-Littman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Littman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Littman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 285,
                                "start": 263
                            }
                        ],
                        "text": "An example follows (Pantel & Lin, 2002a):\n\u03b4ij = fij fij + 1 \u00b7 min (\n\u2211nr k=1 fkj, \u2211nc k=1 fik)\nmin ( \u2211nr k=1 fkj, \u2211nc k=1 fik) + 1 (6)\nnewpmiij = \u03b4ij \u00b7 pmiij (7)\nAnother way to deal with infrequent events is Laplace smoothing of the probability estimates, pij , pi\u2217, and p\u2217j (Turney & Littman, 2003)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 142
                            }
                        ],
                        "text": "The VSM was developed for the SMART information retrieval system (Salton, 1971) by Gerard Salton and his colleagues (Salton, Wong, & Yang, 1975)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2024,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5c8d05e27e36ebd64ee43fe1670262cdcc2123ba",
            "isKey": false,
            "numCitedBy": 1674,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "The evaluative character of a word is called its semantic orientation. Positive semantic orientation indicates praise (e.g., \"honest\", \"intrepid\") and negative semantic orientation indicates criticism (e.g., \"disturbing\", \"superfluous\"). Semantic orientation varies in both direction (positive or negative) and degree (mild to strong). An automated system for measuring semantic orientation would have application in text classification, text filtering, tracking opinions in online discussions, analysis of survey responses, and automated chat systems (chatbots). This article introduces a method for inferring the semantic orientation of a word from its statistical association with a set of positive and negative paradigm words. Two instances of this approach are evaluated, based on two different statistical measures of word association: pointwise mutual information (PMI) and latent semantic analysis (LSA). The method is experimentally tested with 3,596 words (including adjectives, adverbs, nouns, and verbs) that have been manually labeled positive (1,614 words) and negative (1,982 words). The method attains an accuracy of 82.8% on the full test set, but the accuracy rises above 95% when the algorithm is allowed to abstain from classifying mild words."
            },
            "slug": "Measuring-praise-and-criticism:-Inference-of-from-Turney-Littman",
            "title": {
                "fragments": [],
                "text": "Measuring praise and criticism: Inference of semantic orientation from association"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This article introduces a method for inferring the semantic orientation of a word from its statistical association with a set of positive and negative paradigm words, based on two different statistical measures of word association."
            },
            "venue": {
                "fragments": [],
                "text": "TOIS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2256003"
                        ],
                        "name": "Vivi Nastase",
                        "slug": "Vivi-Nastase",
                        "structuredName": {
                            "firstName": "Vivi",
                            "lastName": "Nastase",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vivi Nastase"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400620585"
                        ],
                        "name": "J. Sayyad-Shirabad",
                        "slug": "J.-Sayyad-Shirabad",
                        "structuredName": {
                            "firstName": "Jelber",
                            "lastName": "Sayyad-Shirabad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sayyad-Shirabad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145368345"
                        ],
                        "name": "Marina Sokolova",
                        "slug": "Marina-Sokolova",
                        "structuredName": {
                            "firstName": "Marina",
                            "lastName": "Sokolova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marina Sokolova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66114340"
                        ],
                        "name": "S. Szpakowicz",
                        "slug": "S.-Szpakowicz",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Szpakowicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Szpakowicz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6583037,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "35471fa1234fb7f7dc9586df0c5b23371f806a04",
            "isKey": false,
            "numCitedBy": 87,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the performance of two representations of word meaning in learning noun-modifier semantic relations. One representation is based on lexical resources, in particular WordNet, the other - on a corpus. We experimented with decision trees, instance-based learning and Support Vector Machines. All these methods work well in this learning task. We report high precision, recall and F-score, and small variation in performance across several 10-fold cross-validation runs. The corpus-based method has the advantage of working with data without word-sense annotations and performs well over the baseline. The WordNet-based method, requiring word-sense annotated data, has higher precision."
            },
            "slug": "Learning-Noun-Modifier-Semantic-Relations-with-and-Nastase-Sayyad-Shirabad",
            "title": {
                "fragments": [],
                "text": "Learning Noun-Modifier Semantic Relations with Corpus-based and WordNet-based Features"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "The performance of two representations of word meaning in learning noun-modifier semantic relations are studied, one based on lexical resources, in particular WordNet, and the other - on a corpus."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689647"
                        ],
                        "name": "Peter D. Turney",
                        "slug": "Peter-D.-Turney",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Turney",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter D. Turney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144885169"
                        ],
                        "name": "M. Littman",
                        "slug": "M.-Littman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Littman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Littman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744846"
                        ],
                        "name": "Jeffrey P. Bigham",
                        "slug": "Jeffrey-P.-Bigham",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Bigham",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeffrey P. Bigham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2541093"
                        ],
                        "name": "V. Shnayder",
                        "slug": "V.-Shnayder",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Shnayder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Shnayder"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 118
                            }
                        ],
                        "text": "Latent relation hypothesis: Pairs of words that co-occur in similar patterns tend to have similar semantic relations (Turney et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 94
                            }
                        ],
                        "text": "Several approaches to measuring the semantic similarity of words combine a VSM with a lexicon (Turney et al., 2003; Pantel, 2005; Patwardhan & Pedersen, 2006; Mohammad & Hirst, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 155
                            }
                        ],
                        "text": "However, the reduction is often a good approximation, and there is some evidence that a hybrid approach, combining a VSM with a lexicon, can be beneficial (Turney et al., 2003; Nastase, Sayyad-Shirabad, Sokolova, & Szpakowicz, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Turney et al. (2003) introduced the use of the pair\u2013pattern matrix for measuring the semantic similarity of relations between word pairs; that is, the similarity of row vectors."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29074,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fced0c675eff3e8a9b0c18c169aa87c0d30695e3",
            "isKey": false,
            "numCitedBy": 158,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Existing statistical approaches to natural language problems are very coarse approximations to the true complexity of language processing. As such, no single technique will be best for all problem instances. Many researchers are examining ensemble methods that combine the output of successful, separately developed modules to create more accurate solutions. This paper examines three merging rules for combining probability distributions: the well known mixture rule, the logarithmic rule, and a novel product rule. These rules were applied with state-of-the-art results to two problems commonly used to assess human mastery of lexical semantics|synonym questions and analogy questions. All three merging rules result in ensembles that are more accurate than any of their component modules. The dierences among the three rules are not statistically signicant, but it is suggestive that the popular mixture rule is not the best rule for either of the two problems."
            },
            "slug": "Combining-Independent-Modules-to-Solve-Synonym-and-Turney-Littman",
            "title": {
                "fragments": [],
                "text": "Combining Independent Modules to Solve Multiple-choice Synonym and Analogy Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Three merging rules for combining probability distributions are examined: the well known mixture rule, the logarithmic rule, and a novel product rule that were applied with state-of-the-art results to two problems commonly used to assess human mastery of lexical semantics|synonym questions and analogy questions."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2095398"
                        ],
                        "name": "Alexander Budanitsky",
                        "slug": "Alexander-Budanitsky",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Budanitsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Budanitsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145036961"
                        ],
                        "name": "Graeme Hirst",
                        "slug": "Graeme-Hirst",
                        "structuredName": {
                            "firstName": "Graeme",
                            "lastName": "Hirst",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Graeme Hirst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 838777,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a32b3d027064798fb31ce42894fec31e834f7db",
            "isKey": false,
            "numCitedBy": 1554,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": "The quantification of lexical semantic relatedness has many applications in NLP, and many different measures have been proposed. We evaluate five of these measures, all of which use WordNet as their central resource, by comparing their performance in detecting and correcting real-word spelling errors. An information-content-based measure proposed by Jiang and Conrath is found superior to those proposed by Hirst and St-Onge, Leacock and Chodorow, Lin, and Resnik. In addition, we explain why distributional similarity is not an adequate proxy for lexical semantic relatedness."
            },
            "slug": "Evaluating-WordNet-based-Measures-of-Lexical-Budanitsky-Hirst",
            "title": {
                "fragments": [],
                "text": "Evaluating WordNet-based Measures of Lexical Semantic Relatedness"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An information-content-based measure proposed by Jiang and Conrath is found superior to those proposed by Hirst and St-Onge, Leacock and Chodorow, Lin, and Resnik, and why distributional similarity is not an adequate proxy for lexical semantic relatedness."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708581"
                        ],
                        "name": "Sebastian Pad\u00f3",
                        "slug": "Sebastian-Pad\u00f3",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Pad\u00f3",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Pad\u00f3"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747893"
                        ],
                        "name": "Mirella Lapata",
                        "slug": "Mirella-Lapata",
                        "structuredName": {
                            "firstName": "Mirella",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirella Lapata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5139774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b1a3b5a20e77d8ac94967dc48173c48af3012eaf",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditional vector-based models use word co-occurrence counts from large corpora to represent lexical meaning. In this paper we present a novel approach for constructing semantic spaces that takes syntactic relations into account. We introduce a formalisation for this class of models and evaluate their adequacy on two modelling tasks: semantic priming and automatic discrimination of lexical relations."
            },
            "slug": "Constructing-Semantic-Space-Models-from-Parsed-Pad\u00f3-Lapata",
            "title": {
                "fragments": [],
                "text": "Constructing Semantic Space Models from Parsed Corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel approach for constructing semantic spaces that takes syntactic relations into account is presented, which is a formalisation for this class of models and their adequacy on two modelling tasks is evaluated."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "118049359"
                        ],
                        "name": "W. Jones",
                        "slug": "W.-Jones",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Jones",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2737579"
                        ],
                        "name": "G. Furnas",
                        "slug": "G.-Furnas",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Furnas",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Furnas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 163
                            }
                        ],
                        "text": "\u2026similarity by inversion (13) or subtraction (14).\nsim(x,y) = 1/dist(x,y) (13) sim(x,y) = 1\u2212 dist(x,y) (14)\nMany similarity measures have been proposed in both IR (Jones & Furnas, 1987) and lexical semantics circles (Lin, 1998; Dagan, Lee, & Pereira, 1999; Lee, 1999; Weeds, Weir, & McCarthy, 2004)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9135369,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3a720f42ae2e68d1a6f5fe19f8170532e0caef73",
            "isKey": false,
            "numCitedBy": 293,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We want computer systems that can help us assess the similarity or relevance of existing objects (e.g., documents, functions, commands, etc.) to a statement of our current needs (e.g., the query). Towards this end, a variety of similarity measures have been proposed. However, the relationship between a measure's formula and its performance is not always obvious. A geometric analysis is advanced and its utility demonstrated through its application to six conventional information retrieval similarity measures and a seventh spreading activation measure. All seven similarity measures work with a representational scheme wherein a query and the database objects are represented as vectors of term weights. A geometric analysis characterizes each similarity measure by the nature of its iso\u2010similarity contours in an n\u2010space containing query and object vectors. This analysis reveals important differences among the similarity measures and suggests conditions in which these differences will affect retrieval performance. The cosine coefficient, for example, is shown to be insensitive to between\u2010document differences in the magnitude of term weights while the inner product measure is sometimes overly affected by such differences. The context\u2010sensitive spreading activation measure may overcome both of these limitations and deserves further study. The geometric analysis is intended to complement, and perhaps to guide, the empirical analysis of similarity measures. \u00a9 1987 John Wiley & Sons, Inc."
            },
            "slug": "Pictures-of-relevance:-A-geometric-analysis-of-Jones-Furnas",
            "title": {
                "fragments": [],
                "text": "Pictures of relevance: A geometric analysis of similarity measures"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A geometric analysis is advanced and its utility demonstrated through its application to six conventional information retrieval similarity measures and a seventh spreading activation measure, intended to complement, and perhaps to guide, the empirical analysis of similarity measures."
            },
            "venue": {
                "fragments": [],
                "text": "J. Am. Soc. Inf. Sci."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5737624"
                        ],
                        "name": "E. Rosch",
                        "slug": "E.-Rosch",
                        "structuredName": {
                            "firstName": "Eleanor",
                            "lastName": "Rosch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Rosch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145190958"
                        ],
                        "name": "B. Lloyd",
                        "slug": "B.-Lloyd",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Lloyd",
                            "middleNames": [
                                "Bloom"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Lloyd"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 100
                            }
                        ],
                        "text": "The basic idea of prototype theory is that some members of a category are more central than others (Rosch & Lloyd, 1978; Lakoff, 1987)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 116
                            }
                        ],
                        "text": "We find VSMs especially interesting due to their relation with the distributional hypothesis and related hypotheses (see Section 2.7)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16680251,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "44600addb5fe0f2ca925b318b72732b724ed558b",
            "isKey": true,
            "numCitedBy": 2801,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "ion of those listed by the students? In general, we found that the event name itself combined most readily with superordinate noun categories; thus, one gets dressed with clothes and needs various kitchen utensils to make breakfast. When such activities were analyzed into their script elements, the basic level appeared as the level of abstraction of objects necessary to script the events; e.g., in getting dressed, one puts on pants, sweater, and shoes, and in making breakfast, one cooks eggs in a frying pan. With respect to prototypes, it appears to be those category members judged the more prototypical that have attributes that enable them to fit into the typical and agreed upon script elements. We are presently collecting normative data on the intersection of common events, the objects associated with those events and the other sets of events associated with those objects.2 In addition, object names for eliciting events are varied in level of abstraction and in known prototypicality in given categories. Initial results show a similar pattern to that obtained in the earlier research in which it was found that the more typical members of superordinate categories could replace the superordinate in sentence frames generated by subjects told to \"make up a sentence\" that used the superordinate (Rosch, 1977). That is, the task of using a given concrete noun in a sentence appears to be an indirect method of eliciting a statement about the events in which objects play a part; that indirect method showed clearly that prototypical category members are those that can play the role in events expected of members of that category. The use of deviant forms of object names in narratives accounts for several recently explored effects in the psychological literature. Substituting object names at other than the basic level within scripts results in obviously deviant descriptions. Substitution of superordinates produces just those types of narrative that Bransford and Johnson (1973) have claimed are not comprehended; for example, \" The procedure is actually quite simple. First you arrange things into different groups. Of course, one pile may be sufficient [p. 400].\" It should be noted in the present context that what Bransford and Johnson call context cues are actually names of basic-level events (e.g., washing clothes) and that one function of hearing the event name is to enable the reader to translate the superordinate terms into basic-level objects and actions. Such a translation appears to be a necessary aspect of our ability to match linguistic descriptions to world knowledge in a way that produces the \"click of comprehension.\" On the other hand, substitution of subordinate terms for basic-level object names in scripts gives the effect of satire or snobbery. For example, a review ( Garis, 1975) of a pretentious novel accused of actually being about nothing more than brand-name snobbery concludes, \"And so, after putting away my 10"
            },
            "slug": "Cognition-and-Categorization-Rosch-Lloyd",
            "title": {
                "fragments": [],
                "text": "Cognition and Categorization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725561"
                        ],
                        "name": "Michael J. Cafarella",
                        "slug": "Michael-J.-Cafarella",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Cafarella",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Cafarella"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2339397"
                        ],
                        "name": "Michele Banko",
                        "slug": "Michele-Banko",
                        "structuredName": {
                            "firstName": "Michele",
                            "lastName": "Banko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michele Banko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741101"
                        ],
                        "name": "Oren Etzioni",
                        "slug": "Oren-Etzioni",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Etzioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Etzioni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7142575,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cdb0efae2bad7e09832950423785c1da3299054e",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Facts are naturally organized in terms of entities, classes, and their relationships as in an entity-relationship diagram or a semantic network. Search engines have eschewed such structures because, in the past, their creation and processing have not been practical at Web scale. This paper introduces the extraction graph, a textual approximation to an entity-relationship graph, which is automatically extracted from Web pages. The extraction graph is an intermediate representation that is more informative than a mere page-hyperlink graph but far easier to construct than a semantic network. The paper also introduces TextRunner, a search engine that utilizes this representation to answer complex relational queries that are dicult to answer using today\u2019s search engines or Web Information Extraction (IE) systems. The paper compares TextRunner to a state-of-the-art IE system on list searches, and nds that TextRunner is 40% more precise, with 11% better recall than the IE system. Our experiments, computed over a 90-million page corpus and a 227-million node extraction graph, show how TextRunner will scale to billions of pages."
            },
            "slug": "Relational-Web-Search-Cafarella-Banko",
            "title": {
                "fragments": [],
                "text": "Relational Web Search"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The extraction graph is a textual approximation to an entity-relationship graph, which is automatically extracted from Web pages, and TextRunner, a search engine that utilizes this representation to answer complex relational queries that are difficult to answer using today\u2019s search engines or Web Information Extraction systems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684353"
                        ],
                        "name": "Jennifer Chu-Carroll",
                        "slug": "Jennifer-Chu-Carroll",
                        "structuredName": {
                            "firstName": "Jennifer",
                            "lastName": "Chu-Carroll",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jennifer Chu-Carroll"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2579894"
                        ],
                        "name": "B. Carpenter",
                        "slug": "B.-Carpenter",
                        "structuredName": {
                            "firstName": "Bob",
                            "lastName": "Carpenter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Carpenter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 269,
                                "start": 101
                            }
                        ],
                        "text": "As an alternative to normalizing them, we may reduce their weights when they co-occur in a document (Church, 1995). Feature selection may be viewed as a form of weighting, in which some terms get a weight of zero and hence can be removed from the matrix. Forman (2003) provides a good study of feature selection methods for text classification."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14229502,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbbf137fba6062820b98dba6ec5e500b33d1e2ec",
            "isKey": false,
            "numCitedBy": 203,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a domain-independent, automatically trained natural language call router for directing incoming calls in a call center. Our call router directs customer calls based on their response to an open-ended How may I direct your call? prompt. Routing behavior is trained from a corpus of transcribed and hand-routed calls and then carried out using vector-based information retrieval techniques. Terms consist of n-gram sequences of morphologically reduced content words, while documents representing routing destinations consist of weighted term frequencies derived from calls to that destination in the training corpus. Based on the statistical discriminating power of the n-gram terms extracted from the caller's request, the caller is 1) routed to the appropriate destination, 2) transferred to a human operator, or 3) asked a disambiguation question. In the last case, the system dynamically generates queries tailored to the caller's request and the destinations with which it is consistent, based on our extension of the vector model. Evaluation of the call router performance over a financial services call center using both accurate transcriptions of calls and fairly noisy speech recognizer output demonstrated robustness in the face of speech recognition errors. More specifically, using accurate transcriptions of speech input, our system correctly routed 93.8% of the calls after redirecting 10.2% of all calls to a human operator. Using speech recognizer output with a 23% error rate reduced the number of correctly routed calls by 4%."
            },
            "slug": "Vector-based-Natural-Language-Call-Routing-Chu-Carroll-Carpenter",
            "title": {
                "fragments": [],
                "text": "Vector-based Natural Language Call Routing"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Evaluation of the call router performance over a financial services call center using both accurate transcriptions of calls and fairly noisy speech recognizer output demonstrated robustness in the face of speech recognition errors."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24143598"
                        ],
                        "name": "Peter A. Chew",
                        "slug": "Peter-A.-Chew",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Chew",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter A. Chew"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31431661"
                        ],
                        "name": "Brett W. Bader",
                        "slug": "Brett-W.-Bader",
                        "structuredName": {
                            "firstName": "Brett",
                            "lastName": "Bader",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brett W. Bader"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744364"
                        ],
                        "name": "T. Kolda",
                        "slug": "T.-Kolda",
                        "structuredName": {
                            "firstName": "Tamara",
                            "lastName": "Kolda",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kolda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683403"
                        ],
                        "name": "Ahmed Abdelali",
                        "slug": "Ahmed-Abdelali",
                        "structuredName": {
                            "firstName": "Ahmed",
                            "lastName": "Abdelali",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ahmed Abdelali"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4678124,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bbc1c2ed4be33f71e9fcb4c9df11d0cde80913dd",
            "isKey": false,
            "numCitedBy": 105,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "A standard approach to cross-language information retrieval (CLIR) uses Latent Semantic Analysis (LSA) in conjunction with a multilingual parallel aligned corpus. This approach has been shown to be successful in identifying similar documents across languages - or more precisely, retrieving the most similar document in one language to a query in another language. However, the approach has severe drawbacks when applied to a related task, that of clustering documents \"language-independently\", so that documents about similar topics end up closest to one another in the semantic space regardless of their language. The problem is that documents are generally more similar to other documents in the same language than they are to documents in a different language, but on the same topic. As a result, when using multilingual LSA, documents will in practice cluster by language, not by topic.\n We propose a novel application of PARAFAC2 (which is a variant of PARAFAC, a multi-way generalization of the singular value decomposition [SVD]) to overcome this problem. Instead of forming a single multilingual term-by-document matrix which, under LSA, is subjected to SVD, we form an irregular three-way array, each slice of which is a separate term-by-document matrix for a single language in the parallel corpus. The goal is to compute an SVD for each language such that V (the matrix of right singular vectors) is the same across all languages. Effectively, PARAFAC2 imposes the constraint, not present in standard LSA, that the \"concepts\" in all documents in the parallel corpus are the same regardless of language. Intuitively, this constraint makes sense, since the whole purpose of using a parallel corpus is that exactly the same concepts are expressed in the translations.\n We tested this approach by comparing the performance of PARAFAC2 with standard LSA in solving a particular CLIR problem. From our results, we conclude that PARAFAC2 offers a very promising alternative to LSA not only for multilingual document clustering, but also for solving other problems in cross-language information retrieval."
            },
            "slug": "Cross-language-information-retrieval-using-PARAFAC2-Chew-Bader",
            "title": {
                "fragments": [],
                "text": "Cross-language information retrieval using PARAFAC2"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel application of PARAFAC2 (which is a variant ofPARAFAC, a multi-way generalization of the singular value decomposition [SVD]) is proposed to overcome the problem that documents will in practice cluster by language, not by topic when using multilingual LSA."
            },
            "venue": {
                "fragments": [],
                "text": "KDD '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2469966"
                        ],
                        "name": "R. Girju",
                        "slug": "R.-Girju",
                        "structuredName": {
                            "firstName": "Roxana",
                            "lastName": "Girju",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Girju"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683562"
                        ],
                        "name": "Preslav Nakov",
                        "slug": "Preslav-Nakov",
                        "structuredName": {
                            "firstName": "Preslav",
                            "lastName": "Nakov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Preslav Nakov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2256003"
                        ],
                        "name": "Vivi Nastase",
                        "slug": "Vivi-Nastase",
                        "structuredName": {
                            "firstName": "Vivi",
                            "lastName": "Nastase",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vivi Nastase"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795595"
                        ],
                        "name": "S. Szpakowicz",
                        "slug": "S.-Szpakowicz",
                        "structuredName": {
                            "firstName": "Stan",
                            "lastName": "Szpakowicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Szpakowicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689647"
                        ],
                        "name": "Peter D. Turney",
                        "slug": "Peter-D.-Turney",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Turney",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter D. Turney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2808366"
                        ],
                        "name": "Deniz Yuret",
                        "slug": "Deniz-Yuret",
                        "structuredName": {
                            "firstName": "Deniz",
                            "lastName": "Yuret",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Deniz Yuret"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14624577,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1755f2ac7eb8a091f3613be47da844803df86fc1",
            "isKey": false,
            "numCitedBy": 200,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "The NLP community has shown a renewed interest in deeper semantic analyses, among them automatic recognition of relations between pairs of words in a text. We present an evaluation task designed to provide a framework for comparing different approaches to classifying semantic relations between nominals in a sentence. This is part of SemEval, the 4th edition of the semantic evaluation event previously known as SensEval. We define the task, describe the training/test data and their creation, list the participating systems and discuss their results. There were 14 teams who submitted 15 systems."
            },
            "slug": "SemEval-2007-Task-04:-Classification-of-Semantic-Girju-Nakov",
            "title": {
                "fragments": [],
                "text": "SemEval-2007 Task 04: Classification of Semantic Relations between Nominals"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An evaluation task designed to provide a framework for comparing different approaches to classifying semantic relations between nominals in a sentence as part of SemEval, the 4th edition of the semantic evaluation event previously known as SensEval."
            },
            "venue": {
                "fragments": [],
                "text": "Fourth International Workshop on Semantic Evaluations (SemEval-2007)"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35729970"
                        ],
                        "name": "Yiming Yang",
                        "slug": "Yiming-Yang",
                        "structuredName": {
                            "firstName": "Yiming",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiming Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 140
                            }
                        ],
                        "text": "A measure of document similarity, such as cosine, can be directly applied to document classification by using a nearest-neighbour algorithm (Yang, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 93891,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "890c16ca29a781a7b793c603822ffd57aee9f57f",
            "isKey": false,
            "numCitedBy": 2034,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper focuses on a comparative evaluation of a wide-range of text categorization methods, including previously published results on the Reuters corpus and new results of additional experiments. A controlled study using three classifiers, kNN, LLSF and WORD, was conducted to examine the impact of configuration variations in five versions of Reuters on the observed performance of classifiers. Analysis and empirical evidence suggest that the evaluation results on some versions of Reuters were significantly affected by the inclusion of a large portion of unlabelled documents, mading those results difficult to interpret and leading to considerable confusions in the literature. Using the results evaluated on the other versions of Reuters which exclude the unlabelled documents, the performance of twelve methods are compared directly or indirectly. For indirect compararions, kNN, LLSF and WORD were used as baselines, since they were evaluated on all versions of Reuters that exclude the unlabelled documents. As a global observation, kNN, LLSF and a neural network method had the best performance; except for a Naive Bayes approach, the other learning algorithms also performed relatively well."
            },
            "slug": "An-Evaluation-of-Statistical-Approaches-to-Text-Yang",
            "title": {
                "fragments": [],
                "text": "An Evaluation of Statistical Approaches to Text Categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Analysis and empirical evidence suggest that the evaluation results on some versions of Reuters were significantly affected by the inclusion of a large portion of unlabelled documents, mading those results difficult to interpret and leading to considerable confusions in the literature."
            },
            "venue": {
                "fragments": [],
                "text": "Information Retrieval"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3199842"
                        ],
                        "name": "Barbara Rosario",
                        "slug": "Barbara-Rosario",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Rosario",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barbara Rosario"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716902"
                        ],
                        "name": "Marti A. Hearst",
                        "slug": "Marti-A.-Hearst",
                        "structuredName": {
                            "firstName": "Marti",
                            "lastName": "Hearst",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marti A. Hearst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8570237,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4656da2393dc4dc5935989483a176a07beb59dc1",
            "isKey": false,
            "numCitedBy": 175,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We are developing corpus-based techniques for identifying semantic relations at an intermediate level of description (more specific than those used in case frames, but more general than those used in traditional knowledge representation systems). In this paper we describe a classification algorithm for identifying relationships between two-word noun compounds. We find that a very simple approach using a machine learning algorithm and a domain-specific lexical hierarchy successfully generalizes from training instances, performing better on previously unseen words than a baseline consisting of training on the words themselves."
            },
            "slug": "Classifying-the-Semantic-Relations-in-Noun-via-a-Rosario-Hearst",
            "title": {
                "fragments": [],
                "text": "Classifying the Semantic Relations in Noun Compounds via a Domain-Specific Lexical Hierarchy"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is found that a very simple approach using a machine learning algorithm and a domain-specific lexical hierarchy successfully generalizes from training instances, performing better on previously unseen words than a baseline consisting of training on the words themselves."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836606"
                        ],
                        "name": "T. Landauer",
                        "slug": "T.-Landauer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Landauer",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Landauer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 65
                            }
                        ],
                        "text": "\u201d The term statistical semantics appeared in the work of Furnas, Landauer, Gomez, and Dumais (1983), but it was not defined there."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 194
                            }
                        ],
                        "text": "See http://wordnet.princeton.edu/.\nwords that occur in similar contexts tend to have similar meanings (Wittgenstein, 1953; Harris, 1954; Weaver, 1955; Firth, 1957; Deerwester, Dumais, Landauer, Furnas, & Harshman, 1990)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 248
                            }
                        ],
                        "text": "A measure of distance between vectors can easily be converted to a measure of similarity by inversion (13) or subtraction (14).\nsim(x,y) = 1/dist(x,y) (13) sim(x,y) = 1\u2212 dist(x,y) (14)\nMany similarity measures have been proposed in both IR (Jones & Furnas, 1987) and lexical semantics circles (Lin, 1998; Dagan, Lee, & Pereira, 1999; Lee, 1999; Weeds, Weir, & McCarthy, 2004)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 55
                            }
                        ],
                        "text": "The term statistical semantics appeared in the work of Furnas, Landauer, Gomez, and Dumais (1983), but it was not defined there.\nand the column vectors correspond to documents (web pages, for example)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 86
                            }
                        ],
                        "text": "However, if the reader would like to do some further background reading, we recommend Landauer et al.\u2019s (2007) collection."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11233228,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "ff5ade844825d3d867c67f957313f8193ae4b89d",
            "isKey": true,
            "numCitedBy": 191,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-computational-basis-of-learning-and-from-LSA-Landauer",
            "title": {
                "fragments": [],
                "text": "On the computational basis of learning and cognition: Arguments from LSA"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2500077"
                        ],
                        "name": "Julie Weeds",
                        "slug": "Julie-Weeds",
                        "structuredName": {
                            "firstName": "Julie",
                            "lastName": "Weeds",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Julie Weeds"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35258592"
                        ],
                        "name": "David J. Weir",
                        "slug": "David-J.-Weir",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Weir",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Weir"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145586618"
                        ],
                        "name": "Diana McCarthy",
                        "slug": "Diana-McCarthy",
                        "structuredName": {
                            "firstName": "Diana",
                            "lastName": "McCarthy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diana McCarthy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 137
                            }
                        ],
                        "text": "In one experiment on determining the compositionality of collocations, high-frequency sensitive measures outperformed the other classes (Weeds et al., 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Weeds et al. (2004) studied the linguistic and statistical properties of the similar words returned by various similarity measures and found that the measures can be grouped into three classes:\n1. high-frequency sensitive measures (cosine, Jensen-Shannon, \u03b1-skew, recall),\n2. low-frequency sensitive\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3016990,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "527eb9c939801f1edcedace66eff7bbc02f74e80",
            "isKey": false,
            "numCitedBy": 297,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This work investigates the variation in a word's distributionally nearest neighbours with respect to the similarity measure used. We identify one type of variation as being the relative frequency of the neighbour words with respect to the frequency of the target word. We then demonstrate a three-way connection between relative frequency of similar words, a concept of distributional gnerality and the semantic relation of hyponymy. Finally, we consider the impact that this has on one application of distributional similarity methods (judging the compositionality of collocations)."
            },
            "slug": "Characterising-Measures-of-Lexical-Distributional-Weeds-Weir",
            "title": {
                "fragments": [],
                "text": "Characterising Measures of Lexical Distributional Similarity"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A three-way connection is demonstrated between relative frequency of similar words, a concept of distributional gnerality and the semantic relation of hyponymy in a word's distributionally nearest neighbours with respect to the similarity measure."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418438"
                        ],
                        "name": "Hinrich Sch\u00fctze",
                        "slug": "Hinrich-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Hinrich",
                            "lastName": "Sch\u00fctze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hinrich Sch\u00fctze"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 267
                            }
                        ],
                        "text": "\u2026words according to their parts of speech), word sense tagging (marking ambiguous words according to their intended meanings), and parsing (analyzing the grammatical structure of sentences and marking the words in the sentences according to their grammatical roles) (Manning & Schu\u0308tze, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52800448,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "084c55d6432265785e3ff86a2e900a49d501c00a",
            "isKey": false,
            "numCitedBy": 7801,
            "numCiting": 294,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical approaches to processing natural language text have become dominant in recent years. This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear. The book contains all the theory and algorithms needed for building NLP tools. It provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations. The book covers collocation finding, word sense disambiguation, probabilistic parsing, information retrieval, and other applications."
            },
            "slug": "Foundations-of-statistical-natural-language-Manning-Sch\u00fctze",
            "title": {
                "fragments": [],
                "text": "Foundations of statistical natural language processing"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear and provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations."
            },
            "venue": {
                "fragments": [],
                "text": "SGMD"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34902160"
                        ],
                        "name": "Jeff Mitchell",
                        "slug": "Jeff-Mitchell",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Mitchell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Mitchell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747893"
                        ],
                        "name": "Mirella Lapata",
                        "slug": "Mirella-Lapata",
                        "structuredName": {
                            "firstName": "Mirella",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirella Lapata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18597583,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5d67d1dc671bce42a9daac0c3605adb3fcfc697",
            "isKey": false,
            "numCitedBy": 730,
            "numCiting": 133,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a framework for representing the meaning of phrases and sentences in vector space. Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions. Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task. Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments."
            },
            "slug": "Vector-based-Models-of-Semantic-Composition-Mitchell-Lapata",
            "title": {
                "fragments": [],
                "text": "Vector-based Models of Semantic Composition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Under this framework, a wide range of composition models are introduced which are evaluated empirically on a sentence similarity task and demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144096985"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Miller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3226331"
                        ],
                        "name": "C. Leacock",
                        "slug": "C.-Leacock",
                        "structuredName": {
                            "firstName": "Claudia",
                            "lastName": "Leacock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Leacock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1818801"
                        ],
                        "name": "Randee Tengi",
                        "slug": "Randee-Tengi",
                        "structuredName": {
                            "firstName": "Randee",
                            "lastName": "Tengi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Randee Tengi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144819917"
                        ],
                        "name": "R. Bunker",
                        "slug": "R.-Bunker",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Bunker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bunker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7231199,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "f9a25e0dc776857fc24ebc7115c980312f2719b1",
            "isKey": false,
            "numCitedBy": 725,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "A semantic concordance is a textual corpus and a lexicon so combined that every substantive word in the text is linked to its appropriate sense in the lexicon. Thus it can be viewed either as a corpus in which words have been tagged syntactically and semantically, or as a lexicon in which example sentences can be found for many definitions. A semantic concordance is being constructed to use in studies of sense resolution in context (semantic disambiguation). The Brown Corpus is the text and WordNet is the lexicon. Semantic tags (pointers to WordNet synsets) are inserted in the text manually using an interface, ConText, that was designed to facilitate the task. Another interface supports searches of the tagged text. Some practical uses for semantic concordances am proposed."
            },
            "slug": "A-Semantic-Concordance-Miller-Leacock",
            "title": {
                "fragments": [],
                "text": "A Semantic Concordance"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A semantic concordance is a textual corpus and a lexicon so combined that every substantive word in the text is linked to its appropriate sense in the lexicon."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144541931"
                        ],
                        "name": "Edward E. Smith",
                        "slug": "Edward-E.-Smith",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Smith",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edward E. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2312012"
                        ],
                        "name": "D. Osherson",
                        "slug": "D.-Osherson",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Osherson",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Osherson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1969651"
                        ],
                        "name": "L. Rips",
                        "slug": "L.-Rips",
                        "structuredName": {
                            "firstName": "Lance",
                            "lastName": "Rips",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143918997"
                        ],
                        "name": "M. Keane",
                        "slug": "M.-Keane",
                        "structuredName": {
                            "firstName": "Margaret",
                            "lastName": "Keane",
                            "middleNames": [
                                "M"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Keane"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 67
                            }
                        ],
                        "text": "This approach to measuring relational similarity was introduced by Turney et al. (2003) and examined in more detail by Turney and Littman (2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 67
                            }
                        ],
                        "text": "This approach to measuring relational similarity was introduced by Turney et al. (2003) and examined in more detail by Turney and Littman (2005). Turney (2006) evaluated this approach to relational similarity with 374 multiple-choice analogy questions from the SAT college entrance test, achieving human-level performance (56% correct for the pair\u2013pattern matrix and 57% correct for the average US college applicant)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14830228,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "8bedc7c7fc5a7f056439b6c74ac0680993eaf152",
            "isKey": false,
            "numCitedBy": 247,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a model that accounts for how people construct prototypes for composite concepts out of prototypes for simple concepts. The first component of the model is a prototype representation for simple, noun concepts, such as fruit, which specifies: (1) the relevant attributes of the concepts, (2) the possiblevalues of each attribute, (3) the salience of each value, and (4) the diognosticity of each attribute. The second component of the model specifies procedures for modifying simple prototypes so that they represent new, composite concepts. The procedure for adjectival modification, OS when red modifies fruft, consists of selecting the relevant attribute(s) in the noun concept (color), boosting the diognosticity of that ottribute, and increosing\u2018the salience of the value named by the adfective (red). The procedure for odverbiol modification, OS in very red frutt, consists of multiplication-by-a-scalar of the salience of the relevant volue (red). The outcome of these procedures is a new prototype representation. The third component of the model is Tversky\u2019s (1977) contrast rule for determining the similority between a representation for a prototype and one for on instance. The model is shown to be consistent with previous findings about prototypes in generol, OS well as with specific findings about typicality judgments for adjective-noun conjunctions. Four new experiments provide further detailed support for the model."
            },
            "slug": "Combining-Prototypes:-A-Selective-Modification-Smith-Osherson",
            "title": {
                "fragments": [],
                "text": "Combining Prototypes: A Selective Modification Model"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The model is shown to be consistent with previous findings about prototypes in generol, OS as well as with specific findings about typicality judgments for adjective-noun conjunctions."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1990190"
                        ],
                        "name": "P. Pantel",
                        "slug": "P.-Pantel",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Pantel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pantel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Lin and Pantel (2001) proposed the extended distributional hypothesis, that patterns that co-occur with similar pairs tend to have similar meanings."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Lin and Pantel (2001) introduced the pair\u2013pattern matrix for the purpose of measuring the semantic similarity of patterns; that is, the similarity of column vectors."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 89
                            }
                        ],
                        "text": "The leading algorithms for measuring the similarity of semantic relations also use VSMs (Lin & Pantel, 2001; Turney, 2006; Nakov & Hearst, 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 109
                            }
                        ],
                        "text": "Extended distributional hypothesis: Patterns that co-occur with similar pairs tend to have similar meanings (Lin & Pantel, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 86
                            }
                        ],
                        "text": "Pattern similarity can be used to infer that one sentence is a paraphrase of another (Lin & Pantel, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2971806,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f3f9a848dca0a80ef64987a9dd511ee6b7e19cd1",
            "isKey": false,
            "numCitedBy": 565,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose an unsupervised method for discovering inference rules from text, such as \"X is author of Y \u2248 X wrote Y\", \"X solved Y \u2248 X found a solution to Y\", and \"X caused Y \u2248 Y is triggered by X\". Inference rules are extremely important in many fields such as natural language processing, information retrieval, and artificial intelligence in general. Our algorithm is based on an extended version of Harris' Distributional Hypothesis, which states that words that occurred in the same contexts tend to be similar. Instead of using this hypothesis on words, we apply it to paths in the dependency trees of a parsed corpus."
            },
            "slug": "DIRT-@SBT@discovery-of-inference-rules-from-text-Lin-Pantel",
            "title": {
                "fragments": [],
                "text": "DIRT @SBT@discovery of inference rules from text"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This paper proposes an unsupervised method for discovering inference rules from text, based on an extended version of Harris' Distributional Hypothesis, which states that words that occurred in the same contexts tend to be similar."
            },
            "venue": {
                "fragments": [],
                "text": "KDD '01"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1990190"
                        ],
                        "name": "P. Pantel",
                        "slug": "P.-Pantel",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Pantel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pantel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 179
                            }
                        ],
                        "text": "For example, dog and wolf have a relatively high degree of attributional similarity, whereas dog : bark and cat :meow have a relatively high degree of relational similarity (Turney, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Lin and Pantel (2001) proposed the extended distributional hypothesis, that patterns that co-occur with similar pairs tend to have similar meanings."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Lin and Pantel (2001) introduced the pair\u2013pattern matrix for the purpose of measuring the semantic similarity of patterns; that is, the similarity of column vectors."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 89
                            }
                        ],
                        "text": "The leading algorithms for measuring the similarity of semantic relations also use VSMs (Lin & Pantel, 2001; Turney, 2006; Nakov & Hearst, 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 109
                            }
                        ],
                        "text": "Extended distributional hypothesis: Patterns that co-occur with similar pairs tend to have similar meanings (Lin & Pantel, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 86
                            }
                        ],
                        "text": "Pattern similarity can be used to infer that one sentence is a paraphrase of another (Lin & Pantel, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9548219,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c384d9ee4fd8657b26a8165244eb4ad73df4f492",
            "isKey": false,
            "numCitedBy": 506,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose an unsupervised method for discovering inference rules from text, such as \u201cX is author of Y \u2248 X wrote Y\u201d, \u201cX solved Y \u2248 X found a solution to Y\u201d, and \u201cX caused Y \u2248 Y is triggered by X\u201d. Inference rules are extremely important in many fields such as natural language processing, information retrieval, and artificial intelligence in general. Our algorithm is based on an extended version of Harris\u2019 Distributional Hypothesis, which states that words that occurred in the same contexts tend to be similar. Instead of using this hypothesis on words, we apply it to paths in the dependency trees of a parsed corpus."
            },
            "slug": "DIRT-\u2013-Discovery-of-Inference-Rules-from-Text-Lin-Pantel",
            "title": {
                "fragments": [],
                "text": "DIRT \u2013 Discovery of Inference Rules from Text"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This paper proposes an unsupervised method for discovering inference rules from text, based on an extended version of Harris\u2019 Distributional Hypothesis, which states that words that occurred in the same contexts tend to be similar."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 216
                            }
                        ],
                        "text": "\u2026similarity by inversion (13) or subtraction (14).\nsim(x,y) = 1/dist(x,y) (13) sim(x,y) = 1\u2212 dist(x,y) (14)\nMany similarity measures have been proposed in both IR (Jones & Furnas, 1987) and lexical semantics circles (Lin, 1998; Dagan, Lee, & Pereira, 1999; Lee, 1999; Weeds, Weir, & McCarthy, 2004)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Lin and Pantel (2001) introduced the pair\u2013pattern matrix for the purpose of measuring the semantic similarity of patterns; that is, the similarity of column vectors."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 142
                            }
                        ],
                        "text": "\u2026are derived from the occurrences of the word in various contexts, such as windows of words (Lund & Burgess, 1996), grammatical dependencies (Lin, 1998; Pado\u0301 & Lapata, 2007), and richer contexts consisting of dependency links and selectional preferences on the argument positions (Erk &\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 24
                            }
                        ],
                        "text": "For example, Pantel and Lin (2002a) presented an algorithm that can discover word senses by clustering row vectors in a word\u2013context matrix, using contextual information derived from parsing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 118
                            }
                        ],
                        "text": "By keeping only the context-word dimensions with a PMI above a conservative threshold and setting the others to zero, Lin (1998) showed that the number of comparisons needed to compare vectors greatly decreases while losing little precision in the similarity score between the top-200 most similar\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 200
                            }
                        ],
                        "text": "A word may be represented by a vector in which the elements are derived from the occurrences of the word in various contexts, such as windows of words (Lund & Burgess, 1996), grammatical dependencies (Lin, 1998; Pad\u00f3 & Lapata, 2007), and richer contexts consisting of dependency links and selectional preferences on the argument positions (Erk & Pad\u00f3, 2008); see Sahlgren\u2019s (2006) thesis for a comprehensive study of various contexts."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 349,
                                "start": 0
                            }
                        ],
                        "text": "Lin and Pantel (2001) introduced the pair\u2013pattern matrix for the purpose of measuring the semantic similarity of patterns; that is, the similarity of column vectors. Given a pattern such as \u201cX solves Y \u201d, their algorithm was able to find similar patterns, such as \u201cY is solved by X\u201d, \u201cY is resolved in X\u201d, and \u201cX resolves Y \u201d. Lin and Pantel (2001) proposed the extended distributional hypothesis, that patterns that co-occur with similar pairs tend to have similar meanings."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 98
                            }
                        ],
                        "text": "Chew, Bader, Kolda, and Abdelali (2007) use a term\u2013 document\u2013language third-order tensor for multilingual information retrieval. Turney (2007) uses a word\u2013word\u2013pattern tensor to measure similarity of words. Van de Cruys (2009) uses a verb\u2013subject\u2013object tensor to learn selectional preferences of verbs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 108
                            }
                        ],
                        "text": "Many similarity measures have been proposed in both IR (Jones & Furnas, 1987) and lexical semantics circles (Lin, 1998; Dagan, Lee, & Pereira, 1999; Lee, 1999; Weeds, Weir, & McCarthy, 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 98
                            }
                        ],
                        "text": "Chew, Bader, Kolda, and Abdelali (2007) use a term\u2013 document\u2013language third-order tensor for multilingual information retrieval. Turney (2007) uses a word\u2013word\u2013pattern tensor to measure similarity of words."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 86
                            }
                        ],
                        "text": "Pattern similarity can be used to infer that one sentence is a paraphrase of another (Lin & Pantel, 2001). Turney et al. (2003) introduced the use of the pair\u2013pattern matrix for measuring the semantic similarity of relations between word pairs; that is, the similarity of row vectors."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1005,
                                "start": 17
                            }
                        ],
                        "text": "In computational linguistics, the term semantic similarity is applied to words that share a hypernym (car and bicycle are semantically similar, because they share the hypernym vehicle) (Resnik, 1995). Semantic similarity is a specific type of attributional similarity. We prefer the term taxonomical similarity to the term semantic similarity, because the term semantic similarity is misleading. Intuitively, both attributional and relational similarity involve meaning, so both deserve to be called semantic similarity. Words are semantically associated if they tend to co-occur frequently (e.g., bee and honey) (Chiarello, Burgess, Richards, & Pollock, 1990). Words may be taxonomically similar and semantically associated (doctor and nurse), taxonomically similar but not semantically associated (horse and platypus), semantically associated but not taxonomically similar (cradle and baby), or neither semantically associated nor taxonomically similar (calculus and candy). Sch\u00fctze and Pedersen (1993) defined two ways that words can be distributed in a corpus of text: If two words tend to be neighbours of each other, then they are syntagmatic associates."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 512,
                                "start": 126
                            }
                        ],
                        "text": "In our own experiments of computing the semantic similarity between all pairs of words in a large web crawl, we observed near linear average running time complexity in n. The computational cost can be reduced further by leveraging the element weighting techniques described in Section 4.2. By setting to zero all coordinates that have a low PPMI, PMI or tf-idf score, the coordinate density is dramatically reduced at the cost of losing little discriminative power. In this vein, Bayardo, Ma, and Srikant (2007) described a strategy that omits the coordinates with the highest number of nonzero values."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15698938,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fd1901f34cc3673072264104885d70555b1a4cdc",
            "isKey": true,
            "numCitedBy": 1928,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is."
            },
            "slug": "Automatic-Retrieval-and-Clustering-of-Similar-Words-Lin",
            "title": {
                "fragments": [],
                "text": "Automatic Retrieval and Clustering of Similar Words"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A word similarity measure based on the distributional pattern of words allows the automatically constructed thesaurus to be significantly closer to WordNet than Roget Thesaurus is."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786259"
                        ],
                        "name": "S. Brin",
                        "slug": "S.-Brin",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Brin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Brin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48479130"
                        ],
                        "name": "Lawrence Page",
                        "slug": "Lawrence-Page",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Page",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lawrence Page"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7587743,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10d6778bc45aebcd58d336b4062b935861d2fe8a",
            "isKey": false,
            "numCitedBy": 15466,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Anatomy-of-a-Large-Scale-Hypertextual-Web-Brin-Page",
            "title": {
                "fragments": [],
                "text": "The Anatomy of a Large-Scale Hypertextual Web Search Engine"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Networks"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2001885"
                        ],
                        "name": "Ted Pedersen",
                        "slug": "Ted-Pedersen",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Pedersen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ted Pedersen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 76
                            }
                        ],
                        "text": "Word sense disambiguation: A typical Word Sense Disambiguation (WSD) system (Agirre & Edmonds, 2006; Pedersen, 2006) uses a feature vector representation in which each vector corresponds to a token of a word, not a type (see Section 2."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59727521,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d4d6cbd405783f121ac1c3955ff1a32c94244fb",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter focuses on unsupervised corpus-based methods of word sense discrimination that are knowledge-lean, and do not rely on external knowledge sources such as machine readable dictionaries, concept hierarchies, or sense-tagged text. They do not assign sense tags to words; rather, they discriminate among word meanings based on information found in unannotated corpora. This chapter reviews distributional approaches that rely on monolingual corpora and methods based on translational equivalence as found in word-aligned parallel corpora. These techniques are organized into typeand token-based approaches. The former identify sets of related words, while the latter distinguish among the senses of a word used in multiple contexts."
            },
            "slug": "Unsupervised-Corpus-Based-Methods-for-WSD-Pedersen",
            "title": {
                "fragments": [],
                "text": "Unsupervised Corpus-Based Methods for WSD"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This chapter focuses on unsupervised corpus-based methods of word sense discrimination that are knowledge-lean, and do not rely on external knowledge sources such as machine readable dictionaries, concept hierarchies, or sense-tagged text."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689109"
                        ],
                        "name": "Magnus Sahlgren",
                        "slug": "Magnus-Sahlgren",
                        "structuredName": {
                            "firstName": "Magnus",
                            "lastName": "Sahlgren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Magnus Sahlgren"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11917163,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1521ddb27860cc8834f8a82e62665bf983c8ad2c",
            "isKey": false,
            "numCitedBy": 600,
            "numCiting": 166,
            "paperAbstract": {
                "fragments": [],
                "text": "The word-space model is a computational model of word meaning that utilizes the distributional patterns of words collected over large text data to represent semantic similarity between words in ter ..."
            },
            "slug": "The-Word-Space-Model-:-Using-distributional-to-and-Sahlgren",
            "title": {
                "fragments": [],
                "text": "The Word-Space Model : Using distributional analysis to represent syntagmatic and paradigmatic relations between words in high-dimensional vector spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "The word-space model is a computational model of word meaning that utilizes the distributional patterns of words collected over large text data to represent semantic similarity between words in terabytes of data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3320836"
                        ],
                        "name": "Guihong Cao",
                        "slug": "Guihong-Cao",
                        "structuredName": {
                            "firstName": "Guihong",
                            "lastName": "Cao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guihong Cao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143619007"
                        ],
                        "name": "Jian-Yun Nie",
                        "slug": "Jian-Yun-Nie",
                        "structuredName": {
                            "firstName": "Jian-Yun",
                            "lastName": "Nie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian-Yun Nie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143867241"
                        ],
                        "name": "Jing Bai",
                        "slug": "Jing-Bai",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Bai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Bai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1458999,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "afb57b9207879ee301fc53a01153eb6a27ae2ca8",
            "isKey": false,
            "numCitedBy": 179,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a novel dependency language modeling approach for information retrieval. The approach extends the existing language modeling approach by relaxing the independence assumption. Our goal is to build a language model in which various word relationships can be integrated. In this work, we integrate two types of relationship extracted from WordNet and co-occurrence relationships respectively. The integrated model has been tested on several TREC collections. The results show that our model achieves substantial and significant improvements with respect to the models without these relationships. These results clearly show the benefit of integrating word relationships into language models for IR."
            },
            "slug": "Integrating-word-relationships-into-language-models-Cao-Nie",
            "title": {
                "fragments": [],
                "text": "Integrating word relationships into language models"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The results show that the model achieves substantial and significant improvements with respect to the models without these relationships, and clearly shows the benefit of integrating word relationships into language models for IR."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '05"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145375801"
                        ],
                        "name": "M. Pennacchiotti",
                        "slug": "M.-Pennacchiotti",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Pennacchiotti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pennacchiotti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32323528"
                        ],
                        "name": "D. D. Cao",
                        "slug": "D.-D.-Cao",
                        "structuredName": {
                            "firstName": "Diego",
                            "lastName": "Cao",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. D. Cao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144654543"
                        ],
                        "name": "Roberto Basili",
                        "slug": "Roberto-Basili",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Basili",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roberto Basili"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784172"
                        ],
                        "name": "D. Croce",
                        "slug": "D.-Croce",
                        "structuredName": {
                            "firstName": "Danilo",
                            "lastName": "Croce",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Croce"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46617131"
                        ],
                        "name": "Michael Roth",
                        "slug": "Michael-Roth",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Roth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1627782,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd3ce4274a1a3d14e938104f25225e4b85f6a451",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Most attempts to integrate FrameNet in NLP systems have so far failed because of its limited coverage. In this paper, we investigate the applicability of distributional and WordNet-based models on the task of lexical unit induction, i.e. the expansion of FrameNet with new lexical units. Experimental results show that our distributional and WordNet-based models achieve good level of accuracy and coverage, especially when combined."
            },
            "slug": "Automatic-induction-of-FrameNet-lexical-units-Pennacchiotti-Cao",
            "title": {
                "fragments": [],
                "text": "Automatic induction of FrameNet lexical units"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This paper investigates the applicability of distributional and WordNet-based models on the task of lexical unit induction, i.e. the expansion of FrameNet with new lexical units, and shows good level of accuracy and coverage, especially when combined."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145077269"
                        ],
                        "name": "F. Sebastiani",
                        "slug": "F.-Sebastiani",
                        "structuredName": {
                            "firstName": "Fabrizio",
                            "lastName": "Sebastiani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Sebastiani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 97
                            }
                        ],
                        "text": "For example, a machine learning algorithm can be applied to classifying or clustering documents (Sebastiani, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 84
                            }
                        ],
                        "text": "Gathering a corpus for a new language is generally much easier than building a lexicon, and building a lexicon often involves also gathering a corpus, such as SemCor for WordNet (Miller, Leacock, Tengi, & Bunker, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3091,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b20af22b0734757d9ead382b201a65f9dd637cc",
            "isKey": false,
            "numCitedBy": 8450,
            "numCiting": 224,
            "paperAbstract": {
                "fragments": [],
                "text": "The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last 10 years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation."
            },
            "slug": "Machine-learning-in-automated-text-categorization-Sebastiani",
            "title": {
                "fragments": [],
                "text": "Machine learning in automated text categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This survey discusses the main approaches to text categorization that fall within the machine learning paradigm and discusses in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation."
            },
            "venue": {
                "fragments": [],
                "text": "CSUR"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2325236"
                        ],
                        "name": "Y. Niwa",
                        "slug": "Y.-Niwa",
                        "structuredName": {
                            "firstName": "Yoshiki",
                            "lastName": "Niwa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Niwa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1806193"
                        ],
                        "name": "Y. Nitta",
                        "slug": "Y.-Nitta",
                        "structuredName": {
                            "firstName": "Yoshihiko",
                            "lastName": "Nitta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Nitta"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 115
                            }
                        ],
                        "text": "A variation of PMI is Positive PMI (PPMI), in which all PMI values that are less than zero are replaced with zero (Niwa & Nitta, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2646329,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c989e8aa08b24345419e4528198fe5ea17cc0160",
            "isKey": false,
            "numCitedBy": 140,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "A comparison was made of vectors derived by using ordinary co-occurrence statistics from large text corpora and of vectors derived by measuring the interword distances in dictionary definitions. The precision of word sense disambiguation by using co-occurrence vectors from the 1987 Wall Street Journal (20M total words) was higher than that by using distance vectors from the Collins English Dictionary (60K head words + 1.6M definition words). However, other experimental results suggest that distance vectors contain some different semantic information from co-occurrence vectors."
            },
            "slug": "Co-Occurrence-Vectors-From-Corpora-vs.-Distance-Niwa-Nitta",
            "title": {
                "fragments": [],
                "text": "Co-Occurrence Vectors From Corpora vs. Distance Vectors From Dictionaries"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Experimental results suggest that distance vectors contain some different semantic information from co-occurrence vectors, compared with other experimental results, which suggest that word sense disambiguation is affected by distance vectors."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802811"
                        ],
                        "name": "D. Widdows",
                        "slug": "D.-Widdows",
                        "structuredName": {
                            "firstName": "Dominic",
                            "lastName": "Widdows",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Widdows"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41124197"
                        ],
                        "name": "Kathleen Ferraro",
                        "slug": "Kathleen-Ferraro",
                        "structuredName": {
                            "firstName": "Kathleen",
                            "lastName": "Ferraro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kathleen Ferraro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12317655,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d115d2f55442137893aacd641674c248064c938f",
            "isKey": false,
            "numCitedBy": 183,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the open source SemanticVectors package that efficiently creates semantic vectors for words and documents from a corpus of free text articles. We believe that this package can play an important role in furthering research in distributional semantics, and (perhaps more importantly) can help to significantly reduce the current gap that exists between good research results and valuable applications in production software. Two clear principles that have guided the creation of the package so far include ease-of-use and scalability. The basic package installs and runs easily on any Java-enabled platform, and depends only on Apache Lucene. Dimension reduction is performed using Random Projection, which enables the system to scale much more effectively than other algorithms used for the same purpose. This paper also describes a trial application in the Technology Management domain, which highlights some user-centred design challenges which we believe are also key to successful deployment of this technology."
            },
            "slug": "Semantic-Vectors:-a-Scalable-Open-Source-Package-Widdows-Ferraro",
            "title": {
                "fragments": [],
                "text": "Semantic Vectors: a Scalable Open Source Package and Online Technology Management Application"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "The SemanticVectors package that efficiently creates semantic vectors for words and documents from a corpus of free text articles is described, which can play an important role in furthering research in distributional semantics, and can help to significantly reduce the current gap between good research results and valuable applications in production software."
            },
            "venue": {
                "fragments": [],
                "text": "LREC"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "119515866"
                        ],
                        "name": "G. Ruge",
                        "slug": "G.-Ruge",
                        "structuredName": {
                            "firstName": "Gerd",
                            "lastName": "Ruge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Ruge"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 126
                            }
                        ],
                        "text": "Several researchers have used word\u2013context matrices specifically for the task of assisting or automating thesaurus generation (Crouch, 1988; Grefenstette, 1994; Ruge, 1997; Pantel & Lin, 2002a; Curran & Moens, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 98
                            }
                        ],
                        "text": "Vectors are common in AI and cognitive science; they were common before the VSM was introduced by Salton et al. (1975). The novelty of the VSM was to use frequencies in a corpus of text as a clue for discovering semantic information."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13966972,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "f6e1db64a6e7e724bfc088ed0f3c2fcf3ede06d5",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Is it possible to discover semantic term relations useful for thesauri without any semantic information? Yes, it is. A recent approach for automatic thesaurus construction is based on explicit linguistic knowledge, i.e. a domain independent parser without any semantic component, and implicit linguistic knowledge contained in large amounts of real world texts. Such texts include implicitly the linguistic, especially semantic, knowledge that the authors needed for formulating their texts. This article explains how implicit semantic knowledge can be transformed to an explicit one. Evaluations of quality and performance of the approach are very encouraging."
            },
            "slug": "Automatic-Detection-of-Thesaurus-relations-for-Ruge",
            "title": {
                "fragments": [],
                "text": "Automatic Detection of Thesaurus relations for Information Retrieval Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This article explains how implicit semantic knowledge can be transformed to an explicit one and evaluates of quality and performance of the approach are very encouraging."
            },
            "venue": {
                "fragments": [],
                "text": "Foundations of Computer Science: Potential - Theory - Cognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802811"
                        ],
                        "name": "D. Widdows",
                        "slug": "D.-Widdows",
                        "structuredName": {
                            "firstName": "Dominic",
                            "lastName": "Widdows",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Widdows"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The mathematical operations include vector negation and disjunction, based on quantum logic (Widdows, 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Widdows (2004) gives a gentle introduction to vectors from the perspective of semantics."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17581,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "36eff99a7f23cec395e4efc80ff7f937934c7be6",
            "isKey": false,
            "numCitedBy": 306,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "From Pythagoras's harmonic sequence to Einstein's theory of relativity, geometric models of position, proximity, ratio, and the underlying properties of physical space have provided us with powerful ideas and accurate scientific tools. Currently, similar geometric models are being applied to another type of space the conceptual space of information and meaning, where the contributions of Pythagoras and Einstein are a part of the landscape itself. The rich geometry of conceptual space can be glimpsed, for instance, in internet documents: while the documents themselves define a structure of visual layouts and point-to-point links, search engines create an additional structure by matching keywords to nearby documents in a spatial arrangement of content. What the \"Geometry of Meaning\" provides is a much-needed exploration of computational techniques to represent meaning and of the conceptual spaces on which these representations are founded.\""
            },
            "slug": "Geometry-and-Meaning-Widdows",
            "title": {
                "fragments": [],
                "text": "Geometry and Meaning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "What the \"Geometry of Meaning\" provides is a much-needed exploration of computational techniques to represent meaning and of the conceptual spaces on which these representations are founded."
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2218418"
                        ],
                        "name": "Timothy Chklovski",
                        "slug": "Timothy-Chklovski",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Chklovski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timothy Chklovski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1990190"
                        ],
                        "name": "P. Pantel",
                        "slug": "P.-Pantel",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Pantel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pantel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13507979,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c739b915d633cc3c162e4ef1e57b796c2dc2217",
            "isKey": false,
            "numCitedBy": 497,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Broad-coverage repositories of semantic relations between verbs could benefit many NLP tasks. We present a semi-automatic method for extracting fine-grained semantic relations between verbs. We detect similarity, strength, antonymy, enablement, and temporal happens-before relations between pairs of strongly associated verbs using lexicosyntactic patterns over the Web. On a set of 29,165 strongly associated verb pairs, our extraction algorithm yielded 65.5% accuracy. Analysis of error types shows that on the relation strength we achieved 75% accuracy. We provide the resource, called VERBOCEAN, for download at http://semantics.isi.edu/ocean/."
            },
            "slug": "VerbOcean:-Mining-the-Web-for-Fine-Grained-Semantic-Chklovski-Pantel",
            "title": {
                "fragments": [],
                "text": "VerbOcean: Mining the Web for Fine-Grained Semantic Verb Relations"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A semi-automatic method for extracting fine-grained semantic relations between verbs using lexicosyntactic patterns over the Web, which detects similarity, strength, antonymy, enablement, and temporal happens-before relations between pairs of strongly associated verbs."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143645439"
                        ],
                        "name": "Bin Tan",
                        "slug": "Bin-Tan",
                        "structuredName": {
                            "firstName": "Bin",
                            "lastName": "Tan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bin Tan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682136"
                        ],
                        "name": "Fuchun Peng",
                        "slug": "Fuchun-Peng",
                        "structuredName": {
                            "firstName": "Fuchun",
                            "lastName": "Peng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fuchun Peng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 50
                            }
                        ],
                        "text": "Syntactic annotation includes query segmentation (Tan & Peng, 2008) and part of speech tagging (Barr, Jones, & Regelson, 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17848107,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ea775c61144a28136239f1edfa09b6fedd571db0",
            "isKey": false,
            "numCitedBy": 173,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a novel unsupervised approach to query segmentation, an important task in Web search. We use a generative query model to recover a query's underlying concepts that compose its original segmented form. The model's parameters are estimated using an expectation-maximization (EM) algorithm, optimizing the minimum description length objective function on a partial corpus that is specific to the query. To augment this unsupervised learning, we incorporate evidence from Wikipedia.\n Experiments show that our approach dramatically improves performance over the traditional approach that is based on mutual information, and produces comparable results with a supervised method. In particular, the basic generative language model contributes a 7.4% improvement over the mutual information based method (measured by segment F1 on the Intersection test set). EM optimization further improves the performance by 14.3%. Additional knowledge from Wikipedia provides another improvement of 24.3%, adding up to a total of 46% improvement (from 0.530 to 0.774)."
            },
            "slug": "Unsupervised-query-segmentation-using-generative-Tan-Peng",
            "title": {
                "fragments": [],
                "text": "Unsupervised query segmentation using generative language models and wikipedia"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "A novel unsupervised approach to query segmentation, an important task in Web search, using a generative query model to recover a query's underlying concepts that compose its original segmented form using an expectation-maximization algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "WWW"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2095398"
                        ],
                        "name": "Alexander Budanitsky",
                        "slug": "Alexander-Budanitsky",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Budanitsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Budanitsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145036961"
                        ],
                        "name": "Graeme Hirst",
                        "slug": "Graeme-Hirst",
                        "structuredName": {
                            "firstName": "Graeme",
                            "lastName": "Hirst",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Graeme Hirst"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40770371"
                        ],
                        "name": "K. Alcock",
                        "slug": "K.-Alcock",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Alcock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Alcock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2072593976"
                        ],
                        "name": "Jiang\u2014 Conrath",
                        "slug": "Jiang\u2014-Conrath",
                        "structuredName": {
                            "firstName": "Jiang\u2014",
                            "lastName": "Conrath",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiang\u2014 Conrath"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14764558,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "6db4e86e6377cd703aaaf3a3b471b62e033757ae",
            "isKey": false,
            "numCitedBy": 916,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Five different proposed measures of similarity or semantic distance in WordNet were experimentally compared by examining their performance in a real-word spelling correction system. It was found that Jiang and Conrath\u2019s measure gave the best results overall. That of Hirst and St-Onge seriously over-related, that of Resnik seriously under-related, and those of Lin and of Leacock and Chodorow fell in between."
            },
            "slug": "Semantic-distance-in-WordNet:-An-experimental,-of-Budanitsky-Hirst",
            "title": {
                "fragments": [],
                "text": "Semantic distance in WordNet: An experimental, application-oriented evaluation of five measures"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "Five different proposed measures of similarity or semantic distance in WordNet were experimentally compared by examining their performance in a real-word spelling correction system and found that Jiang and Conrath\u2019s measure gave the best results overall."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49543385"
                        ],
                        "name": "Xiaoyong Liu",
                        "slug": "Xiaoyong-Liu",
                        "structuredName": {
                            "firstName": "Xiaoyong",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoyong Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144456145"
                        ],
                        "name": "W. Bruce Croft",
                        "slug": "W.-Bruce-Croft",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Croft",
                            "middleNames": [
                                "Bruce"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Bruce Croft"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 45558661,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "57444ae44a52df1745d54214168e8d31e6002350",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : This chapter reviews research and applications in statistical language modeling for information retrieval (IR) that has emerged within the past several years as a new probabilistic framework for describing information retrieval processes. Generally speaking, statistical language modeling, or more simply, language modeling (LM), refers to the task of estimating a probability distribution that captures statistical regularities of natural language use. Applied to information retrieval, language modeling refers to the problem of estimating the likelihood that a query and a document could have been generated by the same language model, given the language model of the document and with or without a language model of the query."
            },
            "slug": "Statistical-language-modeling-for-information-Liu-Croft",
            "title": {
                "fragments": [],
                "text": "Statistical language modeling for information retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This chapter reviews research and applications in statistical language modeling for information retrieval (IR) that has emerged within the past several years as a new probabilistic framework for describing information retrieval processes."
            },
            "venue": {
                "fragments": [],
                "text": "Annu. Rev. Inf. Sci. Technol."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683562"
                        ],
                        "name": "Preslav Nakov",
                        "slug": "Preslav-Nakov",
                        "structuredName": {
                            "firstName": "Preslav",
                            "lastName": "Nakov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Preslav Nakov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716902"
                        ],
                        "name": "Marti A. Hearst",
                        "slug": "Marti-A.-Hearst",
                        "structuredName": {
                            "firstName": "Marti",
                            "lastName": "Hearst",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marti A. Hearst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6620221,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4f548dbd6a8738c13bd5d7c6c31f1035d901a946",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The UC Berkeley team participated in the SemEval 2007 Task #4, with an approach that leverages the vast size of the Web in order to build lexically-specific features. The idea is to determine which verbs, prepositions, and conjunctions are used in sentences containing a target word pair, and to compare those to features extracted for other word pairs in order to determine which are most similar. By combining these Web features with words from the sentence context, our team was able to achieve the best results for systems of category C and third best for systems of category A."
            },
            "slug": "UCB:-System-Description-for-SemEval-Task-\\#4-Nakov-Hearst",
            "title": {
                "fragments": [],
                "text": "UCB: System Description for SemEval Task \\#4"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The UC Berkeley team participated in the SemEval 2007 Task #4, with an approach that leverages the vast size of the Web in order to build lexically-specific features."
            },
            "venue": {
                "fragments": [],
                "text": "Fourth International Workshop on Semantic Evaluations (SemEval-2007)"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2365155"
                        ],
                        "name": "S. Deerwester",
                        "slug": "S.-Deerwester",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Deerwester",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Deerwester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728602"
                        ],
                        "name": "S. Dumais",
                        "slug": "S.-Dumais",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Dumais",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dumais"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836606"
                        ],
                        "name": "T. Landauer",
                        "slug": "T.-Landauer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Landauer",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Landauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2737579"
                        ],
                        "name": "G. Furnas",
                        "slug": "G.-Furnas",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Furnas",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Furnas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3154682"
                        ],
                        "name": "R. Harshman",
                        "slug": "R.-Harshman",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Harshman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Harshman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Deerwester et al. (1990) showed how the intuitions of Wittgenstein (1953), Harris (1954), Weaver, and Firth could be used in a practical algorithm."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 130
                            }
                        ],
                        "text": "Truncated SVD applied to document similarity is called Latent Semantic Indexing (LSI), but it is called Latent Semantic Analysis (LSA) when applied to word similarity."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Deerwester et al. (1990) found an elegant way to improve similarity measurements with a mathematical operation on the term\u2013document matrix, X, based on linear algebra."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 115
                            }
                        ],
                        "text": "Cognitive scientists have argued that there are empirical and theoretical reasons for believing that VSMs, such as LSA and HAL, are plausible models of some aspects of human cognition (Landauer et al., 2007)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Deerwester et al. (1990) observed that we can shift the focus to measuring word similarity, instead of document similarity, by looking at row vectors in the term\u2013document matrix, instead of column vectors."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Deerwester et al. (1990) were inspired by the term\u2013document matrix of Salton et al. (1975), but a document is not necessarily the optimal length of text for measuring word similarity."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 54
                            }
                        ],
                        "text": "In cognitive science, Latent Semantic Analysis (LSA) (Deerwester et al., 1990; Landauer & Dumais, 1997), Hyperspace Analogue to Language (HAL) (Lund, Burgess, & Atchley, 1995; Lund & Burgess, 1996), and related research (Landauer, McNamara, Dennis, & Kintsch, 2007) is entirely within the scope of\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 117
                            }
                        ],
                        "text": "A low-RAM algorithm, Multislice Projection, for large sparse tensors is presented and evaluated.17\nSince the work of Deerwester et al. (1990), subsequent research has discovered many alternative matrix smoothing processes, such as Nonnegative Matrix Factorization (NMF) (Lee & Seung, 1999),\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "An important technical advance was the discovery that smoothing the term\u2013document matrix by truncated SVD can improve precision and recall (Deerwester et al., 1990), although few commercial systems use smoothing, due to the computational expense when the document collection is large and dynamic."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 122
                            }
                        ],
                        "text": "Distributional hypothesis: Words that occur in similar contexts tend to have similar meanings (Harris, 1954; Firth, 1957; Deerwester et al., 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 16
                            }
                        ],
                        "text": "Latent meaning: Deerwester et al. (1990) and Landauer and Dumais (1997) describe truncated SVD as a method for discovering latent meaning."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 48
                            }
                        ],
                        "text": "In cognitive science, Latent Semantic Analysis (LSA) (Deerwester et al., 1990; Landauer & Dumais, 1997), Hyperspace Analogue to Language (HAL) (Lund, Burgess, & Atchley, 1995; Lund & Burgess, 1996), and related research (Landauer, McNamara, Dennis, & Kintsch, 2007) is entirely within the scope of VSMs, as defined above, since this research uses vector space models in which the values of the elements are derived from event frequencies, such as the number of times that a given word appears in a given context."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3252915,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "20a80a7356859daa4170fb4da6b87b84adbb547f",
            "isKey": true,
            "numCitedBy": 7019,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (\u201csemantic structure\u201d) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. initial tests find this completely automatic method for retrieval to be promising."
            },
            "slug": "Indexing-by-Latent-Semantic-Analysis-Deerwester-Dumais",
            "title": {
                "fragments": [],
                "text": "Indexing by Latent Semantic Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A new method for automatic indexing and retrieval to take advantage of implicit higher-order structure in the association of terms with documents (\u201csemantic structure\u201d) in order to improve the detection of relevant documents on the basis of terms found in queries."
            },
            "venue": {
                "fragments": [],
                "text": "J. Am. Soc. Inf. Sci."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118077301"
                        ],
                        "name": "Jane Morris",
                        "slug": "Jane-Morris",
                        "structuredName": {
                            "firstName": "Jane",
                            "lastName": "Morris",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jane Morris"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145036961"
                        ],
                        "name": "Graeme Hirst",
                        "slug": "Graeme-Hirst",
                        "structuredName": {
                            "firstName": "Graeme",
                            "lastName": "Hirst",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Graeme Hirst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15754496,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "3319f87827b77d6eff9101b4d8beb913a0035c3f",
            "isKey": false,
            "numCitedBy": 137,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "NLP methods and applications need to take account not only of \"classical\" lexical relations, as found in WordNet, but the less-structural, more context-dependent \"non-classical\" relations that readers intuit in text. In a reader-based study of lexical relations in text, most were found to be of the latter type. The relationships themselves are analyzed, and consequences for NLP are discussed."
            },
            "slug": "Non-Classical-Lexical-Semantic-Relations-Morris-Hirst",
            "title": {
                "fragments": [],
                "text": "Non-Classical Lexical Semantic Relations"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "NLP methods and applications need to take account not only of \"classical\" lexical relations, as found in WordNet, but the less-structural, more context-dependent \"non- classical\" relations that readers intuit in text."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the HLT-NAACL Workshop on Computational Lexical Semantics - CLS '04"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153873834"
                        ],
                        "name": "A. Wong",
                        "slug": "A.-Wong",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Wong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40498308"
                        ],
                        "name": "Chung-Shu Yang",
                        "slug": "Chung-Shu-Yang",
                        "structuredName": {
                            "firstName": "Chung-Shu",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chung-Shu Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 123
                            }
                        ],
                        "text": "Bag of words hypothesis: The frequencies of words in a document tend to indicate the relevance of the document to a query (Salton et al., 1975)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 9
                            }
                        ],
                        "text": "We prefer the term attributional similarity to the term semantic relatedness, because attributional similarity emphasizes the contrast with relational similarity, whereas semantic relatedness could be confused with relational similarity."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 57
                            }
                        ],
                        "text": "VSMs extract knowledge automatically from a given corpus, thus they require much less labour than other approaches to semantics, such as hand-coded knowledge bases and ontologies."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 11
                            }
                        ],
                        "text": "The VSM of Salton et al. (1975) was arguably the first practical, useful algorithm for extracting semantic information from word usage."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 88
                            }
                        ],
                        "text": "The bag of words hypothesis is the basis for applying the VSM to information retrieval (Salton et al., 1975)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 69
                            }
                        ],
                        "text": "Deerwester et al. (1990) were inspired by the term\u2013document matrix of Salton et al. (1975), but a document is not necessarily the optimal length of text for measuring word similarity."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 98
                            }
                        ],
                        "text": "Vectors are common in AI and cognitive science; they were common before the VSM was introduced by Salton et al. (1975)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Salton et al. (1975) focused on measuring document similarity, treating a query to a search engine as a pseudo-document."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6473756,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5f169880e30e1f76827d72f862555d00b01bed9",
            "isKey": true,
            "numCitedBy": 7617,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "In a document retrieval, or other pattern matching environment where stored entities (documents) are compared with each other or with incoming patterns (search requests), it appears that the best indexing (property) space is one where each entity lies as far away from the others as possible; in these circumstances the value of an indexing system may be expressible as a function of the density of the object space; in particular, retrieval performance may correlate inversely with space density. An approach based on space density computations is used to choose an optimum indexing vocabulary for a collection of documents. Typical evaluation results are shown, demonstating the usefulness of the model."
            },
            "slug": "A-vector-space-model-for-automatic-indexing-Salton-Wong",
            "title": {
                "fragments": [],
                "text": "A vector space model for automatic indexing"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An approach based on space density computations is used to choose an optimum indexing vocabulary for a collection of documents, demonstating the usefulness of the model."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2237884"
                        ],
                        "name": "Ergun Bi\u00e7ici",
                        "slug": "Ergun-Bi\u00e7ici",
                        "structuredName": {
                            "firstName": "Ergun",
                            "lastName": "Bi\u00e7ici",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ergun Bi\u00e7ici"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 23
                            }
                        ],
                        "text": "Relational clustering: Bi\u00e7ici and Yuret (2006) clustered word pairs by representing them as row vectors in a pair\u2013pattern matrix."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1082,
                                "start": 23
                            }
                        ],
                        "text": "Relational clustering: Bi\u00e7ici and Yuret (2006) clustered word pairs by representing them as row vectors in a pair\u2013pattern matrix. Davidov and Rappoport (2008) first clustered contexts (patterns) and then identified representative pairs for each context cluster. They used the representative pairs to automatically generate multiple-choice analogy questions, in the style of SAT analogy questions. Relational classification: Chklovski and Pantel (2004) used a pair\u2013pattern matrix to classify pairs of verbs into semantic classes. For example, taint : poison is classified as strength (poisoning is stronger than tainting) and assess : review is classified as enablement (assessing is enabled by reviewing). Turney (2005) used a pair\u2013pattern matrix to classify noun compounds into semantic classes. For example, flu virus is classified as cause (the virus causes the flu), home town is classified as location (the home is located in the town), and weather report is classified as topic (the topic of the report is the weather). Relational search: Cafarella, Banko, and Etzioni (2006) described relational search as the task of searching for entities that satisfy given semantic relations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 23
                            }
                        ],
                        "text": "Relational clustering: Bi\u00e7ici and Yuret (2006) clustered word pairs by representing them as row vectors in a pair\u2013pattern matrix. Davidov and Rappoport (2008) first clustered contexts (patterns) and then identified representative pairs for each context cluster."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 720,
                                "start": 23
                            }
                        ],
                        "text": "Relational clustering: Bi\u00e7ici and Yuret (2006) clustered word pairs by representing them as row vectors in a pair\u2013pattern matrix. Davidov and Rappoport (2008) first clustered contexts (patterns) and then identified representative pairs for each context cluster. They used the representative pairs to automatically generate multiple-choice analogy questions, in the style of SAT analogy questions. Relational classification: Chklovski and Pantel (2004) used a pair\u2013pattern matrix to classify pairs of verbs into semantic classes. For example, taint : poison is classified as strength (poisoning is stronger than tainting) and assess : review is classified as enablement (assessing is enabled by reviewing). Turney (2005) used a pair\u2013pattern matrix to classify noun compounds into semantic classes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 452,
                                "start": 23
                            }
                        ],
                        "text": "Relational clustering: Bi\u00e7ici and Yuret (2006) clustered word pairs by representing them as row vectors in a pair\u2013pattern matrix. Davidov and Rappoport (2008) first clustered contexts (patterns) and then identified representative pairs for each context cluster. They used the representative pairs to automatically generate multiple-choice analogy questions, in the style of SAT analogy questions. Relational classification: Chklovski and Pantel (2004) used a pair\u2013pattern matrix to classify pairs of verbs into semantic classes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14009721,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d20611a45574733b6efc408757538bcff6818727",
            "isKey": true,
            "numCitedBy": 20,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We focus on answering word analogy questions by using clustering techniques. The increased performance in answering word similarity questions can have many possible applications, including question answering and information retrieval. We present an analysis of clustering algorithms\u2019 performance on answering word similarity questions. This paper\u2019s contributions can be summarized as: (i) casting the problem of solving word analogy questions as an instance of learning clusterings of data and measuring the effectiveness of prominent clustering techniques in learning semantic relations; (ii) devising a heuristic approach to combine the results of different clusterings for the purpose of distinctly separating word pair semantics; (iii) answering SAT-type word similarity questions using our technique."
            },
            "slug": "Clustering-Word-Pairs-to-Answer-Analogy-Questions-Bi\u00e7ici",
            "title": {
                "fragments": [],
                "text": "Clustering Word Pairs to Answer Analogy Questions"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This paper presents an analysis of clustering algorithms\u2019 performance on answering word similarity questions and proposes a heuristic approach to combine the results of different clusterings for the purpose of distinctly separating word pair semantics."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1885890"
                        ],
                        "name": "T. V. D. Cruys",
                        "slug": "T.-V.-D.-Cruys",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Cruys",
                            "middleNames": [
                                "V.",
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. V. D. Cruys"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7771877,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "89645fff935867e3eb06b3899d6fc38688ced9ca",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract The distributional similarity methods have proven to be a valuable tool for the induction of semantic similarity. Until now, most algorithms use two-way co-occurrence data to compute the meaning of words. Co-occurrence frequencies, however, need not be pairwise. One can easily imagine situations where it is desirable to investigate co-occurrence frequencies of three modes and beyond. This paper will investigate tensor factorization methods to build a model of three-way co-occurrences. The approach is applied to the problem of selectional preference induction, and automatically evaluated in a pseudo-disambiguation task. The results show that tensor factorization, and non-negative tensor factorization in particular, is a promising tool for Natural Language Processing (nlp)."
            },
            "slug": "A-non-negative-tensor-factorization-model-for-Cruys",
            "title": {
                "fragments": [],
                "text": "A non-negative tensor factorization model for selectional preference induction"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Tensor factorization methods are investigated to build a model of three-way co-occurrence frequencies of three modes and beyond and are shown to be a promising tool for Natural Language Processing (nlp)."
            },
            "venue": {
                "fragments": [],
                "text": "Natural Language Engineering"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145848824"
                        ],
                        "name": "Karen Sp\u00e4rck Jones",
                        "slug": "Karen-Sp\u00e4rck-Jones",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Sp\u00e4rck Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karen Sp\u00e4rck Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 166
                            }
                        ],
                        "text": "The most popular way to formalize this idea for term\u2013document matrices is the tf-idf (term frequency \u00d7 inverse document frequency) family of weighting functions (Spa\u0308rck Jones, 1972)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2996187,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "4f09e6ec1b7d4390d23881852fd7240994abeb58",
            "isKey": false,
            "numCitedBy": 3208,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The exhaustivity of document descriptions and the specificity of index terms are usually regarded as independent. It is suggested that specificity should be interpreted statistically, as a function of term use rather than of term meaning. The effects on retrieval of variations in term specificity are examined, experiments with three test collections showing in particular that frequently\u2010occurring terms are required for good overall performance. It is argued that terms should be weighted according to collection frequency, so that matches on less frequent, more specific, terms are of greater value than matches on frequent terms. Results for the test collections show that considerable improvements in performance are obtained with this very simple procedure."
            },
            "slug": "A-statistical-interpretation-of-term-specificity-in-Jones",
            "title": {
                "fragments": [],
                "text": "A statistical interpretation of term specificity and its application in retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is argued that terms should be weighted according to collection frequency, so that matches on less frequent, more specific, terms are of greater value than matches on frequent terms."
            },
            "venue": {
                "fragments": [],
                "text": "J. Documentation"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145702640"
                        ],
                        "name": "Guido Minnen",
                        "slug": "Guido-Minnen",
                        "structuredName": {
                            "firstName": "Guido",
                            "lastName": "Minnen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guido Minnen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144708726"
                        ],
                        "name": "John A. Carroll",
                        "slug": "John-A.-Carroll",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Carroll",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John A. Carroll"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35096557"
                        ],
                        "name": "Darren Pearce",
                        "slug": "Darren-Pearce",
                        "structuredName": {
                            "firstName": "Darren",
                            "lastName": "Pearce",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Darren Pearce"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 34553826,
            "fieldsOfStudy": [
                "Materials Science"
            ],
            "id": "e979925b15861153a0e9ce8ace39a28d319e613d",
            "isKey": false,
            "numCitedBy": 386,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe two newly developed computational tools for morphological processing: a program for analysis of English inflectional morphology, and a morphological generator, automatically derived from the analyser. The tools are fast, being based on finite-state techniques, have wide coverage, incorporating data from various corpora and machine readable dictionaries, and are robust, in that they are able to deal effectively with unknown words. The tools are freely available. We evaluate the accuracy and speed of both tools and discuss a number of practical applications in which they have been put to use."
            },
            "slug": "Applied-morphological-processing-of-English-Minnen-Carroll",
            "title": {
                "fragments": [],
                "text": "Applied morphological processing of English"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "Two newly developed computational tools for morphological processing are described: a program for analysis of English inflectional morphology, and a morphological generator, automatically derived from the analyser, which are fast, being based on finite-state techniques, and robust, in that they are able to deal effectively with unknown words."
            },
            "venue": {
                "fragments": [],
                "text": "Natural Language Engineering"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2360279"
                        ],
                        "name": "Oren Zamir",
                        "slug": "Oren-Zamir",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Zamir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Zamir"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741101"
                        ],
                        "name": "Oren Etzioni",
                        "slug": "Oren-Etzioni",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Etzioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Etzioni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206134308,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5a2b055c8cded61a1112de0abd06586bd604bd6",
            "isKey": false,
            "numCitedBy": 828,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Grouper:-A-Dynamic-Clustering-Interface-to-Web-Zamir-Etzioni",
            "title": {
                "fragments": [],
                "text": "Grouper: A Dynamic Clustering Interface to Web Search Results"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Networks"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2330066"
                        ],
                        "name": "Manu Konchady",
                        "slug": "Manu-Konchady",
                        "structuredName": {
                            "firstName": "Manu",
                            "lastName": "Konchady",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manu Konchady"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 0
                            }
                        ],
                        "text": "Konchady (2008) explains how to integrate Lucene with LingPipe and GATE for sophisticated semantic processing."
                    },
                    "intents": []
                }
            ],
            "corpusId": 59800160,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8bac25f38fdcba5bf4902e7bd6cb41f19fa4818e",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Lucene, LingPipe, and Gate are popular open source tools to build powerful search applications. Building Search Applications describes functions from Lucene that include indexing, searching, ranking, and spelling correction to build search engines. Use LingPipe and Gate to find the meaning of text to make search applications more useful. With this book you will learn to: - Extract tokens from text using custom tokenizers and analyzers from Lucene, LingPipe, and Gate. - Construct a search engine index with an optional backend database to manage large document collections. - Explore the wide range of Lucene queries to search an index, understand the ranking algorithm for a query, and suggest spelling corrections. - Find the names of people, places, and other entities in text using LingPipe and Gate. - Categorize documents by topic using classifiers and build groups of self-organized documents using clustering algorithms from LingPipe. - Create a Web crawler to scan the We b, Intranet, or desktop using Nutch. - Track the sentiment of articles published on the Web with LingPipe - Detect plagiarism of documents using a registered document collection."
            },
            "slug": "Building-Search-Applications:-Lucene,-Lingpipe,-and-Konchady",
            "title": {
                "fragments": [],
                "text": "Building Search Applications: Lucene, Lingpipe, and Gate"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This book describes functions from Lucene that include indexing, searching, ranking, and spelling correction to build search engines and uses LingPipe and Gate to find the meaning of text to make search applications more useful."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746656"
                        ],
                        "name": "E. Voorhees",
                        "slug": "E.-Voorhees",
                        "structuredName": {
                            "firstName": "Ellen",
                            "lastName": "Voorhees",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Voorhees"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 215762892,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "0d8bf6be792deb9b7e499b361a8332f0dce68089",
            "isKey": false,
            "numCitedBy": 536,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The TREC question answering track is an effort to bring the benefits of large-scale evaluation to bear on the question answering problem. The track contained two tasks in TREC 2002, the main task and the list task. Both tasks required that the answer strings returned by the systems consist of nothing more or less than an answer in contrast to the text snippets containing an answer allowed in previous years. A new evaluation measure in the main task, the confidence-weighted score, tested a system\u2019s ability to recognize when it has found a correct answer. The goal of the question answering (QA) track is to foster research on systems that retrieve answers rather than documents in response to a question, with particular emphasis on systems that can function in unrestricted domains. Now in its fourth year, the tasks in the track have evolved over the years to increase the realism of the task and to focus research on particular aspects of the problem deemed important to improving the state-of-the-art. All of the tasks have involved finding answers to closed-class questions within a large corpus of news text. This paper provides an overview of the TREC 2002 QA track. This year\u2019s track contained two tasks, the main task and the list task. Both tasks were also run in TREC 2001, but systems were required to return exact answers this year. That is, the text string returned by the system in response to a question was required to consist of a complete answer and nothing else, in contrast to earlier years where systems could return text strings that simply contained an answer. To make the paper self-contained, the first section recaps the tasks and evaluation procedures used in the first three tracks. The following sections then describe this year\u2019s tasks. 1 Evolution of the TREC QA Track The task in the first two QA tracks (TRECs 8 and 9) was the same. For each question in the question set, systems retrieved a ranked list of up to five text snippets that contained an answer to the question plus a document that supported the answer. The collection of documents from which the support was drawn was a large set of newswire and newspaper articles. The questions were restricted to factoid questions such as In what year did Joe DiMaggio compile his 56game hitting streak? and Name a film in which Jude Law acted. Each question was guaranteed to have at least one document in the collection that explicitly answered it. The maximum length of the text snippets was either 50 or 250 bytes, depending on the run type. Human assessors read each string and decided whether the string actually did contain an answer to the question in the context provided by the document. Given a set of judgments for the strings, the score computed for a submission was the mean reciprocal rank. An individual question received a score equal to the reciprocal of the rank at which the first correct response was returned, or zero if none of the five responses contained a correct answer. The score for a submission was then the mean of the individual questions\u2019 reciprocal ranks. The TREC-8 track both defined how answer strings were judged, and established that different assessors have different ideas as to what constitutes a correct answer even for the limited type of questions used in the track. A [document-id, answer-string] pair was judged correct if, in the opinion of the assessor, the answer-string contained an answer to the question, the answer-string was responsive to the question, and the document supported the answer. If the answer-string was responsive and contained a correct answer, but the document did not support that answer, the pair was judged \u201cNot supported\u201d. Otherwise, the pair was judged incorrect. Requiring that the answer string be responsive to the question addressed a variety of issues. Answer strings that contained multiple entities of the same semantic category as the correct answer but did not indicate which of those entities was the actual answer (e.g., a list of names in response to a who question) were judged as incorrect. Certain punctuation and units were also required. Thus \u201c5 5 billion\u2019\u2019 was not an acceptable substitute for \u201c5.5 billion\u201d, nor was \u201c500\u201d acceptable when the correct answer was \u201c$500\u201d. Finally, unless the question specifically stated otherwise, correct responses for questions about a famous entity had to refer to the famous entity and not to imitations, copies, etc. For example, two TREC-8 questions asked for the height of the Matterhorn (i.e., the Alp) and the replica of the Matterhorn at Disneyland. Correct responses for one of these questions were incorrect for the other. See [6] for a very detailed discussion of responsiveness. To test whether assessor opinions vary, each TREC-8 question was independently judged by three different assessors. The separate judgments were combined into a single judgment set through adjudication for the official track evaluation, but the individual judgments were used to measure the effect of differences in judgments on systems\u2019 scores. Assessors opinions did vary. For example, assessors differed on how much of a name was required and on the desired granularity of dates and locations. Fortunately, as with document retrieval evaluation, the relative mean reciprocal rank scores between QA systems remain stable despite differences in the judgments used to evaluate them [5]. The TREC 2001 track modified the main task to make it more realistic and introduced two new tasks, the list task and the context task. All runs were restricted to answer strings of maximum length 50 bytes since the results from the earlier tracks clearly demonstrated that allowing 250-byte answer strings was a much simpler problem. In the main task, the guarantee that a question had an answer in the document collection was eliminated. A system returned the string \u201cNIL\u201d to indicate its belief that there was no answer in the document collection. NIL was marked correct if there was no known answer for that question in the collection and incorrect otherwise. The list task required systems to assemble an answer from information located in multiple documents. Such questions are harder to answer than the questions used in the main task since information duplicated in the documents must be detected and reported only once. Each question in the list task specified a particular kind of information to be retrieved, such as Who are 6 actors who have played Tevye in \u201cFiddler on the Roof\u201d?. Systems returned an unordered list of [document-id, answer-string] pairs where each pair represented a single instance. Results were scored using mean accuracy, which is the ratio of the number of distinct correct responses retrieved to the target number of responses requested. The context task was a pilot evaluation for question answering within a particular scenario or context. The task was designed to represent the kind of dialog processing that a system would need to support an interactive user session. Questions were grouped into different series, and the QA system was expected to track the discourse objects across the individual questions of a series. Unfortunately, the results in the pilot were completely dominated by whether or not a system could answer the particular type of question: the ability to correctly answer questions later in a series was uncorrelated with the ability to correctly answer questions earlier in the series. Thus the task was not repeated in TREC 2002. 2 The TREC 2002 QA Track The TREC 2002 track repeated the main and list tasks from 2001, but with the major difference of requiring systems to return exact answers. The change to exact answers was motivated by the belief that a system\u2019s ability to recognize the precise extent of the answer is crucial to improving question answering technology. The problems with using text snippets containing the answer as responses were illustrated in the TREC 2001 track. For example, each of the answer strings shown in Figure 1 was judged correct for the question What river in the US is known as the Big Muddy?, yet earlier responses are clearly better than later ones. Judging only exact answers correct forces systems to demonstrate that they know precisely where the answer lies in such strings. What constitutes an \u201cexact answer\u201d? As with correctness, exactness is essentially a personal opinion. NIST provided guidelines to the assessors so that questions would be judged similarly, but in the end whether or not an answer was exact was up to the assessor. The guidelines given to the assessors are reproduced in Figure 2. Notice that even \u201cgood\u201d responses that contain a correct answer and justification for that answer were considered inexact for the purposes of this evaluation. A system response consisting of an [document-id, answer-string] pair was assigned exactly one judgment by a human assessor as follows: wrong: the answer string does not contain a correct answer or the answer is not responsive; not supported: the answer string contains a correct answer but the document returned does not support that answer; not exact: the answer string contains a correct answer and the document supports that answer, but the string contains more than just the answer (or is missing bits of the answer);"
            },
            "slug": "Overview-of-the-TREC-2002-Question-Answering-Track-Voorhees",
            "title": {
                "fragments": [],
                "text": "Overview of the TREC 2002 Question Answering Track"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper provides an overview of the TREC 2002 QA track, which defined how answer strings were judged, and established that different assessors have different ideas as to what constitutes a correct answer even for the limited type of questions used in the track."
            },
            "venue": {
                "fragments": [],
                "text": "TREC"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144494993"
                        ],
                        "name": "R. Jones",
                        "slug": "R.-Jones",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068774701"
                        ],
                        "name": "Benjamin Rey",
                        "slug": "Benjamin-Rey",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Rey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Rey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734627"
                        ],
                        "name": "Omid Madani",
                        "slug": "Omid-Madani",
                        "structuredName": {
                            "firstName": "Omid",
                            "lastName": "Madani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Omid Madani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153688898"
                        ],
                        "name": "W. Greiner",
                        "slug": "W.-Greiner",
                        "structuredName": {
                            "firstName": "Wiley",
                            "lastName": "Greiner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Greiner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207159138,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0fd30ce8d311cb97725670b5a176cec8ac71677",
            "isKey": false,
            "numCitedBy": 707,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce the notion of query substitution, that is, generating a new query to replace a user's original search query. Our technique uses modifications based on typical substitutions web searchers make to their queries. In this way the new query is strongly related to the original query, containing terms closely related to all of the original terms. This contrasts with query expansion through pseudo-relevance feedback, which is costly and can lead to query drift. This also contrasts with query relaxation through boolean or TFIDF retrieval, which reduces the specificity of the query. We define a scale for evaluating query substitution, and show that our method performs well at generating new queries related to the original queries. We build a model for selecting between candidates, by using a number of features relating the query-candidate pair, and by fitting the model to human judgments of relevance of query suggestions. This further improves the quality of the candidates generated. Experiments show that our techniques significantly increase coverage and effectiveness in the setting of sponsored search."
            },
            "slug": "Generating-query-substitutions-Jones-Rey",
            "title": {
                "fragments": [],
                "text": "Generating query substitutions"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A model for selecting between candidates is built, by using a number of features relating the query-candidate pair, and by fitting the model to human judgments of relevance of query suggestions, which improves the quality of the candidates generated."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '06"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065539629"
                        ],
                        "name": "Howard Johnson",
                        "slug": "Howard-Johnson",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Howard Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145900862"
                        ],
                        "name": "Joel D. Martin",
                        "slug": "Joel-D.-Martin",
                        "structuredName": {
                            "firstName": "Joel",
                            "lastName": "Martin",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joel D. Martin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 159
                            }
                        ],
                        "text": "For instance, Rapp (2003) used a vector-based representation of word meaning to achieve a score of 92.5% on multiple-choice synonym questions from the Test of English as a Foreign Language (TOEFL), whereas the average human score was 64.5%.1 Turney (2006) used a vector-based representation of semantic relations to attain a score of 56% on multiple-choice analogy questions from the SAT college entrance test, compared to an average human score of 57%.2\nIn this survey, we have organized past work with VSMs according to the type of matrix involved: term\u2013document, word\u2013context, and pair\u2013pattern."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 112
                            }
                        ],
                        "text": "The elements in this slice correspond to all the patterns that relate the given TOEFL word to any word in Basic English."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 106
                            }
                        ],
                        "text": "A single word in an agglutinative language may correspond to a sentence of half a dozen words in English (Johnson & Martin, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 8
                            }
                        ],
                        "text": "Even in English, case folding can cause problems, because case sometimes has semantic significance."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 6
                            }
                        ],
                        "text": "Basic English is a highly reduced subset of English, designed to be easy for people to learn."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 24
                            }
                        ],
                        "text": "Case folding is easy in English, but can be problematic in some languages."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 153
                            }
                        ],
                        "text": "In Turney\u2019s (2007) tensor, for example, rows correspond to words from the TOEFL multiple-choice synonym questions, columns correspond to words from Basic English (Ogden, 1930),12 and tubes correspond to patterns that join rows and columns (hence we have a word\u2013word\u2013pattern third-order tensor)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 16
                            }
                        ],
                        "text": "Tokenization of English seems simple at first glance: words are separated by spaces."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 12
                            }
                        ],
                        "text": "An accurate English tokenizer must know how to handle punctuation (e.g., don\u2019t, Jane\u2019s, and/or), hyphenation (e.g., state-of-the-art versus state of the art), and recognize multi-word terms (e.g., Barack Obama and ice hockey) (Manning et al., 2008)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 19
                            }
                        ],
                        "text": "The words of Basic English are listed at http://ogden.basic-english.org/."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 152
                            }
                        ],
                        "text": "Landauer and Dumais (1997) applied truncated SVD to word similarity, achieving human-level scores on multiple-choice synonym questions from the Test of English as a Foreign Language (TOEFL)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 42
                            }
                        ],
                        "text": "This assumption is approximately true for English, and it may work sufficiently well for a basic VSM, but a more advanced VSM requires a more sophisticated approach to tokenization."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 3
                            }
                        ],
                        "text": "In English, affixes are simpler and more regular than in many other languages, and stemming algorithms based on heuristics (rules of thumb) work relatively well (Lovins, 1968; Porter, 1980; Minnen, Carroll, & Pearce, 2001)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2362250,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c2294dbedae7df36a950303b76027b0ab08ac159",
            "isKey": true,
            "numCitedBy": 40,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a simple unsupervised technique for learning morphology by identifying hubs in an automaton. For our purposes, a hub is a node in a graph with in-degree greater than one and out-degree greater than one. We create a word-trie, transform it into a minimal DFA, then identify hubs. Those hubs mark the boundary between root and suffix, achieving similar performance to more complex mixtures of techniques."
            },
            "slug": "Unsupervised-Learning-of-Morphology-for-English-and-Johnson-Martin",
            "title": {
                "fragments": [],
                "text": "Unsupervised Learning of Morphology for English and Inuktitut"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A simple unsupervised technique for learning morphology by identifying hubs in an automaton, achieving similar performance to more complex mixtures of techniques."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1990190"
                        ],
                        "name": "P. Pantel",
                        "slug": "P.-Pantel",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Pantel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pantel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 94
                            }
                        ],
                        "text": "Several approaches to measuring the semantic similarity of words combine a VSM with a lexicon (Turney et al., 2003; Pantel, 2005; Patwardhan & Pedersen, 2006; Mohammad & Hirst, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 546,
                                "start": 129
                            }
                        ],
                        "text": "(contexts) consist of advertiser identifiers (Gleich & Zhukov, 2004) or co-bidded bidterms (second order co-occurrences) (Chang, Pantel, Popescu, & Gabrilovich, 2009). Information extraction: The field of information extraction (IE) includes named entity recognition (NER: recognizing that a chunk of text is the name of an entity, such as a person or a place), relation extraction, event extraction, and fact extraction. Pa\u015fca et al. (2006) demonstrate that a word\u2013context frequency matrix can facilitate fact extraction. Vyas and Pantel (2009) propose a semi-supervised model using a word\u2013context matrix for building and iteratively refining arbitrary classes of named entities."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 442,
                                "start": 129
                            }
                        ],
                        "text": "(contexts) consist of advertiser identifiers (Gleich & Zhukov, 2004) or co-bidded bidterms (second order co-occurrences) (Chang, Pantel, Popescu, & Gabrilovich, 2009). Information extraction: The field of information extraction (IE) includes named entity recognition (NER: recognizing that a chunk of text is the name of an entity, such as a person or a place), relation extraction, event extraction, and fact extraction. Pa\u015fca et al. (2006) demonstrate that a word\u2013context frequency matrix can facilitate fact extraction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 121
                            }
                        ],
                        "text": "Snow, Jurafsky, and Ng (2006) used a pair\u2013pattern matrix to build a hypernym-hyponym taxonomy, whereas Pennacchiotti and Pantel (2006) built a meronymy and causation taxonomy."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 121
                            }
                        ],
                        "text": "Snow, Jurafsky, and Ng (2006) used a pair\u2013pattern matrix to build a hypernym-hyponym taxonomy, whereas Pennacchiotti and Pantel (2006) built a meronymy and causation taxonomy. Turney (2008b) showed how a pair\u2013pattern matrix can distinguish synonyms from antonyms, synonyms from non-synonyms, and taxonomically similar words (hair and fur) from words that are merely semantically associated (cradle and baby)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 570187,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "02d0127113314da26752a55d238689159c414f50",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present an unsupervised methodology for propagating lexical cooccurrence vectors into an ontology such as WordNet. We evaluate the framework on the task of automatically attaching new concepts into the ontology. Experimental results show 73.9% attachment accuracy in the first position and 81.3% accuracy in the top-5 positions. This framework could potentially serve as a foundation for ontologizing lexical-semantic resources and assist the development of other largescale and internally consistent collections of semantic information."
            },
            "slug": "Inducing-Ontological-Co-occurrence-Vectors-Pantel",
            "title": {
                "fragments": [],
                "text": "Inducing Ontological Co-occurrence Vectors"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "An unsupervised methodology for propagating lexical cooccurrence vectors into an ontology such as WordNet is presented and evaluated on the task of automatically attaching new concepts into the ontology."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733593"
                        ],
                        "name": "J. Curran",
                        "slug": "J.-Curran",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Curran",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Curran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2085030"
                        ],
                        "name": "M. Moens",
                        "slug": "M.-Moens",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Moens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Moens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8927694,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ea419a6e8583d12f6b91be48df41edb7a8b8d6b6",
            "isKey": false,
            "numCitedBy": 253,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of semantic resources is common in modern NLP systems, but methods to extract lexical semantics have only recently begun to perform well enough for practical use. We evaluate existing and new similarity metrics for thesaurus extraction, and experiment with the trade-off between extraction performance and efficiency. We propose an approximation algorithm, based on canonical attributes and coarse- and fine-grained matching, that reduces the time complexity and execution time of thesaurus extraction with only a marginal performance penalty."
            },
            "slug": "Improvements-in-Automatic-Thesaurus-Extraction-Curran-Moens",
            "title": {
                "fragments": [],
                "text": "Improvements in Automatic Thesaurus Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "An approximation algorithm is proposed, based on canonical attributes and coarse- and fine-grained matching, that reduces the time complexity and execution time of thesaurus extraction with only a marginal performance penalty."
            },
            "venue": {
                "fragments": [],
                "text": "ACL 2002"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 168
                            }
                        ],
                        "text": "In machine learning, a typical problem is to learn to classify or cluster a set of items (i.e., examples, cases, individuals, entities) represented as feature vectors (Mitchell, 1997; Witten & Frank, 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 73
                            }
                        ],
                        "text": ", examples, cases, individuals, entities) represented as feature vectors (Mitchell, 1997; Witten & Frank, 2005)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6134427,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aab43c9c33af00b718cf2ae374b861d49862a563",
            "isKey": false,
            "numCitedBy": 15727,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Machine Learning is the study of methods for programming computers to learn. Computers are applied to a wide range of tasks, and for most of these it is relatively easy for programmers to design and implement the necessary software. However, there are many tasks for which this is difficult or impossible. These can be divided into four general categories. First, there are problems for which there exist no human experts. For example, in modern automated manufacturing facilities, there is a need to predict machine failures before they occur by analyzing sensor readings. Because the machines are new, there are no human experts who can be interviewed by a programmer to provide the knowledge necessary to build a computer system. A machine learning system can study recorded data and subsequent machine failures and learn prediction rules. Second, there are problems where human experts exist, but where they are unable to explain their expertise. This is the case in many perceptual tasks, such as speech recognition, hand-writing recognition, and natural language understanding. Virtually all humans exhibit expert-level abilities on these tasks, but none of them can describe the detailed steps that they follow as they perform them. Fortunately, humans can provide machines with examples of the inputs and correct outputs for these tasks, so machine learning algorithms can learn to map the inputs to the outputs. Third, there are problems where phenomena are changing rapidly. In finance, for example, people would like to predict the future behavior of the stock market, of consumer purchases, or of exchange rates. These behaviors change frequently, so that even if a programmer could construct a good predictive computer program, it would need to be rewritten frequently. A learning program can relieve the programmer of this burden by constantly modifying and tuning a set of learned prediction rules. Fourth, there are applications that need to be customized for each computer user separately. Consider, for example, a program to filter unwanted electronic mail messages. Different users will need different filters. It is unreasonable to expect each user to program his or her own rules, and it is infeasible to provide every user with a software engineer to keep the rules up-to-date. A machine learning system can learn which mail messages the user rejects and maintain the filtering rules automatically. Machine learning addresses many of the same research questions as the fields of statistics, data mining, and psychology, but with differences of emphasis. Statistics focuses on understanding the phenomena that have generated the data, often with the goal of testing different hypotheses about those phenomena. Data mining seeks to find patterns in the data that are understandable by people. Psychological studies of human learning aspire to understand the mechanisms underlying the various learning behaviors exhibited by people (concept learning, skill acquisition, strategy change, etc.)."
            },
            "slug": "Machine-learning-Dietterich",
            "title": {
                "fragments": [],
                "text": "Machine learning"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Machine learning addresses many of the same research questions as the fields of statistics, data mining, and psychology, but with differences of emphasis."
            },
            "venue": {
                "fragments": [],
                "text": "CSUR"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690470"
                        ],
                        "name": "C. Crouch",
                        "slug": "C.-Crouch",
                        "structuredName": {
                            "firstName": "Carolyn",
                            "lastName": "Crouch",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Crouch"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 126
                            }
                        ],
                        "text": "Several researchers have used word\u2013context matrices specifically for the task of assisting or automating thesaurus generation (Crouch, 1988; Grefenstette, 1994; Ruge, 1997; Pantel & Lin, 2002a; Curran & Moens, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 21958293,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0a4c2e3c9f4906e8ece40a37e3949d06ed81b9a",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The importance of a thesaurus in the successful operation of an information retrieval system is well recognized. Yet techniques which support the automatic generation of thesauri remain largely undiscovered. This paper describes one approach to the automatic generation of global thesauri, based on the discrimination value model of Salton, Yang, and Yu and on an appropriate clustering algorithm. This method has been implemented and applied to two document collections. Preliminary results indicate that this method, which produces improvements in retrieval performance in excess of 10 and 15 percent in the test collections, is viable and worthy of continued investigation."
            },
            "slug": "A-cluster-based-approach-to-thesaurus-construction-Crouch",
            "title": {
                "fragments": [],
                "text": "A cluster-based approach to thesaurus construction"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper describes one approach to the automatic generation of global thesauri, based on the discrimination value model of Salton, Yang, and Yu and on an appropriate clustering algorithm, which has been implemented and applied to two document collections."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '88"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057287718"
                        ],
                        "name": "M. Porter",
                        "slug": "M.-Porter",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Porter",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Porter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 176
                            }
                        ],
                        "text": "In English, affixes are simpler and more regular than in many other languages, and stemming algorithms based on heuristics (rules of thumb) work relatively well (Lovins, 1968; Porter, 1980; Minnen, Carroll, & Pearce, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 14
                            }
                        ],
                        "text": "For instance, Rapp (2003) used a vector-based representation of word meaning to achieve a score of 92."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6093716,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "a651bb7cc7fc68ece0cc66ab921486d163373385",
            "isKey": false,
            "numCitedBy": 6533,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The automatic removal of suffixes from words in English is of particular interest in the field of information retrieval. An algorithm for suffix stripping is described, which has been implemented as a short, fast program in BCPL. Although simple, it performs slightly better than a much more elaborate system with which it has been compared. It effectively works by treating complex suffixes as compounds made up of simple suffixes, and removing the simple suffixes in a number of steps. In each step the removal of the suffix is made to depend upon the form of the remaining stem, which usually involves a measure of its syllable length."
            },
            "slug": "An-algorithm-for-suffix-stripping-Porter",
            "title": {
                "fragments": [],
                "text": "An algorithm for suffix stripping"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "An algorithm for suffix stripping is described, which has been implemented as a short, fast program in BCPL and performs slightly better than a much more elaborate system with which it has been compared."
            },
            "venue": {
                "fragments": [],
                "text": "Program"
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1774745"
                        ],
                        "name": "A. Broder",
                        "slug": "A.-Broder",
                        "structuredName": {
                            "firstName": "Andrei",
                            "lastName": "Broder",
                            "middleNames": [
                                "Z."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Broder"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "dex vectors of each non-unique coordinate of r. Random Indexing was shown to perform as well as LSA on a word synonym selection task (Karlgren &amp; Sahlgren, 2001). Locality sensitive hashing (LSH) (Broder, 1997) is another technique that approximates the similarity matrix with complexity O(n2 r\u03b4 2), where \u03b4 is a constant number of random projections, which controls the accuracy versuse\ufb03ciency tradeo\ufb00.21 LSHi"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "s, such that two similar vectors are likely to have similar \ufb01ngerprints. De\ufb01nitions of LSH functions include the Min-wise independent function, which preserves the Jaccard similarity between vectors (Broder, 1997), and functions that preserve the cosine similarity between vectors (Charikar, 2002). On a word similarity task, Ravichandran et al. (2005) showed that, on average, over 80% of the top-10 similar word"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11748509,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8addb1718c2bc6bbb0d82cd1a57b41198bf65965",
            "isKey": false,
            "numCitedBy": 1822,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Given two documents A and B we define two mathematical notions: their resemblance r(A, B) and their containment c(A, B) that seem to capture well the informal notions of \"roughly the same\" and \"roughly contained.\" The basic idea is to reduce these issues to set intersection problems that can be easily evaluated by a process of random sampling that can be done independently for each document. Furthermore, the resemblance can be evaluated using a fixed size sample for each document. This paper discusses the mathematical properties of these measures and the efficient implementation of the sampling process using Rabin (1981) fingerprints."
            },
            "slug": "On-the-resemblance-and-containment-of-documents-Broder",
            "title": {
                "fragments": [],
                "text": "On the resemblance and containment of documents"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The basic idea is to reduce these issues to set intersection problems that can be easily evaluated by a process of random sampling that could be done independently for each document."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No.97TB100171)"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054859870"
                        ],
                        "name": "James Gorman",
                        "slug": "James-Gorman",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Gorman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Gorman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733593"
                        ],
                        "name": "J. Curran",
                        "slug": "J.-Curran",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Curran",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Curran"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12564904,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0eb7f8ccf96f714998245a8cb1a35690a3adbafb",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Accurately representing synonymy using distributional similarity requires large volumes of data to reliably represent infrequent words. However, the naive nearest-neighbour approach to comparing context vectors extracted from large corpora scales poorly (O(n2) in the vocabulary size).In this paper, we compare several existing approaches to approximating the nearest-neighbour search for distributional similarity. We investigate the trade-off between efficiency and accuracy, and find that SASH (Houle and Sakuma, 2005) provides the best balance."
            },
            "slug": "Scaling-Distributional-Similarity-to-Large-Corpora-Gorman-Curran",
            "title": {
                "fragments": [],
                "text": "Scaling Distributional Similarity to Large Corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper compares several existing approaches to approximating the nearest-neighbour search for distributional similarity, and finds that SASH (Houle and Sakuma, 2005) provides the best balance."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680292"
                        ],
                        "name": "P. Resnik",
                        "slug": "P.-Resnik",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Resnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Resnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 186
                            }
                        ],
                        "text": "In computational linguistics, the term semantic similarity is applied to words that share a hypernym (car and bicycle are semantically similar, because they share the hypernym vehicle) (Resnik, 1995)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 170
                            }
                        ],
                        "text": "Gathering a corpus for a new language is generally much easier than building a lexicon, and building a lexicon often involves also gathering a corpus, such as SemCor for WordNet (Miller, Leacock, Tengi, & Bunker, 1993)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 221,
                                "start": 110
                            }
                        ],
                        "text": "The main alternatives to VSMs for measuring word similarity are approaches that use lexicons, such as WordNet (Resnik, 1995; Jiang & Conrath, 1997; Hirst & St-Onge, 1998; Leacock & Chodrow, 1998; Budanitsky & Hirst, 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 318,
                                "start": 311
                            }
                        ],
                        "text": "For example, the main resource used in Rapp\u2019s (2003) VSM system for measuring word similarity is the British National Corpus (BNC),4 whereas the main resource used in non-VSM systems for measuring word similarity (Hirst & St-Onge, 1998; Leacock & Chodrow, 1998; Jarmasz & Szpakowicz, 2003) is a lexicon, such as WordNet5 or Roget\u2019s Thesaurus."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1752785,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "265be00bf112c6cb2fa3e8176bff8394a114dbde",
            "isKey": true,
            "numCitedBy": 3889,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new measure of semantic similarity in an IS-A taxonomy, based on the notion of information content. Experimental evaluation suggests that the measure performs encouragingly well (a correlation of r = 0.79 with a benchmark set of human similarity judgments, with an upper bound of r = 0.90 for human subjects performing the same task), and significantly better than the traditional edge counting approach (r = 0.66)."
            },
            "slug": "Using-Information-Content-to-Evaluate-Semantic-in-a-Resnik",
            "title": {
                "fragments": [],
                "text": "Using Information Content to Evaluate Semantic Similarity in a Taxonomy"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This paper presents a new measure of semantic similarity in an IS-A taxonomy, based on the notion of information content, which performs encouragingly well and is significantly better than the traditional edge counting approach."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5149784"
                        ],
                        "name": "C. Chiarello",
                        "slug": "C.-Chiarello",
                        "structuredName": {
                            "firstName": "Christine",
                            "lastName": "Chiarello",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Chiarello"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50611682"
                        ],
                        "name": "C. Burgess",
                        "slug": "C.-Burgess",
                        "structuredName": {
                            "firstName": "Curt",
                            "lastName": "Burgess",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burgess"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47070773"
                        ],
                        "name": "L. Richards",
                        "slug": "L.-Richards",
                        "structuredName": {
                            "firstName": "Lorie",
                            "lastName": "Richards",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Richards"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122911260"
                        ],
                        "name": "Alma Pollock",
                        "slug": "Alma-Pollock",
                        "structuredName": {
                            "firstName": "Alma",
                            "lastName": "Pollock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alma Pollock"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 697,
                                "start": 81
                            }
                        ],
                        "text": "Turney (2006) evaluated this approach to relational similarity with 374 multiple-choice analogy questions from the SAT college entrance test, achieving human-level performance (56% correct for the pair\u2013pattern matrix and 57% correct for the average US college applicant). This is the highest performance so far for an algorithm. The best algorithm based on attributional similarity has an accuracy of only 35% (Turney, 2006). The best non-VSM algorithm achieves 43% (Veale, 2004). Pattern similarity: Instead of measuring the similarity between row vectors in a pair\u2013 pattern matrix, we can measure the similarity between columns; that is, we can measure pattern similarity. Lin and Pantel (2001) constructed a pair\u2013pattern matrix in which the patterns were derived from parsed text."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 21461440,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "6ce359aa42f0883c35989153cdb7631a68cb3c7a",
            "isKey": false,
            "numCitedBy": 356,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Semantic-and-associative-priming-in-the-cerebral-\u2026-Chiarello-Burgess",
            "title": {
                "fragments": [],
                "text": "Semantic and associative priming in the cerebral hemispheres: Some words do, some words don't \u2026 sometimes, some places"
            },
            "venue": {
                "fragments": [],
                "text": "Brain and Language"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3332330"
                        ],
                        "name": "George Forman",
                        "slug": "George-Forman",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Forman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George Forman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1401,
                                "start": 53
                            }
                        ],
                        "text": "The simplest way to improve information retrieval performance is to limit the number of vector components. Keeping only components representing the most frequently occurring content words is such a way; however, common words, such as the and have, carry little semantic discrimination power. Simple component smoothing heuristics, based on the properties of the weighting schemes presented in Section 4.2, have been shown to both maintain semantic discrimination power and improve the performance of similarity computations. Computing the similarity between all pairs of vectors, described in Section 4.4, is a computationally intensive task. However, only vectors that share a non-zero coordinate must be compared (i.e., two vectors that do not share a coordinate are dissimilar). Very frequent context words, such as the word the, unfortunately result in most vectors matching a non-zero coordinate. Such words are precisely the contexts that have little semantic discrimination power. Consider the pointwise mutual information weighting described in Section 4.2. Highly weighted dimensions co-occur frequently with only very few words and are by definition highly discriminating contexts (i.e., they have very high association with the words with which they co-occur). By keeping only the context-word dimensions with a PMI above a conservative threshold and setting the others to zero, Lin (1998) showed that the number of comparisons needed to compare vectors greatly decreases while losing little precision in the similarity score between the top-200 most similar words of every word."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 452,
                                "start": 104
                            }
                        ],
                        "text": "5% on the TOEFL questions, Landauer and Dumais (1997) note that, \u201cAlthough we do not know how such a performance would compare, for example, with U.S. school children of a particular age, we have been told that the average score is adequate for admission to many universities.\u201d 2. This is the average score for highschool students in their senior year, applying to US universities. For more discussion of this score, see Section 6.3 in Turney\u2019s (2006) paper."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Forman (2003) provides a good study of feature selection methods for text classification."
                    },
                    "intents": []
                }
            ],
            "corpusId": 809191,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ad32f5295c2e0f7d077fee722aff2998e2af72e7",
            "isKey": true,
            "numCitedBy": 2730,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Machine learning for text classification is the cornerstone of document categorization, news filtering, document routing, and personalization. In text domains, effective feature selection is essential to make the learning task efficient and more accurate. This paper presents an empirical comparison of twelve feature selection methods (e.g. Information Gain) evaluated on a benchmark of 229 text classification problem instances that were gathered from Reuters, TREC, OHSUMED, etc. The results are analyzed from multiple goal perspectives-accuracy, F-measure, precision, and recall-since each is appropriate in different situations. The results reveal that a new feature selection metric we call 'Bi-Normal Separation' (BNS), outperformed the others by a substantial margin in most situations. This margin widened in tasks with high class skew, which is rampant in text classification problems and is particularly challenging for induction algorithms. A new evaluation methodology is offered that focuses on the needs of the data mining practitioner faced with a single dataset who seeks to choose one (or a pair of) metrics that are most likely to yield the best performance. From this perspective, BNS was the top single choice for all goals except precision, for which Information Gain yielded the best result most often. This analysis also revealed, for example, that Information Gain and Chi-Squared have correlated failures, and so they work poorly together. When choosing optimal pairs of metrics for each of the four performance goals, BNS is consistently a member of the pair---e.g., for greatest recall, the pair BNS + F1-measure yielded the best performance on the greatest number of tasks by a considerable margin."
            },
            "slug": "An-Extensive-Empirical-Study-of-Feature-Selection-Forman",
            "title": {
                "fragments": [],
                "text": "An Extensive Empirical Study of Feature Selection Metrics for Text Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An empirical comparison of twelve feature selection methods evaluated on a benchmark of 229 text classification problem instances, revealing that a new feature selection metric, called 'Bi-Normal Separation' (BNS), outperformed the others by a substantial margin in most situations and was the top single choice for all goals except precision."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724629"
                        ],
                        "name": "Marius Pasca",
                        "slug": "Marius-Pasca",
                        "structuredName": {
                            "firstName": "Marius",
                            "lastName": "Pasca",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marius Pasca"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744846"
                        ],
                        "name": "Jeffrey P. Bigham",
                        "slug": "Jeffrey-P.-Bigham",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Bigham",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeffrey P. Bigham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2049693"
                        ],
                        "name": "Andrei Lifchits",
                        "slug": "Andrei-Lifchits",
                        "structuredName": {
                            "firstName": "Andrei",
                            "lastName": "Lifchits",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrei Lifchits"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777385"
                        ],
                        "name": "Alpa Jain",
                        "slug": "Alpa-Jain",
                        "structuredName": {
                            "firstName": "Alpa",
                            "lastName": "Jain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alpa Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17121460,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "708355e97d0a1d9e074261cd6e4ec6cdf2d55031",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "In a new approach to large-scale extraction of facts from unstructured text, distributional similarities become an integral part of both the iterative acquisition of high-coverage contextual extraction patterns, and the validation and ranking of candidate facts. The evaluation measures the quality and coverage of facts extracted from one hundred million Web documents, starting from ten seed facts and using no additional knowledge, lexicons or complex tools."
            },
            "slug": "Names-and-Similarities-on-the-Web:-Fact-Extraction-Pasca-Lin",
            "title": {
                "fragments": [],
                "text": "Names and Similarities on the Web: Fact Extraction in the Fast Lane"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "In a new approach to large-scale extraction of facts from unstructured text, distributional similarities become an integral part of both the iterative acquisition of high-coverage contextual extraction patterns and the validation and ranking of candidate facts."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3133664"
                        ],
                        "name": "D. Cutting",
                        "slug": "D.-Cutting",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Cutting",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cutting"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34165212"
                        ],
                        "name": "Jan O. Pedersen",
                        "slug": "Jan-O.-Pedersen",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Pedersen",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan O. Pedersen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743286"
                        ],
                        "name": "D. Karger",
                        "slug": "D.-Karger",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Karger",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Karger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2016914"
                        ],
                        "name": "J. Tukey",
                        "slug": "J.-Tukey",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Tukey",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tukey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 373655,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "827e1fc081f62f245946df1f347d86af635716eb",
            "isKey": false,
            "numCitedBy": 1191,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Document clustering has not been well received as an information retrieval tool. Objections to its use fall into two main categories: first, that clustering is too slow for large corpora (with running time often quadratic in the number of documents); and second, that clustering does not appreciably improve retrieval.\nWe argue that these problems arise only when clustering is used in an attempt to improve conventional search techniques. However, looking at clustering as an information access tool in its own right obviates these objections, and provides a powerful new access paradigm. We present a document browsing technique that employs document clustering as its primary operation. We also present fast (linear time) clustering algorithms which support this interactive browsing paradigm."
            },
            "slug": "Scatter/Gather:-a-cluster-based-approach-to-large-Cutting-Pedersen",
            "title": {
                "fragments": [],
                "text": "Scatter/Gather: a cluster-based approach to browsing large document collections"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work presents a document browsing technique that employs document clustering as its primary operation, and presents fast (linear time) clustering algorithms which support this interactive browsing paradigm."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '92"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1990190"
                        ],
                        "name": "P. Pantel",
                        "slug": "P.-Pantel",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Pantel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pantel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 184
                            }
                        ],
                        "text": "In applications dealing with polysemy, one approach uses vectors that represent word tokens (Schu\u0308tze, 1998; Agirre & Edmonds, 2006) and another uses vectors that represent word types (Pantel & Lin, 2002a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 13
                            }
                        ],
                        "text": "For example, Pantel and Lin (2002a) presented an algorithm that can discover word senses by clustering row vectors in a word\u2013context matrix, using contextual information derived from parsing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 20
                            }
                        ],
                        "text": "An example follows (Pantel & Lin, 2002a):\n\u03b4ij = fij fij + 1 \u00b7 min (\n\u2211nr k=1 fkj, \u2211nc k=1 fik)\nmin ( \u2211nr k=1 fkj, \u2211nc k=1 fik) + 1 (6)\nnewpmiij = \u03b4ij \u00b7 pmiij (7)\nAnother way to deal with infrequent events is Laplace smoothing of the probability estimates, pij , pi\u2217, and p\u2217j (Turney & Littman, 2003)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 149
                            }
                        ],
                        "text": "An alternative to tf-idf is Pointwise Mutual Information (PMI) (Church & Hanks, 1989; Turney, 2001), which works well for both word\u2013context matrices (Pantel & Lin, 2002a) and term\u2013document matrices (Pantel & Lin, 2002b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 68
                            }
                        ],
                        "text": "The leading algorithms for measuring semantic relatedness use VSMs (Pantel & Lin, 2002a; Rapp, 2003; Turney, Littman, Bigham, & Shnayder, 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12739674,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6e69d3057fa8088c3af1b0e36b872d082997347f",
            "isKey": false,
            "numCitedBy": 198,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Document clustering is useful in many information retrieval tasks: document browsing, organization and viewing of retrieval results, generation of Yahoo-like hierarchies of documents, etc. The general goal of clustering is to group data elements such that the intra-group similarities are high and the inter-group similarities are low. We present a clustering algorithm called CBC (Clustering By Committee) that is shown to produce higher quality clusters in document clustering tasks as compared to several well known clustering algorithms. It initially discovers a set of tight clusters (high intra-group similarity), called committees, that are well scattered in the similarity space (low inter-group similarity). The union of the committees is but a subset of all elements. The algorithm proceeds by assigning elements to their most similar committee. Evaluating cluster quality has always been a difficult task. We present a new evaluation methodology that is based on the editing distance between output clusters and manually constructed classes (the answer key). This evaluation measure is more intuitive and easier to interpret than previous evaluation measures."
            },
            "slug": "Document-clustering-with-committees-Pantel-Lin",
            "title": {
                "fragments": [],
                "text": "Document clustering with committees"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "A new evaluation methodology that is based on the editing distance between output clusters and manually constructed classes (the answer key) is presented, which is more intuitive and easier to interpret than previous evaluation measures."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '02"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1644043304"
                        ],
                        "name": "D. TurneyPeter",
                        "slug": "D.-TurneyPeter",
                        "structuredName": {
                            "firstName": "D",
                            "lastName": "TurneyPeter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. TurneyPeter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1643872837"
                        ],
                        "name": "PantelPatrick",
                        "slug": "PantelPatrick",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "PantelPatrick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "PantelPatrick"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 220956987,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "c572ae1432b5146cd5fdfd7788f141e0993b69a8",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and..."
            },
            "slug": "From-frequency-to-meaning-TurneyPeter-PantelPatrick",
            "title": {
                "fragments": [],
                "text": "From frequency to meaning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35211746"
                        ],
                        "name": "David A. Hull",
                        "slug": "David-A.-Hull",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hull",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. Hull"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 0
                            }
                        ],
                        "text": "Hull (1996) gives a good analysis of normalization for information retrieval."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11971671,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f7237801b84a674c5530f279787d2cf9d922f3e",
            "isKey": false,
            "numCitedBy": 320,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The majority of information retrieval experiments are evaluated by measures such as average precision and average recall. Fundamental decisions about the superiority of one retrieval technique over another are made solely on the basis of these measures. We claim that average performance figures need to be validated with a careful statistical analysis and that there is a great deal of additional information that can be uncovered by looking closely at the results of individual queries. This article is a case study of stemming algorithms which describes a number of novel approaches to evaluation and demonstrates their value. \u00a9 1996 John Wiley & Sons, Inc."
            },
            "slug": "Stemming-Algorithms:-A-Case-Study-for-Detailed-Hull",
            "title": {
                "fragments": [],
                "text": "Stemming Algorithms: A Case Study for Detailed Evaluation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A case study of stemming algorithms is described which describes a number of novel approaches to evaluation and demonstrates their value."
            },
            "venue": {
                "fragments": [],
                "text": "J. Am. Soc. Inf. Sci."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2974669"
                        ],
                        "name": "R. Nosofsky",
                        "slug": "R.-Nosofsky",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Nosofsky",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Nosofsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 103
                            }
                        ],
                        "text": "A natural way to formalize this is to represent concepts as vectors and categories as sets of vectors (Nosofsky, 1986; Smith, Osherson, Rips, & Keane, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8869524,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "527e1fcabce31b8cd2e720576a11f93622aae1ad",
            "isKey": false,
            "numCitedBy": 2581,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "A unified quantitative approach to modeling subjects' identification and categorization of multidimensional perceptual stimuli is proposed and tested. Two subjects identified and categorized the same set of perceptually confusable stimuli varying on separable dimensions. The identification data were modeled using Shepard's (1957) multidimensional scaling-choice framework. This framework was then extended to model the subjects' categorization performance. The categorization model, which generalizes the context theory of classification developed by Medin and Schaffer (1978), assumes that subjects store category exemplars in memory. Classification decisions are based on the similarity of stimuli to the stored exemplars. It is assumed that the same multidimensional perceptual representation underlies performance in both the identification and categorization paradigms. However, because of the influence of selective attention, similarity relationships change systematically across the two paradigms. Some support was gained for the hypothesis that subjects distribute attention among component dimensions so as to optimize categorization performance. Evidence was also obtained that subjects may have augmented their category representations with inferred exemplars. Implications of the results for theories of multidimensional scaling and categorization are discussed."
            },
            "slug": "Attention,-similarity,-and-the-relationship.-Nosofsky",
            "title": {
                "fragments": [],
                "text": "Attention, similarity, and the identification-categorization relationship."
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A unified quantitative approach to modeling subjects' identification and categorization of multidimensional perceptual stimuli is proposed and tested and some support was gained for the hypothesis that subjects distribute attention among component dimensions so as to optimize categorization performance."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of experimental psychology. General"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 101
                            }
                        ],
                        "text": "As an alternative to normalizing them, we may reduce their weights when they co-occur in a document (Church, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16505277,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "13d3ada414ce8071efd165e074bdfd1a5e8ad64a",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "How effective is stemming? Text normalization? Stemming experiments test two hypotheses: one term (+stemmer) or two (\u2013stemmer). The truth lies somewhere in between. The correlations, \u03c1, between a word and its variants (e.g., + s, + ly, +uppercase) tend to be small (refuting the one term hypothesis), but non-negligible (refuting the two term hypothesis). Moreover, \u03c1 varies systematically depending on the words involved; it is relatively large for a good keyword, \u03c1(hostage , hostages) \u223c\u223c0. 5, and small for pairs with little content, \u03c1(anytime, Anytime) \u223c\u223c0, or conflicting content, \u03c1(continental , Continental) \u223c\u223c0."
            },
            "slug": "One-term-or-two-Church",
            "title": {
                "fragments": [],
                "text": "One term or two?"
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '95"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2122755"
                        ],
                        "name": "H. Dang",
                        "slug": "H.-Dang",
                        "structuredName": {
                            "firstName": "Hoa",
                            "lastName": "Dang",
                            "middleNames": [
                                "Trang"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Dang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145580839"
                        ],
                        "name": "Jimmy J. Lin",
                        "slug": "Jimmy-J.-Lin",
                        "structuredName": {
                            "firstName": "Jimmy",
                            "lastName": "Lin",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jimmy J. Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144859929"
                        ],
                        "name": "D. Kelly",
                        "slug": "D.-Kelly",
                        "structuredName": {
                            "firstName": "Diane",
                            "lastName": "Kelly",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kelly"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 63284142,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "901e31635918310569ec0bddf23755154a6df829",
            "isKey": false,
            "numCitedBy": 106,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The TREC 2006 question answering (QA) track contained two tasks: the main task and the complex, interactive question answering (ciQA) task. As in 2005, the main task consisted of series of factoid, list, and \u201cOther\u201d questions organized around a set of targets; in contrast to previous years, the evaluation of factoid and list responses distinguished between answers that were globally correct (with respect to the document collection), and those that were only locally correct (with respect to the supporting document). The ciQA task provided a framework for participants to investigate interaction in the context of complex information needs, and was a blend of the TREC 2005 QA relationship task and the TREC 2005 HARD track. Multiple assessors were used to judge the importance of information nuggets used to evaluate the responses to ciQA and \u201cOther\u201d questions, resulting in an evaluation that is more stable and discriminative than one that uses only a single assessor to judge nugget importance."
            },
            "slug": "Overview-of-the-TREC-2006-Question-Answering-Track-Dang-Lin",
            "title": {
                "fragments": [],
                "text": "Overview of the TREC 2006 Question Answering Track 99"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Multiple assessors were used to judge the importance of information nuggets used to evaluate the responses to ciQA and \u201cOther\u201d questions, resulting in an evaluation that is more stable and discriminative than one that uses only a single assessor to judge nugget importance."
            },
            "venue": {
                "fragments": [],
                "text": "TREC"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145375801"
                        ],
                        "name": "M. Pennacchiotti",
                        "slug": "M.-Pennacchiotti",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Pennacchiotti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pennacchiotti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1990190"
                        ],
                        "name": "P. Pantel",
                        "slug": "P.-Pantel",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Pantel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pantel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 39
                            }
                        ],
                        "text": "For example, the main resource used in Rapp\u2019s (2003) VSM system for measuring word similarity is the British National Corpus (BNC),4 whereas the main resource used in non-VSM systems for measuring word similarity (Hirst & St-Onge, 1998; Leacock & Chodrow, 1998; Jarmasz & Szpakowicz, 2003) is a lexicon, such as WordNet5 or Roget\u2019s Thesaurus."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7992957,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66ed861a6e47041be45e65aef0a73a44c4d35f76",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Many algorithms have been developed to harvest lexical semantic resources, however few have linked the mined knowledge into formal knowledge repositories. In this paper, we propose two algorithms for automatically ontologizing (attaching) semantic relations into WordNet. We present an empirical evaluation on the task of attaching part-of and causation relations, showing an improvement on F-score over a baseline model."
            },
            "slug": "Ontologizing-Semantic-Relations-Pennacchiotti-Pantel",
            "title": {
                "fragments": [],
                "text": "Ontologizing Semantic Relations"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An empirical evaluation on the task of attaching part-of and causation relations is presented, showing an improvement on F-score over a baseline model."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145277401"
                        ],
                        "name": "M. N. Murty",
                        "slug": "M.-N.-Murty",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Murty",
                            "middleNames": [
                                "Narasimha"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. N. Murty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704876"
                        ],
                        "name": "P. Flynn",
                        "slug": "P.-Flynn",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Flynn",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Flynn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12744045,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8ff7b7cf3849640b7cfb8f08e2946fd151fed34c",
            "isKey": false,
            "numCitedBy": 13832,
            "numCiting": 314,
            "paperAbstract": {
                "fragments": [],
                "text": "Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a difficult problem combinatorially, and differences in assumptions and contexts in different communities has made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overviewof pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques, and identify cross-cutting themes and recent advances. We also describe some important applications of clustering algorithms such as image segmentation, object recognition, and information retrieval."
            },
            "slug": "Data-clustering:-a-review-Jain-Murty",
            "title": {
                "fragments": [],
                "text": "Data clustering: a review"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An overview of pattern clustering methods from a statistical pattern recognition perspective is presented, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners."
            },
            "venue": {
                "fragments": [],
                "text": "CSUR"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38070424"
                        ],
                        "name": "R. Ando",
                        "slug": "R.-Ando",
                        "structuredName": {
                            "firstName": "Rie",
                            "lastName": "Ando",
                            "middleNames": [
                                "Kubota"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ando"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 148
                            }
                        ],
                        "text": "\u2026Nonnegative Matrix Factorization (NMF) (Lee & Seung, 1999), Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999), Iterative Scaling (IS) (Ando, 2000), Kernel Principal Components Analysis (KPCA) (Scholkopf, Smola, & Muller, 1997), Latent Dirichlet Allocation (LDA) (Blei et al., 2003),\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10335672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e25c81719351ea0cdac5a075868c363b36e239c7",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel algorithm that creates document vectors with reduced dimensionality. This work was motivated by an application characterizing relationships among documents in a collection. Our algorithm yielded inter-document similarities with an average precision up to 17.8% higher than that of singular value decomposition (SVD) used for Latent Semantic Indexing. The best performance was achieved with dimensional reduction rates that were 43% higher than SVD on average. Our algorithm creates basis vectors for a reduced space by iteratively \u201cscaling\u201d vectors and computing eigenvectors. Unlike SVD, it breaks the symmetry of documents and terms to capture information more evenly across documents. We also discuss correlation with a probabilistic model and evaluate a method for selecting the dimensionality using log-likelihood estimation."
            },
            "slug": "Latent-semantic-space:-iterative-scaling-improves-Ando",
            "title": {
                "fragments": [],
                "text": "Latent semantic space: iterative scaling improves precision of inter-document similarity measurement"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A novel algorithm that creates document vectors with reduced dimensionality by iteratively \"scaling\" vectors and computing eigenvectors is presented, which breaks the symmetry of documents and terms to capture information more evenly across documents."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '00"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4205363"
                        ],
                        "name": "Huanhuan Cao",
                        "slug": "Huanhuan-Cao",
                        "structuredName": {
                            "firstName": "Huanhuan",
                            "lastName": "Cao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huanhuan Cao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71790825"
                        ],
                        "name": "Daxin Jiang",
                        "slug": "Daxin-Jiang",
                        "structuredName": {
                            "firstName": "Daxin",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daxin Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145525190"
                        ],
                        "name": "J. Pei",
                        "slug": "J.-Pei",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Pei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145109822"
                        ],
                        "name": "Qi He",
                        "slug": "Qi-He",
                        "structuredName": {
                            "firstName": "Qi",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qi He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143696247"
                        ],
                        "name": "Zhen Liao",
                        "slug": "Zhen-Liao",
                        "structuredName": {
                            "firstName": "Zhen",
                            "lastName": "Liao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhen Liao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144378760"
                        ],
                        "name": "Enhong Chen",
                        "slug": "Enhong-Chen",
                        "structuredName": {
                            "firstName": "Enhong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Enhong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49404233"
                        ],
                        "name": "Hang Li",
                        "slug": "Hang-Li",
                        "structuredName": {
                            "firstName": "Hang",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hang Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13875928,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eda66713401c9d53a5572f1244b47b00e77d0117",
            "isKey": false,
            "numCitedBy": 591,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Query suggestion plays an important role in improving the usability of search engines. Although some recently proposed methods can make meaningful query suggestions by mining query patterns from search logs, none of them are context-aware - they do not take into account the immediately preceding queries as context in query suggestion. In this paper, we propose a novel context-aware query suggestion approach which is in two steps. In the offine model-learning step, to address data sparseness, queries are summarized into concepts by clustering a click-through bipartite. Then, from session data a concept sequence suffix tree is constructed as the query suggestion model. In the online query suggestion step, a user's search context is captured by mapping the query sequence submitted by the user to a sequence of concepts. By looking up the context in the concept sequence sufix tree, our approach suggests queries to the user in a context-aware manner. We test our approach on a large-scale search log of a commercial search engine containing 1:8 billion search queries, 2:6 billion clicks, and 840 million query sessions. The experimental results clearly show that our approach outperforms two baseline methods in both coverage and quality of suggestions."
            },
            "slug": "Context-aware-query-suggestion-by-mining-and-data-Cao-Jiang",
            "title": {
                "fragments": [],
                "text": "Context-aware query suggestion by mining click-through and session data"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper proposes a novel context-aware query suggestion approach which is in two steps, and outperforms two baseline methods in both coverage and quality of suggestions."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740640"
                        ],
                        "name": "Wessel Kraaij",
                        "slug": "Wessel-Kraaij",
                        "structuredName": {
                            "firstName": "Wessel",
                            "lastName": "Kraaij",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wessel Kraaij"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32112072"
                        ],
                        "name": "Ren\u00e9e Pohlmann",
                        "slug": "Ren\u00e9e-Pohlmann",
                        "structuredName": {
                            "firstName": "Ren\u00e9e",
                            "lastName": "Pohlmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ren\u00e9e Pohlmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 66
                            }
                        ],
                        "text": "In general, normalization increases recall and reduces precision (Kraaij & Pohlmann, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14204534,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "acddd8959a22b55050de23b0810428b80cf100c5",
            "isKey": false,
            "numCitedBy": 212,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous research on stemming has shown both positive and negative effects on retrieval performance. This paper describes an experiment in which several linguistic and non-linguistic stemmers are evaluated on a Dutch test collection. Experiments especially focus on the measurement of Recall. Results show that linguistic stemming restricted to inflection yields a significant improvement over full linguistic and non-linquistic stemming, both in average Precision and R-Recall. Best results are obtained with a linguistic stemmer which is enhanced with compound analysis. This version has a significantly better Recall than a system without stemming, without a significant deterioration of Precision."
            },
            "slug": "Viewing-stemming-as-recall-enhancement-Kraaij-Pohlmann",
            "title": {
                "fragments": [],
                "text": "Viewing stemming as recall enhancement"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Results show that linguistic stemming restricted to inflection yields a significant improvement over full linguistic and non-linquistic stemming, both in average Precision and R-Recall."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '96"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69462054"
                        ],
                        "name": "Otis Gospodnetic",
                        "slug": "Otis-Gospodnetic",
                        "structuredName": {
                            "firstName": "Otis",
                            "lastName": "Gospodnetic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Otis Gospodnetic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27365653"
                        ],
                        "name": "E. Hatcher",
                        "slug": "E.-Hatcher",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Hatcher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Hatcher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3133664"
                        ],
                        "name": "D. Cutting",
                        "slug": "D.-Cutting",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Cutting",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cutting"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60479011,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "202636f452d551548d16a9742f423a0b81f5e9cc",
            "isKey": false,
            "numCitedBy": 829,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Lucene is a rare gem in the Apache Jakarta world?a highly scalable, fast, and pure Java search engine. Its initial attractions are its performance, its simplicity, and its disarming ease-of-use. But there are many caveats, best practices, and examples that users are looking for that are not yet well documented. The lucene-user e-mail list is very active and helpful, but many users seek more guidance and examples. Lucene in Action describes what Lucene is and how it works and most importantly how it can be used in a variety of real-world use cases, such at Nutch. Nutch?an open-source project designed to index the internet very much like Google?is built upon Lucene. Lucene in Action provides readers with best practices, tried and true from the field, including: * Understanding and solving ?analysis paralysis? * Advanced searching techniques, including filtering and custom query parsing techniques. * Handling document types such as Word, PDF, HTML, XML, and others."
            },
            "slug": "Lucene-in-Action-Gospodnetic-Hatcher",
            "title": {
                "fragments": [],
                "text": "Lucene in Action"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Lucene in Action describes what Lucene is and how it works and most importantly how it can be used in a variety of real-world use cases, such at Nutch, an open-source project designed to index the internet very much like Google."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2938286"
                        ],
                        "name": "V. Vyas",
                        "slug": "V.-Vyas",
                        "structuredName": {
                            "firstName": "Vishnu",
                            "lastName": "Vyas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vyas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1990190"
                        ],
                        "name": "P. Pantel",
                        "slug": "P.-Pantel",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Pantel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pantel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14130384,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ca3bbb3105bbf61df291e2a7306dd4b6cb6ab27b",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "State of the art set expansion algorithms produce varying quality expansions for different entity types. Even for the highest quality expansions, errors still occur and manual refinements are necessary for most practical uses. In this paper, we propose algorithms to aide this refinement process, greatly reducing the amount of manual labor required. The methods rely on the fact that most expansion errors are systematic, often stemming from the fact that some seed elements are ambiguous. Using our methods, empirical evidence shows that average R-precision over random entity sets improves by 26% to 51% when given from 5 to 10 manually tagged errors. Both proposed refinement models have linear time complexity in set size allowing for practical online use in set expansion systems."
            },
            "slug": "Semi-Automatic-Entity-Set-Refinement-Vyas-Pantel",
            "title": {
                "fragments": [],
                "text": "Semi-Automatic Entity Set Refinement"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Using the proposed algorithms, empirical evidence shows that average R-precision over random entity sets improves by 26% to 51% when given from 5 to 10 manually tagged errors, greatly reducing the amount of manual labor required."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2939803"
                        ],
                        "name": "Ronan Collobert",
                        "slug": "Ronan-Collobert",
                        "structuredName": {
                            "firstName": "Ronan",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronan Collobert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2617020,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "57458bc1cffe5caa45a885af986d70f723f406b4",
            "isKey": false,
            "numCitedBy": 5024,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance."
            },
            "slug": "A-unified-architecture-for-natural-language-deep-Collobert-Weston",
            "title": {
                "fragments": [],
                "text": "A unified architecture for natural language processing: deep neural networks with multitask learning"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "This work describes a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense using a language model."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695574"
                        ],
                        "name": "T. Veale",
                        "slug": "T.-Veale",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Veale",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Veale"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "of relations. As with word\u2013context matrices, the main alternatives are approaches that use lexicons (Rosario &amp; Hearst, 2001; Rosario, Hearst, &amp; Fillmore, 2002; Nastase &amp; Szpakowicz, 2003; Veale, 2003, 2004). The idea is to reduce relational similarity to attributional similarity, sim r(a : b,c : d) \u2248 sim a(a,c) + sim a(b,d), and then use a lexicon to measure attributional similarity. As we discus"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17083890,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c68c4e1386652160bde4b166ba536d0976688516",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Innovative applications often occur at the juncture of radically different domains of research. This paper describes an emerging application, called the analogical thesaurus, that arises at the boundary of two very different domains, the highly applied domain of information retrieval, and the esoteric domain of lexical metaphor interpretation. This application has the potential to not just to improve the utility of conventional electronic thesauri, but to serve as an intelligent mapping component in any system that uses analogical reasoning or case-"
            },
            "slug": "The-Analogical-Thesaurus-Veale",
            "title": {
                "fragments": [],
                "text": "The Analogical Thesaurus"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "An emerging application, called the analogical thesaurus, that arises at the boundary of two very different domains, the highly applied domain of information retrieval, and the esoteric domain of lexical metaphor interpretation is described."
            },
            "venue": {
                "fragments": [],
                "text": "IAAI"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770124"
                        ],
                        "name": "Sunita Sarawagi",
                        "slug": "Sunita-Sarawagi",
                        "structuredName": {
                            "firstName": "Sunita",
                            "lastName": "Sarawagi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sunita Sarawagi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2988182"
                        ],
                        "name": "Alok Kirpal",
                        "slug": "Alok-Kirpal",
                        "structuredName": {
                            "firstName": "Alok",
                            "lastName": "Kirpal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alok Kirpal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12230460,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7cb3cb75a60d4e446d6da43801895c6e98655f90",
            "isKey": false,
            "numCitedBy": 362,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present an efficient, scalable and general algorithm for performing set joins on predicates involving various similarity measures like intersect size, Jaccard-coefficient, cosine similarity, and edit-distance. This expands the existing suite of algorithms for set joins on simpler predicates such as, set containment, equality and non-zero overlap. We start with a basic inverted index based probing method and add a sequence of optimizations that result in one to two orders of magnitude improvement in running time. The algorithm folds in a data partitioning strategy that can work efficiently with an index compressed to fit in any available amount of main memory. The optimizations used in our algorithm generalize to several weighted and unweighted measures of partial word overlap between sets."
            },
            "slug": "Efficient-set-joins-on-similarity-predicates-Sarawagi-Kirpal",
            "title": {
                "fragments": [],
                "text": "Efficient set joins on similarity predicates"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "This paper presents an efficient, scalable and general algorithm for performing set joins on predicates involving various similarity measures like intersect size, Jaccard-coefficient, cosine similarity, and edit-distance that generalize to several weighted and unweighted measures of partial word overlap between sets."
            },
            "venue": {
                "fragments": [],
                "text": "SIGMOD '04"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115651440"
                        ],
                        "name": "Daniel D. Lee",
                        "slug": "Daniel-D.-Lee",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Lee",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel D. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144924970"
                        ],
                        "name": "H. Seung",
                        "slug": "H.-Seung",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Seung",
                            "middleNames": [
                                "Sebastian"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Seung"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 265
                            }
                        ],
                        "text": "A low-RAM algorithm, Multislice Projection, for large sparse tensors is presented and evaluated.17\nSince the work of Deerwester et al. (1990), subsequent research has discovered many alternative matrix smoothing processes, such as Nonnegative Matrix Factorization (NMF) (Lee & Seung, 1999), Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999), Iterative Scaling (IS) (Ando, 2000), Kernel Principal Components Analysis (KPCA) (Scholkopf, Smola, & Muller, 1997), Latent Dirichlet Allocation (LDA) (Blei et al., 2003), and Discrete Component Analysis (DCA) (Buntine & Jakulin, 2006)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 144
                            }
                        ],
                        "text": "\u2026et al. (1990), subsequent research has discovered many alternative matrix smoothing processes, such as Nonnegative Matrix Factorization (NMF) (Lee & Seung, 1999), Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999), Iterative Scaling (IS) (Ando, 2000), Kernel Principal Components\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4428232,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "29bae9472203546847ec1352a604566d0f602728",
            "isKey": false,
            "numCitedBy": 11312,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Is perception of the whole based on perception of its parts? There is psychological and physiological evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign."
            },
            "slug": "Learning-the-parts-of-objects-by-non-negative-Lee-Seung",
            "title": {
                "fragments": [],
                "text": "Learning the parts of objects by non-negative matrix factorization"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An algorithm for non-negative matrix factorization is demonstrated that is able to learn parts of faces and semantic features of text and is in contrast to other methods that learn holistic, not parts-based, representations."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708114"
                        ],
                        "name": "Katrin Erk",
                        "slug": "Katrin-Erk",
                        "structuredName": {
                            "firstName": "Katrin",
                            "lastName": "Erk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katrin Erk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 0
                            }
                        ],
                        "text": "Erk (2007) presented a system in which a word\u2013context frequency matrix was used to improve the performance of semantic role labeling. Pennacchiotti, Cao, Basili, Croce, and Roth (2008) show that word\u2013context matrices can reliably predict the semantic frame to which an unknown lexical unit refers, with good levels of accuracy."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 0
                            }
                        ],
                        "text": "Erk (2007) presented a system in which a word\u2013context frequency matrix was used to improve the performance of semantic role labeling."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2032205,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3cce57b4eb593359862817e93eb47e8655520652",
            "isKey": false,
            "numCitedBy": 161,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new, simple model for the automatic induction of selectional preferences, using corpus-based semantic similarity metrics. Focusing on the task of semantic role labeling, we compute selectional preferences for semantic roles. In evaluations the similarity-based model shows lower error rates than both Resnik\u2019s WordNet-based model and the EM-based clustering model, but has coverage problems."
            },
            "slug": "A-Simple,-Similarity-based-Model-for-Selectional-Erk",
            "title": {
                "fragments": [],
                "text": "A Simple, Similarity-based Model for Selectional Preferences"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A new, simple model for the automatic induction of selectional preferences, using corpus-based semantic similarity metrics, focuses on the task of semantic role labeling and shows lower error rates than both Resnik's WordNet-based model and the EM-based clustering model."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118415117"
                        ],
                        "name": "Michael P. Jones",
                        "slug": "Michael-P.-Jones",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jones",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael P. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10796472"
                        ],
                        "name": "James H. Martin",
                        "slug": "James-H.-Martin",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Martin",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James H. Martin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8592573,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "232e66748382ded9d217de554574fbf70df0f6b6",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Contextual spelling errors are defined as the use of an incorrect, though valid, word in a particular sentence or context. Traditional spelling checkers flag misspelled words, but they do not typically attempt to identify words that are used incorrectly in a sentence. We explore the use of Latent Semantic Analysis for correcting these incorrectly used words and the results are compared to earlier work based on a Bayesian classifier."
            },
            "slug": "Contextual-Spelling-Correction-Using-Latent-Jones-Martin",
            "title": {
                "fragments": [],
                "text": "Contextual Spelling Correction Using Latent Semantic Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The use of Latent Semantic Analysis for correcting incorrectly used words is explored and the results are compared to earlier work based on a Bayesian classifier."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721801"
                        ],
                        "name": "C. Fellbaum",
                        "slug": "C.-Fellbaum",
                        "structuredName": {
                            "firstName": "Christiane",
                            "lastName": "Fellbaum",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Fellbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144096985"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Miller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 63043568,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "afb180e7615fedf3479707aea9db9ee156f9406e",
            "isKey": false,
            "numCitedBy": 1659,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Introducfion, Training and Testing Data, Experiment 1: The Local Context Classifier, Experiment 2: Measuring Word Similarity In Wordnet, Experiment 3: Combining Local Context and Wordnet Similarity Measures, Conclusions, References"
            },
            "slug": "Combining-Local-Context-and-Wordnet-Similarity-for-Fellbaum-Miller",
            "title": {
                "fragments": [],
                "text": "Combining Local Context and Wordnet Similarity for Word Sense Identification"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This chapter contains sections titled: Introducfion, Training and Testing Data, Experiment 1: The Local Context Classifier, Experiment 2: Measuring Word Similarity In Wordnet, and Combining Local Context and Wordnet Similarity Measures."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2920325"
                        ],
                        "name": "Chien-Kang Huang",
                        "slug": "Chien-Kang-Huang",
                        "structuredName": {
                            "firstName": "Chien-Kang",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chien-Kang Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745331"
                        ],
                        "name": "Lee-Feng Chien",
                        "slug": "Lee-Feng-Chien",
                        "structuredName": {
                            "firstName": "Lee-Feng",
                            "lastName": "Chien",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lee-Feng Chien"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143735694"
                        ],
                        "name": "Y. Oyang",
                        "slug": "Y.-Oyang",
                        "structuredName": {
                            "firstName": "Yen-Jen",
                            "lastName": "Oyang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Oyang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9522695,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2289bbc55457e602dd6dfece8d96633bd327bad9",
            "isKey": false,
            "numCitedBy": 297,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes an effective term suggestion approach to interactive Web search. Conventional approaches to making term suggestions involve extracting co-occurring keyterms from highly ranked retrieved documents. Such approaches must deal with term extraction difficulties and interference from irrelevant documents, and, more importantly, have difficulty extracting terms that are conceptually related but do not frequently co-occur in documents. In this paper, we present a new, effective log-based approach to relevant term extraction and term suggestion. Using this approach, the relevant terms suggested for a user query are those that co-occur in similar query sessions from search engine logs, rather than in the retrieved documents. In addition, the suggested terms in each interactive search step can be organized according to its relevance to the entire query session, rather than to the most recent single query as in conventional approaches. The proposed approach was tested using a proxy server log containing about two million query transactions submitted to search engines in Taiwan. The obtained experimental results show that the proposed approach can provide organized and highly relevant terms, and can exploit the contextual information in a user's query session to make more effective suggestions."
            },
            "slug": "Relevant-term-suggestion-in-interactive-web-search-Huang-Chien",
            "title": {
                "fragments": [],
                "text": "Relevant term suggestion in interactive web search based on contextual information in query session logs"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Experimental results show that the proposed approach to relevant term extraction and term suggestion can provide organized and highly relevant terms, and can exploit the contextual information in a user's query session to make more effective suggestions."
            },
            "venue": {
                "fragments": [],
                "text": "J. Assoc. Inf. Sci. Technol."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144456145"
                        ],
                        "name": "W. Bruce Croft",
                        "slug": "W.-Bruce-Croft",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Croft",
                            "middleNames": [
                                "Bruce"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Bruce Croft"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 223
                            }
                        ],
                        "text": "The clusters may be partitional (flat) (Cutting, Karger, Pedersen, & Tukey, 1992; Pantel & Lin, 2002b) or they may have a hierarchical structure (groups of groups) (Zhao & Karypis, 2002); they may be non-overlapping (hard) (Croft, 1977) or overlapping (soft) (Zamir & Etzioni, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 46278993,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "419f7857622c5d3e42edec080b1dcab99c2fdf64",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "A method for clustering large files of documents using a clustering algorithm which takes O(n2) operations (single-link) is proposed. This method is tested on a file of 11,613 documents derived from an operational system. One property of the generated cluster hierarchy (hierarchy connection percentage) is examined and it indicates that the hierarchy is similar to those from other test collections. A comparison of clustering times with other methods showsthat large files can be clustered by single-link in a time at least comparable to various heuristic algorithms which theoretically require fewer operations."
            },
            "slug": "Clustering-large-files-of-documents-using-the-Croft",
            "title": {
                "fragments": [],
                "text": "Clustering large files of documents using the single-link method"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A comparison of clustering times with other methods show that large files can be clustered by single-link in a time at least comparable to various heuristic algorithms which theoretically require fewer operations."
            },
            "venue": {
                "fragments": [],
                "text": "J. Am. Soc. Inf. Sci."
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796335"
                        ],
                        "name": "D. Blei",
                        "slug": "D.-Blei",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Blei",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Blei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 43
                            }
                        ],
                        "text": "However, there are efficient forms of SVD (Brand, 2006; Gorrell, 2006). 21. LSH stems from work by Rabin (1981), who proposed the use of hash functions from random irreducible polynomials to create short fingerprints of collections of documents."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 233,
                                "start": 216
                            }
                        ],
                        "text": "\u2026Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999), Iterative Scaling (IS) (Ando, 2000), Kernel Principal Components Analysis (KPCA) (Scholkopf, Smola, & Muller, 1997), Latent Dirichlet Allocation (LDA) (Blei et al., 2003), and Discrete Component Analysis (DCA) (Buntine & Jakulin, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 389,
                                "start": 370
                            }
                        ],
                        "text": "(1990), subsequent research has discovered many alternative matrix smoothing processes, such as Nonnegative Matrix Factorization (NMF) (Lee & Seung, 1999), Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999), Iterative Scaling (IS) (Ando, 2000), Kernel Principal Components Analysis (KPCA) (Scholkopf, Smola, & Muller, 1997), Latent Dirichlet Allocation (LDA) (Blei et al., 2003), and Discrete Component Analysis (DCA) (Buntine & Jakulin, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 62
                            }
                        ],
                        "text": "Newer generative models, such as Latent Dirichlet Allocation (LDA) (Blei, Ng, & Jordan, 2003), directly model this intuition."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 503,
                                "start": 500
                            }
                        ],
                        "text": "A low-RAM algorithm, Multislice Projection, for large sparse tensors is presented and evaluated.17\nSince the work of Deerwester et al. (1990), subsequent research has discovered many alternative matrix smoothing processes, such as Nonnegative Matrix Factorization (NMF) (Lee & Seung, 1999), Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999), Iterative Scaling (IS) (Ando, 2000), Kernel Principal Components Analysis (KPCA) (Scholkopf, Smola, & Muller, 1997), Latent Dirichlet Allocation (LDA) (Blei et al., 2003), and Discrete Component Analysis (DCA) (Buntine & Jakulin, 2006)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3177797,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f198043a866e9187925a8d8db9a55e3bfdd47f2c",
            "isKey": true,
            "numCitedBy": 30947,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Latent-Dirichlet-Allocation-Blei-Ng",
            "title": {
                "fragments": [],
                "text": "Latent Dirichlet Allocation"
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38070424"
                        ],
                        "name": "R. Ando",
                        "slug": "R.-Ando",
                        "structuredName": {
                            "firstName": "Rie",
                            "lastName": "Ando",
                            "middleNames": [
                                "Kubota"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ando"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117881943"
                        ],
                        "name": "Tong Zhang",
                        "slug": "Tong-Zhang",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13650160,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "944e1a7b2c5c62e952418d7684e3cade89c76f87",
            "isKey": false,
            "numCitedBy": 1414,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don't have a complete understanding of their effectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Specifically we consider learning predictive structures on hypothesis spaces (that is, what kind of classifiers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the effectiveness of the proposed algorithms in the semi-supervised learning setting."
            },
            "slug": "A-Framework-for-Learning-Predictive-Structures-from-Ando-Zhang",
            "title": {
                "fragments": [],
                "text": "A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper presents a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data, and algorithms for structural learning will be proposed, and computational issues will be investigated."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3154682"
                        ],
                        "name": "R. Harshman",
                        "slug": "R.-Harshman",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Harshman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Harshman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 128
                            }
                        ],
                        "text": "For higher-order tensors, there are operations that are analogous to truncated SVD, such as parallel factor analysis (PARAFAC) (Harshman, 1970), canonical decomposition (CANDECOMP) (Carroll & Chang, 1970) (equivalent to PARAFAC but discovered independently), and Tucker decomposition (Tucker, 1966)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6816804,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c5b28cae82b14417f1250e58bb241367248e827d",
            "isKey": false,
            "numCitedBy": 2918,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Simple structure and other common principles of factor rotation do not in general provide strong grounds for attributing explanatory significance to the factors which they select. In contrast, it is shown that an extension of Cattell's principle of rotation to Proportional Profiles (PP) offers a basis for determining explanatory factors for three-way or higher order multi-mode data. Conceptual models are developed for two basic patterns of multi-mode data variation, systemand object-variation, and PP analysis is found to apply in the system-variation case. Although PP was originally formulated as a principle of rotation to be used with classic two-way factor analysis, it is shown to embody a latent three-mode factor model, which is here made explicit and generalized frown two to N \"parallel occasions\". As originally formulated, PP rotation was restricted to orthogonal factors. The generalized PP model is demonstrated to give unique \"correct\" solutions with oblique, non-simple structure, and even non-linear factor structures. A series of tests, conducted with synthetic data of known factor composition, demonstrate the capabilities of linear and non-linear versions of the model, provide data on the minimal necessary conditions of uniqueness, and reveal the properties of the analysis procedures when these minimal conditions are not fulfilled. In addition, a mathematical proof is presented for the uniqueness of the solution given certain conditions on the data. Three-mode PP factor analysis is applied to a three-way set of real data consisting of the fundamental and first three formant frequencies of 11 persons saying 8 vowels. A unique solution is extracted, consisting of three factors which are highly meaningful and consistent with prior knowledge and theory concerning vowel quality. The relationships between the three-mode PP model and Tucker's multi-modal model, McDonald's non-linear model and Carroll and Chang's multi-dimensional scaling model are explored."
            },
            "slug": "Foundations-of-the-PARAFAC-procedure:-Models-and-an-Harshman",
            "title": {
                "fragments": [],
                "text": "Foundations of the PARAFAC procedure: Models and conditions for an \"explanatory\" multi-model factor analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "It is shown that an extension of Cattell's principle of rotation to Proportional Profiles (PP) offers a basis for determining explanatory factors for three-way or higher order multi-mode data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2448400"
                        ],
                        "name": "Genevieve Gorrell",
                        "slug": "Genevieve-Gorrell",
                        "structuredName": {
                            "firstName": "Genevieve",
                            "lastName": "Gorrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Genevieve Gorrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 42
                            }
                        ],
                        "text": "However, there are efficient forms of SVD (Brand, 2006; Gorrell, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1471171,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fcbf884f925e79241d481d6aefe7a2689028feab",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm based on the Generalized Hebbian Algorithm is described that allows the singular value decomposition of a dataset to be learned based on single observation pairs presented serially. The algorithm has minimal memory requirements, and is therefore interesting in the natural language domain, where very large datasets are often used, and datasets quickly become intractable. The technique is demonstrated on the task of learning word and letter bigram pairs from text."
            },
            "slug": "Generalized-Hebbian-Algorithm-for-Incremental-Value-Gorrell",
            "title": {
                "fragments": [],
                "text": "Generalized Hebbian Algorithm for Incremental Singular Value Decomposition in Natural Language Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "An algorithm based on the Generalized Hebbian Algorithm is described that allows the singular value decomposition of a dataset to be learned based on single observation pairs presented serially, which is interesting in the natural language domain, where very large datasets are often used, and datasets quickly become intractable."
            },
            "venue": {
                "fragments": [],
                "text": "EACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399974318"
                        ],
                        "name": "Ying Zhao",
                        "slug": "Ying-Zhao",
                        "structuredName": {
                            "firstName": "Ying",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ying Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50877490"
                        ],
                        "name": "G. Karypis",
                        "slug": "G.-Karypis",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Karypis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Karypis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207676894,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "320bdc47fc1e8682308d2008f25dbc2939d0a892",
            "isKey": false,
            "numCitedBy": 691,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Fast and high-quality document clustering algorithms play an important role in providing intuitive navigation and browsing mechanisms by organizing large amounts of information into a small number of meaningful clusters. In particular, hierarchical clustering solutions provide a view of the data at different levels of granularity, making them ideal for people to visualize and interactively explore large document collections.In this paper we evaluate different partitional and agglomerative approaches for hierarchical clustering. Our experimental evaluation showed that partitional algorithms always lead to better clustering solutions than agglomerative algorithms, which suggests that partitional clustering algorithms are well-suited for clustering large document datasets due to not only their relatively low computational requirements, but also comparable or even better clustering performance. We present a new class of clustering algorithms called constrained agglomerative algorithms that combine the features of both partitional and agglomerative algorithms. Our experimental results showed that they consistently lead to better hierarchical solutions than agglomerative or partitional algorithms alone."
            },
            "slug": "Evaluation-of-hierarchical-clustering-algorithms-Zhao-Karypis",
            "title": {
                "fragments": [],
                "text": "Evaluation of hierarchical clustering algorithms for document datasets"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is suggested that partitional clustering algorithms are well-suited for clustering large document datasets due to not only their relatively low computational requirements, but also comparable or even better clustering performance."
            },
            "venue": {
                "fragments": [],
                "text": "CIKM '02"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716902"
                        ],
                        "name": "Marti A. Hearst",
                        "slug": "Marti-A.-Hearst",
                        "structuredName": {
                            "firstName": "Marti",
                            "lastName": "Hearst",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marti A. Hearst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 10
                            }
                        ],
                        "text": "Nakov and Hearst (2007) achieved good results using a pair\u2013pattern matrix."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 166
                            }
                        ],
                        "text": "Document segmentation: The task of document segmentation is to partition a document into sections, where each section focuses on a different subtopic of the document (Hearst, 1997; Choi, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8574660,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cb91a9ef1723440bd35a3e5965a2e180ad1ab36f",
            "isKey": false,
            "numCitedBy": 1404,
            "numCiting": 94,
            "paperAbstract": {
                "fragments": [],
                "text": "TextTiling is a technique for subdividing texts into multi-paragraph units that represent passages, or subtopics. The discourse cues for identifying major subtopic shifts are patterns of lexical co-occurrence and distribution. The algorithm is fully implemented and is shown to produce segmentation that corresponds well to human judgments of the subtopic boundaries of 12 texts. Multi-paragraph subtopic segmentation should be useful for many text analysis tasks, including information retrieval and summarization."
            },
            "slug": "Text-Tiling:-Segmenting-Text-into-Multi-paragraph-Hearst",
            "title": {
                "fragments": [],
                "text": "Text Tiling: Segmenting Text into Multi-paragraph Subtopic Passages"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The algorithm is fully implemented and is shown to produce segmentation that corresponds well to human judgments of the subtopic boundaries of 12 texts, which should be useful for many text analysis tasks, including information retrieval and summarization."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145163573"
                        ],
                        "name": "A. Singhal",
                        "slug": "A.-Singhal",
                        "structuredName": {
                            "firstName": "Amit",
                            "lastName": "Singhal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Singhal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798723"
                        ],
                        "name": "Mandar Mitra",
                        "slug": "Mandar-Mitra",
                        "structuredName": {
                            "firstName": "Mandar",
                            "lastName": "Mitra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mandar Mitra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144009691"
                        ],
                        "name": "C. Buckley",
                        "slug": "C.-Buckley",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Buckley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Buckley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 30988838,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d4a871356f01fdbf0df6c687fc7dcc5de195b9e0",
            "isKey": false,
            "numCitedBy": 153,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Document-Length-Normalization-Singhal-Salton",
            "title": {
                "fragments": [],
                "text": "Document Length Normalization"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Process. Manag."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746017"
                        ],
                        "name": "G. Grefenstette",
                        "slug": "G.-Grefenstette",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Grefenstette",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Grefenstette"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 126
                            }
                        ],
                        "text": "Several researchers have used word\u2013context matrices specifically for the task of assisting or automating thesaurus generation (Crouch, 1988; Grefenstette, 1994; Ruge, 1997; Pantel & Lin, 2002a; Curran & Moens, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Grefenstette (1994) presents a good study of linguistic processing for word\u2013context VSMs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59167516,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4471e3117cdac2fae74d305d54b237bb3addd749",
            "isKey": false,
            "numCitedBy": 873,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface. 1. Introduction. 2. Semantic Extraction. 3. Sextant. 4. Evaluation. 5. Applications. 6. Conclusion. 1: Preprocesors. 2. Webster Stopword List. 3: Similarity List. 4: Semantic Clustering. 5: Automatic Thesaurus Generation. 6. Corpora Treated. Index."
            },
            "slug": "Explorations-in-automatic-thesaurus-discovery-Grefenstette",
            "title": {
                "fragments": [],
                "text": "Explorations in automatic thesaurus discovery"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The aim of this monograph is to provide a catalog of words and phrases used in ThesaurusGeneration, as well as some examples of other writers' work, which have been used in similar contexts."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144009691"
                        ],
                        "name": "C. Buckley",
                        "slug": "C.-Buckley",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Buckley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Buckley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 0
                            }
                        ],
                        "text": "Salton and Buckley (1988) defined a large family of tf-idf weighting functions and evaluated them on information retrieval tasks, demonstrating that tf-idf weighting can yield significant improvements over raw frequency."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7725217,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e50a316f97c9a405aa000d883a633bd5707f1a34",
            "isKey": false,
            "numCitedBy": 9462,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Term-Weighting-Approaches-in-Automatic-Text-Salton-Buckley",
            "title": {
                "fragments": [],
                "text": "Term-Weighting Approaches in Automatic Text Retrieval"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Process. Manag."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778725"
                        ],
                        "name": "J. Breese",
                        "slug": "J.-Breese",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Breese",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Breese"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48099028"
                        ],
                        "name": "D. Heckerman",
                        "slug": "D.-Heckerman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Heckerman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Heckerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1772349"
                        ],
                        "name": "C. Kadie",
                        "slug": "C.-Kadie",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Kadie",
                            "middleNames": [
                                "Myers"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Kadie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2885948,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "36b4a92c8eca6fd6d1b8588fc1fd0e3f89a16623",
            "isKey": false,
            "numCitedBy": 5604,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Collaborative filtering or recommender systems use a database about user preferences to predict additional topics or products a new user might like. In this paper we describe several algorithms designed for this task, including techniques based on correlation coefficients, vector-based similarity calculations, and statistical Bayesian methods. We compare the predictive accuracy of the various methods in a set of representative problem domains. We use two basic classes of evaluation metrics. The first characterizes accuracy over a set of individual predictions in terms of average absolute deviation. The second estimates the utility of a ranked list of suggested items. This metric uses an estimate of the probability that a user will see a recommendation in an ordered list. \n \nExperiments were run for datasets associated with 3 application areas, 4 experimental protocols, and the 2 evaluation metr rics for the various algorithms. Results indicate that for a wide range of conditions, Bayesian networks with decision trees at each node and correlation methods outperform Bayesian-clustering and vector-similarity methods. Between correlation and Bayesian networks, the preferred method depends on the nature of the dataset, nature of the application (ranked versus one-by-one presentation), and the availability of votes with which to make predictions. Other considerations include the size of database, speed of predictions, and learning time."
            },
            "slug": "Empirical-Analysis-of-Predictive-Algorithms-for-Breese-Heckerman",
            "title": {
                "fragments": [],
                "text": "Empirical Analysis of Predictive Algorithms for Collaborative Filtering"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "Several algorithms designed for collaborative filtering or recommender systems are described, including techniques based on correlation coefficients, vector-based similarity calculations, and statistical Bayesian methods, to compare the predictive accuracy of the various methods in a set of representative problem domains."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2913681"
                        ],
                        "name": "Stefanie Tellex",
                        "slug": "Stefanie-Tellex",
                        "structuredName": {
                            "firstName": "Stefanie",
                            "lastName": "Tellex",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefanie Tellex"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6104312"
                        ],
                        "name": "Boris Katz",
                        "slug": "Boris-Katz",
                        "structuredName": {
                            "firstName": "Boris",
                            "lastName": "Katz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Boris Katz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145580839"
                        ],
                        "name": "Jimmy J. Lin",
                        "slug": "Jimmy-J.-Lin",
                        "structuredName": {
                            "firstName": "Jimmy",
                            "lastName": "Lin",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jimmy J. Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144893917"
                        ],
                        "name": "A. Fernandes",
                        "slug": "A.-Fernandes",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Fernandes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Fernandes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38102633"
                        ],
                        "name": "Gregory A. Marton",
                        "slug": "Gregory-A.-Marton",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Marton",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gregory A. Marton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 3
                            }
                        ],
                        "text": "In Turney\u2019s (2007) tensor, for example, rows correspond to words from the TOEFL multiple-choice synonym questions, columns correspond to words from Basic English (Ogden, 1930),12 and tubes correspond to patterns that join rows and columns (hence we have a word\u2013word\u2013pattern third-order tensor)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 102
                            }
                        ],
                        "text": "Vector-based similarity measurements are often used for both document retrieval and passage retrieval (Tellex et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 846801,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "828c773ec37c9f3e5e245780ba37dae7466acdd4",
            "isKey": false,
            "numCitedBy": 334,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Passage retrieval is an important component common to many question answering systems. Because most evaluations of question answering systems focus on end-to-end performance, comparison of common components becomes difficult. To address this shortcoming, we present a quantitative evaluation of various passage retrieval algorithms for question answering, implemented in a framework called Pauchok. We present three important findings: Boolean querying schemes perform well in the question answering task. The performance differences between various passage retrieval algorithms vary with the choice of document retriever, which suggests significant interactions between document retrieval and passage retrieval. The best algorithms in our evaluation employ density-based measures for scoring query terms. Our results reveal future directions for passage retrieval and question answering."
            },
            "slug": "Quantitative-evaluation-of-passage-retrieval-for-Tellex-Katz",
            "title": {
                "fragments": [],
                "text": "Quantitative evaluation of passage retrieval algorithms for question answering"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work presents a quantitative evaluation of various passage retrieval algorithms for question answering, implemented in a framework called Pauchok, and presents three important findings: Boolean querying schemes perform well in the question answering task."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144876441"
                        ],
                        "name": "Xing Wei",
                        "slug": "Xing-Wei",
                        "structuredName": {
                            "firstName": "Xing",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xing Wei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682136"
                        ],
                        "name": "Fuchun Peng",
                        "slug": "Fuchun-Peng",
                        "structuredName": {
                            "firstName": "Fuchun",
                            "lastName": "Peng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fuchun Peng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2936837"
                        ],
                        "name": "Beno\u00eet Dumoulin",
                        "slug": "Beno\u00eet-Dumoulin",
                        "structuredName": {
                            "firstName": "Beno\u00eet",
                            "lastName": "Dumoulin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beno\u00eet Dumoulin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15600228,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db70090f17bb4459ac4e570a3426b02a8caee7cf",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a statistical model for abbreviation disambiguation in Web search, based on analysis of Web data resources, including anchor text, click log and query log. By combining evidence from multiple sources, we are able to accurately disambiguate the abbreviation in queries. Experiments on real Web search queries show promising results."
            },
            "slug": "Analyzing-web-text-association-to-disambiguate-in-Wei-Peng",
            "title": {
                "fragments": [],
                "text": "Analyzing web text association to disambiguate abbreviation in queries"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A statistical model for abbreviation disambiguation in Web search is introduced, based on analysis of Web data resources, including anchor text, click log and query log, and is able to accuratelydisambiguate the abbreviation in queries."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1824057"
                        ],
                        "name": "Florent Monay",
                        "slug": "Florent-Monay",
                        "structuredName": {
                            "firstName": "Florent",
                            "lastName": "Monay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Florent Monay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403029865"
                        ],
                        "name": "D. G\u00e1tica-P\u00e9rez",
                        "slug": "D.-G\u00e1tica-P\u00e9rez",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "G\u00e1tica-P\u00e9rez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. G\u00e1tica-P\u00e9rez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 120
                            }
                        ],
                        "text": "Wittgenstein\u2019s intuition might be better captured by a matrix that combines words with other modalities, such as images (Monay & Gatica-Perez, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1007967,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c253729d6170b31972ded6bfec1ea502f3ff86e",
            "isKey": false,
            "numCitedBy": 281,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Image auto-annotation, i.e., the association of words to whole images, has attracted considerable attention. In particular, unsupervised, probabilistic latent variable models of text and image features have shown encouraging results, but their performance with respect to other approaches remains unknown. In this paper, we apply and compare two simple latent space models commonly used in text analysis, namely Latent Semantic Analysis (LSA) and Probabilistic LSA (PLSA). Annotation strategies for each model are discussed. Remarkably, we found that, on a 8000-image dataset, a classic LSA model defined on keywords and a very basic image representation performed as well as much more complex, state-of-the-art methods. Furthermore, non-probabilistic methods (LSA and direct image matching) outperformed PLSA on the same dataset."
            },
            "slug": "On-image-auto-annotation-with-latent-space-models-Monay-G\u00e1tica-P\u00e9rez",
            "title": {
                "fragments": [],
                "text": "On image auto-annotation with latent space models"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper applies and compares two simple latent space models commonly used in text analysis, namely Latent Semantic Analysis (LSA) and Probabilistic LSA (PLSA), and found that, on a 8000-image dataset, a classic LSA model defined on keywords and a very basic image representation performed as well as much more complex, state-of-the-art methods."
            },
            "venue": {
                "fragments": [],
                "text": "MULTIMEDIA '03"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745732"
                        ],
                        "name": "M. Charikar",
                        "slug": "M.-Charikar",
                        "structuredName": {
                            "firstName": "Moses",
                            "lastName": "Charikar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Charikar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 209
                            }
                        ],
                        "text": "Definitions of LSH functions include the Min-wise independent function, which preserves the Jaccard similarity between vectors (Broder, 1997), and functions that preserve the cosine similarity between vectors (Charikar, 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4229473,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "42bb0ac384fb87933be67f63b98d90a45d2fe6e9",
            "isKey": false,
            "numCitedBy": 2207,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "(MATH) A locality sensitive hashing scheme is a distribution on a family $\\F$ of hash functions operating on a collection of objects, such that for two objects <i>x,y</i>, <b>Pr</b><sub><i>h</i></sub>\u03b5F[<i>h</i>(<i>x</i>) = <i>h</i>(<i>y</i>)] = sim(<i>x,y</i>), where <i>sim</i>(<i>x,y</i>) \u03b5 [0,1] is some similarity function defined on the collection of objects. Such a scheme leads to a compact representation of objects so that similarity of objects can be estimated from their compact sketches, and also leads to efficient algorithms for approximate nearest neighbor search and clustering. Min-wise independent permutations provide an elegant construction of such a locality sensitive hashing scheme for a collection of subsets with the set similarity measure <i>sim</i>(<i>A,B</i>) = \\frac{|A &Pgr; B|}{|A &Pgr B|}.(MATH) We show that rounding algorithms for LPs and SDPs used in the context of approximation algorithms can be viewed as locality sensitive hashing schemes for several interesting collections of objects. Based on this insight, we construct new locality sensitive hashing schemes for:<ol><li>A collection of vectors with the distance between \u2192 \\over <i>u</i> and \u2192 \\over <i>v</i> measured by \u00d8(\u2192 \\over <i>u</i>, \u2192 \\over <i>v</i>)/\u03c0, where \u00d8(\u2192 \\over <i>u</i>, \u2192 \\over <i>v</i>) is the angle between \u2192 \\over <i>u</i>) and \u2192 \\over <i>v</i>). This yields a sketching scheme for estimating the cosine similarity measure between two vectors, as well as a simple alternative to minwise independent permutations for estimating set similarity.</li><li>A collection of distributions on <i>n</i> points in a metric space, with distance between distributions measured by the Earth Mover Distance (<b>EMD</b>), (a popular distance measure in graphics and vision). Our hash functions map distributions to points in the metric space such that, for distributions <i>P</i> and <i>Q</i>, <b>EMD</b>(<i>P,Q</i>) &xie; <b>E</b><sub>h\u03b5\\F</sub> [<i>d</i>(<i>h</i>(<i>P</i>),<i>h</i>(<i>Q</i>))] &xie; <i>O</i>(log <i>n</i> log log <i>n</i>). <b>EMD</b>(<i>P, Q</i>).</li></ol>."
            },
            "slug": "Similarity-estimation-techniques-from-rounding-Charikar",
            "title": {
                "fragments": [],
                "text": "Similarity estimation techniques from rounding algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is shown that rounding algorithms for LPs and SDPs used in the context of approximation algorithms can be viewed as locality sensitive hashing schemes for several interesting collections of objects."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '02"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71084179"
                        ],
                        "name": "Muthu Dayalan",
                        "slug": "Muthu-Dayalan",
                        "structuredName": {
                            "firstName": "Muthu",
                            "lastName": "Dayalan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Muthu Dayalan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 0
                            }
                        ],
                        "text": "Dean and Ghemawat (2008) provide a good primer on MapReduce programming."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 17
                            }
                        ],
                        "text": "Word similarity: Deerwester et al. (1990) discovered that we can measure word similarity by comparing row vectors in a term\u2013document matrix. Landauer and Dumais (1997) evaluated this approach with 80 multiple-choice synonym questions from the Test of English as a Foreign Language (TOEFL), achieving human-level performance (64."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 17
                            }
                        ],
                        "text": "Word similarity: Deerwester et al. (1990) discovered that we can measure word similarity by comparing row vectors in a term\u2013document matrix."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 67055872,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "627be67feb084f1266cfc36e5aed3c3e7e6ce5f0",
            "isKey": true,
            "numCitedBy": 7853,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day."
            },
            "slug": "MapReduce:-simplified-data-processing-on-large-Dayalan",
            "title": {
                "fragments": [],
                "text": "MapReduce: simplified data processing on large clusters"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This presentation explains how the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3530486"
                        ],
                        "name": "R. Darnell",
                        "slug": "R.-Darnell",
                        "structuredName": {
                            "firstName": "R\u00e9gna",
                            "lastName": "Darnell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Darnell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 67
                            }
                        ],
                        "text": "words that occur in similar contexts tend to have similar meanings (Wittgenstein, 1953; Harris, 1954; Weaver, 1955; Firth, 1957; Deerwester, Dumais, Landauer, Furnas, & Harshman, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 137
                            }
                        ],
                        "text": "See http://wordnet.princeton.edu/.\nwords that occur in similar contexts tend to have similar meanings (Wittgenstein, 1953; Harris, 1954; Weaver, 1955; Firth, 1957; Deerwester, Dumais, Landauer, Furnas, & Harshman, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 119
                            }
                        ],
                        "text": "Statistical semantics hypothesis: Statistical patterns of human word usage can be used to figure out what people mean (Weaver, 1955; Furnas et al., 1983)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 149
                            }
                        ],
                        "text": "\u2026(e.g., the word brick, spoken in the context of the physical activity of building a house), but the main context for a word is often other words.11\nWeaver (1955) argued that word sense disambiguation for machine translation should be based on the co-occurrence frequency of the context words near\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 215102261,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "5839ad026ee43f3b72493c416dddb4203791714d",
            "isKey": true,
            "numCitedBy": 861,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "?well worth reproducing in English.?Edts., I. M. Gazette.] On the loss of the epithelium of the intestinal canal, consequent on the excessive secretion of fluid from its surface. We are all well acquainted with the fact, that in certain diseases the outer layers of the epithelial cells protecting the skin are thrown off in flakes ; and I believe that it is the same in Asiatic cholera as regards epithelial cells lining the surface of the jntestinal mucous membrane?a matter of greater pathological significance than that concerning the skin, because the intestinal epithelium is intended to guard more delicate and important structures than the cells that cover the cutis. The symptoms of cholera, however, are very much dependent on this desquamation of the epithelia?a fact which may be demonstrated by the aid of the microscope; but we are not to suppose that all parts of the intestinal canal are equally affected in cholera. The epithelium of the stomach suffers less than that of the intestines, and the upper part of the small intestines is not so deeply involved in the disease as the lower part of the ileum. In the duodenum, where the peristaltic action of the canal is not very strong, you often find the epithelial cells lining the mucous membrane ; the cells are loosened, but riot detached, because this part of the canal has less mechanical work to do than the lower portion of the gut. The valvulae conniventes (kerkring), which are large and closely approximated in the second part of the duodenum, protect by covering in the epithelial celh that lie between them, but on the surface of these folds wo shall observe the commencement of the desquamative process which is so marked in the ileum. We shall see with the naked eye that the epithelium, which should cover the valvulae conniventes, has disappeared in places, leaving small isolated patches of the denuded mucous membrane. In their early stages, these spots are distinguishable by their whiter colour, and by a soft velvet-like texture, which may be well demonstrated if a spot of this kind is isolates1, and fixed on a plate under the object glass of the microscope, little water being allowed to trickle over it. You may also in this way examine the villi, which are clearly denuded of epithelial cells in the patches of the valvulae conniventes above referred to. In some parts wo notice that a space evidently extends through the length of the villi, and externally the villi are covered"
            },
            "slug": "Translation-Darnell",
            "title": {
                "fragments": [],
                "text": "Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "This chapter discusses the loss of the epithelium of the intestinal canal, consequent on the excessive secretion of fluid from its surface, and examines the villi, which are clearly denuded of epithelial cells in the patches of the valvulae conniventes above referred to."
            },
            "venue": {
                "fragments": [],
                "text": "The Indian medical gazette"
            },
            "year": 1873
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30182834"
                        ],
                        "name": "G. Linden",
                        "slug": "G.-Linden",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Linden",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Linden"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157122032"
                        ],
                        "name": "Brent Smith",
                        "slug": "Brent-Smith",
                        "structuredName": {
                            "firstName": "Brent",
                            "lastName": "Smith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brent Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073999382"
                        ],
                        "name": "J. York",
                        "slug": "J.-York",
                        "structuredName": {
                            "firstName": "Jeremy",
                            "lastName": "York",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. York"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14604122,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "da8b0378174bc25ed174be36a1c725787b81854d",
            "isKey": false,
            "numCitedBy": 5412,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Recommendation algorithms are best known for their use on e-commerce Web sites, where they use input about a customer's interests to generate a list of recommended items. Many applications use only the items that customers purchase and explicitly rate to represent their interests, but they can also use other attributes, including items viewed, demographic data, subject interests, and favorite artists. At Amazon.com, we use recommendation algorithms to personalize the online store for each customer. The store radically changes based on customer interests, showing programming titles to a software engineer and baby toys to a new mother. There are three common approaches to solving the recommendation problem: traditional collaborative filtering, cluster models, and search-based methods. Here, we compare these methods with our algorithm, which we call item-to-item collaborative filtering. Unlike traditional collaborative filtering, our algorithm's online computation scales independently of the number of customers and number of items in the product catalog. Our algorithm produces recommendations in real-time, scales to massive data sets, and generates high quality recommendations."
            },
            "slug": "Amazon.com-Recommendations:-Item-to-Item-Filtering-Linden-Smith",
            "title": {
                "fragments": [],
                "text": "Amazon.com Recommendations: Item-to-Item Collaborative Filtering"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work compares three common approaches to solving the recommendation problem: traditional collaborative filtering, cluster models, and search-based methods, and their algorithm, which is called item-to-item collaborative filtering."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Internet Comput."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145421878"
                        ],
                        "name": "R. Sproat",
                        "slug": "R.-Sproat",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sproat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sproat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145085715"
                        ],
                        "name": "Thomas Emerson",
                        "slug": "Thomas-Emerson",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Emerson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Emerson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 152
                            }
                        ],
                        "text": "A more sophisticated approach is to match the input text against entries in a lexicon, but the matching often does not determine a unique tokenization (Sproat & Emerson, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2776693,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "602af317fe1ff52f254a824df5880505e086c76d",
            "isKey": false,
            "numCitedBy": 230,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents the results from the ACL-SIGHAN-sponsored First International Chinese Word Segmentation Bakeoff held in 2003 and reported in conjunction with the Second SIGHAN Workshop on Chinese Language Processing, Sapporo, Japan. We give the motivation for having an international segmentation contest (given that there have been two within-China contests to date) and we report on the results of this first international contest, analyze these results, and make some recommendations for the future."
            },
            "slug": "The-First-International-Chinese-Word-Segmentation-Sproat-Emerson",
            "title": {
                "fragments": [],
                "text": "The First International Chinese Word Segmentation Bakeoff"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper presents the results from the ACL-SIGHAN-sponsored First International Chinese Word Segmentation Bakeoff held in 2003 and reported in conjunction with the Second SIGHAN Workshop on Chinese Language Processing, Sapporo, Japan."
            },
            "venue": {
                "fragments": [],
                "text": "SIGHAN"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692160"
                        ],
                        "name": "R. Bayardo",
                        "slug": "R.-Bayardo",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Bayardo",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bayardo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47009838"
                        ],
                        "name": "Y. Ma",
                        "slug": "Y.-Ma",
                        "structuredName": {
                            "firstName": "Yiming",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Ma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34641476"
                        ],
                        "name": "R. Srikant",
                        "slug": "R.-Srikant",
                        "structuredName": {
                            "firstName": "Ramakrishnan",
                            "lastName": "Srikant",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Srikant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 689,
                                "start": 23
                            }
                        ],
                        "text": "15 Another approach is Brand\u2019s (2006) incremental truncated SVD algorithm.16 Yet another approach is Gorrell\u2019s (2006) Hebbian algorithm for incremental truncated SVD. Brand\u2019s and Gorrell\u2019s algorithms both introduce interesting new ways of handling missing values, instead of treating them as zero values. For higher-order tensors, there are operations that are analogous to truncated SVD, such as parallel factor analysis (PARAFAC) (Harshman, 1970), canonical decomposition (CANDECOMP) (Carroll & Chang, 1970) (equivalent to PARAFAC but discovered independently), and Tucker decomposition (Tucker, 1966). For an overview of tensor decompositions, see the surveys of Kolda and Bader (2009) or Acar and Yener (2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 23
                            }
                        ],
                        "text": "15 Another approach is Brand\u2019s (2006) incremental truncated SVD algorithm."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 23
                            }
                        ],
                        "text": "15 Another approach is Brand\u2019s (2006) incremental truncated SVD algorithm.16 Yet another approach is Gorrell\u2019s (2006) Hebbian algorithm for incremental truncated SVD."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5996048,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f5f2cc33e5bc6368d55955cdaf3bf20b9cc30c72",
            "isKey": true,
            "numCitedBy": 747,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Given a large collection of sparse vector data in a high dimensional space, we investigate the problem of finding all pairs of vectors whose similarity score (as determined by a function such as cosine distance) is above a given threshold. We propose a simple algorithm based on novel indexing and optimization strategies that solves this problem without relying on approximation methods or extensive parameter tuning. We show the approach efficiently handles a variety of datasets across a wide setting of similarity thresholds, with large speedups over previous state-of-the-art approaches."
            },
            "slug": "Scaling-up-all-pairs-similarity-search-Bayardo-Ma",
            "title": {
                "fragments": [],
                "text": "Scaling up all pairs similarity search"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This work proposes a simple algorithm based on novel indexing and optimization strategies that solves the problem of finding all pairs of vectors whose similarity score is above a given threshold without relying on approximation methods or extensive parameter tuning."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764547"
                        ],
                        "name": "M. Sahami",
                        "slug": "M.-Sahami",
                        "structuredName": {
                            "firstName": "Mehran",
                            "lastName": "Sahami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sahami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728602"
                        ],
                        "name": "S. Dumais",
                        "slug": "S.-Dumais",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Dumais",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dumais"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48099028"
                        ],
                        "name": "D. Heckerman",
                        "slug": "D.-Heckerman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Heckerman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Heckerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145479841"
                        ],
                        "name": "E. Horvitz",
                        "slug": "E.-Horvitz",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Horvitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Horvitz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5301578,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "916177807ecbf0d78f2f64d756d4cecbaa3102a3",
            "isKey": false,
            "numCitedBy": 1598,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "In addressing the growing problem of junk E-mail on the Internet, we examine methods for the automated construction of filters to eliminate such unwanted messages from a user\u2019s mail stream. By casting this problem in a decision theoretic framework, we are able to make use of probabilistic learning methods in conjunction with a notion of differential misclassification cost to produce filters Which are especially appropriate for the nuances of this task. While this may appear, at first, to be a straight-forward text classification problem, we show that by considering domain-specific features of this problem in addition to the raw text of E-mail messages, we can produce much more accurate filters. Finally, we show the efficacy of such filters in a real world usage scenario, arguing that this technology is mature enough for deployment."
            },
            "slug": "A-Bayesian-Approach-to-Filtering-Junk-E-Mail-Sahami-Dumais",
            "title": {
                "fragments": [],
                "text": "A Bayesian Approach to Filtering Junk E-Mail"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "This work examines methods for the automated construction of filters to eliminate such unwanted messages from a user\u2019s mail stream, and shows the efficacy of such filters in a real world usage scenario, arguing that this technology is mature enough for deployment."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI 1998"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064774858"
                        ],
                        "name": "Deepak Ravichandran",
                        "slug": "Deepak-Ravichandran",
                        "structuredName": {
                            "firstName": "Deepak",
                            "lastName": "Ravichandran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Deepak Ravichandran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1990190"
                        ],
                        "name": "P. Pantel",
                        "slug": "P.-Pantel",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Pantel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pantel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144547315"
                        ],
                        "name": "E. Hovy",
                        "slug": "E.-Hovy",
                        "structuredName": {
                            "firstName": "Eduard",
                            "lastName": "Hovy",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Hovy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6884128,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "826be441da031fde467f786426e075d723239a72",
            "isKey": false,
            "numCitedBy": 242,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we explore the power of randomized algorithm to address the challenge of working with very large amounts of data. We apply these algorithms to generate noun similarity lists from 70 million pages. We reduce the running time from quadratic to practically linear in the number of elements to be computed."
            },
            "slug": "Randomized-Algorithms-and-NLP:-Using-Locality-Hash-Ravichandran-Pantel",
            "title": {
                "fragments": [],
                "text": "Randomized Algorithms and NLP: Using Locality Sensitive Hash Functions for High Speed Noun Clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "The power of randomized algorithm is explored to address the challenge of working with very large amounts of data and the running time from quadratic to practically linear in the number of elements to be computed."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3298207"
                        ],
                        "name": "P. Kanerva",
                        "slug": "P.-Kanerva",
                        "structuredName": {
                            "firstName": "Pentti",
                            "lastName": "Kanerva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Kanerva"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 79
                            }
                        ],
                        "text": "Random Indexing, an approximation technique based on Sparse Distributed Memory (Kanerva, 1993), computes the pairwise similarity between all rows (or vectors) of a matrix with complexity O(nrnc\u03b41), where \u03b41 is a fixed constant representing the length of the index vectors assigned to each column."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6073397,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a0b1d1e2a99c4d9c67750c231b3883e5976717d4",
            "isKey": false,
            "numCitedBy": 140,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "Described here is sparse distributed memory (SDM) as a neural-net associative memory. It is characterized by two weight matrices and by a large internal dimension - the number of hidden units is much larger than the number of input or output units. The first matrix, A, is fixed and possibly random, and the second matrix, C, is modifiable. The SDM is compared and contrasted to (1) computer memory, (2) correlation-matrix memory, (3) feet-forward artificial neural network, (4) cortex of the cerebellum, (5) Marr and Albus models of the cerebellum, and (6) Albus' cerebellar model arithmetic computer (CMAC). Several variations of the basic SDM design are discussed: the selected-coordinate and hyperplane designs of Jaeckel, the pseudorandom associative neural memory of Hassoun, and SDM with real-valued input variables by Prager and Fallside. SDM research conducted mainly at the Research Institute for Advanced Computer Science (RIACS) in 1986-1991 is highlighted."
            },
            "slug": "Sparse-distributed-memory-and-related-models-Kanerva",
            "title": {
                "fragments": [],
                "text": "Sparse distributed memory and related models"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Several variations of the basic SDM design are discussed: the selected-coordinate and hyperplane designs of Jaeckel, the pseudorandom associative neural memory of Hassoun, and SDM with real-valued input variables by Prager and Fallside."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145810617"
                        ],
                        "name": "Lillian Lee",
                        "slug": "Lillian-Lee",
                        "structuredName": {
                            "firstName": "Lillian",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lillian Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 256
                            }
                        ],
                        "text": "\u2026similarity by inversion (13) or subtraction (14).\nsim(x,y) = 1/dist(x,y) (13) sim(x,y) = 1\u2212 dist(x,y) (14)\nMany similarity measures have been proposed in both IR (Jones & Furnas, 1987) and lexical semantics circles (Lin, 1998; Dagan, Lee, & Pereira, 1999; Lee, 1999; Weeds, Weir, & McCarthy, 2004)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 108
                            }
                        ],
                        "text": "Many similarity measures have been proposed in both IR (Jones & Furnas, 1987) and lexical semantics circles (Lin, 1998; Dagan, Lee, & Pereira, 1999; Lee, 1999; Weeds, Weir, & McCarthy, 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 0
                            }
                        ],
                        "text": "Lee (1999) proposed that, for finding word similarities, measures that focused more on overlapping coordinates and less on the importance of negative features (i.e., coordinates where one word has a nonzero value and the other has a zero value) appear to perform better."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6305097,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6f3250ba47fdb413a0c113cc16d274517864f8ab",
            "isKey": false,
            "numCitedBy": 661,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We study distributional similarity measures for the purpose of improving probability estimation for unseen cooccurrences. Our contributions are three-fold: an empirical comparison of a broad range of measures; a classification of similarity functions based on the information that they incorporate; and the introduction of a novel function that is superior at evaluating potential proxy distributions."
            },
            "slug": "Measures-of-Distributional-Similarity-Lee",
            "title": {
                "fragments": [],
                "text": "Measures of Distributional Similarity"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work presents an empirical comparison of a broad range of measures; a classification of similarity functions based on the information that they incorporate; and the introduction of a novel function that is superior at evaluating potential proxy distributions."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143928505"
                        ],
                        "name": "T. Elsayed",
                        "slug": "T.-Elsayed",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Elsayed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Elsayed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145580839"
                        ],
                        "name": "Jimmy J. Lin",
                        "slug": "Jimmy-J.-Lin",
                        "structuredName": {
                            "firstName": "Jimmy",
                            "lastName": "Lin",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jimmy J. Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737250"
                        ],
                        "name": "D. Oard",
                        "slug": "D.-Oard",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Oard",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Oard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 954470,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "08fd33cb1c8837d374bc4c863a09cc792f6c52f2",
            "isKey": false,
            "numCitedBy": 237,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a MapReduce algorithm for computing pairwise document similarity in large document collections. MapReduce is an attractive framework because it allows us to decompose the inner products involved in computing document similarity into separate multiplication and summation stages in a way that is well matched to efficient disk access patterns across several machines. On a collection consisting of approximately 900,000 newswire articles, our algorithm exhibits linear growth in running time and space in terms of the number of documents."
            },
            "slug": "Pairwise-Document-Similarity-in-Large-Collections-Elsayed-Lin",
            "title": {
                "fragments": [],
                "text": "Pairwise Document Similarity in Large Collections with MapReduce"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "This paper presents a MapReduce algorithm for computing pairwise document similarity in large document collections that exhibits linear growth in running time and space in terms of the number of documents."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144741762"
                        ],
                        "name": "P. Resnick",
                        "slug": "P.-Resnick",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Resnick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Resnick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32895600"
                        ],
                        "name": "N. Iacovou",
                        "slug": "N.-Iacovou",
                        "structuredName": {
                            "firstName": "Neophytos",
                            "lastName": "Iacovou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Iacovou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48185525"
                        ],
                        "name": "M. Suchak",
                        "slug": "M.-Suchak",
                        "structuredName": {
                            "firstName": "Mitesh",
                            "lastName": "Suchak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Suchak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073783270"
                        ],
                        "name": "P. Bergstrom",
                        "slug": "P.-Bergstrom",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bergstrom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bergstrom"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2579342"
                        ],
                        "name": "J. Riedl",
                        "slug": "J.-Riedl",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Riedl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Riedl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2616594,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "64a717dc148b76070bbb6a3190a7e05bb9734400",
            "isKey": false,
            "numCitedBy": 5750,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Collaborative filters help people make choices based on the opinions of other people. GroupLens is a system for collaborative filtering of netnews, to help people find articles they will like in the huge stream of available articles. News reader clients display predicted scores and make it easy for users to rate articles after they read them. Rating servers, called Better Bit Bureaus, gather and disseminate the ratings. The rating servers predict scores based on the heuristic that people who agreed in the past will probably agree again. Users can protect their privacy by entering ratings under pseudonyms, without reducing the effectiveness of the score prediction. The entire architecture is open: alternative software for news clients and Better Bit Bureaus can be developed independently and can interoperate with the components we have developed."
            },
            "slug": "GroupLens:-an-open-architecture-for-collaborative-Resnick-Iacovou",
            "title": {
                "fragments": [],
                "text": "GroupLens: an open architecture for collaborative filtering of netnews"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "GroupLens is a system for collaborative filtering of netnews, to help people find articles they will like in the huge stream of available articles, and protect their privacy by entering ratings under pseudonyms, without reducing the effectiveness of the score prediction."
            },
            "venue": {
                "fragments": [],
                "text": "CSCW '94"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2428338"
                        ],
                        "name": "Freddy Y. Y. Choi",
                        "slug": "Freddy-Y.-Y.-Choi",
                        "structuredName": {
                            "firstName": "Freddy",
                            "lastName": "Choi",
                            "middleNames": [
                                "Y.",
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Freddy Y. Y. Choi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 368,
                                "start": 181
                            }
                        ],
                        "text": "Document segmentation: The task of document segmentation is to partition a document into sections, where each section focuses on a different subtopic of the document (Hearst, 1997; Choi, 2000). We may treat the document as a series of blocks, where a block is a sentence or a paragraph. The problem is to detect a topic shift from one block to the next. Hearst (1997) and Choi (2000) both use the cosine between columns in a word\u2013block frequency matrix to measure the semantic similarity of blocks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2891,
                                "start": 110
                            }
                        ],
                        "text": "Landauer and Dumais (1997) applied truncated SVD to word similarity, achieving human-level scores on multiple-choice synonym questions from the Test of English as a Foreign Language (TOEFL). Truncated SVD applied to document similarity is called Latent Semantic Indexing (LSI), but it is called Latent Semantic Analysis (LSA) when applied to word similarity. There are several ways of thinking about how truncated SVD works. We will first present the math behind truncated SVD and then describe four ways of looking at it: latent meaning, noise reduction, high-order co-occurrence, and sparsity reduction. SVD decomposes X into the product of three matrices U\u03a3VT, where U and V are in column orthonormal form (i.e., the columns are orthogonal and have unit length, UTU = VTV = I) and \u03a3 is a diagonal matrix of singular values (Golub & Van Loan, 1996). If X is of rank r, then \u03a3 is also of rank r. Let \u03a3k, where k < r, be the diagonal matrix formed from the top k singular values, and let Uk and Vk be the matrices produced by selecting the corresponding columns from U and V. The matrix Uk\u03a3kV k is the matrix of rank k that best approximates the original matrix X, in the sense that it minimizes the approximation errors. That is, X\u0302 = Uk\u03a3kV k minimizes \u2016X\u0302 \u2212 X\u2016F over all matrices X\u0302 of rank k, where \u2016 . . . \u2016F denotes the Frobenius norm (Golub & Van Loan, 1996). Latent meaning: Deerwester et al. (1990) and Landauer and Dumais (1997) describe truncated SVD as a method for discovering latent meaning. Suppose we have a word\u2013 context matrix X. The truncated SVD, X\u0302 = Uk\u03a3kV k , creates a low-dimensional linear mapping between row space (words) and column space (contexts). This low-dimensional mapping captures the latent (hidden) meaning in the words and the contexts. Limiting the number of latent dimensions (k < r) forces a greater correspondence between words and contexts. This forced correspondence between words and contexts improves the similarity measurement. Noise reduction: Rapp (2003) describes truncated SVD as a noise reduction technique. We may think of the matrix X\u0302 = Uk\u03a3kV k as a smoothed version of the original matrix X. The matrix Uk maps the row space (the space spanned by the rows of X) into a smaller k-dimensional space and the matrix Vk maps the column space (the space spanned by the columns of X) into the same k-dimensional space. The diagonal matrix \u03a3k specifies the weights in this reduced k-dimensional space. The singular values in \u03a3 are ranked in descending order of the amount of variation in X that they fit. If we think of the matrix X as being composed of a mixture of signal and noise, with more signal than noise, then Uk\u03a3kV k mostly captures the variation in X that is due to the signal, whereas the remaining vectors in U\u03a3VT are mostly fitting the variation in X that is due to the noise. High-order co-occurrence: Landauer and Dumais (1997) also describe truncated SVD as a method for discovering high-order co-occurrence."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 166
                            }
                        ],
                        "text": "Document segmentation: The task of document segmentation is to partition a document into sections, where each section focuses on a different subtopic of the document (Hearst, 1997; Choi, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3282,
                                "start": 110
                            }
                        ],
                        "text": "Landauer and Dumais (1997) applied truncated SVD to word similarity, achieving human-level scores on multiple-choice synonym questions from the Test of English as a Foreign Language (TOEFL). Truncated SVD applied to document similarity is called Latent Semantic Indexing (LSI), but it is called Latent Semantic Analysis (LSA) when applied to word similarity. There are several ways of thinking about how truncated SVD works. We will first present the math behind truncated SVD and then describe four ways of looking at it: latent meaning, noise reduction, high-order co-occurrence, and sparsity reduction. SVD decomposes X into the product of three matrices U\u03a3VT, where U and V are in column orthonormal form (i.e., the columns are orthogonal and have unit length, UTU = VTV = I) and \u03a3 is a diagonal matrix of singular values (Golub & Van Loan, 1996). If X is of rank r, then \u03a3 is also of rank r. Let \u03a3k, where k < r, be the diagonal matrix formed from the top k singular values, and let Uk and Vk be the matrices produced by selecting the corresponding columns from U and V. The matrix Uk\u03a3kV k is the matrix of rank k that best approximates the original matrix X, in the sense that it minimizes the approximation errors. That is, X\u0302 = Uk\u03a3kV k minimizes \u2016X\u0302 \u2212 X\u2016F over all matrices X\u0302 of rank k, where \u2016 . . . \u2016F denotes the Frobenius norm (Golub & Van Loan, 1996). Latent meaning: Deerwester et al. (1990) and Landauer and Dumais (1997) describe truncated SVD as a method for discovering latent meaning. Suppose we have a word\u2013 context matrix X. The truncated SVD, X\u0302 = Uk\u03a3kV k , creates a low-dimensional linear mapping between row space (words) and column space (contexts). This low-dimensional mapping captures the latent (hidden) meaning in the words and the contexts. Limiting the number of latent dimensions (k < r) forces a greater correspondence between words and contexts. This forced correspondence between words and contexts improves the similarity measurement. Noise reduction: Rapp (2003) describes truncated SVD as a noise reduction technique. We may think of the matrix X\u0302 = Uk\u03a3kV k as a smoothed version of the original matrix X. The matrix Uk maps the row space (the space spanned by the rows of X) into a smaller k-dimensional space and the matrix Vk maps the column space (the space spanned by the columns of X) into the same k-dimensional space. The diagonal matrix \u03a3k specifies the weights in this reduced k-dimensional space. The singular values in \u03a3 are ranked in descending order of the amount of variation in X that they fit. If we think of the matrix X as being composed of a mixture of signal and noise, with more signal than noise, then Uk\u03a3kV k mostly captures the variation in X that is due to the signal, whereas the remaining vectors in U\u03a3VT are mostly fitting the variation in X that is due to the noise. High-order co-occurrence: Landauer and Dumais (1997) also describe truncated SVD as a method for discovering high-order co-occurrence. Direct co-occurrence (firstorder co-occurrence) is when two words appear in identical contexts. Indirect co-occurrence (high-order co-occurrence) is when two words appear in similar contexts. Similarity of contexts may be defined recursively in terms of lower-order co-occurrence. Lemaire and Denhi\u00e8re (2006) demonstrate that truncated SVD can discover high-order co-occurrence."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2003,
                                "start": 110
                            }
                        ],
                        "text": "Landauer and Dumais (1997) applied truncated SVD to word similarity, achieving human-level scores on multiple-choice synonym questions from the Test of English as a Foreign Language (TOEFL). Truncated SVD applied to document similarity is called Latent Semantic Indexing (LSI), but it is called Latent Semantic Analysis (LSA) when applied to word similarity. There are several ways of thinking about how truncated SVD works. We will first present the math behind truncated SVD and then describe four ways of looking at it: latent meaning, noise reduction, high-order co-occurrence, and sparsity reduction. SVD decomposes X into the product of three matrices U\u03a3VT, where U and V are in column orthonormal form (i.e., the columns are orthogonal and have unit length, UTU = VTV = I) and \u03a3 is a diagonal matrix of singular values (Golub & Van Loan, 1996). If X is of rank r, then \u03a3 is also of rank r. Let \u03a3k, where k < r, be the diagonal matrix formed from the top k singular values, and let Uk and Vk be the matrices produced by selecting the corresponding columns from U and V. The matrix Uk\u03a3kV k is the matrix of rank k that best approximates the original matrix X, in the sense that it minimizes the approximation errors. That is, X\u0302 = Uk\u03a3kV k minimizes \u2016X\u0302 \u2212 X\u2016F over all matrices X\u0302 of rank k, where \u2016 . . . \u2016F denotes the Frobenius norm (Golub & Van Loan, 1996). Latent meaning: Deerwester et al. (1990) and Landauer and Dumais (1997) describe truncated SVD as a method for discovering latent meaning. Suppose we have a word\u2013 context matrix X. The truncated SVD, X\u0302 = Uk\u03a3kV k , creates a low-dimensional linear mapping between row space (words) and column space (contexts). This low-dimensional mapping captures the latent (hidden) meaning in the words and the contexts. Limiting the number of latent dimensions (k < r) forces a greater correspondence between words and contexts. This forced correspondence between words and contexts improves the similarity measurement. Noise reduction: Rapp (2003) describes truncated SVD as a noise reduction technique."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1302,
                                "start": 181
                            }
                        ],
                        "text": "Document segmentation: The task of document segmentation is to partition a document into sections, where each section focuses on a different subtopic of the document (Hearst, 1997; Choi, 2000). We may treat the document as a series of blocks, where a block is a sentence or a paragraph. The problem is to detect a topic shift from one block to the next. Hearst (1997) and Choi (2000) both use the cosine between columns in a word\u2013block frequency matrix to measure the semantic similarity of blocks. A topic shift is signaled by a drop in the cosine between consecutive blocks. The word\u2013block matrix can be viewed as a small term\u2013document matrix, where the corpus is a single document and the documents are blocks. Question answering: Given a simple question, the task in Question Answering (QA) is to find a short answer to the question by searching in a large corpus. A typical question is, \u201cHow many calories are there in a Big Mac?\u201d Most algorithms for QA have four components, question analysis, document retrieval, passage retrieval, and answer extraction (Tellex, Katz, Lin, Fern, & Marton, 2003; Dang, Lin, & Kelly, 2006). Vector-based similarity measurements are often used for both document retrieval and passage retrieval (Tellex et al., 2003). Call routing: Chu-carroll and Carpenter (1999) present a vector-based system for automatically routing telephone calls, based on the caller\u2019s spoken answer to the question,"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 15
                            }
                        ],
                        "text": "5% on multiple-choice synonym questions from the Test of English as a Foreign Language (TOEFL), whereas the average human score was 64.5%.1 Turney (2006) used a vector-based representation of semantic relations to attain a score of 56% on multiple-choice analogy questions from the SAT college entrance test, compared to an average human score of 57%."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1407,
                                "start": 110
                            }
                        ],
                        "text": "Landauer and Dumais (1997) applied truncated SVD to word similarity, achieving human-level scores on multiple-choice synonym questions from the Test of English as a Foreign Language (TOEFL). Truncated SVD applied to document similarity is called Latent Semantic Indexing (LSI), but it is called Latent Semantic Analysis (LSA) when applied to word similarity. There are several ways of thinking about how truncated SVD works. We will first present the math behind truncated SVD and then describe four ways of looking at it: latent meaning, noise reduction, high-order co-occurrence, and sparsity reduction. SVD decomposes X into the product of three matrices U\u03a3VT, where U and V are in column orthonormal form (i.e., the columns are orthogonal and have unit length, UTU = VTV = I) and \u03a3 is a diagonal matrix of singular values (Golub & Van Loan, 1996). If X is of rank r, then \u03a3 is also of rank r. Let \u03a3k, where k < r, be the diagonal matrix formed from the top k singular values, and let Uk and Vk be the matrices produced by selecting the corresponding columns from U and V. The matrix Uk\u03a3kV k is the matrix of rank k that best approximates the original matrix X, in the sense that it minimizes the approximation errors. That is, X\u0302 = Uk\u03a3kV k minimizes \u2016X\u0302 \u2212 X\u2016F over all matrices X\u0302 of rank k, where \u2016 . . . \u2016F denotes the Frobenius norm (Golub & Van Loan, 1996). Latent meaning: Deerwester et al. (1990) and Landauer and Dumais (1997) describe truncated SVD as a method for discovering latent meaning."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2958363,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "448aa9b04905e421a8ef6e864c183f7ca6a7bb45",
            "isKey": true,
            "numCitedBy": 668,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a method for linear text segmentation which is twice as accurate and over seven times as fast as the state-of-the-art (Reynar, 1998). Inter-sentence similarity is replaced by rank in the local context. Boundary locations are discovered by divisive clustering."
            },
            "slug": "Advances-in-domain-independent-linear-text-Choi",
            "title": {
                "fragments": [],
                "text": "Advances in domain independent linear text segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper describes a method for linear text segmentation which is twice as accurate and over seven times as fast as the state-of-the-art (Reynar, 1998)."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144996667"
                        ],
                        "name": "Beno\u00eet Lemaire",
                        "slug": "Beno\u00eet-Lemaire",
                        "structuredName": {
                            "firstName": "Beno\u00eet",
                            "lastName": "Lemaire",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beno\u00eet Lemaire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2496651"
                        ],
                        "name": "G. Denhi\u00e8re",
                        "slug": "G.-Denhi\u00e8re",
                        "structuredName": {
                            "firstName": "Guy",
                            "lastName": "Denhi\u00e8re",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Denhi\u00e8re"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 335,
                                "start": 73
                            }
                        ],
                        "text": "We assume our reader has a basic understanding of vectors, matrices, and linear algebra, such as one might acquire from an introductory undergraduate course in linear algebra, or from a text book (Golub & Van Loan, 1996). The basic concepts of vectors and matrices are more important here than the mathematical details. Widdows (2004) gives a gentle introduction to vectors from the perspective of semantics."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 662,
                                "start": 73
                            }
                        ],
                        "text": "We assume our reader has a basic understanding of vectors, matrices, and linear algebra, such as one might acquire from an introductory undergraduate course in linear algebra, or from a text book (Golub & Van Loan, 1996). The basic concepts of vectors and matrices are more important here than the mathematical details. Widdows (2004) gives a gentle introduction to vectors from the perspective of semantics. We also assume our reader has some familiarity with computational linguistics or information retrieval. Manning et al. (2008) provide a good introduction to information retrieval. For computational linguistics, we recommend Manning and Sch\u00fctze\u2019s (1999) text."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 535,
                                "start": 73
                            }
                        ],
                        "text": "We assume our reader has a basic understanding of vectors, matrices, and linear algebra, such as one might acquire from an introductory undergraduate course in linear algebra, or from a text book (Golub & Van Loan, 1996). The basic concepts of vectors and matrices are more important here than the mathematical details. Widdows (2004) gives a gentle introduction to vectors from the perspective of semantics. We also assume our reader has some familiarity with computational linguistics or information retrieval. Manning et al. (2008) provide a good introduction to information retrieval."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 9
                            }
                        ],
                        "text": "Elsayed, Lin, and Oard (2008) proposed a MapReduce implementation deployed using Hadoop, an open-source software package implementing the MapReduce framework and distributed file system."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2270635,
            "fieldsOfStudy": [
                "Philosophy",
                "Computer Science"
            ],
            "id": "8509b5cdc10869711c8c5e1666cc542155e4bbb5",
            "isKey": true,
            "numCitedBy": 91,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Nous presentons un modele informatique de la construction de la signification des mots par l'exposition aux textes, dans le but de simuler, paragraphe apres paragraphe, les effets des valeurs de cooccurrence sur les similarites semantiques intermots. La similarite est ici consideree comme une association semantique. Les resultats montrent que la similarite entre deux mots M1 et M2 augmente fortement avec leur cooccurrence, diminue avec l'occurrence de M1 sans M2 ou de M2 sans M1, et augmente legerement avec des cooccurrences d'ordre superieur. Operationnaliser la similarite par la frequence de cooccurrence introduit donc probablement un biais : tout d'abord, il existe des cas pour lesquels il existe une similarite sans cooccurrence, et d'autre part, la frequence de cooccurrence surestime la similarite."
            },
            "slug": "Effects-of-High-Order-Co-occurrences-on-Word-Lemaire-Denhi\u00e8re",
            "title": {
                "fragments": [],
                "text": "Effects of High-Order Co-occurrences on Word Semantic Similarities"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "La similarite entre deux mots M1 et M2 augmente fortement avec leur cooccurrence, diminue avec l'occurrence de M1 sans M2 ou de M2 sans M1, and augmente legerement avec des cooccurrences d'ordre superieur."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709413"
                        ],
                        "name": "C. J. V. Rijsbergen",
                        "slug": "C.-J.-V.-Rijsbergen",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Rijsbergen",
                            "middleNames": [
                                "J.",
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. J. V. Rijsbergen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5312750,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "99614334a3e9b809e43384777409af7eccde3db6",
            "isKey": false,
            "numCitedBy": 491,
            "numCiting": 231,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface Prologue 1. Introduction 2. On sets and kinds in IR 3. Vector and Hilbert spaces 4. Linear transformations, operators and matrices 5. Conditional logic in IR 6. The geometry of IR Appendix I. Linear algebra Appendix II. Quantum mechanics Appendix III. Probability Bibliography Index."
            },
            "slug": "The-geometry-of-information-retrieval-Rijsbergen",
            "title": {
                "fragments": [],
                "text": "The geometry of information retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "The geometry of IR is studied through the lens of linear algebra, quantum mechanics, and vector and Hilbert spaces, with a focus on linear transformations, operators and matrices."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744364"
                        ],
                        "name": "T. Kolda",
                        "slug": "T.-Kolda",
                        "structuredName": {
                            "firstName": "Tamara",
                            "lastName": "Kolda",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kolda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31431661"
                        ],
                        "name": "Brett W. Bader",
                        "slug": "Brett-W.-Bader",
                        "structuredName": {
                            "firstName": "Brett",
                            "lastName": "Bader",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brett W. Bader"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 44
                            }
                        ],
                        "text": "The generalization of a matrix is a tensor (Kolda & Bader, 2009; Acar & Yener, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 104
                            }
                        ],
                        "text": "In applications dealing with polysemy, one approach uses vectors that represent word tokens (Schu\u0308tze, 1998; Agirre & Edmonds, 2006) and another uses vectors that represent word types (Pantel & Lin, 2002a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 61
                            }
                        ],
                        "text": "For an overview of tensor decompositions, see the surveys of Kolda and Bader (2009) or Acar and Yener (2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16074195,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "87e43e9eba01a4eb03436c9946bf6aa031a5d5af",
            "isKey": false,
            "numCitedBy": 7198,
            "numCiting": 343,
            "paperAbstract": {
                "fragments": [],
                "text": "This survey provides an overview of higher-order tensor decompositions, their applications, and available software. A tensor is a multidimensional or $N$-way array. Decompositions of higher-order tensors (i.e., $N$-way arrays with $N \\geq 3$) have applications in psycho-metrics, chemometrics, signal processing, numerical linear algebra, computer vision, numerical analysis, data mining, neuroscience, graph analysis, and elsewhere. Two particular tensor decompositions can be considered to be higher-order extensions of the matrix singular value decomposition: CANDECOMP/PARAFAC (CP) decomposes a tensor as a sum of rank-one tensors, and the Tucker decomposition is a higher-order form of principal component analysis. There are many other tensor decompositions, including INDSCAL, PARAFAC2, CANDELINC, DEDICOM, and PARATUCK2 as well as nonnegative variants of all of the above. The N-way Toolbox, Tensor Toolbox, and Multilinear Engine are examples of software packages for working with tensors."
            },
            "slug": "Tensor-Decompositions-and-Applications-Kolda-Bader",
            "title": {
                "fragments": [],
                "text": "Tensor Decompositions and Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This survey provides an overview of higher-order tensor decompositions, their applications, and available software."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM Rev."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689647"
                        ],
                        "name": "Peter D. Turney",
                        "slug": "Peter-D.-Turney",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Turney",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter D. Turney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Turney (2007) uses a word\u2013word\u2013pattern tensor to measure similarity of words."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 144
                            }
                        ],
                        "text": "We mention both approaches to polysemy in Section 6, due to their similarity and close relationship, although a defining characteristic of the VSM is that it is concerned with frequencies (see Section 1.1), and frequency is a property of types, not tokens."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Turney (2007) gives an empirical evaluation of how well four different Tucker decomposition algorithms scale up for large sparse third-order tensors."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1218598,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c1e6cd8b0cc4076f52f32f3d86d3638bf872d969",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Higher-order tensor decompositions are analogous to the familiar Singular Value Decomposition (SVD), but they transcend the limitations of matrices (second-order tensors). SVD is a powerful tool that has achieved impressive results in information retrieval, collaborative filtering, computational linguistics, computational vision, and other fields. However, SVD is limited to two-dimensional arrays of data (two modes), and many potential applications have three or more modes, which require higher-order tensor decompositions. This paper evaluates four algorithms for higher-order tensor decomposition: Higher-Order Singular Value Decomposition (HO-SVD), Higher-Order Orthogonal Iteration (HOOI), Slice Projection (SP), and Multislice Projection (MP). We measure the time (elapsed run time), space (RAM and disk space requirements), and fit (tensor reconstruction accuracy) of the four algorithms, under a variety of conditions. We find that standard implementations of HO-SVD and HOOI do not scale up to larger tensors, due to increasing RAM requirements. We recommend HOOI for tensors that are small enough for the available RAM and MP for larger tensors."
            },
            "slug": "Empirical-Evaluation-of-Four-Tensor-Decomposition-Turney",
            "title": {
                "fragments": [],
                "text": "Empirical Evaluation of Four Tensor Decomposition Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is found that standard implementations of HO-SVD and HOOI do not scale up to larger tensors, due to increasing RAM requirements, and is recommended to recommend HooI for tensors that are small enough for the available RAM and MP for larger Tensors."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2259699"
                        ],
                        "name": "Ji-Rong Wen",
                        "slug": "Ji-Rong-Wen",
                        "structuredName": {
                            "firstName": "Ji-Rong",
                            "lastName": "Wen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ji-Rong Wen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143619007"
                        ],
                        "name": "Jian-Yun Nie",
                        "slug": "Jian-Yun-Nie",
                        "structuredName": {
                            "firstName": "Jian-Yun",
                            "lastName": "Nie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian-Yun Nie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108698841"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10279153,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "27154320a85ad803fe7e9ba1521807fb6ea5b4b6",
            "isKey": false,
            "numCitedBy": 501,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "In order to increase retrieval precision, some new search engines provide manually verified answers to Frequently Asked Queries (FAQs). An underlying task is the identification of FAQs. This paper describes our attempt to cluster similar queries according to their contents as well as user logs. Our preliminary results show that the resulting clusters provide useful information for FAQ identification."
            },
            "slug": "Clustering-user-queries-of-a-search-engine-Wen-Nie",
            "title": {
                "fragments": [],
                "text": "Clustering user queries of a search engine"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The attempt to cluster similar queries according to their contents as well as user logs is described, and preliminary results show that the resulting clusters provide useful information for FAQ identification."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '01"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3139393"
                        ],
                        "name": "E. Acar",
                        "slug": "E.-Acar",
                        "structuredName": {
                            "firstName": "Evrim",
                            "lastName": "Acar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Acar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7834060"
                        ],
                        "name": "B. Yener",
                        "slug": "B.-Yener",
                        "structuredName": {
                            "firstName": "B\u00fclent",
                            "lastName": "Yener",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yener"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 87
                            }
                        ],
                        "text": "For an overview of tensor decompositions, see the surveys of Kolda and Bader (2009) or Acar and Yener (2009). Turney (2007) gives an empirical evaluation of how well four different Tucker decomposition algorithms scale up for large sparse third-order tensors."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 402,
                                "start": 87
                            }
                        ],
                        "text": "For an overview of tensor decompositions, see the surveys of Kolda and Bader (2009) or Acar and Yener (2009). Turney (2007) gives an empirical evaluation of how well four different Tucker decomposition algorithms scale up for large sparse third-order tensors. A low-RAM algorithm, Multislice Projection, for large sparse tensors is presented and evaluated.17 Since the work of Deerwester et al. (1990), subsequent research has discovered many alternative matrix smoothing processes, such as Nonnegative Matrix Factorization (NMF) (Lee & Seung, 1999), Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999), Iterative Scaling (IS) (Ando, 2000), Kernel Principal Components Analysis (KPCA) (Scholkopf, Smola, & Muller, 1997), Latent Dirichlet Allocation (LDA) (Blei et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 65
                            }
                        ],
                        "text": "The generalization of a matrix is a tensor (Kolda & Bader, 2009; Acar & Yener, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 87
                            }
                        ],
                        "text": "For an overview of tensor decompositions, see the surveys of Kolda and Bader (2009) or Acar and Yener (2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1658970,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5979666682daad35c7cf72e139902c000cbd6b37",
            "isKey": true,
            "numCitedBy": 403,
            "numCiting": 120,
            "paperAbstract": {
                "fragments": [],
                "text": "Two-way arrays or matrices are often not enough to represent all the information in the data and standard two-way analysis techniques commonly applied on matrices may fail to find the underlying structures in multi-modal datasets. Multiway data analysis has recently become popular as an exploratory analysis tool in discovering the structures in higher-order datasets, where data have more than two modes. We provide a review of significant contributions in the literature on multiway models, algorithms as well as their applications in diverse disciplines including chemometrics, neuroscience, social network analysis, text mining and computer vision."
            },
            "slug": "Unsupervised-Multiway-Data-Analysis:-A-Literature-Acar-Yener",
            "title": {
                "fragments": [],
                "text": "Unsupervised Multiway Data Analysis: A Literature Survey"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work provides a review of significant contributions in the literature on multiway models, algorithms as well as their applications in diverse disciplines including chemometrics, neuroscience, social network analysis, text mining and computer vision."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Knowledge and Data Engineering"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153071103"
                        ],
                        "name": "J. Carroll",
                        "slug": "J.-Carroll",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Carroll",
                            "middleNames": [
                                "Douglas"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Carroll"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32078324"
                        ],
                        "name": "J. Chang",
                        "slug": "J.-Chang",
                        "structuredName": {
                            "firstName": "Jih",
                            "lastName": "Chang",
                            "middleNames": [
                                "Jie"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Chang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 182
                            }
                        ],
                        "text": "For higher-order tensors, there are operations that are analogous to truncated SVD, such as parallel factor analysis (PARAFAC) (Harshman, 1970), canonical decomposition (CANDECOMP) (Carroll & Chang, 1970) (equivalent to PARAFAC but discovered independently), and Tucker decomposition (Tucker, 1966)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 50364581,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "757cb62e3d1c0643c9f83bf57d45e427bd76e235",
            "isKey": false,
            "numCitedBy": 4134,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "An individual differences model for multidimensional scaling is outlined in which individuals are assumed differentially to weight the several dimensions of a common \u201cpsychological space\u201d. A corresponding method of analyzing similarities data is proposed, involving a generalization of \u201cEckart-Young analysis\u201d to decomposition of three-way (or higher-way) tables. In the present case this decomposition is applied to a derived three-way table of scalar products between stimuli for individuals. This analysis yields a stimulus by dimensions coordinate matrix and a subjects by dimensions matrix of weights. This method is illustrated with data on auditory stimuli and on perception of nations."
            },
            "slug": "Analysis-of-individual-differences-in-scaling-via-Carroll-Chang",
            "title": {
                "fragments": [],
                "text": "Analysis of individual differences in multidimensional scaling via an n-way generalization of \u201cEckart-Young\u201d decomposition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704065"
                        ],
                        "name": "D. Gentner",
                        "slug": "D.-Gentner",
                        "structuredName": {
                            "firstName": "Dedre",
                            "lastName": "Gentner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gentner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 94
                            }
                        ],
                        "text": "The distinction between attributional and relational similarity has been explored in depth by Gentner (1983). The attributional similarity between two words a and b, sima(a, b) \u2208 , depends on the degree of correspondence between the properties of a and b."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 148
                            }
                        ],
                        "text": "The term semantic relatedness in computational linguistics (Budanitsky & Hirst, 2001) corresponds to attributional similarity in cognitive science (Gentner, 1983)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 94
                            }
                        ],
                        "text": "The distinction between attributional and relational similarity has been explored in depth by Gentner (1983)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5371492,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "c0373426c8e5579dcff60cc0bd930277822edc7d",
            "isKey": true,
            "numCitedBy": 2309,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "ion Anomaly No. of No. of attributes relations mopped to mapped to target target"
            },
            "slug": "Structure-Mapping:-A-Theoretical-Framework-for-Gentner",
            "title": {
                "fragments": [],
                "text": "Structure-Mapping: A Theoretical Framework for Analogy"
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50477565"
                        ],
                        "name": "L. Tucker",
                        "slug": "L.-Tucker",
                        "structuredName": {
                            "firstName": "Ledyard",
                            "lastName": "Tucker",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Tucker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 285
                            }
                        ],
                        "text": "For higher-order tensors, there are operations that are analogous to truncated SVD, such as parallel factor analysis (PARAFAC) (Harshman, 1970), canonical decomposition (CANDECOMP) (Carroll & Chang, 1970) (equivalent to PARAFAC but discovered independently), and Tucker decomposition (Tucker, 1966)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 44301099,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6661789de63b3cebe2eafdd7e9e7a316ad1f0b8f",
            "isKey": false,
            "numCitedBy": 3232,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The model for three-mode factor analysis is discussed in terms of newer applications of mathematical processes including a type of matrix process termed the Kronecker product and the definition of combination variables. Three methods of analysis to a type of extension of principal components analysis are discussed. Methods II and III are applicable to analysis of data collected for a large sample of individuals. An extension of the model is described in which allowance is made for unique variance for each combination variable when the data are collected for a large sample of individuals."
            },
            "slug": "Some-mathematical-notes-on-three-mode-factor-Tucker",
            "title": {
                "fragments": [],
                "text": "Some mathematical notes on three-mode factor analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The model for three-mode factor analysis is discussed in terms of newer applications of mathematical processes including a type of matrix process termed the Kronecker product and the definition of combination variables."
            },
            "venue": {
                "fragments": [],
                "text": "Psychometrika"
            },
            "year": 1966
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144865353"
                        ],
                        "name": "B. Pang",
                        "slug": "B.-Pang",
                        "structuredName": {
                            "firstName": "Bo",
                            "lastName": "Pang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Pang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145810617"
                        ],
                        "name": "Lillian Lee",
                        "slug": "Lillian-Lee",
                        "structuredName": {
                            "firstName": "Lillian",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lillian Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066721"
                        ],
                        "name": "Shivakumar Vaithyanathan",
                        "slug": "Shivakumar-Vaithyanathan",
                        "structuredName": {
                            "firstName": "Shivakumar",
                            "lastName": "Vaithyanathan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shivakumar Vaithyanathan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7105713,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "12d0353ce8b41b7e5409e5a4a611110aee33c7bc",
            "isKey": false,
            "numCitedBy": 8511,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging."
            },
            "slug": "Thumbs-up-Sentiment-Classification-using-Machine-Pang-Lee",
            "title": {
                "fragments": [],
                "text": "Thumbs up? Sentiment Classification using Machine Learning Techniques"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This work considers the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative, and concludes by examining factors that make the sentiment classification problem more challenging."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109560510"
                        ],
                        "name": "Soo-Min Kim",
                        "slug": "Soo-Min-Kim",
                        "structuredName": {
                            "firstName": "Soo-Min",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Soo-Min Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1990190"
                        ],
                        "name": "P. Pantel",
                        "slug": "P.-Pantel",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Pantel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pantel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2218418"
                        ],
                        "name": "Timothy Chklovski",
                        "slug": "Timothy-Chklovski",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Chklovski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timothy Chklovski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145375801"
                        ],
                        "name": "M. Pennacchiotti",
                        "slug": "M.-Pennacchiotti",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Pennacchiotti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pennacchiotti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15829121,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4492611e06d61d127d8b51d70d1be654a109209d",
            "isKey": false,
            "numCitedBy": 512,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "User-supplied reviews are widely and increasingly used to enhance e-commerce and other websites. Because reviews can be numerous and varying in quality, it is important to assess how helpful each review is. While review helpfulness is currently assessed manually, in this paper we consider the task of automatically assessing it. Experiments using SVM regression on a variety of features over Amazon.com product reviews show promising results, with rank correlations of up to 0.66. We found that the most useful features include the length of the review, its unigrams, and its product rating."
            },
            "slug": "Automatically-Assessing-Review-Helpfulness-Kim-Pantel",
            "title": {
                "fragments": [],
                "text": "Automatically Assessing Review Helpfulness"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper considers the task of automatically assessing review helpfulness, and finds that the most useful features include the length of the review, its unigrams, and its product rating."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70219052"
                        ],
                        "name": "Wray L. Buntine",
                        "slug": "Wray-L.-Buntine",
                        "structuredName": {
                            "firstName": "Wray",
                            "lastName": "Buntine",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wray L. Buntine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2962063"
                        ],
                        "name": "Aleks Jakulin",
                        "slug": "Aleks-Jakulin",
                        "structuredName": {
                            "firstName": "Aleks",
                            "lastName": "Jakulin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aleks Jakulin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14527203,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ec67db75e3d1fc3822b168d8d9ee45baab17389",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "This article presents a unified theory for analysis of components in discrete data, and compares the methods with techniques such as independent component analysis, non-negative matrix factorisation and latent Dirichlet allocation. The main families of algorithms discussed are a variational approximation, Gibbs sampling, and Rao-Blackwellised Gibbs sampling. Applications are presented for voting records from the United States Senate for 2003, and for the Reuters-21578 newswire collection."
            },
            "slug": "Discrete-Component-Analysis-Buntine-Jakulin",
            "title": {
                "fragments": [],
                "text": "Discrete Component Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A unified theory for analysis of components in discrete data is presented, and the methods compared with techniques such as independent component analysis, non-negative matrix factorisation and latent Dirichlet allocation are compared."
            },
            "venue": {
                "fragments": [],
                "text": "SLSFS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757913"
                        ],
                        "name": "D. Gleich",
                        "slug": "D.-Gleich",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Gleich",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gleich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143839070"
                        ],
                        "name": "L. Zhukov",
                        "slug": "L.-Zhukov",
                        "structuredName": {
                            "firstName": "Leonid",
                            "lastName": "Zhukov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Zhukov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7613032,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "78c3185be3defd7762f2c472b318c02eacc1aa45",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we consider the application of the singular value decomposition (SVD) to a search term suggestion system in a pay-for-performance search market. We propose a positive and negative refinement method based on orthogonal subspace projections. We demonstrate that SVD subspace-based methods: 1) expand coverage by reordering the results, and 2) enhance the clustered structure of the data. The numerical experiments reported in this paper were performed on Overture's pay-per-performance search market data."
            },
            "slug": "SVD-based-term-suggestion-and-ranking-system-Gleich-Zhukov",
            "title": {
                "fragments": [],
                "text": "SVD based term suggestion and ranking system"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is demonstrated that SVD subspace-based methods: 1) expand coverage by reordering the results, and 2) enhance the clustered structure of the data."
            },
            "venue": {
                "fragments": [],
                "text": "Fourth IEEE International Conference on Data Mining (ICDM'04)"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803434"
                        ],
                        "name": "R. Larson",
                        "slug": "R.-Larson",
                        "structuredName": {
                            "firstName": "Ray",
                            "lastName": "Larson",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Larson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 87
                            }
                        ],
                        "text": "Most search engines use VSMs to measure the similarity between a query and a document (Manning et al., 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 245,
                                "start": 225
                            }
                        ],
                        "text": "An accurate English tokenizer must know how to handle punctuation (e.g., don\u2019t, Jane\u2019s, and/or), hyphenation (e.g., state-of-the-art versus state of the art), and recognize multi-word terms (e.g., Barack Obama and ice hockey) (Manning et al., 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Manning et al. (2008) provide a good introduction to information retrieval."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 94
                            }
                        ],
                        "text": "The performance of an information retrieval system is often measured by precision and recall (Manning et al., 2008)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 18
                            }
                        ],
                        "text": "The VSM was developed for the SMART information retrieval system (Salton, 1971) by Gerard Salton and his colleagues (Salton, Wong, & Yang, 1975)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 97
                            }
                        ],
                        "text": "This phrase was taken from the Faculty Profile of George Furnas at the University of Michigan,\nhttp://www.si.umich.edu/people/faculty-detail.htm?sid=41."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 87
                            }
                        ],
                        "text": "A token is a single instance of a symbol, whereas a type is a general class of tokens (Manning et al., 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 62
                            }
                        ],
                        "text": "Other popular measures are the Dice and Jaccard coefficients (Manning et al., 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 32493971,
            "fieldsOfStudy": [
                "Computer Science",
                "Environmental Science",
                "Psychology"
            ],
            "id": "5f3b50c6c826ad105163b09d53e1eb498a4b3994",
            "isKey": true,
            "numCitedBy": 7737,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction To Information Retrieval Overdrive Digital. Introduction To Information Retrieval. Introduction To Information Retrieval Putao Ufcg. Introduction To Information Retrieval Arbeitsbereiche. Introduction To Information Retrieval. Introduction To Information Retrieval Stanford Nlp Group. Introduction To Information Retrieval Cs Ucr Edu. Introduction To Information Retrieval By Christopher D. Introduction To Information Retrieval Book. Information Retrieval The Mit Press. Introduction Information Retrieval Uvm. Information Retrieval Lmu Munich. Introduction To Information Retrieval Stanford University. Introduction To Information Retrieval. Introduction To Information Retrieval Amp Models Slideshare. Introduction To Information Retrieval Kangwon Ac Kr. Information Retrieval. Introduction To Information Retrieval Assets. Introduction To Information Retrieval. Introduction To Information Retrieval"
            },
            "slug": "Introduction-to-Information-Retrieval-Larson",
            "title": {
                "fragments": [],
                "text": "Introduction to Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This chapter discusses Information Retrieval, the science and technology behind information retrieval and retrieval, and some of the techniques used in the retrieval of information."
            },
            "venue": {
                "fragments": [],
                "text": "J. Assoc. Inf. Sci. Technol."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9419406"
                        ],
                        "name": "I. Witten",
                        "slug": "I.-Witten",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Witten",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Witten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143713826"
                        ],
                        "name": "Eibe Frank",
                        "slug": "Eibe-Frank",
                        "structuredName": {
                            "firstName": "Eibe",
                            "lastName": "Frank",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eibe Frank"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 184
                            }
                        ],
                        "text": "In machine learning, a typical problem is to learn to classify or cluster a set of items (i.e., examples, cases, individuals, entities) represented as feature vectors (Mitchell, 1997; Witten & Frank, 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 152
                            }
                        ],
                        "text": "\u2026(2003) VSM system for measuring word similarity is the British National Corpus (BNC),4 whereas the main resource used in non-VSM systems for measuring word similarity (Hirst & St-Onge, 1998; Leacock & Chodrow, 1998; Jarmasz & Szpakowicz, 2003) is a lexicon, such as WordNet5 or Roget\u2019s Thesaurus."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61973115,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "26ae952599aa9ba5815a80356024258247fc2b10",
            "isKey": false,
            "numCitedBy": 5714,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "1. What's It All About? 2. Input: Concepts, Instances, Attributes 3. Output: Knowledge Representation 4. Algorithms: The Basic Methods 5. Credibility: Evaluating What's Been Learned 6. Implementations: Real Machine Learning Schemes 7. Moving On: Engineering The Input And Output 8. Nuts And Bolts: Machine Learning Algorithms In Java 9. Looking Forward"
            },
            "slug": "Data-mining:-practical-machine-learning-tools-and-Witten-Frank",
            "title": {
                "fragments": [],
                "text": "Data mining: practical machine learning tools and techniques with Java implementations"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This presentation discusses the design and implementation of machine learning algorithms in Java, as well as some of the techniques used to develop and implement these algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "SGMD"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795338"
                        ],
                        "name": "P. Ladefoged",
                        "slug": "P.-Ladefoged",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Ladefoged",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Ladefoged"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61963270,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "d2a95d4bb26c210b6350e0b0ff56c3ab42bc9e2c",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : ;Contents: An 'unbiased' procedure for comparing degree of lateralization of dichotically presented stimuli; Lateralization and the critical period; Language processing in the brain; Language lateralization and grammars; Analysis of tongue parameters for vowels; Discussion paper on speech physiology."
            },
            "slug": "UCLA-Working-Papers-in-Phonetics,-23.-Ladefoged",
            "title": {
                "fragments": [],
                "text": "UCLA Working Papers in Phonetics, 23."
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "An 'unbiased' procedure for comparing degree of lateralization of dichotically presented stimuli and language processing in the brain is presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144549270"
                        ],
                        "name": "M. Brand",
                        "slug": "M.-Brand",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Brand",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Brand"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 52
                            }
                        ],
                        "text": "Random indexing (Sahlgren, 2005) or incremental SVD (Brand, 2006) may help to address these scaling issues."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 42
                            }
                        ],
                        "text": "However, there are efficient forms of SVD (Brand, 2006; Gorrell, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11877024,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c92642c04eda20476fb6c41f1ceb6fa84e7a35d4",
            "isKey": false,
            "numCitedBy": 453,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Fast-low-rank-modifications-of-the-thin-singular-Brand",
            "title": {
                "fragments": [],
                "text": "Fast low-rank modifications of the thin singular value decomposition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145428555"
                        ],
                        "name": "J. Gilbert",
                        "slug": "J.-Gilbert",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Gilbert",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gilbert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145263592"
                        ],
                        "name": "C. Moler",
                        "slug": "C.-Moler",
                        "structuredName": {
                            "firstName": "Cleve",
                            "lastName": "Moler",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Moler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143986781"
                        ],
                        "name": "R. Schreiber",
                        "slug": "R.-Schreiber",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schreiber",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schreiber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 119805771,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d433fe5e194a15e3facd9a90f7be4f9432946e36",
            "isKey": false,
            "numCitedBy": 595,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "The matrix computation language and environment MATLAB is extended to include sparse matrix storage and operations. The only change to the outward appearance of the MATLAB language is a pair of commands to create full or sparse matrices. Nearly all the operations of MATLAB now apply equally to full or sparse matrices, without any explicit action by the user. The sparse data structure represents a matrix in space proportional to the number of nonzero entries, and most of the operations compute sparse results in time proportional to the number of arithmetic operations on nonzeros."
            },
            "slug": "Sparse-Matrices-in-MATLAB:-Design-and-Gilbert-Moler",
            "title": {
                "fragments": [],
                "text": "Sparse Matrices in MATLAB: Design and Implementation"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "The matrix computation language and environment MATLAB is extended to include sparse matrix storage and operations, and nearly all the operations of MATLAB now apply equally to full or sparse matrices, without any explicit action by the user."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Matrix Anal. Appl."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068061920"
                        ],
                        "name": "Wm. R. Wright",
                        "slug": "Wm.-R.-Wright",
                        "structuredName": {
                            "firstName": "Wm.",
                            "lastName": "Wright",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wm. R. Wright"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 126
                            }
                        ],
                        "text": "This intimate connection between the distributional hypothesis and VSMs is a strong motivation for taking a close look at VSMs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 62
                            }
                        ],
                        "text": "Many techniques for vector analysis, such as factor analysis (Spearman, 1904), were pioneered in psychometrics."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 144456697,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "0f3829ddab6429ccd237fa73fc010cee6c03bb0e",
            "isKey": false,
            "numCitedBy": 1760,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Chap. I. Introductory. PAGE I. Signs of Weakness in Experimental Psychology 202 2. The Cause of this Weakness 203 3. The Identities of Science 204 4. Scope of the Present Experiments 205 Chap. II. Historical and Critical I. History of Previous Researches 206 2. Conclusions to be drawn from these Previous Researches 219 3. Criticism of Prevalent Working Methods 222 Chap. III. Preliminary Investigation I. Obviation of the Four Faults Quoted 225 2. Definition of the Correspondence Sought 226 3. Irrelevancies from Practice 227 (a) Pitch 228"
            },
            "slug": "General-Intelligence,-Objectively-Determined-and-Wright",
            "title": {
                "fragments": [],
                "text": "General Intelligence, Objectively Determined and Measured."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1905
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144977509"
                        ],
                        "name": "Jin Shin",
                        "slug": "Jin-Shin",
                        "structuredName": {
                            "firstName": "Jin",
                            "lastName": "Shin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jin Shin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109828882"
                        ],
                        "name": "Sang Joon Kim",
                        "slug": "Sang-Joon-Kim",
                        "structuredName": {
                            "firstName": "Sang",
                            "lastName": "Kim",
                            "middleNames": [
                                "Joon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sang Joon Kim"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 97
                            }
                        ],
                        "text": "In information theory, a surprising event has higher information content than an expected event (Shannon, 1948)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5747983,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d12a1d23b21a9b170118a56386552bc5d4727de",
            "isKey": false,
            "numCitedBy": 47459,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper opened the new area the information theory. Before this paper, most people believed that the only way to make the error probability of transmission as small as desired is to reduce the data rate (such as a long repetition scheme). However, surprisingly this paper revealed that it does not need to reduce the data rate for achieving that much of small errors. It proved that we can get some positive data rate that has the same small error probability and also there is an upper bound of the data rate, which means we cannot achieve the data rate with any encoding scheme that has small enough error probability over the upper bound."
            },
            "slug": "A-Mathematical-Theory-of-Communication-Shin-Kim",
            "title": {
                "fragments": [],
                "text": "A Mathematical Theory of Communication"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "It is proved that the authors can get some positive data rate that has the same small error probability and also there is an upper bound of the data rate, which means they cannot achieve the data rates with any encoding scheme that has small enough error probability over the upper bound."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2031081"
                        ],
                        "name": "E. Vozalis",
                        "slug": "E.-Vozalis",
                        "structuredName": {
                            "firstName": "Emmanouil",
                            "lastName": "Vozalis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Vozalis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722096"
                        ],
                        "name": "K. Margaritis",
                        "slug": "K.-Margaritis",
                        "structuredName": {
                            "firstName": "Konstantinos",
                            "lastName": "Margaritis",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Margaritis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 113
                            }
                        ],
                        "text": "From this perspective, truncated SVD is a way of simulating the missing text, compensating for the lack of data (Vozalis & Margaritis, 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15606571,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "65431c3a27f01e70bb5e4b1e68fc6a91249f6114",
            "isKey": false,
            "numCitedBy": 123,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work, we will provide a brief review of different recommender systems\u2019 algorithms, which have been proposed in the recent literature. First, we will present the basic recommender systems\u2019 challenges and problems. Then, we will give an overview of association rules, memorybased, model-based and hybrid recommendation algorithms. Finally, evaluation metrics to measure the performance of those systems will be discussed. Keywords\u2014 Collaborative Filtering, Recommender Systems, Machine Learning"
            },
            "slug": "Analysis-of-Recommender-Systems\u2019-Algorithms-Vozalis-Margaritis",
            "title": {
                "fragments": [],
                "text": "Analysis of Recommender Systems\u2019 Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This work will provide a brief review of different recommender systems\u2019 algorithms, which have been proposed in the recent literature, and an overview of association rules, memorybased, model-based and hybrid recommendation algorithms."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116169554"
                        ],
                        "name": "William Chang",
                        "slug": "William-Chang",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1990190"
                        ],
                        "name": "P. Pantel",
                        "slug": "P.-Pantel",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Pantel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pantel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36445704"
                        ],
                        "name": "Ana-Maria Popescu",
                        "slug": "Ana-Maria-Popescu",
                        "structuredName": {
                            "firstName": "Ana-Maria",
                            "lastName": "Popescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ana-Maria Popescu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718798"
                        ],
                        "name": "E. Gabrilovich",
                        "slug": "E.-Gabrilovich",
                        "structuredName": {
                            "firstName": "Evgeniy",
                            "lastName": "Gabrilovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Gabrilovich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17494612,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "55ec153fffa6abb05133bbb3251f1555bef4e787",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "In online advertising, pervasive in commercial search engines, advertisers typically bid on few terms, and the scarcity of data makes ad matching difficult. Suggesting additional bidterms can significantly improve ad clickability and conversion rates. In this paper, we present a large-scale bidterm suggestion system that models an advertiser's intent and finds new bidterms consistent with that intent. Preliminary experiments show that our system significantly increases the coverage of a state of the art production system used at Yahoo while maintaining comparable precision."
            },
            "slug": "Towards-intent-driven-bidterm-suggestion-Chang-Pantel",
            "title": {
                "fragments": [],
                "text": "Towards intent-driven bidterm suggestion"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A large-scale bidterm suggestion system that models an advertiser's intent and finds new bidterms consistent with that intent, and preliminary experiments show that the system significantly increases the coverage of a state of the art production system used at Yahoo while maintaining comparable precision."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7831590,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1ad15c08556c8f8e3739703857ea01077ce738c5",
            "isKey": false,
            "numCitedBy": 2055,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method for performing a nonlinear form of Principal Component Analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in highdimensional feature spaces, related to input space by some nonlinear map; for instance the space of all possible d-pixel products in images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition."
            },
            "slug": "Kernel-Principal-Component-Analysis-Sch\u00f6lkopf-Smola",
            "title": {
                "fragments": [],
                "text": "Kernel Principal Component Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A new method for performing a nonlinear form of Principal Component Analysis by the use of integral operator kernel functions is proposed and experimental results on polynomial feature extraction for pattern recognition are presented."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143716922"
                        ],
                        "name": "G. Lakoff",
                        "slug": "G.-Lakoff",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Lakoff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lakoff"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 121
                            }
                        ],
                        "text": "The basic idea of prototype theory is that some members of a category are more central than others (Rosch & Lloyd, 1978; Lakoff, 1987)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 2
                            }
                        ],
                        "text": "The distributional hypothesis is that\n1."
                    },
                    "intents": []
                }
            ],
            "corpusId": 142299281,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "d03e21ac514278f6860908dacd133a2485c13cbc",
            "isKey": false,
            "numCitedBy": 5038,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Women,-Fire,-and-Dangerous-Things-Lakoff",
            "title": {
                "fragments": [],
                "text": "Women, Fire, and Dangerous Things"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "65739159"
                        ],
                        "name": "A. Chrz\u0229szczyk",
                        "slug": "A.-Chrz\u0229szczyk",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Chrz\u0229szczyk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Chrz\u0229szczyk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1920259"
                        ],
                        "name": "J. Kochanowski",
                        "slug": "J.-Kochanowski",
                        "structuredName": {
                            "firstName": "Janusz",
                            "lastName": "Kochanowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kochanowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 19
                            }
                        ],
                        "text": "Minimizing the the Frobenius norm \u2016X\u0302\u2212X\u2016F will minimize the noise, if the noise has a Gaussian distribution."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 197
                            }
                        ],
                        "text": "We assume our reader has a basic understanding of vectors, matrices, and linear algebra, such as one might acquire from an introductory undergraduate course in linear algebra, or from a text book (Golub & Van Loan, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 218
                            }
                        ],
                        "text": "SVD decomposes X into the product of three matrices U\u03a3VT, where U and V are in column orthonormal form (i.e., the columns are orthogonal and have unit length, UTU = VTV = I) and \u03a3 is a diagonal matrix of singular values (Golub & Van Loan, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 109
                            }
                        ],
                        "text": "That is, X\u0302 = Uk\u03a3kV T\nk minimizes \u2016X\u0302\u2212X\u2016F over all matrices X\u0302 of rank k, where \u2016 . . . \u2016F denotes the Frobenius norm (Golub & Van Loan, 1996)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 291377,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "444d70e3331b5083b40ef32e49390ef683a65e67",
            "isKey": true,
            "numCitedBy": 8368,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Matrix-Computations-Chrz\u0229szczyk-Kochanowski",
            "title": {
                "fragments": [],
                "text": "Matrix Computations"
            },
            "venue": {
                "fragments": [],
                "text": "Encyclopedia of Parallel Computing"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69351840"
                        ],
                        "name": "C. Ogden",
                        "slug": "C.-Ogden",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Ogden",
                            "middleNames": [
                                "Kay"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Ogden"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 162
                            }
                        ],
                        "text": "In Turney\u2019s (2007) tensor, for example, rows correspond to words from the TOEFL multiple-choice synonym questions, columns correspond to words from Basic English (Ogden, 1930),12 and tubes correspond to patterns that join rows and columns (hence we have a word\u2013word\u2013pattern third-order tensor)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 13
                            }
                        ],
                        "text": "The words of Basic English are listed at http://ogden.basic-english.org/."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 106
                            }
                        ],
                        "text": "The elements in this slice correspond to all the patterns that relate the given TOEFL word to any word in Basic English."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Basic English is a highly reduced subset of English, designed to be easy for people to learn."
                    },
                    "intents": []
                }
            ],
            "corpusId": 60519611,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "3971b3dda9e14ce8313f23659ff8170d7102ced8",
            "isKey": true,
            "numCitedBy": 228,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Basic-English-:-a-general-introduction-with-rules-Ogden",
            "title": {
                "fragments": [],
                "text": "Basic English : a general introduction with rules and grammar"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1930
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 121
                            }
                        ],
                        "text": "The distributional hypothesis in linguistics is that words that occur in similar contexts tend to have similar meanings (Harris, 1954)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 75
                            }
                        ],
                        "text": "Deerwester et al. (1990) showed how the intuitions of Wittgenstein (1953), Harris (1954), Weaver, and Firth could be used in a practical algorithm."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 123
                            }
                        ],
                        "text": "See http://wordnet.princeton.edu/.\nwords that occur in similar contexts tend to have similar meanings (Wittgenstein, 1953; Harris, 1954; Weaver, 1955; Firth, 1957; Deerwester, Dumais, Landauer, Furnas, & Harshman, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 112
                            }
                        ],
                        "text": "Reprinted with permission.\nquery is represented as a point in the same space as the documents (the query is a pseudodocument)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 95
                            }
                        ],
                        "text": "Distributional hypothesis: Words that occur in similar contexts tend to have similar meanings (Harris, 1954; Firth, 1957; Deerwester et al., 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Distributional structure. Word"
            },
            "venue": {
                "fragments": [],
                "text": "Distributional structure. Word"
            },
            "year": 1954
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2469966"
                        ],
                        "name": "R. Girju",
                        "slug": "R.-Girju",
                        "structuredName": {
                            "firstName": "Roxana",
                            "lastName": "Girju",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Girju"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683562"
                        ],
                        "name": "Preslav Nakov",
                        "slug": "Preslav-Nakov",
                        "structuredName": {
                            "firstName": "Preslav",
                            "lastName": "Nakov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Preslav Nakov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2256003"
                        ],
                        "name": "Vivi Nastase",
                        "slug": "Vivi-Nastase",
                        "structuredName": {
                            "firstName": "Vivi",
                            "lastName": "Nastase",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vivi Nastase"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795595"
                        ],
                        "name": "S. Szpakowicz",
                        "slug": "S.-Szpakowicz",
                        "structuredName": {
                            "firstName": "Stan",
                            "lastName": "Szpakowicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Szpakowicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689647"
                        ],
                        "name": "Peter D. Turney",
                        "slug": "Peter-D.-Turney",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Turney",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter D. Turney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2808366"
                        ],
                        "name": "Deniz Yuret",
                        "slug": "Deniz-Yuret",
                        "structuredName": {
                            "firstName": "Deniz",
                            "lastName": "Yuret",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Deniz Yuret"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 219306279,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "3fe924b38355b16063bd910b2dead6c797586d56",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "SemEval-2007-Task-04:-Classification-of-Semantic-Girju-Nakov",
            "title": {
                "fragments": [],
                "text": "SemEval-2007 Task 04: Classification of Semantic Relations between Nominals"
            },
            "venue": {
                "fragments": [],
                "text": "*SEMEVAL"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2808366"
                        ],
                        "name": "Deniz Yuret",
                        "slug": "Deniz-Yuret",
                        "structuredName": {
                            "firstName": "Deniz",
                            "lastName": "Yuret",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Deniz Yuret"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 215881965,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1bb1488931004a5e3cb170323f630ff994a38772",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Clustering-Word-Pairs-to-Answer-Analogy-Questions-Yuret",
            "title": {
                "fragments": [],
                "text": "Clustering Word Pairs to Answer Analogy Questions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144191879"
                        ],
                        "name": "J. Firth",
                        "slug": "J.-Firth",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Firth",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Firth"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 13
                            }
                        ],
                        "text": "The documents are sorted in order of increasing distance (decreasing semantic similarity) from the query and then presented to the user."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 151
                            }
                        ],
                        "text": "See http://wordnet.princeton.edu/.\nwords that occur in similar contexts tend to have similar meanings (Wittgenstein, 1953; Harris, 1954; Weaver, 1955; Firth, 1957; Deerwester, Dumais, Landauer, Furnas, & Harshman, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 109
                            }
                        ],
                        "text": "Distributional hypothesis: Words that occur in similar contexts tend to have similar meanings (Harris, 1954; Firth, 1957; Deerwester et al., 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 208093066,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "88b3959b6f5333e5358eac43970a5fa29b54642c",
            "isKey": false,
            "numCitedBy": 1923,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Synopsis-of-Linguistic-Theory,-1930-1955-Firth",
            "title": {
                "fragments": [],
                "text": "A Synopsis of Linguistic Theory, 1930-1955"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1957
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14664289"
                        ],
                        "name": "P. Stone",
                        "slug": "P.-Stone",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Stone",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Stone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39396941"
                        ],
                        "name": "D. Dunphy",
                        "slug": "D.-Dunphy",
                        "structuredName": {
                            "firstName": "Dexter",
                            "lastName": "Dunphy",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Dunphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49826480"
                        ],
                        "name": "Marshall S. Smith",
                        "slug": "Marshall-S.-Smith",
                        "structuredName": {
                            "firstName": "Marshall",
                            "lastName": "Smith",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marshall S. Smith"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60936250,
            "fieldsOfStudy": [
                "Sociology"
            ],
            "id": "c13d3506f716fb9fb0c417b5132144db42f80dd4",
            "isKey": false,
            "numCitedBy": 1759,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-general-inquirer:-A-computer-approach-to-Stone-Dunphy",
            "title": {
                "fragments": [],
                "text": "The general inquirer: A computer approach to content analysis."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742359"
                        ],
                        "name": "Jussi Karlgren",
                        "slug": "Jussi-Karlgren",
                        "structuredName": {
                            "firstName": "Jussi",
                            "lastName": "Karlgren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jussi Karlgren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689109"
                        ],
                        "name": "Magnus Sahlgren",
                        "slug": "Magnus-Sahlgren",
                        "structuredName": {
                            "firstName": "Magnus",
                            "lastName": "Sahlgren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Magnus Sahlgren"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122118215,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "b1649d21217a439d885e929e08d4ec38d83d33a1",
            "isKey": false,
            "numCitedBy": 165,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "From-Words-to-Understanding-Karlgren-Sahlgren",
            "title": {
                "fragments": [],
                "text": "From Words to Understanding"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "19132136"
                        ],
                        "name": "W. N. Locke",
                        "slug": "W.-N.-Locke",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Locke",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. N. Locke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40231439"
                        ],
                        "name": "A. Booth",
                        "slug": "A.-Booth",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Booth",
                            "middleNames": [
                                "Donald"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Booth"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62740389,
            "fieldsOfStudy": [
                "Psychology",
                "Computer Science"
            ],
            "id": "7b6021277d673b3af54e28e4e87dc144da403ffe",
            "isKey": false,
            "numCitedBy": 120,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Machine-Translation-of-Languages:-Fourteen-Essays-Locke-Booth",
            "title": {
                "fragments": [],
                "text": "Machine Translation of Languages: Fourteen Essays"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1955
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14664289"
                        ],
                        "name": "P. Stone",
                        "slug": "P.-Stone",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Stone",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Stone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48267824"
                        ],
                        "name": "D. Ogilvie",
                        "slug": "D.-Ogilvie",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Ogilvie",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ogilvie"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 147212147,
            "fieldsOfStudy": [
                "Sociology",
                "Computer Science"
            ],
            "id": "92cee8c0e159561aa1b3ad3f55c10bd57e5d5853",
            "isKey": false,
            "numCitedBy": 998,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Extracting-Information.-(Book-Reviews:-The-General-Stone-Ogilvie",
            "title": {
                "fragments": [],
                "text": "Extracting Information. (Book Reviews: The General Inquirer. A Computer Approach to Content Analysis)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2240660"
                        ],
                        "name": "B. Dasarathy",
                        "slug": "B.-Dasarathy",
                        "structuredName": {
                            "firstName": "Belur",
                            "lastName": "Dasarathy",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Dasarathy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 90
                            }
                        ],
                        "text": "For classification, a nearest-neighbour algorithm can use cosine as a measure of nearness (Dasarathy, 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60461418,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b1d3ec2e6fe49aaf8dc068b8a812e9ef3f163fa",
            "isKey": false,
            "numCitedBy": 1939,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Nearest-neighbor-(NN)-norms:-NN-pattern-techniques-Dasarathy",
            "title": {
                "fragments": [],
                "text": "Nearest neighbor (NN) norms: NN pattern classification techniques"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2737579"
                        ],
                        "name": "G. Furnas",
                        "slug": "G.-Furnas",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Furnas",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Furnas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836606"
                        ],
                        "name": "T. Landauer",
                        "slug": "T.-Landauer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Landauer",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Landauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40994979"
                        ],
                        "name": "L. M. G\u00f3mez",
                        "slug": "L.-M.-G\u00f3mez",
                        "structuredName": {
                            "firstName": "Luc\u00eda",
                            "lastName": "G\u00f3mez",
                            "middleNames": [
                                "Mart\u00edn"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. M. G\u00f3mez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728602"
                        ],
                        "name": "S. Dumais",
                        "slug": "S.-Dumais",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Dumais",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dumais"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 133
                            }
                        ],
                        "text": "Statistical semantics hypothesis: Statistical patterns of human word usage can be used to figure out what people mean (Weaver, 1955; Furnas et al., 1983)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60454845,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "76c523a0b590d8d183f1e94dd75bec06f2366cb2",
            "isKey": false,
            "numCitedBy": 169,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Statistical-semantics:-analysis-of-the-potential-of-Furnas-Landauer",
            "title": {
                "fragments": [],
                "text": "Statistical semantics: analysis of the potential performance of keyword information systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "C\nL ]\n4 M"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 110
                            }
                        ],
                        "text": "A popular list of stop words is the set of 571 common words included in the source code for the SMART system (Salton, 1971).14\nIn some languages (e.g., Chinese), words are not separated by spaces."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 66
                            }
                        ],
                        "text": "The VSM was developed for the SMART information retrieval system (Salton, 1971) by Gerard Salton and his colleagues (Salton, Wong, & Yang, 1975)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61113802,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "939d409a615d4b88c0a84e1f9b99ed67a9053208",
            "isKey": false,
            "numCitedBy": 3147,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-SMART-Retrieval-System\u2014Experiments-in-Automatic-Salton",
            "title": {
                "fragments": [],
                "text": "The SMART Retrieval System\u2014Experiments in Automatic Document Processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2679783"
                        ],
                        "name": "M. Hassoun",
                        "slug": "M.-Hassoun",
                        "structuredName": {
                            "firstName": "Mohamad",
                            "lastName": "Hassoun",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hassoun"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59869535,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "904cd811a1f2ffc57d1c88beb153bb47fc406733",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Associative-neural-memories-Hassoun",
            "title": {
                "fragments": [],
                "text": "Associative neural memories"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733049"
                        ],
                        "name": "Eneko Agirre",
                        "slug": "Eneko-Agirre",
                        "structuredName": {
                            "firstName": "Eneko",
                            "lastName": "Agirre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eneko Agirre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145660941"
                        ],
                        "name": "P. Edmonds",
                        "slug": "P.-Edmonds",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Edmonds",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Edmonds"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 108
                            }
                        ],
                        "text": "In applications dealing with polysemy, one approach uses vectors that represent word tokens (Schu\u0308tze, 1998; Agirre & Edmonds, 2006) and another uses vectors that represent word types (Pantel & Lin, 2002a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59924769,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6694f7e7815dddfb397dfa59a9aca99a92e422c5",
            "isKey": false,
            "numCitedBy": 162,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Word-Sense-Disambiguation:-Algorithms-and-(Text,-Agirre-Edmonds",
            "title": {
                "fragments": [],
                "text": "Word Sense Disambiguation: Algorithms and Applications (Text, Speech and Language Technology)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2036336"
                        ],
                        "name": "P. Foltz",
                        "slug": "P.-Foltz",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Foltz",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Foltz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2444937"
                        ],
                        "name": "Darrell Laham",
                        "slug": "Darrell-Laham",
                        "structuredName": {
                            "firstName": "Darrell",
                            "lastName": "Laham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Darrell Laham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836606"
                        ],
                        "name": "T. Landauer",
                        "slug": "T.-Landauer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Landauer",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Landauer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59672746,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "295a48ed1ca5eb9b8fa2befcdb7fb7c3e9cbac53",
            "isKey": false,
            "numCitedBy": 254,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-intelligent-essay-assessor:-Applications-to-Foltz-Laham",
            "title": {
                "fragments": [],
                "text": "The intelligent essay assessor: Applications to educational technology"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3226331"
                        ],
                        "name": "C. Leacock",
                        "slug": "C.-Leacock",
                        "structuredName": {
                            "firstName": "Claudia",
                            "lastName": "Leacock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Leacock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736799"
                        ],
                        "name": "M. Chodorow",
                        "slug": "M.-Chodorow",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Chodorow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Chodorow"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 191
                            }
                        ],
                        "text": "\u2026(2003) VSM system for measuring word similarity is the British National Corpus (BNC),4 whereas the main resource used in non-VSM systems for measuring word similarity (Hirst & St-Onge, 1998; Leacock & Chodrow, 1998; Jarmasz & Szpakowicz, 2003) is a lexicon, such as WordNet5 or Roget\u2019s Thesaurus."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59721988,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54cd5ea7f987a23d663605640113859207490400",
            "isKey": false,
            "numCitedBy": 1053,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Combining-local-context-and-wordnet-similarity-for-Leacock-Chodorow",
            "title": {
                "fragments": [],
                "text": "Combining local context and wordnet similarity for word sense identification"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712211"
                        ],
                        "name": "G. Golub",
                        "slug": "G.-Golub",
                        "structuredName": {
                            "firstName": "Gene",
                            "lastName": "Golub",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Golub"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2077001"
                        ],
                        "name": "C. Loan",
                        "slug": "C.-Loan",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Loan",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Loan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 218523127,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "64b3435826a94ddd269b330e6254579f3244f214",
            "isKey": false,
            "numCitedBy": 574,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Matrix-Computations,-Third-Edition-Golub-Loan",
            "title": {
                "fragments": [],
                "text": "Matrix Computations, Third Edition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36568337"
                        ],
                        "name": "B. Ross",
                        "slug": "B.-Ross",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Ross",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ross"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17140139,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "3c94b8e34dec3df82bc64f27d36211ffbabb119b",
            "isKey": false,
            "numCitedBy": 1730,
            "numCiting": 272,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Psychology-of-Learning-and-Motivation:-Advances-Ross",
            "title": {
                "fragments": [],
                "text": "The Psychology of Learning and Motivation: Advances in Research and Theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144741427"
                        ],
                        "name": "C. Pollard",
                        "slug": "C.-Pollard",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Pollard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Pollard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Widdows (2004) gives a gentle introduction to vectors from the perspective of semantics."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 34
                            }
                        ],
                        "text": "See http://code.google.com/p/semanticvectors/."
                    },
                    "intents": []
                }
            ],
            "corpusId": 18314894,
            "fieldsOfStudy": [],
            "id": "6c974a7d091913b486f51d983ead7023dd5e8de2",
            "isKey": false,
            "numCitedBy": 2281,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Center-for-the-Study-of-Language-and-Information-Pollard",
            "title": {
                "fragments": [],
                "text": "Center for the Study of Language and Information"
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fingerprinting by random polynomials. Tech. rep., Center for research in Computing technology"
            },
            "venue": {
                "fragments": [],
                "text": "Fingerprinting by random polynomials. Tech. rep., Center for research in Computing technology"
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semantic and associative priming in highdimensional semantic space"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 17th Annual Conference of the Cognitive Science Society"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Discrete component analysis. In Subspace, Latent Structure and Feature Selection: Statistical and Optimization Perspectives Workshop at SLSFS"
            },
            "venue": {
                "fragments": [],
                "text": "Discrete component analysis. In Subspace, Latent Structure and Feature Selection: Statistical and Optimization Perspectives Workshop at SLSFS"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Spamcop: A spam classification and organization program"
            },
            "venue": {
                "fragments": [],
                "text": "Learning for Text Categorization: Papers from the AAAI 1998 Workshop"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 128
                            }
                        ],
                        "text": "For higher-order tensors, there are operations that are analogous to truncated SVD, such as parallel factor analysis (PARAFAC) (Harshman, 1970), canonical decomposition (CANDECOMP) (Carroll & Chang, 1970) (equivalent to PARAFAC but discovered independently), and Tucker decomposition (Tucker, 1966)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Foundations of the parafac procedure: Models and conditions for an \" explanatory \" multi-modal factor analysis. UCLA Working Papers in Phonetics"
            },
            "venue": {
                "fragments": [],
                "text": "Foundations of the parafac procedure: Models and conditions for an \" explanatory \" multi-modal factor analysis. UCLA Working Papers in Phonetics"
            },
            "year": 1970
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Exploring noun-modifier semantic relations"
            },
            "venue": {
                "fragments": [],
                "text": "Fifth International Workshop on Computational Semantics (IWCS-5)"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Evaluating wordnet-based measures of semantic distance"
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A vector is a first-order tensor and a matrix is a second-order tensor"
            },
            "venue": {
                "fragments": [],
                "text": "A vector is a first-order tensor and a matrix is a second-order tensor"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 136
                            }
                        ],
                        "text": "It is commonly said in IR that, properly normalized, the difference in retrieval performance using different measures is insignificant (van Rijsbergen, 1979)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information Retrieval. Butterworths"
            },
            "venue": {
                "fragments": [],
                "text": "Information Retrieval. Butterworths"
            },
            "year": 1979
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Relational web search. Tech. rep., University of Washington, Department of Computer Science and Engineering"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report 2006-04-02"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Document length normalization. Information Processing and Management"
            },
            "venue": {
                "fragments": [],
                "text": "Document length normalization. Information Processing and Management"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 140
                            }
                        ],
                        "text": "It is commonly said in IR that, properly normalized, the difference in retrieval performance using different measures is insignificant (van Rijsbergen, 1979)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information Retrieval. Butterworths"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Modern Information Retrieval"
            },
            "venue": {
                "fragments": [],
                "text": "Modern Information Retrieval"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fingerprinting by random polynomials"
            },
            "venue": {
                "fragments": [],
                "text": "Tech. rep., Center for research in Computing technology, Harvard University. Technical Report TR-15-81."
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fully automatic cross-language document retrieval using latent semantic indexing"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Sixth Annual Conference of the UW Centre for the New Oxford English Dictionary and Text Research"
            },
            "year": 1990
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 82,
            "methodology": 58
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 208,
        "totalPages": 21
    },
    "page_url": "https://www.semanticscholar.org/paper/From-Frequency-to-Meaning:-Vector-Space-Models-of-Turney-Pantel/3a0e788268fafb23ab20da0e98bb578b06830f7d?sort=total-citations"
}