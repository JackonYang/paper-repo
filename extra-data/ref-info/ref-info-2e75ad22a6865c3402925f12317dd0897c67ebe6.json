{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685978"
                        ],
                        "name": "B. Taskar",
                        "slug": "B.-Taskar",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Taskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Taskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730156"
                        ],
                        "name": "Carlos Guestrin",
                        "slug": "Carlos-Guestrin",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Guestrin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlos Guestrin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 118
                            }
                        ],
                        "text": "Finally, there has been an independent formulation of this solution, known as Max Margin Markov networks (M3 nets) by Taskar et al. (2004), in which the above formulation is simplified, not needing exponentially many Lagrange multipliers to solve it."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 34
                            }
                        ],
                        "text": "This data set was preprocessed by Taskar et al. (2004) to test the M3 network."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 172
                            }
                        ],
                        "text": "We have discussed CRF (Lafferty et al. (2001)), B-CRF (Qi et al. (2005)), K-CRF (Altun et al. (2004a,b); Lafferty et al. (2004)), HM-SVM (Altun et al. (2003)) and M3 nets (Taskar et al. (2004))."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 264
                            }
                        ],
                        "text": "We have presented a compact notation that can be used to represent Conditional Random Fields Lafferty et al. (2001), Bayesian CRFs Qi et al. (2005), Kernel CRFs Altun et al. (2004a,b); Lafferty et al. (2004) and Maximum Margin Markov networks Altun et al. (2003); Taskar et al. (2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 193
                            }
                        ],
                        "text": "There has been several extensions to CRFs using kernels (Altun et al. (2004a,b); Lafferty et al. (2004)), Bayesian learning (Qi et al. (2005)) and maximum margin solution (Altun et al. (2003); Taskar et al. (2004))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 277,
                                "start": 257
                            }
                        ],
                        "text": "\u2026\u2211 y \u03b1n,y ( T\u2211 t=1 w>t ft(xn,ynt)\u2212 T\u2211 t=1 w>t ft(xn,yt)\n\u2212 L\u2211\nl=1\n[1\u2212 \u03b4(ynl, yl)] + \u03ben )\n(1.13)\nin which we have substituted o(xn,y) by its definition in (1.8) and we have defined the margin per node to be the Hamming distance between the true and compared labels as proposed by Taskar et al. (2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 3
                            }
                        ],
                        "text": "In Taskar et al. (2004) the authors reported an error rate of around 13% in the letter recognition task, for the same partition of the data used in this experiment."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 122
                            }
                        ],
                        "text": "8) and we have defined the margin per node to be the Hamming distance between the true and compared labels as proposed by Taskar et al. (2004). Now for each training sample and each configuration in every clique, we define: \u03b2 n,yt = \u2211 y\u223cyt \u03b1n,y \u2200n, \u2200t, \u2200yt (1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 107
                            }
                        ],
                        "text": "We have first solved this experiment using 1 group for training and the other 9 for validation, as done in Taskar et al. (2004), and we have repeated it over the 10 different sets."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 201720,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c450531e1121cfb657be5195e310217a4675397",
            "isKey": true,
            "numCitedBy": 1477,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "In typical classification tasks, we seek a function which assigns a label to a single object. Kernel-based approaches, such as support vector machines (SVMs), which maximize the margin of confidence of the classifier, are the method of choice for many such tasks. Their popularity stems both from the ability to use high-dimensional feature spaces, and from their strong theoretical guarantees. However, many real-world tasks involve sequential, spatial, or structured data, where multiple labels must be assigned. Existing kernel-based methods ignore structure in the problem, assigning labels independently to each object, losing much useful information. Conversely, probabilistic graphical models, such as Markov networks, can represent correlations between labels, by exploiting problem structure, but cannot handle high-dimensional feature spaces, and lack strong theoretical generalization guarantees. In this paper, we present a new framework that combines the advantages of both approaches: Maximum margin Markov (M3) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data. We present an efficient algorithm for learning M3 networks based on a compact quadratic program formulation. We provide a new theoretical bound for generalization in structured domains. Experiments on the task of handwritten character recognition and collective hypertext classification demonstrate very significant gains over previous approaches."
            },
            "slug": "Max-Margin-Markov-Networks-Taskar-Guestrin",
            "title": {
                "fragments": [],
                "text": "Max-Margin Markov Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Maximum margin Markov (M3) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data, and a new theoretical bound for generalization in structured domains is provided."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783941"
                        ],
                        "name": "Y. Altun",
                        "slug": "Y.-Altun",
                        "structuredName": {
                            "firstName": "Yasemin",
                            "lastName": "Altun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Altun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 99
                            }
                        ],
                        "text": "This approach to solve CRFs using kernels was independently proposed by Lafferty et al. (2004) and Altun et al. (2004a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 24
                            }
                        ],
                        "text": "This is the proposal in Altun et al. (2004b) using the Laplace approximation over the posterior of the weights, p(w|Y,X)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8403013,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a2182e5a37f5fc04ce23bd2f4d6b5070382c8c5e",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Many real-world classification tasks involve the prediction of multiple, inter-dependent class labels. A prototypical case of this sort deals with prediction of a sequence of labels for a sequence of observations. Such problems arise naturally in the context of annotating and segmenting observation sequences. This paper generalizes Gaussian Process classification to predict multiple labels by taking dependencies between neighboring labels into account. Our approach is motivated by the desire to retain rigorous probabilistic semantics, while overcoming limitations of parametric methods like Conditional Random Fields, which exhibit conceptual and computational difficulties in high-dimensional input spaces. Experiments on named entity recognition and pitch accent prediction tasks demonstrate the competitiveness of our approach."
            },
            "slug": "Gaussian-process-classification-for-segmenting-and-Altun-Hofmann",
            "title": {
                "fragments": [],
                "text": "Gaussian process classification for segmenting and annotating sequences"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Gaussian Process classification is generalized to predict multiple labels by taking dependencies between neighboring labels into account, motivated by the desire to retain rigorous probabilistic semantics, while overcoming limitations of parametric methods like Conditional Random Fields."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1831395"
                        ],
                        "name": "H. Wallach",
                        "slug": "H.-Wallach",
                        "structuredName": {
                            "firstName": "Hanna",
                            "lastName": "Wallach",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Wallach"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Advances in Neural Information Processing Systems 4: Proceedings of the 1991 Conference, John E. Moody, Stephen J. Hanson, and Richard P. Lippmann, eds., 1992."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 72
                            }
                        ],
                        "text": "This functional is convex and can be solved using different techniques (Wallach (2002))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16992489,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cb0690094be9d21334745f917b0adc4d87e0e898",
            "isKey": false,
            "numCitedBy": 182,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "This thesis explores a number of parameter estimation techniques for conditional random fields, a recently introduced [31] probabilistic model for labelling and segmenting sequential data. Theoretical and practical disadvantages of the training techniques reported in current literature on CRFs are discussed. We hypothesise that general numerical optimisation techniques result in improved performance over iterative scaling algorithms for training CRFs. Experiments run on a a subset of a well-known text chunking data set [28] confirm that this is indeed the case. This is a highly promising result, indicating that such parameter estimation techniques make CRFs a practical and efficient choice for labelling sequential data, as well as a theoretically sound and principled probabilistic framework."
            },
            "slug": "Efficient-Training-of-Conditional-Random-Fields-Wallach",
            "title": {
                "fragments": [],
                "text": "Efficient Training of Conditional Random Fields"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This thesis explores a number of parameter estimation techniques for conditional random fields, a recently introduced probabilistic model for labelling and segmenting sequential data, and hypothesises that general numerical optimisation techniques result in improved performance over iterative scaling algorithms for training CRFs."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783941"
                        ],
                        "name": "Y. Altun",
                        "slug": "Y.-Altun",
                        "structuredName": {
                            "firstName": "Yasemin",
                            "lastName": "Altun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Altun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765700"
                        ],
                        "name": "Ioannis Tsochantaridis",
                        "slug": "Ioannis-Tsochantaridis",
                        "structuredName": {
                            "firstName": "Ioannis",
                            "lastName": "Tsochantaridis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ioannis Tsochantaridis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9699301,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5fe5ed2a3b50becdbbcd17e7733653d5ef6ac398",
            "isKey": false,
            "numCitedBy": 556,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a novel discriminative learning technique for label sequences based on a combination of the two most successful learning algorithms, Support Vector Machines and Hidden Markov Models which we call Hidden Markov Support Vector Machine. The proposed architecture handles dependencies between neighboring labels using Viterbi decoding. In contrast to standard HMM training, the learning procedure is discriminative and is based on a maximum/soft margin criterion. Compared to previous methods like Conditional Random Fields, Maximum Entropy Markov Models and label sequence boosting, HM-SVMs have a number of advantages. Most notably, it is possible to learn non-linear discriminant functions via kernel functions. At the same time, HM-SVMs share the key advantages with other discriminative methods, in particular the capability to deal with overlapping features. We report experimental evaluations on two tasks, named entity recognition and part-of-speech tagging, that demonstrate the competitiveness of the proposed approach."
            },
            "slug": "Hidden-Markov-Support-Vector-Machines-Altun-Tsochantaridis",
            "title": {
                "fragments": [],
                "text": "Hidden Markov Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "This paper presents a novel discriminative learning technique for label sequences based on a combination of the two most successful learning algorithms, Support Vector Machines and Hidden Markov Models which it is called HM-SVMs and handles dependencies between neighboring labels using Viterbi decoding."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693407"
                        ],
                        "name": "K. Crammer",
                        "slug": "K.-Crammer",
                        "structuredName": {
                            "firstName": "Koby",
                            "lastName": "Crammer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Crammer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10151608,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "cfc6d0c8260594ebc5dd20ee558d29b1014ed41a",
            "isKey": false,
            "numCitedBy": 2191,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we describe the algorithmic implementation of multiclass kernel-based vector machines. Our starting point is a generalized notion of the margin to multiclass problems. Using this notion we cast multiclass categorization problems as a constrained optimization problem with a quadratic objective function. Unlike most of previous approaches which typically decompose a multiclass problem into multiple independent binary classification tasks, our notion of margin yields a direct method for training multiclass predictors. By using the dual of the optimization problem we are able to incorporate kernels with a compact set of constraints and decompose the dual problem into multiple optimization problems of reduced size. We describe an efficient fixed-point algorithm for solving the reduced optimization problems and prove its convergence. We then discuss technical details that yield significant running time improvements for large datasets. Finally, we describe various experiments with our approach comparing it to previously studied kernel-based methods. Our experiments indicate that for multiclass problems we attain state-of-the-art accuracy."
            },
            "slug": "On-the-Algorithmic-Implementation-of-Multiclass-Crammer-Singer",
            "title": {
                "fragments": [],
                "text": "On the Algorithmic Implementation of Multiclass Kernel-based Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This paper describes the algorithmic implementation of multiclass kernel-based vector machines using a generalized notion of the margin to multiclass problems, and describes an efficient fixed-point algorithm for solving the reduced optimization problems and proves its convergence."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 103
                            }
                        ],
                        "text": "In particular, in the pattern recognition field, the appearance of the Support Vector Machines (SVMs) (Boser et al. (1992)) blossomed the research into new machine learning algorithms and they bought the kernel concept into the machine learning community (Scho\u0308lkopf and Smola (2001))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Published by Morgan-Kaufmann"
                    },
                    "intents": []
                }
            ],
            "corpusId": 207165665,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2599131a4bc2fa957338732a37c744cfe3e17b24",
            "isKey": false,
            "numCitedBy": 10843,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms."
            },
            "slug": "A-training-algorithm-for-optimal-margin-classifiers-Boser-Guyon",
            "title": {
                "fragments": [],
                "text": "A training algorithm for optimal margin classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented, applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '92"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766703"
                        ],
                        "name": "A. Elisseeff",
                        "slug": "A.-Elisseeff",
                        "structuredName": {
                            "firstName": "Andr\u00e9",
                            "lastName": "Elisseeff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Elisseeff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 60
                            }
                        ],
                        "text": "There are two alternative formulation for multi-class SVMs: Weston and Watkins (1998) and Crammer and Singer (2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 30
                            }
                        ],
                        "text": "There is a different approach Weston et al. (2002), which uses a kernel over the labels to deal with the complexity of the addressed problem."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 813046,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "359300ac4a78687d60e18681925709b404b8fa54",
            "isKey": false,
            "numCitedBy": 183,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the learning problem of finding a dependency between a general class of objects and another, possibly different, general class of objects. The objects can be for example: vectors, images, strings, trees or graphs. Such a task is made possible by employing similarity measures in both input and output spaces using kernel functions, thus embedding the objects into vector spaces. We experimentally validate our approach on several tasks: mapping strings to strings, pattern recognition, and reconstruction from partial images."
            },
            "slug": "Kernel-Dependency-Estimation-Weston-Chapelle",
            "title": {
                "fragments": [],
                "text": "Kernel Dependency Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This work considers the learning problem of finding a dependency between a general class of objects and another, possibly different, generalclass of objects, made possible by employing similarity measures in both input and output spaces using kernel functions, thus embedding the objects into vector spaces."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20730631"
                        ],
                        "name": "R. Kassel",
                        "slug": "R.-Kassel",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Kassel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kassel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 29
                            }
                        ],
                        "text": "The dataset was collected by Kassel (1995) and contains around 6877 words of varying length (4 to 15 letters) written by 150 different subjects."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 329,
                                "start": 29
                            }
                        ],
                        "text": "The dataset was collected by Kassel (1995) and contains around 6877 words of varying length (4 to 15 letters) written by 150 different subjects. For this dataset the input space is partitioned as the output for each letter, we have a 16 \u00d7 8 binary image in the input space. This data set was preprocessed by Taskar et al. (2004) to test the M(3) network."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 37004361,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d3b97beda33f190487eaeb411ee81627f2efcff6",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Speech and handwriting are manifestations of a common need for linguistic communication. The similar nature of speech and handwriting recognition problems suggests that a largely shared solution may be possible. Recent advances in speech recognition can be partly attributed to changes in the research paradigm. These changes include using large corpora of common training and testing data, adopting statistical modeling over rule-based approaches, and ensuring meaningful comparisons between candidate technologies. The resulting improvements in system performance and robustness permit the study of increasingly difficult recognition tasks. \nThe primary goal of my thesis is to compare handwriting representations for on-line, printed, alphanumeric character recognition without striving to construct the highest-performance system. My studies are based on a carefully collected body of data containing some 87,000 characters from 150 writers. Material was selected automatically to ensure compact coverage of significant letter sequences. Subjects were instructed and prompted so as to minimally influence the writing they produced. A time-aligned transcription was entered for all of this data. I conducted an authentication study to understand better the classification difficulty of this writing. Only 81.7% of testing characters were identified correctly. \nI examined a number of potential representations for handwriting classification including bitmaps, projections, transforms, chain codes, and point-sampling, paying particular attention to pen motion as an information source. All experiments were based on Gaussian mixture models because of their flexibility. The best representation features Cartesian coordinates of 10 equally-spaced samples along the pen trajectory. Without the benefit of relative size information, this representation resulted in 77.2% correct character classification on testing data. \nFinally, I adapted the scSUMMIT segment-based speech recognition system developed at MIT to handwriting. Segmentation is based primarily on pen-lifts, but strokes are divided to account for connected character pairs. The parameter described above is computed for each segment and the resulting graph passed to the recognition engine for classification and search. This system was able to correctly recognize 65.1% of the test-set characters. Incorporating a bigram character grammar with perplexity 11.3 improved this performance to 76.4%. (Copies available exclusively from MIT Libraries, Rm. 14-0551, Cambridge, MA 02139-4307. Ph. 617-253-5668; Fax 617-253-1690.)"
            },
            "slug": "A-comparison-of-approaches-to-on-line-handwritten-Kassel",
            "title": {
                "fragments": [],
                "text": "A comparison of approaches to on-line handwritten character recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The primary goal of this thesis is to compare handwriting representations for on-line, printed, alphanumeric character recognition without striving to construct the highest-performance system."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52626911"
                        ],
                        "name": "T. Minka",
                        "slug": "T.-Minka",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Minka",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Minka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1768942,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "45b4dde8e0945912a39666f2715cdf10a4445b1c",
            "isKey": false,
            "numCitedBy": 537,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The generative aspect model is an extension of the multinomial model for text that allows word probabilities to vary stochastically across documents. Previous results with aspect models have been promising, but hindered by the computational difficulty of carrying out inference and learning. This paper demonstrates that the simple variational methods of Blei et al. (2001) can lead to inaccurate inferences and biased learning for the generative aspect model. We develop an alternative approach that leads to higher accuracy at comparable cost. An extension of Expectation-Propagation is used for inference and then embedded in an EM algorithm for learning. Experimental results are presented for both synthetic and real data sets."
            },
            "slug": "Expectation-Propogation-for-the-Generative-Aspect-Minka-Lafferty",
            "title": {
                "fragments": [],
                "text": "Expectation-Propogation for the Generative Aspect Model"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper demonstrates that the simple variational methods of Blei et al. (2001) can lead to inaccurate inferences and biased learning for the generative aspect model, and develops an alternative approach that leads to higher accuracy at comparable cost."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 23
                            }
                        ],
                        "text": "We have discussed CRF (Lafferty et al. (2001)), B-CRF (Qi et al. (2005)), K-CRF (Altun et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 72
                            }
                        ],
                        "text": "This approach to solve CRFs using kernels was independently proposed by Lafferty et al. (2004) and Altun et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 93
                            }
                        ],
                        "text": "We have presented a compact notation that can be used to represent Conditional Random Fields Lafferty et al. (2001), Bayesian CRFs Qi et al. (2005), Kernel CRFs Altun et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 23
                            }
                        ],
                        "text": "We have discussed CRF (Lafferty et al. (2001)), B-CRF (Qi et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 93
                            }
                        ],
                        "text": "We have presented a compact notation that can be used to represent Conditional Random Fields Lafferty et al. (2001), Bayesian CRFs Qi et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 92
                            }
                        ],
                        "text": "There are many different machine learning problems that can be represented by this setting (Dietterich (2002)), such as optical character recognition, part-of-speech tagging, collective web page classification, mobile fraud detection, or pitch accent prediction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 38476,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23c3953fb45536c9129e86ac7a23098bd9f1381d",
            "isKey": true,
            "numCitedBy": 674,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical learning problems in many fields involve sequential data. This paper formalizes the principal learning tasks and describes the methods that have been developed within the machine learning research community for addressing these problems. These methods include sliding window methods, recurrent sliding windows, hidden Markov models, conditional random fields, and graph transformer networks. The paper also discusses some open research issues."
            },
            "slug": "Machine-Learning-for-Sequential-Data:-A-Review-Dietterich",
            "title": {
                "fragments": [],
                "text": "Machine Learning for Sequential Data: A Review"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This paper formalizes the principal learning tasks and describes the methods that have been developed within the machine learning research community for addressing these problems, including sliding window methods, recurrent sliding windows, hidden Markov models, conditional random fields, and graph transformer networks."
            },
            "venue": {
                "fragments": [],
                "text": "SSPR/SPR"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114755206"
                        ],
                        "name": "Yuan Qi",
                        "slug": "Yuan-Qi",
                        "structuredName": {
                            "firstName": "Yuan",
                            "lastName": "Qi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuan Qi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778989"
                        ],
                        "name": "M. Szummer",
                        "slug": "M.-Szummer",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Szummer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Szummer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52626911"
                        ],
                        "name": "T. Minka",
                        "slug": "T.-Minka",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Minka",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Minka"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 55
                            }
                        ],
                        "text": "We have discussed CRF (Lafferty et al. (2001)), B-CRF (Qi et al. (2005)), K-CRF (Altun et al. (2004a,b); Lafferty et al. (2004)), HM-SVM (Altun et al. (2003)) and M3 nets (Taskar et al. (2004))."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Advances in Neural Information Processing Systems 4: Proceedings of the 1991 Conference, John E. Moody, Stephen J. Hanson, and Richard P. Lippmann, eds., 1992."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 119
                            }
                        ],
                        "text": "Advances in Neural Information Processing Systems 7: Proceedings of the 1994 Conference, Gerald Tesauro, David S. Touretzky, and Todd K. Leen, eds., 1995."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 131
                            }
                        ],
                        "text": "We have presented a compact notation that can be used to represent Conditional Random Fields Lafferty et al. (2001), Bayesian CRFs Qi et al. (2005), Kernel CRFs Altun et al. (2004a,b); Lafferty et al. (2004) and Maximum Margin Markov networks Altun et al. (2003); Taskar et al. (2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 125
                            }
                        ],
                        "text": "There has been several extensions to CRFs using kernels (Altun et al. (2004a,b); Lafferty et al. (2004)), Bayesian learning (Qi et al. (2005)) and maximum margin solution (Altun et al. (2003); Taskar et al. (2004))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 45
                            }
                        ],
                        "text": "Bayesian Conditional Random Fields (B-CRFs) (Qi et al. (2005)) have been recently proposed, in which the posterior over the weight (1.2) is approximated by a Gaussian using the Power Expectation Propagation (EP) algorithm (Minka and Lafferty (2002))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Published by Morgan-Kaufmann"
                    },
                    "intents": []
                }
            ],
            "corpusId": 14284377,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4477a7902f86c6c1354eaa5d77cafc5ab50b94c7",
            "isKey": true,
            "numCitedBy": 80,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose Bayesian Conditional Random Fields (BCRFs) for classifying interdependent and structured data, such as sequences, images or webs. BCRFs are a Bayesian approach to training and inference with conditional random fields, which were previously trained by maximizing likelihood (ML) (Lafferty et al., 2001). Our framework eliminates the problem of overfitting, and offers the full advantages of a Bayesian treatment. Unlike the ML approach, we estimate the posterior distribution of the model parameters during training, and average over this posterior during inference. We apply an extension of EP method, the power EP method, to incorporate the partition function. For algorithmic stability and accuracy, we flatten the approximation structures to avoid two-level approximations. We demonstrate the superior prediction accuracy of BCRFs over conditional random fields trained with ML or MAP on synthetic and real datasets."
            },
            "slug": "Bayesian-Conditional-Random-Fields-Qi-Szummer",
            "title": {
                "fragments": [],
                "text": "Bayesian Conditional Random Fields"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This framework eliminates the problem of overfitting, and offers the full advantages of a Bayesian treatment, and demonstrates the superior prediction accuracy of BCRFs over conditional random fields trained with ML or MAP on synthetic and real datasets."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113414328"
                        ],
                        "name": "Fernando Pereira",
                        "slug": "Fernando-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 125
                            }
                        ],
                        "text": "This sum can be efficiently computed using a forward-backward algorithm if the proposed graph for the CRF has no cycles, see Lafferty et al. (2001) for further details."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Lafferty et al. (2001) define Conditional Random Fields (CRFs) as:\nDefinition 1 (Conditional Random Field) Let G = (V, E) be a graph such that yn is indexed by the vertices of G, then (xn,yn) is a conditional random field in case, when conditioned on xn, the random variables ynl obey the Markov\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 23
                            }
                        ],
                        "text": "We have discussed CRF (Lafferty et al. (2001)), B-CRF (Qi et al. (2005)), K-CRF (Altun et al. (2004a,b); Lafferty et al. (2004)), HM-SVM (Altun et al. (2003)) and M3 nets (Taskar et al. (2004))."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 93
                            }
                        ],
                        "text": "We have presented a compact notation that can be used to represent Conditional Random Fields Lafferty et al. (2001), Bayesian CRFs Qi et al. (2005), Kernel CRFs Altun et al. (2004a,b); Lafferty et al. (2004) and Maximum Margin Markov networks Altun et al. (2003); Taskar et al. (2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Lafferty et al. (2001) propose to use an undirected acyclic graph over the labels."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 205
                            }
                        ],
                        "text": "\u2026the weights, is:\np(y\u2217|x\u2217,D) = \u222b p(y\u2217|x\u2217,w)p(w|Y,X)dw (1.4)\nThe complexity of this problem grows exponentially in L \u2013the length of the label vector\u2013 and we need to simplify it to work with large values of q and/or L. Lafferty et al. (2001) propose to use an undirected acyclic graph over the labels."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 22
                            }
                        ],
                        "text": "In their seminal work Lafferty et al. (2001) assume the output for each sample can be expressed as a set of interdependent labels and that this dependency can be captured by an undirected graphical model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Lafferty et al. (2001) propose to train the parameters of the CRF model by maximum likelihood (ML)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 0
                            }
                        ],
                        "text": "Lafferty et al. (2001) propose to use an undirected acyclic graph over the labels. This graph makes it possible to efficiently compute the denominator in (1.1) using a forward-backward algorithm. Lafferty et al. (2001) define Conditional Random Fields (CRFs) as:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 219683473,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4ba954b0412773d047dc41231c733de0c1f4926",
            "isKey": true,
            "numCitedBy": 13413,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data."
            },
            "slug": "Conditional-Random-Fields:-Probabilistic-Models-for-Lafferty-McCallum",
            "title": {
                "fragments": [],
                "text": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work presents iterative parameter estimation algorithms for conditional random fields and compares the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1832364"
                        ],
                        "name": "Xiaojin Zhu",
                        "slug": "Xiaojin-Zhu",
                        "structuredName": {
                            "firstName": "Xiaojin",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojin Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47909531"
                        ],
                        "name": "Yan Liu",
                        "slug": "Yan-Liu",
                        "structuredName": {
                            "firstName": "Yan",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yan Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 105
                            }
                        ],
                        "text": "We have discussed CRF (Lafferty et al. (2001)), B-CRF (Qi et al. (2005)), K-CRF (Altun et al. (2004a,b); Lafferty et al. (2004)), HM-SVM (Altun et al. (2003)) and M3 nets (Taskar et al. (2004))."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Advances in Neural Information Processing Systems 7: Proceedings of the 1994 Conference, Gerald Tesauro, David S. Touretzky, and Todd K. Leen, eds., 1995."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 185
                            }
                        ],
                        "text": "We have presented a compact notation that can be used to represent Conditional Random Fields Lafferty et al. (2001), Bayesian CRFs Qi et al. (2005), Kernel CRFs Altun et al. (2004a,b); Lafferty et al. (2004) and Maximum Margin Markov networks Altun et al. (2003); Taskar et al. (2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 81
                            }
                        ],
                        "text": "There has been several extensions to CRFs using kernels (Altun et al. (2004a,b); Lafferty et al. (2004)), Bayesian learning (Qi et al. (2005)) and maximum margin solution (Altun et al. (2003); Taskar et al. (2004))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 72
                            }
                        ],
                        "text": "This approach to solve CRFs using kernels was independently proposed by Lafferty et al. (2004) and Altun et al. (2004a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Published by Morgan-Kaufmann"
                    },
                    "intents": []
                }
            ],
            "corpusId": 3096549,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0e785b4d666158c58fdd1885b8af4908b8e23eed",
            "isKey": true,
            "numCitedBy": 176,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Kernel conditional random fields (KCRFs) are introduced as a framework for discriminative modeling of graph-structured data. A representer theorem for conditional graphical models is given which shows how kernel conditional random fields arise from risk minimization procedures defined using Mercer kernels on labeled graphs. A procedure for greedily selecting cliques in the dual representation is then proposed, which allows sparse representations. By incorporating kernels and implicit feature spaces into conditional graphical models, the framework enables semi-supervised learning algorithms for structured data through the use of graph kernels. The framework and clique selection methods are demonstrated in synthetic data experiments, and are also applied to the problem of protein secondary structure prediction."
            },
            "slug": "Kernel-conditional-random-fields:-representation-Lafferty-Zhu",
            "title": {
                "fragments": [],
                "text": "Kernel conditional random fields: representation and clique selection"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "Kernel conditional random fields are introduced as a framework for discriminative modeling of graph-structured data and a procedure for greedily selecting cliques in the dual representation is proposed, which allows sparse representations."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145349582"
                        ],
                        "name": "Sanjiv Kumar",
                        "slug": "Sanjiv-Kumar",
                        "structuredName": {
                            "firstName": "Sanjiv",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sanjiv Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Published by Morgan-Kaufmann"
                    },
                    "intents": []
                }
            ],
            "corpusId": 1282113,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "015293bf7c4cf7ce50a01ce1ceb11f584d123d25",
            "isKey": false,
            "numCitedBy": 257,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present Discriminative Random Fields (DRF), a discriminative framework for the classification of natural image regions by incorporating neighborhood spatial dependencies in the labels as well as the observed data. The proposed model exploits local discriminative models and allows to relax the assumption of conditional independence of the observed data given the labels, commonly used in the Markov Random Field (MRF) framework. The parameters of the DRF model are learned using penalized maximum pseudo-likelihood method. Furthermore, the form of the DRF model allows the MAP inference for binary classification problems using the graph min-cut algorithms. The performance of the model was verified on the synthetic as well as the real-world images. The DRF model outperforms the MRF model in the experiments."
            },
            "slug": "Discriminative-Fields-for-Modeling-Spatial-in-Kumar-Hebert",
            "title": {
                "fragments": [],
                "text": "Discriminative Fields for Modeling Spatial Dependencies in Natural Images"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The proposed DRF model exploits local discriminative models and allows to relax the assumption of conditional independence of the observed data given the labels, commonly used in the Markov Random Field (MRF) framework."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711460"
                        ],
                        "name": "Chih-Jen Lin",
                        "slug": "Chih-Jen-Lin",
                        "structuredName": {
                            "firstName": "Chih-Jen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Jen Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 525138,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d3c20c4e1a49a3257bbdef5f7d8abf0d6a11c284",
            "isKey": false,
            "numCitedBy": 104,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "In this article, we discuss issues about formulations of support vector machines (SVM) from an optimization point of view. First, SVMs map training data into a higher- (maybe infinite-) dimensional space. Currently primal and dual formulations of SVM are derived in the finite dimensional space and readily extend to the infinite-dimensional space. We rigorously discuss the primal-dual relation in the infinite-dimensional spaces. Second, SVM formulations contain penalty terms, which are different from unconstrained penalty functions in optimization. Traditionally unconstrained penalty functions approximate a constrained problem as the penalty parameter increases. We are interested in similar properties for SVM formulations. For two of the most popular SVM formulations, we show that one enjoys properties of exact penalty functions, but the other is only like traditional penalty functions, which converge when the penalty parameter goes to infinity."
            },
            "slug": "Formulations-of-Support-Vector-Machines:-A-Note-an-Lin",
            "title": {
                "fragments": [],
                "text": "Formulations of Support Vector Machines: A Note from an Optimization Point of View"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "For two of the most popular SVM formulations, it is shown that one enjoys properties of exact penalty functions, but the other is only like traditional penalty functions which converge when the penalty parameter goes to infinity."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783941"
                        ],
                        "name": "Y. Altun",
                        "slug": "Y.-Altun",
                        "structuredName": {
                            "firstName": "Yasemin",
                            "lastName": "Altun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Altun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 103
                            }
                        ],
                        "text": "In particular, in the pattern recognition field, the appearance of the Support Vector Machines (SVMs) (Boser et al. (1992)) blossomed the research into new machine learning algorithms and they bought the kernel concept into the machine learning community (Sch\u00f6lkopf and Smola (2001))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1118,
                                "start": 103
                            }
                        ],
                        "text": "In particular, in the pattern recognition field, the appearance of the Support Vector Machines (SVMs) (Boser et al. (1992)) blossomed the research into new machine learning algorithms and they bought the kernel concept into the machine learning community (Sch\u00f6lkopf and Smola (2001)). Nevertheless, most real-life applications of pattern recognition cannot be readily cast as a binary (or multiclass) learning problem, because they present an inherent structure that cannot be exploited by general classification algorithms. Some of these applications such as speech or object recognition have developed their own research field. They use a mixture of machine learning tools with specific knowledge about each application to provide meaningful solutions to these relevant problems. But there are many others that can benefit from a machine learning approach, if we are capable of embedding their structure into a generic pattern recognition tool. Conditional Random Fields (CRFs) address this general problem as an extension of logistic regression for multiclass problems. In their seminal work Lafferty et al. (2001) assume the output for each sample can be expressed as a set of interdependent labels and that this dependency can be captured by an undirected graphical model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 57
                            }
                        ],
                        "text": "There has been several extensions to CRFs using kernels (Altun et al. (2004a,b); Lafferty et al. (2004)), Bayesian learning (Qi et al. (2005)) and maximum margin solution (Altun et al. (2003); Taskar et al. (2004))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 99
                            }
                        ],
                        "text": "This approach to solve CRFs using kernels was independently proposed by Lafferty et al. (2004) and Altun et al. (2004a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 57
                            }
                        ],
                        "text": "There has been several extensions to CRFs using kernels (Altun et al. (2004a,b); Lafferty et al. (2004)), Bayesian learning (Qi et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 24
                            }
                        ],
                        "text": "This is the proposal in Altun et al. (2004b) using the Laplace approximation over the posterior of the weights, p(w|Y,X)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 57
                            }
                        ],
                        "text": "There has been several extensions to CRFs using kernels (Altun et al. (2004a,b); Lafferty et al. (2004)), Bayesian learning (Qi et al. (2005)) and maximum margin solution (Altun et al. (2003); Taskar et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1523,
                                "start": 103
                            }
                        ],
                        "text": "In particular, in the pattern recognition field, the appearance of the Support Vector Machines (SVMs) (Boser et al. (1992)) blossomed the research into new machine learning algorithms and they bought the kernel concept into the machine learning community (Sch\u00f6lkopf and Smola (2001)). Nevertheless, most real-life applications of pattern recognition cannot be readily cast as a binary (or multiclass) learning problem, because they present an inherent structure that cannot be exploited by general classification algorithms. Some of these applications such as speech or object recognition have developed their own research field. They use a mixture of machine learning tools with specific knowledge about each application to provide meaningful solutions to these relevant problems. But there are many others that can benefit from a machine learning approach, if we are capable of embedding their structure into a generic pattern recognition tool. Conditional Random Fields (CRFs) address this general problem as an extension of logistic regression for multiclass problems. In their seminal work Lafferty et al. (2001) assume the output for each sample can be expressed as a set of interdependent labels and that this dependency can be captured by an undirected graphical model. They exploit the graphical model structure to avoid the exponential growth of the complexity with the number of labels in each output. There are many different machine learning problems that can be represented by this setting (Dietterich (2002)), such as optical character recognition, part-of-speech tagging, collective web page classification, mobile fraud detection, or pitch accent prediction."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 819,
                                "start": 11
                            }
                        ],
                        "text": "(2004) and Altun et al. (2004a). We refer to the general approach as Kernel Conditional Random Fields (K-CRFs). In both of these papers the authors propose to simplify the solution by forcing (in a smart and controlled way) that some \u03b2 should be zero at the solution. The runtime complexity to infer the label of a new input sequence does not grow with the number of training samples. Otherwise all \u03b2 are nonzero due to the applied loss-function (logistic regression). Another option to get a sparse solution in terms of the \u03b2 is to change the loss-function by a hinge-loss, which is presented in the following section. One natural extension of the Bayesian CRF is solving the integral in (1.4) using kernels. We refer to this approach as Gaussian Processes CRFs (GP-CRFs). This is the proposal in Altun et al. (2004b) using the Laplace approximation over the posterior of the weights, p(w|Y,X)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 57
                            }
                        ],
                        "text": "There has been several extensions to CRFs using kernels (Altun et al. (2004a,b); Lafferty et al. (2004)), Bayesian learning (Qi et al. (2005)) and maximum margin solution (Altun et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 11
                            }
                        ],
                        "text": "(2004) and Altun et al. (2004a). We refer to the general approach as Kernel Conditional Random Fields (K-CRFs)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1114784,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "c03edc1b78c7aca86d3034aafe8b215a8608cd7d",
            "isKey": true,
            "numCitedBy": 58,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we define conditional random fields in reproducing kernel Hilbert spaces and show connections to Gaussian Process classification. More specifically, we prove decomposition results for undirected graphical models and we give constructions for kernels. Finally we present efficient means of solving the optimization problem using reduced rank decompositions and we show how stationarity can be exploited efficiently in the optimization process."
            },
            "slug": "Exponential-Families-for-Conditional-Random-Fields-Altun-Smola",
            "title": {
                "fragments": [],
                "text": "Exponential Families for Conditional Random Fields"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper proves decomposition results for undirected graphical models and gives constructions for kernels and shows how stationarity can be exploited efficiently in the optimization process."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101750654"
                        ],
                        "name": "G. Kimeldorf",
                        "slug": "G.-Kimeldorf",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Kimeldorf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kimeldorf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 160
                            }
                        ],
                        "text": "7) fulfils the conditions of the Representer theorem in Sch\u00f6lkopf and Smola (2001) (which is a generalization of the Representer theorem originally proposed by Kimeldorf and Wahba (1971))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 125
                            }
                        ],
                        "text": "This sum can be efficiently computed using a forward-backward algorithm if the proposed graph for the CRF has no cycles, see Lafferty et al. (2001) for further details."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 942,
                                "start": 125
                            }
                        ],
                        "text": "This sum can be efficiently computed using a forward-backward algorithm if the proposed graph for the CRF has no cycles, see Lafferty et al. (2001) for further details. In (1.7) we readily see that the loss-function compares the true output o(xn,yn) with all the other outputs log (\u2211 y exp (o(xn,y)) ) . This loss-function is always positive log (\u2211 y exp (o(xn,y)) ) > log (exp (o(xn,yn))) = o(xn,yn) and it is close to zero iff o(xn,yn) \u00c0 o(xn,y) \u2200y 6= yn. The norm of w is a regulariser to avoid overfitting. This functional is convex and can be solved using different techniques (Wallach (2002)). The inference phase can be done using a Viterbi-like algorithm over the labels in the graph. Bayesian Conditional Random Fields (B-CRFs) (Qi et al. (2005)) have been recently proposed, in which the posterior over the weight (1.2) is approximated by a Gaussian using the Power Expectation Propagation (EP) algorithm (Minka and Lafferty (2002))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 181
                            }
                        ],
                        "text": "Its functional in (1.7) fulfils the conditions of the Representer theorem in Scho\u0308lkopf and Smola (2001) (which is a generalization of the Representer theorem originally proposed by Kimeldorf and Wahba (1971))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1301,
                                "start": 125
                            }
                        ],
                        "text": "This sum can be efficiently computed using a forward-backward algorithm if the proposed graph for the CRF has no cycles, see Lafferty et al. (2001) for further details. In (1.7) we readily see that the loss-function compares the true output o(xn,yn) with all the other outputs log (\u2211 y exp (o(xn,y)) ) . This loss-function is always positive log (\u2211 y exp (o(xn,y)) ) > log (exp (o(xn,yn))) = o(xn,yn) and it is close to zero iff o(xn,yn) \u00c0 o(xn,y) \u2200y 6= yn. The norm of w is a regulariser to avoid overfitting. This functional is convex and can be solved using different techniques (Wallach (2002)). The inference phase can be done using a Viterbi-like algorithm over the labels in the graph. Bayesian Conditional Random Fields (B-CRFs) (Qi et al. (2005)) have been recently proposed, in which the posterior over the weight (1.2) is approximated by a Gaussian using the Power Expectation Propagation (EP) algorithm (Minka and Lafferty (2002)). Once the posterior has been estimated, predictions can be also made using an EP algorithm that considers an independent distribution for each label. CRFs were initially proposed to solve the multi-label problem using a known set of features (f(xn,yn)). Its functional in (1.7) fulfils the conditions of the Representer theorem in Sch\u00f6lkopf and Smola (2001) (which is a generalization of the Representer theorem originally proposed by Kimeldorf and Wahba (1971))."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 598,
                                "start": 125
                            }
                        ],
                        "text": "This sum can be efficiently computed using a forward-backward algorithm if the proposed graph for the CRF has no cycles, see Lafferty et al. (2001) for further details. In (1.7) we readily see that the loss-function compares the true output o(xn,yn) with all the other outputs log (\u2211 y exp (o(xn,y)) ) . This loss-function is always positive log (\u2211 y exp (o(xn,y)) ) > log (exp (o(xn,yn))) = o(xn,yn) and it is close to zero iff o(xn,yn) \u00c0 o(xn,y) \u2200y 6= yn. The norm of w is a regulariser to avoid overfitting. This functional is convex and can be solved using different techniques (Wallach (2002))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 755,
                                "start": 125
                            }
                        ],
                        "text": "This sum can be efficiently computed using a forward-backward algorithm if the proposed graph for the CRF has no cycles, see Lafferty et al. (2001) for further details. In (1.7) we readily see that the loss-function compares the true output o(xn,yn) with all the other outputs log (\u2211 y exp (o(xn,y)) ) . This loss-function is always positive log (\u2211 y exp (o(xn,y)) ) > log (exp (o(xn,yn))) = o(xn,yn) and it is close to zero iff o(xn,yn) \u00c0 o(xn,y) \u2200y 6= yn. The norm of w is a regulariser to avoid overfitting. This functional is convex and can be solved using different techniques (Wallach (2002)). The inference phase can be done using a Viterbi-like algorithm over the labels in the graph. Bayesian Conditional Random Fields (B-CRFs) (Qi et al. (2005)) have been recently proposed, in which the posterior over the weight (1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 121062339,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3065c5e37a0c1f1be365e88ddf2d5cd02faa5db1",
            "isKey": true,
            "numCitedBy": 1330,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Some-results-on-Tchebycheffian-spline-functions-Kimeldorf-Wahba",
            "title": {
                "fragments": [],
                "text": "Some results on Tchebycheffian spline functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 282,
                                "start": 256
                            }
                        ],
                        "text": "In particular, in the pattern recognition field, the appearance of the Support Vector Machines (SVMs) (Boser et al. (1992)) blossomed the research into new machine learning algorithms and they bought the kernel concept into the machine learning community (Scho\u0308lkopf and Smola (2001))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Published by Morgan-Kaufmann"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 77
                            }
                        ],
                        "text": "Its functional in (1.7) fulfils the conditions of the Representer theorem in Scho\u0308lkopf and Smola (2001) (which is a generalization of the Representer theorem originally proposed by Kimeldorf and Wahba (1971))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29871328,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5051890e501117097eeffbd8ded87694f0d8063",
            "isKey": true,
            "numCitedBy": 6578,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "All rights reserved. No part of this book may be reproduced in any form by any electronic or mechanical means (including photocopying, recording, or information storage and retrieval) without permission in writing from the publisher."
            },
            "slug": "Learning-with-kernels-Smola",
            "title": {
                "fragments": [],
                "text": "Learning with kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "This book is intended to be a guide to the art of self-consistency and should not be used as a substitute for a comprehensive guide to self-confidence."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50333420"
                        ],
                        "name": "S. Becker",
                        "slug": "S.-Becker",
                        "structuredName": {
                            "firstName": "Suzanna",
                            "lastName": "Becker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Becker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61652888,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b313a0581253191f291f923185a691b260d2bfee",
            "isKey": false,
            "numCitedBy": 261,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Editors.-Advances-in-Neural-Information-Processing-Dietterich-Becker",
            "title": {
                "fragments": [],
                "text": "Editors. Advances in Neural Information Processing Systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4562073"
                        ],
                        "name": "C. Watkins",
                        "slug": "C.-Watkins",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Watkins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Watkins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 101
                            }
                        ],
                        "text": "For this problem, in which the number of possible labellings grows exponentially, the formulation by Weston and Watkins (1998) can result in an exponential growth in the number of nonzero support vectors and therefore it is more advisable to use the formulation by Crammer and Singer (2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 60
                            }
                        ],
                        "text": "There are two alternative formulation for multi-class SVMs: Weston and Watkins (1998) and Crammer and Singer (2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 3
                            }
                        ],
                        "text": "In Weston and Watkins (1998), the M-SVM penalises any possible labelling that provides a larger output than the true labelling."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7359186,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "89a37349688b49bbfc9fd643db5a41b9071f9ca2",
            "isKey": false,
            "numCitedBy": 1308,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multi-Class-Support-Vector-Machines-Weston-Watkins",
            "title": {
                "fragments": [],
                "text": "Multi-Class Support Vector Machines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Minka . Bayesian conditional random fields"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Minka . Bayesian conditional random fields"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Minka . Bayesian conditional random fields"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics"
            },
            "year": 2001
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 11,
            "methodology": 12
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 23,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Predicting-Structured-Data-Bakir-Hofmann/2e75ad22a6865c3402925f12317dd0897c67ebe6?sort=total-citations"
}