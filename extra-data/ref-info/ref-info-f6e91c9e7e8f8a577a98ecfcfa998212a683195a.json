{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2308463"
                        ],
                        "name": "Salah El Hihi",
                        "slug": "Salah-El-Hihi",
                        "structuredName": {
                            "firstName": "Salah",
                            "lastName": "Hihi",
                            "middleNames": [
                                "El"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Salah El Hihi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2843869,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b13813b49f160e1a2010c44bd4fb3d09a28446e3",
            "isKey": false,
            "numCitedBy": 332,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We have already shown that extracting long-term dependencies from sequential data is difficult, both for determimstic dynamical systems such as recurrent networks, and probabilistic models such as hidden Markov models (HMMs) or input/output hidden Markov models (IOHMMs). In practice, to avoid this problem, researchers have used domain specific a-priori knowledge to give meaning to the hidden or state variables representing past context. In this paper, we propose to use a more general type of a-priori knowledge, namely that the temporal dependencies are structured hierarchically. This implies that long-term dependencies are represented by variables with a long time scale. This principle is applied to a recurrent network which includes delays and multiple time scales. Experiments confirm the advantages of such structures. A similar approach is proposed for HMMs and IOHMMs."
            },
            "slug": "Hierarchical-Recurrent-Neural-Networks-for-Hihi-Bengio",
            "title": {
                "fragments": [],
                "text": "Hierarchical Recurrent Neural Networks for Long-Term Dependencies"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper proposes to use a more general type of a-priori knowledge, namely that the temporal dependencies are structured hierarchically, which implies that long-term dependencies are represented by variables with a long time scale."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688235"
                        ],
                        "name": "P. Frasconi",
                        "slug": "P.-Frasconi",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Frasconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frasconi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206457500,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0be39ee052d246ae99c082a565aba25b811be2d",
            "isKey": false,
            "numCitedBy": 6141,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered."
            },
            "slug": "Learning-long-term-dependencies-with-gradient-is-Bengio-Simard",
            "title": {
                "fragments": [],
                "text": "Learning long-term dependencies with gradient descent is difficult"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work shows why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases, and exposes a trade-off between efficient learning by gradient descent and latching on information for long periods."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2797623"
                        ],
                        "name": "H. Siegelmann",
                        "slug": "H.-Siegelmann",
                        "structuredName": {
                            "firstName": "Hava",
                            "lastName": "Siegelmann",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Siegelmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35216199"
                        ],
                        "name": "B. Horne",
                        "slug": "B.-Horne",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Horne",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Horne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157784"
                        ],
                        "name": "C. Lee Giles",
                        "slug": "C.-Lee-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee Giles"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1872756,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "19b7b3a330d8f4f07cd8752d1e58cccd7f419fe4",
            "isKey": false,
            "numCitedBy": 425,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, fully connected recurrent neural networks have been proven to be computationally rich-at least as powerful as Turing machines. This work focuses on another network which is popular in control applications and has been found to be very effective at learning a variety of problems. These networks are based upon Nonlinear AutoRegressive models with eXogenous Inputs (NARX models), and are therefore called NARX networks. As opposed to other recurrent networks, NARX networks have a limited feedback which comes only from the output neuron rather than from hidden states. They are formalized by y(t)=Psi(u(t-n(u)), ..., u(t-1), u(t), y(t-n(y)), ..., y(t-1)) where u(t) and y(t) represent input and output of the network at time t, n(u) and n(y) are the input and output order, and the function Psi is the mapping performed by a Multilayer Perceptron. We constructively prove that the NARX networks with a finite number of parameters are computationally as strong as fully connected recurrent networks and thus Turing machines. We conclude that in theory one can use the NARX models, rather than conventional recurrent networks without any computational loss even though their feedback is limited. Furthermore, these results raise the issue of what amount of feedback or recurrence is necessary for any network to be Turing equivalent and what restrictions on feedback limit computational power."
            },
            "slug": "Computational-capabilities-of-recurrent-NARX-neural-Siegelmann-Horne",
            "title": {
                "fragments": [],
                "text": "Computational capabilities of recurrent NARX neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is constructively proved that the NARX networks with a finite number of parameters are computationally as strong as fully connected recurrent networks and thus Turing machines, raising the issue of what amount of feedback or recurrence is necessary for any network to be Turing equivalent and what restrictions on feedback limit computational power."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Man Cybern. Part B"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308557"
                        ],
                        "name": "S. Hochreiter",
                        "slug": "S.-Hochreiter",
                        "structuredName": {
                            "firstName": "Sepp",
                            "lastName": "Hochreiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hochreiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1915014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44d2abe2175df8153f465f6c39b68b76a0d40ab9",
            "isKey": false,
            "numCitedBy": 51648,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms."
            },
            "slug": "Long-Short-Term-Memory-Hochreiter-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Long Short-Term Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A novel, efficient, gradient based method called long short-term memory (LSTM) is introduced, which can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 218
                            }
                        ],
                        "text": "As an alternative to using di erent learning algorithms, one suggestion has been to alter the input data so that it represents a reduced description that makes global features more explicit and more readily detectable [21, 29, 28]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14426348,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f7c4048a03281e976f28d35c2f9fef3a58346e6",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Do you want your neural net algorithm to learn sequences? Do not limit yourself to conventional gradient descent (or approximations thereof). Instead, use your sequence learning algorithm (any will do) to implement the following method for history compression. No matter what your final goals are, train a network to predict its next input from the previous ones. Since only unpredictable inputs convey new information, ignore all predictable inputs but let all unexpected inputs (plus information about the time step at which they occurred) become inputs to a higher-level network of the same kind (working on a slower, self-adjusting time scale). Go on building a hierarchy of such networks. This principle reduces the descriptions of event sequences without loss of information, thus easing supervised or reinforcement learning tasks. Alternatively, you may use two recurrent networks to collapse a multi-level predictor hierarchy into a single recurrent net. Experiments show that systems based on these principles can require less computation per time step and many fewer training sequences than conventional training algorithms for recurrent nets. Finally you can modify the above method such that predictability is not defined in a yes-or-no fashion but in a continuous fashion."
            },
            "slug": "Learning-Unambiguous-Reduced-Sequence-Descriptions-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Learning Unambiguous Reduced Sequence Descriptions"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Experiments show that systems based on these principles can require less computation per time step and many fewer training sequences than conventional training algorithms for recurrent nets."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2192631"
                        ],
                        "name": "Hong-Te Su",
                        "slug": "Hong-Te-Su",
                        "structuredName": {
                            "firstName": "Hong-Te",
                            "lastName": "Su",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hong-Te Su"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47621280"
                        ],
                        "name": "T. McAvoy",
                        "slug": "T.-McAvoy",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "McAvoy",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. McAvoy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47055692"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 138
                            }
                        ],
                        "text": "3 NARX networks An important class of discrete{time nonlinear systems is the Nonlinear AutoRegressive with eXogenous inputs (NARX) model2 [3, 19, 20, 34, 35]: y(t) = f u(t Du); : : : ; u(t 1); u(t); y(t Dy); : : : ; y(t 1) ; (9) where u(t) and y(t) represent input and output of the network at time t, Du andDy are the input and output order, and the function f is a nonlinear function."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 140
                            }
                        ],
                        "text": "It has been demonstrated that they are well suited for modeling nonlinear systems such as heat exchangers [3], waste water treatment plants [34, 35], catalytic reforming systems in a petroleum re nery [35], nonlinear oscillations associated with multi{legged locomotion in biological systems [36], time series [4], and various arti cial nonlinear systems [3, 22, 26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 96394873,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "61f4edc8e0f8cb550dceada70e22269760ef826b",
            "isKey": false,
            "numCitedBy": 170,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer feedforward networks have been used successfully for nonlinear system identification by using them as discrete-time dynamic models. In the past, feedforward networks have been adapted as one-step-ahead predictors; however, in model predictive control the model has to be iterated to predict many time steps ahead into the future. Therefore, the feedforward network is chained to itself to go as far as needed in the future, and this chaining may result in large errors. As an alternative to using the one-step-ahead approach, a feedforward network is chained to itself during the training. This training procedure is referred to as a parallel identification method since the network is in parallel with the system to be identified. A feedforward network used in the parallel approach results in an external recurrent network. The learning algorithm for external recurrent networks is derived using ordered derivatives. The network is used to identify the dynamic behavior of a biological wastewater treatment plant and a catalytic reformer in a petroleum refinery"
            },
            "slug": "Long-term-predictions-of-chemical-processes-using-a-Su-McAvoy",
            "title": {
                "fragments": [],
                "text": "Long-term predictions of chemical processes using recurrent neural networks: a parallel training approach"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1974892"
                        ],
                        "name": "G. Puskorius",
                        "slug": "G.-Puskorius",
                        "structuredName": {
                            "firstName": "Gintaras",
                            "lastName": "Puskorius",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Puskorius"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48041290"
                        ],
                        "name": "L. Feldkamp",
                        "slug": "L.-Feldkamp",
                        "structuredName": {
                            "firstName": "Lee",
                            "lastName": "Feldkamp",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Feldkamp"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 829774,
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "id": "0e63335010c6d3a56ffba62595118447ff9e8734",
            "isKey": false,
            "numCitedBy": 571,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Although the potential of the powerful mapping and representational capabilities of recurrent network architectures is generally recognized by the neural network research community, recurrent neural networks have not been widely used for the control of nonlinear dynamical systems, possibly due to the relative ineffectiveness of simple gradient descent training algorithms. Developments in the use of parameter-based extended Kalman filter algorithms for training recurrent networks may provide a mechanism by which these architectures will prove to be of practical value. This paper presents a decoupled extended Kalman filter (DEKF) algorithm for training of recurrent networks with special emphasis on application to control problems. We demonstrate in simulation the application of the DEKF algorithm to a series of example control problems ranging from the well-known cart-pole and bioreactor benchmark problems to an automotive subsystem, engine idle speed control. These simulations suggest that recurrent controller networks trained by Kalman filter methods can combine the traditional features of state-space controllers and observers in a homogeneous architecture for nonlinear dynamical systems, while simultaneously exhibiting less sensitivity than do purely feedforward controller networks to changes in plant parameters and measurement noise."
            },
            "slug": "Neurocontrol-of-nonlinear-dynamical-systems-with-Puskorius-Feldkamp",
            "title": {
                "fragments": [],
                "text": "Neurocontrol of nonlinear dynamical systems with Kalman filter trained recurrent networks"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "These simulations suggest that recurrent controller networks trained by Kalman filter methods can combine the traditional features of state-space controllers and observers in a homogeneous architecture for nonlinear dynamical systems, while simultaneously exhibiting less sensitivity than do purely feedforward controller networks to changes in plant parameters and measurement noise."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18271205,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "50c770b425a5bb25c77387f687a9910a9d130722",
            "isKey": false,
            "numCitedBy": 428,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous neural network learning algorithms for sequence processing are computationally expensive and perform poorly when it comes to long time lags. This paper first introduces a simple principle for reducing the descriptions of event sequences without loss of information. A consequence of this principle is that only unexpected inputs can be relevant. This insight leads to the construction of neural architectures that learn to divide and conquer by recursively decomposing sequences. I describe two architectures. The first functions as a self-organizing multilevel hierarchy of recurrent networks. The second, involving only two recurrent networks, tries to collapse a multilevel predictor hierarchy into a single recurrent net. Experiments show that the system can require less computation per time step and many fewer training sequences than conventional training algorithms for recurrent nets."
            },
            "slug": "Learning-Complex,-Extended-Sequences-Using-the-of-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Learning Complex, Extended Sequences Using the Principle of History Compression"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A simple principle for reducing the descriptions of event sequences without loss of information is introduced and this insight leads to the construction of neural architectures that learn to divide and conquer by recursively decomposing sequences."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39782670"
                        ],
                        "name": "D. Seidl",
                        "slug": "D.-Seidl",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Seidl",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Seidl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3169483"
                        ],
                        "name": "R. Lorenz",
                        "slug": "R.-Lorenz",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Lorenz",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lorenz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 114
                            }
                        ],
                        "text": "1 Introduction Recurrent Neural Networks (RNNs) are capable of representing arbitrary nonlinear dynamical systems [30, 32, 33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 56923464,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "86510e07e79d82b8318e881233704fa7429cf939",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "It is shown that the structure of the standard recurrent neural network has the capacity to model a broad class of nonlinear dynamic systems. The key result is that the structure of the recurrent neural network permits the internal formation of a single hidden layer/linear output layer feedforward neural network to approximate the next system state as a function of the current system state and the inputs. The recurrent nature of the network allows the single weight matrix to serve as both the input and output weight matrices of the internal feedforward network.<<ETX>>"
            },
            "slug": "A-structure-by-which-a-recurrent-neural-network-can-Seidl-Lorenz",
            "title": {
                "fragments": [],
                "text": "A structure by which a recurrent neural network can approximate a nonlinear dynamic system"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "It is shown that the structure of the standard recurrent neural network has the capacity to model a broad class of nonlinear dynamic systems."
            },
            "venue": {
                "fragments": [],
                "text": "IJCNN-91-Seattle International Joint Conference on Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2442252"
                        ],
                        "name": "P. Poddar",
                        "slug": "P.-Poddar",
                        "structuredName": {
                            "firstName": "Pinaki",
                            "lastName": "Poddar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Poddar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143928513"
                        ],
                        "name": "K. Unnikrishnan",
                        "slug": "K.-Unnikrishnan",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Unnikrishnan",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Unnikrishnan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 84
                            }
                        ],
                        "text": "It may also be possible to obtain similar results for the architectures proposed in [6, 9, 24, 37]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 60512655,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "505ceb957fb88ba0bb1cdb4485572ad06e3bbfdb",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present a feed-forward neural network architecture that can be used for nonlinear autoregressive prediction of multivariate time-series. It uses specialized neurons (called memory neurons) to store past activations of the network in an efficient fashion. The network learns to be a nonlinear predictor of the appropriate order to model temporal waveforms of speech signals. Arrays of such networks can be used to build real-time classifiers of speech sounds. Experiments where memory-neuron networks are trained to predict speech waveforms and sequences of spectral frames are described. Performance of the network for prediction of time-series with minimal a priori assumptions of its statistical properties is shown to be better than linear autoregressive models.<<ETX>>"
            },
            "slug": "Nonlinear-prediction-of-speech-signals-using-memory-Poddar-Unnikrishnan",
            "title": {
                "fragments": [],
                "text": "Nonlinear prediction of speech signals using memory neuron networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Performance of the network for prediction of time-series with minimal a priori assumptions of its statistical properties is shown to be better than linear autoregressive models."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks for Signal Processing Proceedings of the 1991 IEEE Workshop"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33259680"
                        ],
                        "name": "J. Connor",
                        "slug": "J.-Connor",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Connor",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Connor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705299"
                        ],
                        "name": "L. Atlas",
                        "slug": "L.-Atlas",
                        "structuredName": {
                            "firstName": "Les",
                            "lastName": "Atlas",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Atlas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107683368"
                        ],
                        "name": "R. Martin",
                        "slug": "R.-Martin",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Martin",
                            "middleNames": [
                                "Douglas"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Martin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 313,
                                "start": 310
                            }
                        ],
                        "text": "It has been demonstrated that they are well suited for modeling nonlinear systems such as heat exchangers [3], waste water treatment plants [34, 35], catalytic reforming systems in a petroleum re nery [35], nonlinear oscillations associated with multi{legged locomotion in biological systems [36], time series [4], and various arti cial nonlinear systems [3, 22, 26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9249486,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "06778bd87125a28f0d045e0221ca1b8ad1d469b6",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "There exist large classes of time series, such as those with nonlinear moving average components, that are not well modeled by feedforward networks or linear models, but can be modeled by recurrent networks. We show that recurrent neural networks are a type of nonlinear autoregressive-moving average (NARMA) model. Practical ability will be shown in the results of a competition sponsored by the Puget Sound Power and Light Company, where the recurrent networks gave the best performance on electric load forecasting."
            },
            "slug": "Recurrent-Networks-and-NARMA-Modeling-Connor-Atlas",
            "title": {
                "fragments": [],
                "text": "Recurrent Networks and NARMA Modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "It is shown that recurrent neural networks are a type of nonlinear autoregressive-moving average (NARMA) model, which is well modeled by feedforward networks or linear models, but can be modeled by recurrent networks."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144473519"
                        ],
                        "name": "M. Mozer",
                        "slug": "M.-Mozer",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Mozer",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mozer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5355536,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e141d68065ce638f9fc4f006eab2f66711e89768",
            "isKey": false,
            "numCitedBy": 160,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning structure in temporally-extended sequences is a difficult computational problem because only a fraction of the relevant information is available at any instant. Although variants of back propagation can in principle be used to find structure in sequences, in practice they are not sufficiently powerful to discover arbitrary contingencies, especially those spanning long temporal intervals or involving high order statistics. For example, in designing a connectionist network for music composition, we have encountered the problem that the net is able to learn musical structure that occurs locally in time--e.g., relations among notes within a musical phrase--but not structure that occurs over longer time periods--e.g., relations among phrases. To address this problem, we require a means of constructing a reduced description of the sequence that makes global aspects more explicit or more readily detectable. I propose to achieve this using hidden units that operate with different time constants. Simulation experiments indicate that slower time-scale hidden units are able to pick up global structure, structure that simply can not be learned by standard back propagation."
            },
            "slug": "Induction-of-Multiscale-Temporal-Structure-Mozer",
            "title": {
                "fragments": [],
                "text": "Induction of Multiscale Temporal Structure"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Simulation experiments indicate that slower time-scale hidden units are able to pick up global structure, structure that simply can not be learned by standard back propagation, using hidden units that operate with different time constants."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3327108"
                        ],
                        "name": "B. D. Vries",
                        "slug": "B.-D.-Vries",
                        "structuredName": {
                            "firstName": "Bert",
                            "lastName": "Vries",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. D. Vries"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143961030"
                        ],
                        "name": "J. Pr\u00edncipe",
                        "slug": "J.-Pr\u00edncipe",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Pr\u00edncipe",
                            "middleNames": [
                                "Carlos"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pr\u00edncipe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 958138,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a00986a3a7385020d9c50cdb76cb6a6b106b217f",
            "isKey": false,
            "numCitedBy": 276,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-gamma-model--A-new-neural-model-for-temporal-Vries-Pr\u00edncipe",
            "title": {
                "fragments": [],
                "text": "The gamma model--A new neural model for temporal processing"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2884373"
                        ],
                        "name": "J. Elman",
                        "slug": "J.-Elman",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Elman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Elman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2763403,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "668087f0ae7ce1de6e0bd0965dbb480c08103260",
            "isKey": false,
            "numCitedBy": 9858,
            "numCiting": 111,
            "paperAbstract": {
                "fragments": [],
                "text": "Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction."
            },
            "slug": "Finding-Structure-in-Time-Elman",
            "title": {
                "fragments": [],
                "text": "Finding Structure in Time"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory and suggests a method for representing lexical categories and the type/token distinction is developed."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35216199"
                        ],
                        "name": "B. Horne",
                        "slug": "B.-Horne",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Horne",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Horne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157784"
                        ],
                        "name": "C. Lee Giles",
                        "slug": "C.-Lee-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee Giles"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 266
                            }
                        ],
                        "text": "Furthermore, we have previously reported that gradient-descent learning is more e ective in NARX networks than in recurrent neural network architectures with \\hidden states\" when applied to problems including grammatical inference and nonlinear system identi cation [11, 15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 285,
                                "start": 277
                            }
                        ],
                        "text": "This has been observed previously, in the sense that gradient-descent learning appeared to be more e ective in NARX networks than in recurrent neural network architectures that have \\hidden states\" on problems including grammatical inference and nonlinear system identi cation [11, 15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2529208,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f481b68ad71a118afb6e33a7cf4c59f5c6948c5",
            "isKey": false,
            "numCitedBy": 126,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Many different discrete-time recurrent neural network architectures have been proposed. However, there has been virtually no effort to compare these architectures experimentally. In this paper we review and categorize many of these architectures and compare how they perform on various classes of simple problems including grammatical inference and nonlinear system identification."
            },
            "slug": "An-experimental-comparison-of-recurrent-neural-Horne-Giles",
            "title": {
                "fragments": [],
                "text": "An experimental comparison of recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This paper reviews and categorizes many of these discrete-time recurrent neural network architectures and compares how they perform on various classes of simple problems including grammatical inference and nonlinear system identification."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2169620365"
                        ],
                        "name": "Si-Zhao Qin",
                        "slug": "Si-Zhao-Qin",
                        "structuredName": {
                            "firstName": "Si-Zhao",
                            "lastName": "Qin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Si-Zhao Qin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2192631"
                        ],
                        "name": "Hong-Te Su",
                        "slug": "Hong-Te-Su",
                        "structuredName": {
                            "firstName": "Hong-Te",
                            "lastName": "Su",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hong-Te Su"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47621280"
                        ],
                        "name": "T. McAvoy",
                        "slug": "T.-McAvoy",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "McAvoy",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. McAvoy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 37391678,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "59f884480d293672213ca315beec332943b64434",
            "isKey": false,
            "numCitedBy": 201,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Four types of neural net learning rules are discussed for dynamic system identification. It is shown that the feedforward network (FFN) pattern learning rule is a first-order approximation of the FFN-batch learning rule. As a result, pattern learning is valid for nonlinear activation networks provided the learning rate is small. For recurrent types of networks (RecNs), RecN-pattern learning is different from RecN-batch learning. However, the difference can be controlled by using small learning rates. While RecN-batch learning is strict in a mathematical sense, RecN-pattern learning is simple to implement and can be implemented in a real-time manner. Simulation results agree very well with the theorems derived. It is shown by simulation that for system identification problems, recurrent networks are less sensitive to noise."
            },
            "slug": "Comparison-of-four-neural-net-learning-methods-for-Qin-Su",
            "title": {
                "fragments": [],
                "text": "Comparison of four neural net learning methods for dynamic system identification"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "It is shown that the feedforward network (FFN) pattern learning rule is a first-order approximation of the FFN-batch learning rule, and is valid for nonlinear activation networks provided the learning rate is small."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2058200225"
                        ],
                        "name": "R. Leighton",
                        "slug": "R.-Leighton",
                        "structuredName": {
                            "firstName": "Russell",
                            "lastName": "Leighton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Leighton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52081124"
                        ],
                        "name": "B. C. Conrath",
                        "slug": "B.-C.-Conrath",
                        "structuredName": {
                            "firstName": "Bartley",
                            "lastName": "Conrath",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. C. Conrath"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 78
                            }
                        ],
                        "text": "In particular we hypothesize that any network that uses tapped delay feedback [1, 18] would demonstrate improved performance on problems involving long{term dependencies."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61372790,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e0ce3be924f955db5c1e3931fcdc7511e58518d",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Describes an extension to error backpropagation that allows the nodes in a neural network to encode state information in an autoregressive 'memory'. This neural model gives such networks the ability to learn to recognize sequences and context-sensitive patterns. Building upon the work of A. Wieland (1990) concerning nodes with a single feedback connection, the authors generalize the method to n feedback connections and address stability issues. The learning algorithm is derived, and a few applications are presented.<<ETX>>"
            },
            "slug": "The-autoregressive-backpropagation-algorithm-Leighton-Conrath",
            "title": {
                "fragments": [],
                "text": "The autoregressive backpropagation algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "An extension to error backpropagation that allows the nodes in a neural network to encode state information in an autoregressive 'memory' gives such networks the ability to learn to recognize sequences and context-sensitive patterns."
            },
            "venue": {
                "fragments": [],
                "text": "IJCNN-91-Seattle International Joint Conference on Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2392662"
                        ],
                        "name": "O. Nerrand",
                        "slug": "O.-Nerrand",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Nerrand",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Nerrand"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399659418"
                        ],
                        "name": "P. Roussel-Ragot",
                        "slug": "P.-Roussel-Ragot",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Roussel-Ragot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Roussel-Ragot"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3078169"
                        ],
                        "name": "L. Personnaz",
                        "slug": "L.-Personnaz",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Personnaz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Personnaz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144097910"
                        ],
                        "name": "G. Dreyfus",
                        "slug": "G.-Dreyfus",
                        "structuredName": {
                            "firstName": "G\u00e9rard",
                            "lastName": "Dreyfus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Dreyfus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060680777"
                        ],
                        "name": "S. Marcos",
                        "slug": "S.-Marcos",
                        "structuredName": {
                            "firstName": "Sylvie",
                            "lastName": "Marcos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Marcos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1621201,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "065531e12839d10b01b582ff9428d538ceaa6af1",
            "isKey": false,
            "numCitedBy": 199,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper proposes a general framework that encompasses the training of neural networks and the adaptation of filters. We show that neural networks can be considered as general nonlinear filters that can be trained adaptively, that is, that can undergo continual training with a possibly infinite number of time-ordered examples. We introduce the canonical form of a neural network. This canonical form permits a unified presentation of network architectures and of gradient-based training algorithms for both feedforward networks (transversal filters) and feedback networks (recursive filters). We show that several algorithms used classically in linear adaptive filtering, and some algorithms suggested by other authors for training neural networks, are special cases in a general classification of training algorithms for feedback networks."
            },
            "slug": "Neural-Networks-and-Nonlinear-Adaptive-Filtering:-Nerrand-Roussel-Ragot",
            "title": {
                "fragments": [],
                "text": "Neural Networks and Nonlinear Adaptive Filtering: Unifying Concepts and New Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that neural networks can be considered as general nonlinear filters that can be trained adaptively, that is, that can undergo continual training with a possibly infinite number of time-ordered examples."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144288586"
                        ],
                        "name": "A. Back",
                        "slug": "A.-Back",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Back",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Back"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733691"
                        ],
                        "name": "A. Tsoi",
                        "slug": "A.-Tsoi",
                        "structuredName": {
                            "firstName": "Ah",
                            "lastName": "Tsoi",
                            "middleNames": [
                                "Chung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tsoi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 78
                            }
                        ],
                        "text": "In particular we hypothesize that any network that uses tapped delay feedback [1, 18] would demonstrate improved performance on problems involving long{term dependencies."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7792757,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "c9282390b4ebe10861d54a1066d5f3907a54608a",
            "isKey": false,
            "numCitedBy": 236,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "A new neural network architecture involving either local feedforward global feedforward, and/or local recurrent global feedforward structure is proposed. A learning rule minimizing a mean square error criterion is derived. The performance of this algorithm (local recurrent global feedforward architecture) is compared with a local-feedforward global-feedforward architecture. It is shown that the local-recurrent global-feedforward model performs better than the local-feedforward global-feedforward model."
            },
            "slug": "FIR-and-IIR-Synapses,-a-New-Neural-Network-for-Time-Back-Tsoi",
            "title": {
                "fragments": [],
                "text": "FIR and IIR Synapses, a New Neural Network Architecture for Time Series Modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "It is shown that the local-recurrent global-feedforward model performs better than the local/local recurrent global feedforward model and the learning rule minimizing a mean square error criterion is derived."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688235"
                        ],
                        "name": "P. Frasconi",
                        "slug": "P.-Frasconi",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Frasconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frasconi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467467"
                        ],
                        "name": "M. Gori",
                        "slug": "M.-Gori",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Gori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35251916"
                        ],
                        "name": "Marco Maggini",
                        "slug": "Marco-Maggini",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Maggini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Maggini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2540925"
                        ],
                        "name": "G. Soda",
                        "slug": "G.-Soda",
                        "structuredName": {
                            "firstName": "Giovanni",
                            "lastName": "Soda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Soda"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14979740,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bc948468c35d7201260c73cad05d14a3fb17e4da",
            "isKey": false,
            "numCitedBy": 93,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Proposes a novel unified approach for integrating explicit knowledge and learning by example in recurrent networks. The explicit knowledge is represented by automaton rules, which are directly injected into the connections of a network. This can be accomplished by using a technique based on linear programming, instead of learning from random initial weights. Learning is conceived as a refinement process and is mainly responsible for uncertain information management. We present preliminary results for problems of automatic speech recognition. >"
            },
            "slug": "Unified-Integration-of-Explicit-Knowledge-and-by-in-Frasconi-Gori",
            "title": {
                "fragments": [],
                "text": "Unified Integration of Explicit Knowledge and Learning by Example in Recurrent Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A novel unified approach for integrating explicit knowledge and learning by example in recurrent networks is proposed, which is accomplished by using a technique based on linear programming, instead of learning from random initial weights."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Knowl. Data Eng."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2192631"
                        ],
                        "name": "Hong-Te Su",
                        "slug": "Hong-Te-Su",
                        "structuredName": {
                            "firstName": "Hong-Te",
                            "lastName": "Su",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hong-Te Su"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47621280"
                        ],
                        "name": "T. McAvoy",
                        "slug": "T.-McAvoy",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "McAvoy",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. McAvoy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 26378932,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f7b3c8c5e9c9f1e610187f3c6d886278c6b155b",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Neurl networks have been widely used in many research areas including nonlinear system identification. In the present study, a recurrent neural network, as an alternative to feed-forward networks, has been used successfully to identify the dynamic behavior of a biological wastewater treatment plant. An approach to deriving the learning algorithm for recurrent networks is discussed. In comparison to a feed-forward network, the recurrent network produces superior results for long-term predictions."
            },
            "slug": "Identification-of-Chemical-Processes-using-Networks-Su-McAvoy",
            "title": {
                "fragments": [],
                "text": "Identification of Chemical Processes using Recurrent Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A recurrent neural network is used as an alternative to feed-forward networks to identify the dynamic behavior of a biological wastewater treatment plant and an approach to deriving the learning algorithm for recurrent networks is discussed."
            },
            "venue": {
                "fragments": [],
                "text": "1991 American Control Conference"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688235"
                        ],
                        "name": "P. Frasconi",
                        "slug": "P.-Frasconi",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Frasconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frasconi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467467"
                        ],
                        "name": "M. Gori",
                        "slug": "M.-Gori",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Gori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2540925"
                        ],
                        "name": "G. Soda",
                        "slug": "G.-Soda",
                        "structuredName": {
                            "firstName": "Giovanni",
                            "lastName": "Soda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Soda"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 79
                            }
                        ],
                        "text": "This e ect is called the problem of vanishing gradient, or forgetting behavior [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 84
                            }
                        ],
                        "text": "It may also be possible to obtain similar results for the architectures proposed in [6, 9, 24, 37]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 33575306,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9a607334c1d963b4af29676578e1ef6aa11ba6e7",
            "isKey": false,
            "numCitedBy": 202,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we investigate the capabilities of local feedback multilayered networks, a particular class of recurrent networks, in which feedback connections are only allowed from neurons to themselves. In this class, learning can be accomplished by an algorithm that is local in both space and time. We describe the limits and properties of these networks and give some insights on their use for solving practical problems."
            },
            "slug": "Local-Feedback-Multilayered-Networks-Frasconi-Gori",
            "title": {
                "fragments": [],
                "text": "Local Feedback Multilayered Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "The capabilities of local feedback multilayered networks, a particular class of recurrent networks, in which feedback connections are only allowed from neurons to themselves are investigated."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2797623"
                        ],
                        "name": "H. Siegelmann",
                        "slug": "H.-Siegelmann",
                        "structuredName": {
                            "firstName": "Hava",
                            "lastName": "Siegelmann",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Siegelmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790264"
                        ],
                        "name": "Eduardo Sontag",
                        "slug": "Eduardo-Sontag",
                        "structuredName": {
                            "firstName": "Eduardo",
                            "lastName": "Sontag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eduardo Sontag"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 44597102,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a49498e51840165d55b6badd4b52e34d17860bc0",
            "isKey": false,
            "numCitedBy": 834,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper deals with finite networks which consist of interconnections of synchronously evolving processors. Each processor updates its state by applying a \u201csigmoidal\u201d scalar nonlinearity to a linear combination of the previous states of all units. We prove that one may simulate all Turing Machines by rational nets. In particular, one can do this in linear time, and there is a net made up of about 1,000 processors which computes a universal partial-recursive function. Products (high order nets) are not required, contrary to what had been stated in the literature. Furthermore, we assert a similar theorem about non-deterministic Turing Machines. Consequences for undecidability and complexity issues about nets are discussed too."
            },
            "slug": "On-the-computational-power-of-neural-nets-Siegelmann-Sontag",
            "title": {
                "fragments": [],
                "text": "On the Computational Power of Neural Nets"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is proved that one may simulate all Turing Machines by rational nets in linear time, and there is a net made up of about 1,000 processors which computes a universal partial-recursive function."
            },
            "venue": {
                "fragments": [],
                "text": "J. Comput. Syst. Sci."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48385057"
                        ],
                        "name": "E. Wan",
                        "slug": "E.-Wan",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Wan",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Wan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 84
                            }
                        ],
                        "text": "It may also be possible to obtain similar results for the architectures proposed in [6, 9, 24, 37]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 12652643,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "667c2b372d3387011510a21cc7e0b267e36259dd",
            "isKey": false,
            "numCitedBy": 162,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "A neural network architecture, which models synapses as Finite Impulse Response (FIR) linear lters, is discussed for use in time series prediction. Analysis and methodology are detailed in the context of the Santa Fe Institute Time Series Prediction Competition. Results of the competition show that the FIR network performed remarkably well on a chaotic laser intensity time series."
            },
            "slug": "Time-series-prediction-by-using-a-connectionist-Wan",
            "title": {
                "fragments": [],
                "text": "Time series prediction by using a connectionist network with internal delay lines"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A neural network architecture, which models synapses as Finite Impulse Response (FIR) linear lters, is discussed for use in time series prediction and results show that the FIR network performed remarkably well on a chaotic laser intensity time series."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790264"
                        ],
                        "name": "Eduardo Sontag",
                        "slug": "Eduardo-Sontag",
                        "structuredName": {
                            "firstName": "Eduardo",
                            "lastName": "Sontag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eduardo Sontag"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 114
                            }
                        ],
                        "text": "1 Introduction Recurrent Neural Networks (RNNs) are capable of representing arbitrary nonlinear dynamical systems [30, 32, 33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 125270141,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8b88440b5728e4599a9c9e9622e28011cd4edd5a",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Systems-Combining-Linearity-and-Saturations,-and-of-Sontag",
            "title": {
                "fragments": [],
                "text": "Systems Combining Linearity and Saturations, and Relations of \u201cNeural Nets\u201d"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52129535"
                        ],
                        "name": "I. J. Leontaritis",
                        "slug": "I.-J.-Leontaritis",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Leontaritis",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. J. Leontaritis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719377"
                        ],
                        "name": "S. Billings",
                        "slug": "S.-Billings",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Billings",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Billings"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 119817033,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "dae7ab44d1a2c72c0c65910837f0bfeda41f6904",
            "isKey": false,
            "numCitedBy": 1161,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Recursive input-output models for non-linear multivariate discrete-time systems are derived, and sufficient conditions for their existence are defined. The paper is divided into two parts. The first part introduces and defines concepts such as Nerode realization, multistructural forms and results from differential geometry which are then used to derive a recursive input-output model for multivariable deterministic non-linear systems. The second part introduces several examples, compares the derived model with other representations and extends the results to create prediction error or innovation input-output models for non-linear stochastic systems. These latter models are the generalization of the multivariable ARM AX models for linear systems and are referred to as NARMAX or Non-linear AutoRegressive Moving Average models with exogenous inputs."
            },
            "slug": "Input-output-parametric-models-for-non-linear-Part-Leontaritis-Billings",
            "title": {
                "fragments": [],
                "text": "Input-output parametric models for non-linear systems Part II: stochastic non-linear systems"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Recursive input-output models for non-linear multivariate discrete-time systems are derived, and sufficient conditions for their existence are defined."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144473519"
                        ],
                        "name": "M. Mozer",
                        "slug": "M.-Mozer",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Mozer",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mozer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6037212,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f53d2b9b8fa9bc2407475ea251aa9d21aac22bd7",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Algorithmic music composition involves the use of rules to generate melodies. One simple but interesting technique is to select notes sequentially according to a transition table that specifies the probability of the next note as a function of the previous context. We describe an extension of this transition table approach using a recurrent connectionist network called CONCERT. CONCERT is trained on a set of melodies written in a certain style and then is able to compose new melodies in the same style. A central ingredient of CONCERT is the incorporation of psychologically-grounded representations of pitch, duration, and harmony. CONCERT was tested on sets of examples artificially generated according to simple rules and was shown to learn the underlying structure, even where other approaches failed. In larger experiments, CONCERT was trained on sets of J. S. Bach pieces, traditional European folk melodies, and waltzes, and was then allowed to compose novel melodies. Although the compositions are surprisingly pleasant, CONCERT has difficulty capturing the global structure of a composition. We describe an improved algorithm that is better able to induce temporal structure at multiple scales."
            },
            "slug": "Neural-network-music-composition-and-the-induction-Mozer",
            "title": {
                "fragments": [],
                "text": "Neural network music composition and the induction of multiscale temporal structure"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An improved algorithm is described that is better able to induce temporal structure at multiple scales and trained on a set of melodies written in a certain style and then is able to compose new melodies in the same style."
            },
            "venue": {
                "fragments": [],
                "text": "Wissensbasierte Systeme"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118528593"
                        ],
                        "name": "Sheng Chen",
                        "slug": "Sheng-Chen",
                        "structuredName": {
                            "firstName": "Sheng",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sheng Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719377"
                        ],
                        "name": "S. Billings",
                        "slug": "S.-Billings",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Billings",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Billings"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51283223"
                        ],
                        "name": "P. Grant",
                        "slug": "P.-Grant",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Grant",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Grant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2684962,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b235711b35468bed92c4ea5c089221f81abd962",
            "isKey": false,
            "numCitedBy": 960,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Multi-layered neural networks offer an exciting alternative for modelling complex non-linear systems. This paper investigates the identification of discrete-time nonlinear systems using neural networks with a single hidden layer. New parameter estimation algorithms are derived for the neural network model based on a prediction error formulation and the application to both simulated and real data is included to demonstrate the effectiveness of the neural network approach."
            },
            "slug": "Non-linear-system-identification-using-neural-Chen-Billings",
            "title": {
                "fragments": [],
                "text": "Non-linear system identification using neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This paper investigates the identification of discrete-time nonlinear systems using neural networks with a single hidden layer using new parameter estimation algorithms derived for the neural network model based on a prediction error formulation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2429288"
                        ],
                        "name": "M. Garzon",
                        "slug": "M.-Garzon",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Garzon",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Garzon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143814533"
                        ],
                        "name": "F. Botelho",
                        "slug": "F.-Botelho",
                        "structuredName": {
                            "firstName": "Fernanda",
                            "lastName": "Botelho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Botelho"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6191341,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5d5c69c7180411ce01bc9168281428094ae95e9c",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We prove that except possibly for small exceptional sets, discrete-time analog neural nets are globally observable, i.e. all their corrupted pseudo-orbits on computer simulations actually reflect the true dynamical behavior of the network. Locally finite discrete (boolean) neural networks are observable without exception."
            },
            "slug": "Observability-of-Neural-Network-Behavior-Garzon-Botelho",
            "title": {
                "fragments": [],
                "text": "Observability of Neural Network Behavior"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "It is proved that except possibly for small exceptional sets, discrete-time analog neural nets are globally observable, i.e. all their corrupted pseudo-orbits on computer simulations actually reflect the true dynamical behavior of the network."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1456777230"
                        ],
                        "name": "E. Rios-Patron",
                        "slug": "E.-Rios-Patron",
                        "structuredName": {
                            "firstName": "Ernesto",
                            "lastName": "Rios-Patron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Rios-Patron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144960825"
                        ],
                        "name": "R. Braatz",
                        "slug": "R.-Braatz",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Braatz",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Braatz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7259480,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "10d3578f425f42c6115c69f8404fc720fb76c305",
            "isKey": false,
            "numCitedBy": 1821,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Referring to the above said paper by Narendra-Parthasarathy (ibid., vol.1, p4-27 (1990)), it is noted that the given Example 2 (p.15) has a third equilibrium state corresponding to the point (0.5, 0.5)."
            },
            "slug": "On-the-\"Identification-and-control-of-dynamical-Rios-Patron-Braatz",
            "title": {
                "fragments": [],
                "text": "On the \"Identification and control of dynamical systems using neural networks\""
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "BPTTinvolves two phases: unfolding the network in time and backpropagating the error through theunfolded network."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "The network was trained using a simple BPTT algorithmwith a learning rate 0:01 for a maximumof 200 epochs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "The network was run using a simple BPTT algorithm with a learning rate of 0:1 for a maximumof 100 epochs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 462,
                                "start": 454
                            }
                        ],
                        "text": "where is a learning rate, and rw is the row vector operator rw = @ @w1 @ @w2 : : : @ @wn : (5) By using the Chain Rule, the gradient can be expanded rwC =Xp (yp(T ) dp)0 rx(T )yp(T )rwx(T ) : (6) We can expand this further by assuming that the weights at di erent time indices are independent and computing the partial gradient with respect to these weights, which is the methodology used to derive algorithms such as Backpropagation Through Time (BPTT) [27, 38]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 653,
                                "start": 649
                            }
                        ],
                        "text": "Gradient-descent is an algorithm which iteratively updates the weights in proportion tothe gradient w = rwC : (4)1We deal only with problems in which the target output is presented at the end of the sequence.4\nwhere is a learning rate, and rw is the row vector operatorrw = @@w1 @@w2 : : : @@wn : (5)By using the Chain Rule, the gradient can be expandedrwC =Xp (yp(T ) dp)0 rx(T )yp(T )rwx(T ) : (6)We can expand this further by assuming that the weights at di erent time indices are independentand computing the partial gradient with respect to these weights, which is the methodology used toderive algorithms such as Backpropagation Through Time (BPTT) [27, 38]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 62245742,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "111fd833a4ae576cfdbb27d87d2f8fc0640af355",
            "isKey": true,
            "numCitedBy": 19355,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-internal-representations-by-error-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning internal representations by error propagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2829148"
                        ],
                        "name": "E. Coven",
                        "slug": "E.-Coven",
                        "structuredName": {
                            "firstName": "Ethan",
                            "lastName": "Coven",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Coven"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102406355"
                        ],
                        "name": "I. Kan",
                        "slug": "I.-Kan",
                        "structuredName": {
                            "firstName": "Ittai",
                            "lastName": "Kan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Kan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9861772"
                        ],
                        "name": "J. Yorke",
                        "slug": "J.-Yorke",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Yorke",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Yorke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 54001296,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ddb05c875af8752c29482645d5e9a6446d8a86df",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "On montre que les applications en tente ont la propriete d'ombrage pour presque tous les parametres s, mais elles n'ont pas cette propriete pour un ensemble dense non denombrable des parametres. On montre que pour une application tente, chaque pseudo-orbite peut etre approchee par une orbite actuelle d'une application en tente avec une boucle peut etre legerement plus grande"
            },
            "slug": "Pseudo-orbit-shadowing-in-the-family-of-tent-maps-Coven-Kan",
            "title": {
                "fragments": [],
                "text": "Pseudo-orbit shadowing in the family of tent maps"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 93
                            }
                        ],
                        "text": "1 The latching problem We explored a slight modi cation on the latching problem described in [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[2], who showed that if a system is to latch information robustly, then the fraction of the gradient due to information n time steps in the past approaches zero as n becomes large."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[2] have analytically explained why learning problems with long{term dependencies is di cult."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 109
                            }
                        ],
                        "text": "For example, gradient{based methods can be abandoned completely in favor of alternative optimization methods [2, 25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 187
                            }
                        ],
                        "text": "However, the algorithms investigated so far either perform just as poorly on problems involving long{term dependencies, or, when they are better, require far more computational resources [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 77
                            }
                        ],
                        "text": "In this section we brie y describe some of the key aspects of the results in [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 276,
                                "start": 273
                            }
                        ],
                        "text": "Another possibility is to modify conventional gradient-descent by more heavily weighing the fraction of the gradient due to information far in the past, but there is no guarantee that such a modi ed algorithm would converge to a minimum of the error surface being searched [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 85
                            }
                        ],
                        "text": "Here we give one such example, which is based upon the latching problem described in [2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[2] showed that if the network satis es their de nition of robustly latching information, i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning long-term dependencies with gradient is  di cult"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Neural Networks, 5(2):157{166,"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2797623"
                        ],
                        "name": "H. Siegelmann",
                        "slug": "H.-Siegelmann",
                        "structuredName": {
                            "firstName": "Hava",
                            "lastName": "Siegelmann",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Siegelmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790264"
                        ],
                        "name": "Eduardo Sontag",
                        "slug": "Eduardo-Sontag",
                        "structuredName": {
                            "firstName": "Eduardo",
                            "lastName": "Sontag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eduardo Sontag"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 114
                            }
                        ],
                        "text": "1 Introduction Recurrent Neural Networks (RNNs) are capable of representing arbitrary nonlinear dynamical systems [30, 32, 33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 116726833,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79914a9898941527c5a3f4d1d456b733a029699d",
            "isKey": false,
            "numCitedBy": 87,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Computational-power-of-neural-networks-Siegelmann-Sontag",
            "title": {
                "fragments": [],
                "text": "Computational power of neural networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720911"
                        ],
                        "name": "T. Kailath",
                        "slug": "T.-Kailath",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Kailath",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kailath"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 86
                            }
                        ],
                        "text": "From a system perspective, it is preferrable to put equations into a state space form [16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 125373463,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a4491e8ceb8b713baa15d348bed2d5383f163233",
            "isKey": false,
            "numCitedBy": 5721,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Linear-Systems-Kailath",
            "title": {
                "fragments": [],
                "text": "Linear Systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895771"
                        ],
                        "name": "D. Zipser",
                        "slug": "D.-Zipser",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zipser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zipser"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14792754,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10dae7fca6b65b61d155a622f0c6ca2bc3922251",
            "isKey": false,
            "numCitedBy": 567,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Gradient-based-learning-algorithms-for-recurrent-Williams-Zipser",
            "title": {
                "fragments": [],
                "text": "Gradient-based learning algorithms for recurrent networks and their computational complexity"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Uniied integration of explicit rules and learning by example in recurrent networks"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Knowledge and Data Engineering"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Identiication and control of dynamical systems using neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On encoding nonlinear oscillations in neural networks for locomotion"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. 8th Yale Wkshp. Adaptive Learning Syst.s"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 139
                            }
                        ],
                        "text": "Finally, it has been suggested that a network architecture that operates on multiple time scales might be useful for tackling this problem [12, 13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Scheduling of modular architectures for inductive infer-  ence of regular grammars"
            },
            "venue": {
                "fragments": [],
                "text": "ECAI'94 Workshop on Combining Symbolic and Connectionist  Processing, Amsterdam, pages 78{87. Wiley, August"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 173
                            }
                        ],
                        "text": "We focus on a class of architectures based upon Nonlinear AutoRegressive models with eXogenous inputs (NARX models), and are therefore called NARX recurrent neural networks [3, 22]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 366,
                                "start": 355
                            }
                        ],
                        "text": "It has been demonstrated that they are well suited for modeling nonlinear systems such as heat exchangers [3], waste water treatment plants [34, 35], catalytic reforming systems in a petroleum re nery [35], nonlinear oscillations associated with multi{legged locomotion in biological systems [36], time series [4], and various arti cial nonlinear systems [3, 22, 26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 131
                            }
                        ],
                        "text": "When the function f can be approximated by a Multilayer Perceptron, the resulting system is called a NARX recurrent neural network [3, 22]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Identi cation and control of dynamical systems using  neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Neural Networks, 1:4{27, March"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning long - term dependencies with gradient is difticult"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Truns . Neurul Networks"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Long short term memory. Forschungsberichte  K\u007f  unstliche Intelligenz FKI-207-95, Institut f\u007f  ur Informatik, Technische Universit\u007f  at M\u007f"
            },
            "venue": {
                "fragments": [],
                "text": "unchen,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Observability of neural-network behavior Representation and learning in recurrent neural-network architectures"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Injormation Processing Systems 6 Proc. 8th Yale Wkshp. Adaptive Learning Sysr"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Long short term memory. Forschungsberichte K unstliche Intelligenz FKI-207-95, Institut f ur Informatik, Technische Universitt at M unchen"
            },
            "venue": {
                "fragments": [],
                "text": "Long short term memory. Forschungsberichte K unstliche Intelligenz FKI-207-95, Institut f ur Informatik, Technische Universitt at M unchen"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 296,
                                "start": 292
                            }
                        ],
                        "text": "It has been demonstrated that they are well suited for modeling nonlinear systems such as heat exchangers [3], waste water treatment plants [34, 35], catalytic reforming systems in a petroleum re nery [35], nonlinear oscillations associated with multi{legged locomotion in biological systems [36], time series [4], and various arti cial nonlinear systems [3, 22, 26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On encoding nonlinear oscillations in neural networks for locomotion"
            },
            "venue": {
                "fragments": [],
                "text": " Proceedings of the Eighth Yale Workshop on Adaptive and Learning Systems, pages 14{20,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "System identiication : Theory for the user"
            },
            "venue": {
                "fragments": [],
                "text": "System identiication : Theory for the user"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Syslenz Identijication: Theoryfur the User"
            },
            "venue": {
                "fragments": [],
                "text": "Syslenz Identijication: Theoryfur the User"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 266
                            }
                        ],
                        "text": "Furthermore, we have previously reported that gradient-descent learning is more e ective in NARX networks than in recurrent neural network architectures with \\hidden states\" when applied to problems including grammatical inference and nonlinear system identi cation [11, 15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 285,
                                "start": 277
                            }
                        ],
                        "text": "This has been observed previously, in the sense that gradient-descent learning appeared to be more e ective in NARX networks than in recurrent neural network architectures that have \\hidden states\" on problems including grammatical inference and nonlinear system identi cation [11, 15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Representation and learning in recurrent neural network architec-  tures"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Eigth Yale Workshop on Adaptive and Learning Systems, pages  128{134,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Long short-term memory Furschungsherichte Kiinstliche Intelligenz FKI-207-95, Institut fur Informatik An experimental compa neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems 7"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 138
                            }
                        ],
                        "text": "3 NARX networks An important class of discrete{time nonlinear systems is the Nonlinear AutoRegressive with eXogenous inputs (NARX) model2 [3, 19, 20, 34, 35]: y(t) = f u(t Du); : : : ; u(t 1); u(t); y(t Dy); : : : ; y(t 1) ; (9) where u(t) and y(t) represent input and output of the network at time t, Du andDy are the input and output order, and the function f is a nonlinear function."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "System identi cation : Theory for the user"
            },
            "venue": {
                "fragments": [],
                "text": "Prentice-Hall, Englewood Cli s, NJ, "
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Unified integration of explicit rules and learning by example in recurrent networks"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans . Knowledge Data Eng ."
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Time series prediction by using a connectionist network with internal delay lines , \u201d in Time Serie . r Prediction , A . S . Weigend and"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Representation and learning in recurrent neural network architectures"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Eigth Yale Workshop on Adaptive and Learning Systems"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Unified integration of explicit rules and learning by example in recurrent networks"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Knowledge Data Eng"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The autoregressive backpropagation algorithm Input-output parametric models for nonlinear systems: Part I: Deterministic nonlinear systems"
            },
            "venue": {
                "fragments": [],
                "text": "Pmc. Int. Joint Con$ Neural Networks"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 16
                            }
                        ],
                        "text": "It can be shown [8] that the network is robust to perturbations in the range [ 0:155; 0:155]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Uni ed integration of explicit rules and learning  by example in recurrent networks"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Knowledge and Data Engineering,  7(2):340{346,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Scheduling of modular architectures for inductive inference of regular grammars"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. ECAI'94 Wkshp. Combining Symbolic Connecrionist Procedures"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning long-term dependencies with gradient is diicult"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "tures for inductive inference of regular grammars,"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. ECAI\u201994 Wkshp. Combining Symbolic Connecrionist Procedures,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Comparison of four neuralnet learning methods for dynamic system identification"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neurul Networks"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recurrent networks and NARMA modeling , \u201d in Advances in Neurul Infiormatiou Processing Systems 4 ,"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 138
                            }
                        ],
                        "text": "3 NARX networks An important class of discrete{time nonlinear systems is the Nonlinear AutoRegressive with eXogenous inputs (NARX) model2 [3, 19, 20, 34, 35]: y(t) = f u(t Du); : : : ; u(t 1); u(t); y(t Dy); : : : ; y(t 1) ; (9) where u(t) and y(t) represent input and output of the network at time t, Du andDy are the input and output order, and the function f is a nonlinear function."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Input{output parametric models for non{linear systems: Part  I: deterministic non{linear systems"
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Control, 41(2):303{328,"
            },
            "year": 1985
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 18,
            "methodology": 4,
            "result": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 63,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/Learning-long-term-dependencies-in-NARX-recurrent-Lin-Horne/f6e91c9e7e8f8a577a98ecfcfa998212a683195a?sort=total-citations"
}