{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2155703534"
                        ],
                        "name": "Jing Zhang",
                        "slug": "Jing-Zhang",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46772547"
                        ],
                        "name": "Xilin Chen",
                        "slug": "Xilin-Chen",
                        "structuredName": {
                            "firstName": "Xilin",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xilin Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2714754"
                        ],
                        "name": "Andreas Hanneman",
                        "slug": "Andreas-Hanneman",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Hanneman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Hanneman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118579343"
                        ],
                        "name": "Jie Yang",
                        "slug": "Jie-Yang",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "The recent attempts have moved toward automatic detection and recognition of signs from natural scenes [30], [33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7832131,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9b2b1e9755b1ff1fa2df74bd53212e8f0c0113fb",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a robust approach for recognition of text embedded in natural scenes. Instead of using binary information as most other OCR systems do, we extract features from intensity of an image directly. We utilize a local intensity normalization method to effectively handle lighting variations. We then employ Gabor transform to obtain local features, and use the linear discriminant analysis (LDA) for selection and classification of features. The proposed method has been applied to a Chinese sign recognition task. The system can recognize a vocabulary of 3755 level I Chinese characters in the Chinese national standard character set GB2312-80 with various print fonts. We tested the system on 1630 test characters in sign images captured from the natural scenes, and the recognition accuracy was 92.46%. We have integrated the system into our automatic Chinese sign translation system."
            },
            "slug": "A-robust-approach-for-recognition-of-text-embedded-Zhang-Chen",
            "title": {
                "fragments": [],
                "text": "A robust approach for recognition of text embedded in natural scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "A robust approach for recognition of text embedded in natural scenes by utilizing a local intensity normalization method to effectively handle lighting variations and using the linear discriminant analysis (LDA) for selection and classification of features."
            },
            "venue": {
                "fragments": [],
                "text": "Object recognition supported by user interaction for service robots"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2452419"
                        ],
                        "name": "H. Yoshimura",
                        "slug": "H.-Yoshimura",
                        "structuredName": {
                            "firstName": "Hiroshi",
                            "lastName": "Yoshimura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Yoshimura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8268643"
                        ],
                        "name": "M. Etoh",
                        "slug": "M.-Etoh",
                        "structuredName": {
                            "firstName": "Minoru",
                            "lastName": "Etoh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Etoh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71290316"
                        ],
                        "name": "K. Kondo",
                        "slug": "K.-Kondo",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Kondo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kondo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771769"
                        ],
                        "name": "N. Yokoya",
                        "slug": "N.-Yokoya",
                        "structuredName": {
                            "firstName": "Naokazu",
                            "lastName": "Yokoya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Yokoya"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 117
                            }
                        ],
                        "text": "In OCR applications, Gabor wavelet has been applied to binary images [5], [6] and recently applied to a video stream [32]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 34363569,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "630effc5ef9b289e90e624cbfe658dabf77d3ef5",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a gray-scale character recognition method for video indexing. It is robust against the problems of binarization against a complex background and low resolution. Unlike a traditional character recognition scheme through image binarization, we directly extract Gabor features (called Gabor jets) from video contents. The use of the Gabor filters contributes to freeing a tricky binarization process for cluttered images, and furthermore provides localized directional edge features, which have phase-shift invariance to edge positions. To form a feature vector to be classified, we accumulate the extracted Gabor features along projection lines in local regions, and then categorize them with a standard LVQ classifier. The projective accumulation provides robustness under character deformation caused by variation of font types or imprecise segmentation. We compare the proposed method by experiments with a typical OCR method, for which correct binarization is advantageously given. The proposed method shows similar or superior performance to the other method in understanding video captions."
            },
            "slug": "Gray-scale-character-recognition-by-Gabor-jets-Yoshimura-Etoh",
            "title": {
                "fragments": [],
                "text": "Gray-scale character recognition by Gabor jets projection"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "A gray-scale character recognition method for video indexing that directly extracts Gabor features (called Gabor jets) from video contents and provides robustness under character deformation caused by variation of font types or imprecise segmentation."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 15th International Conference on Pattern Recognition. ICPR-2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144117646"
                        ],
                        "name": "Jiang Gao",
                        "slug": "Jiang-Gao",
                        "structuredName": {
                            "firstName": "Jiang",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiang Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118579343"
                        ],
                        "name": "Jie Yang",
                        "slug": "Jie-Yang",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16214442,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "758db8b752a0ec38d8df9dc6d8e81bfdfb7289a1",
            "isKey": false,
            "numCitedBy": 127,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new adaptive algorithm for automatic detection of text from a natural scene. The initial cues of text regions are first detected from the captured image/video. An adaptive color modeling and searching algorithm is then utilized near the initial text cues, to discriminate text/non-text regions. EM optimization algorithm is used for color modeling, under the constraint of text layout relations for a specific language. The proposed algorithm combines the advantages of several previous approaches for text detection, and utilizes a focus-of-attention approach for text finding. The whole algorithm is applied in a prototype system that can automatically detect and recognize sign input from a video camera, and translate the signs into English text or voice streams. We present evaluation results of our algorithm on this system."
            },
            "slug": "An-adaptive-algorithm-for-text-detection-from-Gao-Yang",
            "title": {
                "fragments": [],
                "text": "An adaptive algorithm for text detection from natural scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A new adaptive algorithm for automatic detection of text from a natural scene that combines the advantages of several previous approaches for text detection, and utilizes a focus-of-attention approach for text finding is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108487560"
                        ],
                        "name": "J. C. Lee",
                        "slug": "J.-C.-Lee",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lee",
                            "middleNames": [
                                "Chung-Mong"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. C. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2257741"
                        ],
                        "name": "A. Kankanhalli",
                        "slug": "A.-Kankanhalli",
                        "structuredName": {
                            "firstName": "Atreyi",
                            "lastName": "Kankanhalli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kankanhalli"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 107
                            }
                        ],
                        "text": "Some other researchers published their efforts on container text detection and recognition later [1], [3], [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11365167,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "07a0d7a8938fa04cad3a1afc28c3dcaf1724bde8",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We have developed a generalized alphanumeric character extraction algorithm that can efficiently and accurately locate and extract characters from complex scene images. A scene image may be complex due to the following reasons: (1) the characters are embedded in an image with other objects, such as structural bars, company logos and smears; (2) the characters may be painted or printed in any color including white, and the background color may differ only slightly from that of the characters; (3) the font, size and format of the characters may be different; and (4) the lighting may be uneven. The main contribution of this research is that it permits the quick and accurate extraction of characters in a complex scene. A coarse search technique is used to locate potential characters, and then a fine grouping technique is used to extract characters accurately. Several additional techniques in the postprocessing phase eliminate spurious as well as overlapping characters. Experimental results of segmenting characters written on cargo container surfaces show that the system is feasible under real-life constraints. The program has been installed as part of a vision system which verifies container codes on vehicles passing through the Port of Singapore."
            },
            "slug": "Automatic-Extraction-of-Characters-in-Complex-Scene-Lee-Kankanhalli",
            "title": {
                "fragments": [],
                "text": "Automatic Extraction of Characters in Complex Scene Images"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A generalized alphanumeric character extraction algorithm that can efficiently and accurately locate and extract characters from complex scene images is developed that permits the quick and accurate extraction of characters in a complex scene."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Pattern Recognit. Artif. Intell."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118579343"
                        ],
                        "name": "Jie Yang",
                        "slug": "Jie-Yang",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46772547"
                        ],
                        "name": "Xilin Chen",
                        "slug": "Xilin-Chen",
                        "structuredName": {
                            "firstName": "Xilin",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xilin Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2155703534"
                        ],
                        "name": "Jing Zhang",
                        "slug": "Jing-Zhang",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48379623"
                        ],
                        "name": "Y. Zhang",
                        "slug": "Y.-Zhang",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 98
                            }
                        ],
                        "text": "We have successfully applied the proposed methods in developing a Chinese sign translation system [31]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17007111,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "32f952bd7e089ce0962cff84a762e489a3e2277d",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Large amounts of information are embedded in natural scenes. Signs are good examples of natural objects with high information content. In this paper, we discuss problems in automatic detection and translation of text from natural scenes. We describe the chal1enges of automatic text detection and propose methods to address these chal1enges. We extend example based machine translation technology for sign translation and present a prototype system for Chinese sign translation. This system is capable of capturing images, automatically detecting and recognizing text, and translating the text into English. The translation can be displayed on a palm size PDA, or synthesized as a voice output message over the earphones."
            },
            "slug": "Automatic-detection-and-translation-of-text-from-Yang-Chen",
            "title": {
                "fragments": [],
                "text": "Automatic detection and translation of text from natural scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper extends example based machine translation technology for sign translation and presents a prototype system for Chinese sign translation capable of capturing images, automatically detecting and recognizing text, and translating the text into English."
            },
            "venue": {
                "fragments": [],
                "text": "2002 IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3314902"
                        ],
                        "name": "Edward K. Wong",
                        "slug": "Edward-K.-Wong",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Wong",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edward K. Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50133585"
                        ],
                        "name": "Minya Chen",
                        "slug": "Minya-Chen",
                        "structuredName": {
                            "firstName": "Minya",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minya Chen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 177
                            }
                        ],
                        "text": "text detection and recognition task, an image sequence provides a lot of useful information that can be used to detect text and enhance the image\u2019s resolution [13], [14], [20], [27]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 44601365,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b84b397a325afb12f768080705f5bb7b719397f0",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We report on the development and implementation of a new algorithm for extracting text in digitized color video. The algorithm operates by locating potential text line segments from horizontal scan lines. Detected text line segments are expanded or combined with text line segments from adjacent scan lines to form larger text blocks, which are then subject to filtering and refinement. Text pixels within text blocks are then found using bi-color clustering and connected-component analysis. Morphological contour smoothing and morphological resolution enhancement algorithms are then applied to the detected binary texts to enhance their visual quality. The implemented algorithm has fast execution time and is effective in detecting text in difficult cases, such as scenes with highly textured background, and scenes with small text. A variety of color images digitized from broadcast television are used to test the algorithm and excellent performance result was obtained."
            },
            "slug": "A-robust-algorithm-for-text-extraction-in-color-Wong-Chen",
            "title": {
                "fragments": [],
                "text": "A robust algorithm for text extraction in color video"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The implemented algorithm has fast execution time and is effective in detecting text in difficult cases, such as scenes with highly textured background, and scenes with small text."
            },
            "venue": {
                "fragments": [],
                "text": "2000 IEEE International Conference on Multimedia and Expo. ICME2000. Proceedings. Latest Advances in the Fast Changing World of Multimedia (Cat. No.00TH8532)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2316043"
                        ],
                        "name": "Qiang Huo",
                        "slug": "Qiang-Huo",
                        "structuredName": {
                            "firstName": "Qiang",
                            "lastName": "Huo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiang Huo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068908988"
                        ],
                        "name": "Yong Ge",
                        "slug": "Yong-Ge",
                        "structuredName": {
                            "firstName": "Yong",
                            "lastName": "Ge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yong Ge"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2418275"
                        ],
                        "name": "Zhi-Dan Feng",
                        "slug": "Zhi-Dan-Feng",
                        "structuredName": {
                            "firstName": "Zhi-Dan",
                            "lastName": "Feng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhi-Dan Feng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14399071,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd356b57ecee905f476796e46570f321639301b9",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We have developed a Chinese OCR engine for machine printed documents. Currently, our OCR engine can support a vocabulary of 6921 characters which include 6707 simplified Chinese characters in GB2312-80, 12 frequently used GBK Chinese characters, 62 alphanumeric characters, 140 punctuation marks and symbols. The supported font styles include Song, Fang Song, Kat, He, Yuan, LiShu, WeiBei, XingKai, etc. The averaged character recognition accuracy is above 99% for newspaper quality documents with a recognition speed of about 250 characters per second on a Pentium III-450 MHz PC yet only consuming less than 2 MB memory. We describe the key technologies we used to construct the above recognizer. Among them, we highlight three key techniques contributing to the high recognition accuracy, namely the use of Gabor features, the use of discriminative feature extraction, and the use of minimum classification error as a criterion for model training."
            },
            "slug": "High-performance-Chinese-OCR-based-on-Gabor-feature-Huo-Ge",
            "title": {
                "fragments": [],
                "text": "High performance Chinese OCR based on Gabor features, discriminative feature extraction and model training"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Three key techniques contributing to the high recognition accuracy are highlighted, namely theuse of Gabor features, the use of discriminative feature extraction, and theUse of minimum classification error as a criterion for model training."
            },
            "venue": {
                "fragments": [],
                "text": "2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.01CH37221)"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708785"
                        ],
                        "name": "J. Ohya",
                        "slug": "J.-Ohya",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Ohya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ohya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2019875"
                        ],
                        "name": "A. Shio",
                        "slug": "A.-Shio",
                        "structuredName": {
                            "firstName": "Akio",
                            "lastName": "Shio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Shio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49052113"
                        ],
                        "name": "S. Akamatsu",
                        "slug": "S.-Akamatsu",
                        "structuredName": {
                            "firstName": "Shigeru",
                            "lastName": "Akamatsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Akamatsu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1565945,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e94d1ff801fce49eea8d8aa51a477b130ca755de",
            "isKey": false,
            "numCitedBy": 263,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "An effective algorithm for character recognition in scene images is studied. Scene images are segmented into regions by an image segmentation method based on adaptive thresholding. Character candidate regions are detected by observing gray-level differences between adjacent regions. To ensure extraction of multisegment characters as well as single-segment characters, character pattern candidates are obtained by associating the detected regions according to their positions and gray levels. A character recognition process selects patterns with high similarities by calculating the similarities between character pattern candidates and the standard patterns in a dictionary and then comparing the similarities to the thresholds. A relaxational approach to determine character patterns updates the similarities by evaluating the interactions between categories of patterns, and finally character patterns and their recognition results are obtained. Highly promising experimental results have been obtained using the method on 100 images involving characters of different sizes and formats under uncontrolled lighting. >"
            },
            "slug": "Recognizing-Characters-in-Scene-Images-Ohya-Shio",
            "title": {
                "fragments": [],
                "text": "Recognizing Characters in Scene Images"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "An effective algorithm for character recognition in scene images is studied and highly promising experimental results have been obtained using the method on 100 images involving characters of different sizes and formats under uncontrolled lighting."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116415943"
                        ],
                        "name": "B. Yu",
                        "slug": "B.-Yu",
                        "structuredName": {
                            "firstName": "Bin",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5196787,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f565f502ad1acb81c5659b051c04683a34ed138f",
            "isKey": false,
            "numCitedBy": 576,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic text location (without character recognition capabilities) deals with extracting image regions that contain text only. The images of these regions can then be fed to an optical character recognition module or highlighted for users. This is very useful in a number of applications such as database indexing and converting paper documents to their electronic versions. The performance of our automatic text location algorithm is shown in several applications. Compared with some traditional text location methods, our method has the following advantages: 1) low computational cost; 2) robust to font size; and 3) high accuracy."
            },
            "slug": "Automatic-text-location-in-images-and-video-frames-Jain-Yu",
            "title": {
                "fragments": [],
                "text": "Automatic text location in images and video frames"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Compared with some traditional text location methods, this method has the following advantages: 1) low computational cost; 2) robust to font size; and 3) high accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720965"
                        ],
                        "name": "Jeremiah D. Deng",
                        "slug": "Jeremiah-D.-Deng",
                        "structuredName": {
                            "firstName": "Jeremiah",
                            "lastName": "Deng",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeremiah D. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40392393"
                        ],
                        "name": "Kwok-Ping Chan",
                        "slug": "Kwok-Ping-Chan",
                        "structuredName": {
                            "firstName": "Kwok-Ping",
                            "lastName": "Chan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kwok-Ping Chan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2119041337"
                        ],
                        "name": "Yinglin Yu",
                        "slug": "Yinglin-Yu",
                        "structuredName": {
                            "firstName": "Yinglin",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yinglin Yu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17025221,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d672f180ce923a3331a15a5b52b4bf06133a3ac2",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "So far the bottleneck of Chinese recognition, especially handwritten recognition, still lies in the effectiveness of feature-extraction to cater for various distortions and position shifting. In the paper, a novel method is proposed by applying a set of Gabor spatial filters with different directions and spatial frequencies to character images, in an effort to reach the optimum trade-off between feature stability and feature localization. While a classic self-organizing map is used for unsupervised clustering feature codes, a multi-staged LVQ with a fuzzy judgement unit is applied for the final recognition on the feature mapping result.<<ETX>>"
            },
            "slug": "Handwritten-Chinese-character-recognition-using-and-Deng-Chan",
            "title": {
                "fragments": [],
                "text": "Handwritten Chinese character recognition using spatial Gabor filters and self-organizing feature maps"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel method is proposed by applying a set of Gabor spatial filters with different directions and spatial frequencies to character images, in an effort to reach the optimum trade-off between feature stability and feature localization."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1st International Conference on Image Processing"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143863244"
                        ],
                        "name": "Xiansheng Hua",
                        "slug": "Xiansheng-Hua",
                        "structuredName": {
                            "firstName": "Xiansheng",
                            "lastName": "Hua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiansheng Hua"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120640002"
                        ],
                        "name": "Wenyin Liu",
                        "slug": "Wenyin-Liu",
                        "structuredName": {
                            "firstName": "Wenyin",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenyin Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108698841"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15798803,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c44fc2f6d748f54badcaf86feef8eb347d0b1c2",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Text presented in videos provides important supplemental information for video indexing and retrieval. Many efforts have been made for text detection in videos. However, there is still a lack of performance evaluation protocols for video text detection. In this paper, we propose an objective and comprehensive performance evaluation protocol for video text detection algorithms. The protocol includes a positive set and a negative set of indices at the textbox level, which evaluate the detection quality in terms of both location accuracy and fragmentation of the detected textboxes. In the protocol, we assign a detection difficulty (DD) level to each ground truth textbox. The performance indices can then be normalized with respect to the textbox DD level and are therefore tolerant to different ground-truth difficulties to a certain degree. We also assign a detectability index (DI) value to each ground-truth textbox. The overall detection rate is the DI-weighted average of the detection qualities of all ground-truth textboxes, which makes the detection rate more accurate to reveal the real performance. The automatic performance evaluation scheme has been applied to performance evaluation of a text detection approach to determine the best thresholds that can yield the best detection results. The protocol has also been employed to compare the performances of several text detection systems. Hence, we believe that the proposed protocol can be used to compare the performance of different video/image text detection algorithms/systems and can even help improve, select, and design new text detection methods."
            },
            "slug": "An-automatic-performance-evaluation-protocol-for-Hua-Liu",
            "title": {
                "fragments": [],
                "text": "An automatic performance evaluation protocol for video text detection algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An objective and comprehensive performance evaluation protocol for video text detection algorithms, which includes a positive set and a negative set of indices at the textbox level, which evaluate the detection quality in terms of both location accuracy and fragmentation of the detected textboxes."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Circuits and Systems for Video Technology"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739319"
                        ],
                        "name": "R. Lienhart",
                        "slug": "R.-Lienhart",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Lienhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lienhart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 131
                            }
                        ],
                        "text": "In order to detect different sizes of text, the multiresolution/multiscale method has been used in text detection algorithms [13], [14], [28]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 165
                            }
                        ],
                        "text": "text detection and recognition task, an image sequence provides a lot of useful information that can be used to detect text and enhance the image\u2019s resolution [13], [14], [20], [27]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8416045,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0dd1def5778f24c2c5a5f1c114846326e8f86123",
            "isKey": false,
            "numCitedBy": 143,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Efficient indexing and retrieval of digital video is an important aspect of video databases. One powerful index for retrieval is the text appearing in them. It enables content- based browsing. We present our methods for automatic segmentation and recognition of text in digital videos. The algorithms we propose make use of typical characteristics of text in videos in order to enable and enhance segmentation and recognition performance. Especially the inter-frame dependencies of the characters provide new possibilities for their refinement. Then, a straightforward indexing and retrieval scheme is introduced. It is used in the experiments to demonstrate that the proposed text segmentation and text recognition algorithms are suitable for indexing and retrieval of relevant video scenes in and from a video data base. Our experimental results are very encouraging and suggest that these algorithms can be used in video retrieval applications as well as to recognize higher semantics in video."
            },
            "slug": "Automatic-text-recognition-for-video-indexing-Lienhart",
            "title": {
                "fragments": [],
                "text": "Automatic text recognition for video indexing"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The proposed text segmentation and text recognition algorithms are suitable for indexing and retrieval of relevant video scenes in and from a video data base and suggest that they can be used in video retrieval applications as well as to recognize higher semantics in video."
            },
            "venue": {
                "fragments": [],
                "text": "MULTIMEDIA '96"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143863244"
                        ],
                        "name": "Xiansheng Hua",
                        "slug": "Xiansheng-Hua",
                        "structuredName": {
                            "firstName": "Xiansheng",
                            "lastName": "Hua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiansheng Hua"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120640002"
                        ],
                        "name": "Wenyin Liu",
                        "slug": "Wenyin-Liu",
                        "structuredName": {
                            "firstName": "Wenyin",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenyin Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108698841"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 72
                            }
                        ],
                        "text": "14 shows some examples of the video images from Microsoft Research Asia [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1989026,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "844ceedec94d46b15198de140646e0f2ae953eb0",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an objective, comprehensive and difficulty-independent performance evaluation protocol for video text detection algorithms. The protocol includes a positive set and a negative set of indices at textbox level, which evaluate the detection quality in terms of both location accuracy and fragmentation of the detected textboxes. In the protocol, we assign a detection difficulty (DD) level to each ground truth textbox. The performance indices can then be normalized with respect to the textbox DD level and are therefore independent of the ground truth difficulty. We also assign a detection importance (DI) level to each ground truth textbox. The overall detection rate is the DI-weighted average of the detection qualities of all ground truth textboxes, which makes the detection rate more accurate to reveal the real performance. The automatic performance evaluation scheme has been applied on a text detection approach to determine the best parameters that can yield the best detection results."
            },
            "slug": "Automatic-performance-evaluation-for-video-text-Hua-Liu",
            "title": {
                "fragments": [],
                "text": "Automatic performance evaluation for video text detection"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "An objective, comprehensive and difficulty-independent performance evaluation protocol for video text detection algorithms that includes a positive set and a negative set of indices at textbox level, which evaluate the detection quality in terms of both location accuracy and fragmentation of the detected textboxes."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Sixth International Conference on Document Analysis and Recognition"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48764203"
                        ],
                        "name": "Victor Wu",
                        "slug": "Victor-Wu",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Victor Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758550"
                        ],
                        "name": "R. Manmatha",
                        "slug": "R.-Manmatha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Manmatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manmatha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31338632"
                        ],
                        "name": "E. Riseman",
                        "slug": "E.-Riseman",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Riseman",
                            "middleNames": [
                                "M."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Riseman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 137
                            }
                        ],
                        "text": "In order to detect different sizes of text, the multiresolution/multiscale method has been used in text detection algorithms [13], [14], [28]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 95
                            }
                        ],
                        "text": "The work is related to existing research in text detection from general backgrounds [9], [18], [28], video OCR [20], and recognition of text on special objects such as license plates and containers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": ", texture and color analysis [11], [28]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1830124,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e5c342ba0edbebadc7c95c7e59d1bef87d7e4add",
            "isKey": false,
            "numCitedBy": 451,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "A robust system is proposed to automatically detect and extract text in images from different sources, including video, newspapers, advertisements, stock certificates, photographs, and checks. Text is first detected using multiscale texture segmentation and spatial cohesion constraints, then cleaned up and extracted using a histogram-based binarization algorithm. An automatic performance evaluation scheme is also proposed."
            },
            "slug": "TextFinder:-An-Automatic-System-to-Detect-and-Text-Wu-Manmatha",
            "title": {
                "fragments": [],
                "text": "TextFinder: An Automatic System to Detect and Recognize Text In Images"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A robust system is proposed to automatically detect and extract text in images from different sources, including video, newspapers, advertisements, stock certificates, photographs, and checks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115196978"
                        ],
                        "name": "Huiping Li",
                        "slug": "Huiping-Li",
                        "structuredName": {
                            "firstName": "Huiping",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huiping Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3272081"
                        ],
                        "name": "O. Kia",
                        "slug": "O.-Kia",
                        "structuredName": {
                            "firstName": "Omid",
                            "lastName": "Kia",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Kia"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 125
                            }
                        ],
                        "text": "In order to detect different sizes of text, the multiresolution/multiscale method has been used in text detection algorithms [13], [14], [28]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 88
                            }
                        ],
                        "text": "Discrete cosine transform (DCT) and wavelet transform are widely used for area analysis [13], [15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 159
                            }
                        ],
                        "text": "text detection and recognition task, an image sequence provides a lot of useful information that can be used to detect text and enhance the image\u2019s resolution [13], [14], [20], [27]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15485643,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f8f5c282dc11937d29183b955dc3e4fbb677571b",
            "isKey": false,
            "numCitedBy": 652,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "Text that appears in a scene or is graphically added to video can provide an important supplemental source of index information as well as clues for decoding the video's structure and for classification. In this work, we present algorithms for detecting and tracking text in digital video. Our system implements a scale-space feature extractor that feeds an artificial neural processor to detect text blocks. Our text tracking scheme consists of two modules: a sum of squared difference (SSD)-based module to find the initial position and a contour-based module to refine the position. Experiments conducted with a variety of video sources show that our scheme can detect and track text robustly."
            },
            "slug": "Automatic-text-detection-and-tracking-in-digital-Li-Doermann",
            "title": {
                "fragments": [],
                "text": "Automatic text detection and tracking in digital video"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work presents algorithms for detecting and tracking text in digital video that implements a scale-space feature extractor that feeds an artificial neural processor to detect text blocks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118579343"
                        ],
                        "name": "Jie Yang",
                        "slug": "Jie-Yang",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144117646"
                        ],
                        "name": "Jiang Gao",
                        "slug": "Jiang-Gao",
                        "structuredName": {
                            "firstName": "Jiang",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiang Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48379623"
                        ],
                        "name": "Y. Zhang",
                        "slug": "Y.-Zhang",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 103
                            }
                        ],
                        "text": "The recent attempts have moved toward automatic detection and recognition of signs from natural scenes [30], [33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9974904,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "27876ea01c6300eea6ae7e0fab93b3007b865bfe",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Signs are everywhere in our lives. They make our lives easier when we are familiar with them. But sometimes they also pose problems. For example, a tourist might not be able to understand signs in a foreign country. In this paper, we present our efforts towards automatic sign translation. We discuss methods for automatic sign detection. We describe sign translation using example based machine translation technology. We use a user-centered approach in developing an automatic sign translation system. The approach takes advantage of human intelligence in selecting an area of interest and domain for translation if needed. A user can determine which sign is to be translated if multiple signs have been detected within the image. The selected part of the image is then processed, recognized, and translated. We have developed a prototype system that can recognize Chinese signs input from a video camera which is a common gadget for a tourist, and translate them into English text or voice stream."
            },
            "slug": "Towards-Automatic-Sign-Translation-Yang-Gao",
            "title": {
                "fragments": [],
                "text": "Towards Automatic Sign Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A prototype system that can recognize Chinese signs input from a video camera which is a common gadget for a tourist, and translate them into English text or voice stream is developed."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152718968"
                        ],
                        "name": "Li Wang",
                        "slug": "Li-Wang",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145820949"
                        ],
                        "name": "T. Pavlidis",
                        "slug": "T.-Pavlidis",
                        "structuredName": {
                            "firstName": "Theodosios",
                            "lastName": "Pavlidis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Pavlidis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "Wang and Pavlidis showed the advantages of direct feature extraction from gray scale image for OCR [24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 32554888,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "658f7d47c1d268c7e676906892826ae2f8bcbab8",
            "isKey": false,
            "numCitedBy": 209,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "A method for feature extraction directly from gray-scale images of scanned documents without the usual step of binarization is presented. This approach eliminates binarization by extracting features directly from gray-scale images. In this method, a digitized gray-scale image is treated as a noisy sampling of the underlying continuous surface and desired features are obtained by extracting and assembling topographic characteristics of this surface. The advantages and effectiveness of the approach are both shown theoretically and demonstrated through preliminary experiments of the proposed method. >"
            },
            "slug": "Direct-Gray-Scale-Extraction-of-Features-for-Wang-Pavlidis",
            "title": {
                "fragments": [],
                "text": "Direct Gray-Scale Extraction of Features for Character Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A method for feature extraction directly from gray-scale images of scanned documents without the usual step of binarization is presented and the advantages and effectiveness are both shown theoretically and demonstrated through preliminary experiments of the proposed method."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145820949"
                        ],
                        "name": "T. Pavlidis",
                        "slug": "T.-Pavlidis",
                        "structuredName": {
                            "firstName": "Theodosios",
                            "lastName": "Pavlidis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Pavlidis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 9
                            }
                        ],
                        "text": "Wang and Pavlidis showed the advantages of direct feature extraction from gray scale image for OCR [24]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 50
                            }
                        ],
                        "text": "Intensity-based OCR, first introduced by Pavlidis [19], can avoid information loss during binarization, which is irretrievable in subsequent procedure(s)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 27068580,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd36aa1fcfb2fc07d34adddab03a5872bf5519b4",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Recognition-of-printed-text-under-realistic-Pavlidis",
            "title": {
                "fragments": [],
                "text": "Recognition of printed text under realistic conditions"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110062608"
                        ],
                        "name": "Toshio Sato",
                        "slug": "Toshio-Sato",
                        "structuredName": {
                            "firstName": "Toshio",
                            "lastName": "Sato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Toshio Sato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2816639"
                        ],
                        "name": "Ellen K. Hughes",
                        "slug": "Ellen-K.-Hughes",
                        "structuredName": {
                            "firstName": "Ellen",
                            "lastName": "Hughes",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ellen K. Hughes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116645471"
                        ],
                        "name": "Michael A. Smith",
                        "slug": "Michael-A.-Smith",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Smith",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael A. Smith"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The work is related to existing research in text detection from general backgrounds [9], [18], [28], video OCR [ 20 ], and recognition of text on special objects such as license plates and containers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In such a text detection and recognition task, an image sequence provides a lot of useful information that can be used to detect text and enhance the image\u2019s resolution [13], [14], [ 20 ], [27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 43395565,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "67c4ed0ef1c978defe1c44868029790aaad21752",
            "isKey": false,
            "numCitedBy": 275,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Video OCR is a technique that can greatly help to locate topics of interest in a large digital news video archive via the automatic extraction and reading of captions and annotations. News captions generally provide vital search information about the video being presented, the names of people and places or descriptions of objects. In this paper, two difficult problems of character recognition for videos are addressed: low resolution characters and extremely complex backgrounds. We apply an interpolation filter, multi-frame integration and a combination of four filters to solve these problems. Segmenting characters is done by a recognition-based segmentation method and intermediate character recognition results are used to improve the segmentation. The overall recognition results are good enough for use in news indexing. Performing video OCR on news video and combining its results with other video understanding techniques will improve the overall understanding of the news video content."
            },
            "slug": "Video-OCR-for-digital-news-archive-Sato-Kanade",
            "title": {
                "fragments": [],
                "text": "Video OCR for digital news archive"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper applies an interpolation filter, multi-frame integration and a combination of four filters to solve the problems of character recognition for videos: low resolution characters and extremely complex backgrounds."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1998 IEEE International Workshop on Content-Based Access of Image and Video Database"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682986"
                        ],
                        "name": "R. Mullot",
                        "slug": "R.-Mullot",
                        "structuredName": {
                            "firstName": "R\u00e9my",
                            "lastName": "Mullot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mullot"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144925238"
                        ],
                        "name": "C. Olivier",
                        "slug": "C.-Olivier",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Olivier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Olivier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "134138337"
                        ],
                        "name": "J. Bourdon",
                        "slug": "J.-Bourdon",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Bourdon",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bourdon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40269893"
                        ],
                        "name": "P. Courtellemont",
                        "slug": "P.-Courtellemont",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Courtellemont",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Courtellemont"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685183"
                        ],
                        "name": "J. Labiche",
                        "slug": "J.-Labiche",
                        "structuredName": {
                            "firstName": "Jacques",
                            "lastName": "Labiche",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Labiche"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3346993"
                        ],
                        "name": "Y. Lecourtier",
                        "slug": "Y.-Lecourtier",
                        "structuredName": {
                            "firstName": "Yves",
                            "lastName": "Lecourtier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Lecourtier"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 79
                            }
                        ],
                        "text": "reported early attempts on container and car license plate recognition in 1991 [17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 63650619,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "0f6287be1d8217ca589ab73fd6a4dff8d3a55c61",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Three methods of character area localization in noisy images are proposed, constituting the first treatment of automatic recognition of container or wagon identity numbers, or registration plates of cars. The first method, implemented on a two-transputer network, involves a cognitive approach to segmentation, by the search for vertical segments. The second method, based on signal processing, realizes an AR-modeling of the image background, and uses a rupture detection process. The third method is based on the localization of pertinent transitions, i.e. an increase of the gray level crossing over a given percentage of the scale. These methods have been compared on photographs (512*512 pixels on 256 gray levels) of containers and registration plates. The three methods, debugged and checked with the same images, are shown to give good results.<<ETX>>"
            },
            "slug": "Automatic-extraction-methods-of-container-identity-Mullot-Olivier",
            "title": {
                "fragments": [],
                "text": "Automatic extraction methods of container identity number and registration plates of cars"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "Three methods of character area localization in noisy images are proposed, constituting the first treatment of automatic recognition of container or wagon identity numbers, or registration plates of cars, on a two-transputer network."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IECON '91: 1991 International Conference on Industrial Electronics, Control and Instrumentation"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115439468"
                        ],
                        "name": "Yuntao Cui",
                        "slug": "Yuntao-Cui",
                        "structuredName": {
                            "firstName": "Yuntao",
                            "lastName": "Cui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuntao Cui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1391129943"
                        ],
                        "name": "Qian Huang",
                        "slug": "Qian-Huang",
                        "structuredName": {
                            "firstName": "Qian",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qian Huang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 2459300,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e75b09dff3656243fde5cacf9baf1c42a4ea972",
            "isKey": false,
            "numCitedBy": 122,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a new approach to extract characters on a license plate of a moving vehicle given a sequence of perspective distortion corrected license plate images. We model the extraction of characters as a Markov random field (MRF). With the MRF modeling, the extraction of characters is formulated as the problem of maximizing the a posteriori probability based on given prior and observations. A genetic algorithm with local greedy mutation operator is employed to optimize the objective function. Experiments and comparison study were conducted. It is shown that our approach provides better performance than other single frame methods."
            },
            "slug": "Character-extraction-of-license-plates-from-video-Cui-Huang",
            "title": {
                "fragments": [],
                "text": "Character extraction of license plates from video"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "It is shown that this approach to extract characters on a license plate of a moving vehicle given a sequence of perspective distortion corrected license plate images provides better performance than other single frame methods."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110062608"
                        ],
                        "name": "Toshio Sato",
                        "slug": "Toshio-Sato",
                        "structuredName": {
                            "firstName": "Toshio",
                            "lastName": "Sato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Toshio Sato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2816639"
                        ],
                        "name": "Ellen K. Hughes",
                        "slug": "Ellen-K.-Hughes",
                        "structuredName": {
                            "firstName": "Ellen",
                            "lastName": "Hughes",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ellen K. Hughes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116645471"
                        ],
                        "name": "Michael A. Smith",
                        "slug": "Michael-A.-Smith",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Smith",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael A. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 111
                            }
                        ],
                        "text": "The work is related to existing research in text detection from general backgrounds [9], [18], [28], video OCR [20], and recognition of text on special objects such as license plates and containers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 171
                            }
                        ],
                        "text": "text detection and recognition task, an image sequence provides a lot of useful information that can be used to detect text and enhance the image\u2019s resolution [13], [14], [20], [27]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12703346,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71d684a6ddbdc3f816e678e4f2ca9ec0a58f3387",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Video OCR is a technique that can greatly help to locate topics of interest in a large digital news video archive via the automatic extraction and reading of captions and annotations. News captions generally provide vital search information about the video being presented { the names of people and places or descriptions of objects. In this paper, two di cult problems of character recognition for videos are addressed: low resolution characters and extremely complex backgrounds. We apply an interpolation lter, multi-frame integration and a combination of four lters to solve these problems. Segmenting characters is done by a recognition-based segmentation method and intermediate character recognition results are used to improve the segmentation. The overall recognition results are good enough for use in news indexing. Performing Video OCR on news video and combining its results with other video understanding techniques will improve the overall understanding of the news video content."
            },
            "slug": "Video-OCR-for-Digital-News-Archives-Sato-Kanade",
            "title": {
                "fragments": [],
                "text": "Video OCR for Digital News Archives"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper applies an interpolation, multi-frame integration and a combination of four lters to solve the problems of character recognition for videos: low resolution characters and extremely complex backgrounds."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764131"
                        ],
                        "name": "R. Mehrotra",
                        "slug": "R.-Mehrotra",
                        "structuredName": {
                            "firstName": "Rajiv",
                            "lastName": "Mehrotra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mehrotra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697423"
                        ],
                        "name": "K. Namuduri",
                        "slug": "K.-Namuduri",
                        "structuredName": {
                            "firstName": "Kamesh",
                            "lastName": "Namuduri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Namuduri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144735955"
                        ],
                        "name": "N. Ranganathan",
                        "slug": "N.-Ranganathan",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Ranganathan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ranganathan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 149
                            }
                        ],
                        "text": "Because of its superior mathematic properties, Gabor wavelet has been widely used for data compression [21], face recognition [25], texture analysis [16], handwriting recognition [4] and other image processing tasks in recent years."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 33886049,
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "id": "5ac53aba05c24af6d00e4eed7c54b5f7ad9eb760",
            "isKey": false,
            "numCitedBy": 344,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Gabor-filter-based-edge-detection-Mehrotra-Namuduri",
            "title": {
                "fragments": [],
                "text": "Gabor filter-based edge detection"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780236"
                        ],
                        "name": "Y. Hamamoto",
                        "slug": "Y.-Hamamoto",
                        "structuredName": {
                            "firstName": "Yoshihiko",
                            "lastName": "Hamamoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Hamamoto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798048"
                        ],
                        "name": "S. Uchimura",
                        "slug": "S.-Uchimura",
                        "structuredName": {
                            "firstName": "Shunji",
                            "lastName": "Uchimura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Uchimura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3215317"
                        ],
                        "name": "K. Masamizu",
                        "slug": "K.-Masamizu",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Masamizu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Masamizu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758524"
                        ],
                        "name": "S. Tomita",
                        "slug": "S.-Tomita",
                        "structuredName": {
                            "firstName": "Shingo",
                            "lastName": "Tomita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Tomita"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 74
                            }
                        ],
                        "text": "In OCR applications, Gabor wavelet has been applied to binary images [5], [6] and recently applied to a video stream [32]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 41069560,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a220c9b4fd1d04633f70f6dd7526f54e1b137302",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "A method for handprinted Chinese character recognition based on Gabor filters is proposed. The Gabor approach to character recognition is intuitively appealing because it is inspired by a multi-channel filtering theory for processing visual information in the early stages of the human visual system. The performance of a character recognition system using Gabor features is demonstrated on the ETL-8 character set. Mental results show that the Gabor features yielded an error rate of 2.4% versus the error rate of 4.4% obtained by using a popular feature extraction method."
            },
            "slug": "Recognition-of-handprinted-Chinese-characters-using-Hamamoto-Uchimura",
            "title": {
                "fragments": [],
                "text": "Recognition of handprinted Chinese characters using Gabor features"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A method for handprinted Chinese character recognition based on Gabor filters is proposed, inspired by a multi-channel filtering theory for processing visual information in the early stages of the human visual system."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd International Conference on Document Analysis and Recognition"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48133392"
                        ],
                        "name": "Shintaro Kumano",
                        "slug": "Shintaro-Kumano",
                        "structuredName": {
                            "firstName": "Shintaro",
                            "lastName": "Kumano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shintaro Kumano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2029613685"
                        ],
                        "name": "Miyamoto Kazumasa",
                        "slug": "Miyamoto-Kazumasa",
                        "structuredName": {
                            "firstName": "Miyamoto",
                            "lastName": "Kazumasa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Miyamoto Kazumasa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33080731"
                        ],
                        "name": "M. Tamagawa",
                        "slug": "M.-Tamagawa",
                        "structuredName": {
                            "firstName": "Mitsuaki",
                            "lastName": "Tamagawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Tamagawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105617385"
                        ],
                        "name": "H. Ikeda",
                        "slug": "H.-Ikeda",
                        "structuredName": {
                            "firstName": "Hiroaki",
                            "lastName": "Ikeda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ikeda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2072346943"
                        ],
                        "name": "Koji Kan",
                        "slug": "Koji-Kan",
                        "structuredName": {
                            "firstName": "Koji",
                            "lastName": "Kan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Koji Kan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 29
                            }
                        ],
                        "text": ", texture and color analysis [11], [28]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 18368273,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "b812ab61b0f08d5313d73c6c83c19cbbf37f4b43",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper reports the development of a container identification mark or number recognition system designed for application to a container terminal. The recognition system recognizes the number or mark on the back surface of a container in an outdoor environment by photographing the mark by a camera system installed on the container gate. Containers in many cases differ in color as well as in layout depending on their owners; the layouts commonly contain one to four horizontal columns or rows of writing or more rarely vertical rows of writing. The container number recognition system is constructed from an illumination intensity sensor and illumination system for handling the outdoor illumination changes, a shutter speed control device, and devices such as filters for handling various container colors. In addition, the proposed system uses a character recognition scheme based on a dynamic design method for recognizing differing character string layouts in container marks or numbers. Field tests have been conducted to obtain a recognition rate of 92.8% for all data, a recognition rate of 97.9% for effective or appropriate data excluding data outside the field of vision, and an average recognition speed of less than 2 seconds. \u00a9 2004 Wiley Periodicals, Inc. Electron Comm Jpn Pt 2, 87(12): 38\u201350, 2004; Published online in Wiley InterScience (www.interscience.wiley.com). DOI 10.1002/ecjb.20134"
            },
            "slug": "Development-of-a-container-identification-mark-Kumano-Kazumasa",
            "title": {
                "fragments": [],
                "text": "Development of a container identification mark recognition system"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "The development of a container identification mark or number recognition system designed for application to a container terminal that uses a character recognition scheme based on a dynamic design method for recognizing differing character string layouts in container marks or numbers."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34340513"
                        ],
                        "name": "H. Szu",
                        "slug": "H.-Szu",
                        "structuredName": {
                            "firstName": "Harold",
                            "lastName": "Szu",
                            "middleNames": [],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Szu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2341494"
                        ],
                        "name": "B. Telfer",
                        "slug": "B.-Telfer",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Telfer",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Telfer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116497553"
                        ],
                        "name": "Joseph Garcia",
                        "slug": "Joseph-Garcia",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Garcia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph Garcia"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 103
                            }
                        ],
                        "text": "Because of its superior mathematic properties, Gabor wavelet has been widely used for data compression [21], face recognition [25], texture analysis [16], handwriting recognition [4] and other image processing tasks in recent years."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 37124862,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8dea180f96d3076f00b8178ed91a0373babc7ea2",
            "isKey": false,
            "numCitedBy": 90,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Wavelet-transforms-and-neural-networks-for-and-Szu-Telfer",
            "title": {
                "fragments": [],
                "text": "Wavelet transforms and neural networks for compression and recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3115459"
                        ],
                        "name": "Young-Kyu Lim",
                        "slug": "Young-Kyu-Lim",
                        "structuredName": {
                            "firstName": "Young-Kyu",
                            "lastName": "Lim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Young-Kyu Lim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2520496"
                        ],
                        "name": "Song-Ha Choi",
                        "slug": "Song-Ha-Choi",
                        "structuredName": {
                            "firstName": "Song-Ha",
                            "lastName": "Choi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Ha Choi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50112753"
                        ],
                        "name": "Seong-Whan Lee",
                        "slug": "Seong-Whan-Lee",
                        "structuredName": {
                            "firstName": "Seong-Whan",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seong-Whan Lee"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 94
                            }
                        ],
                        "text": "Discrete cosine transform (DCT) and wavelet transform are widely used for area analysis [13], [15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 39670541,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "12ea3d31c0dfb93433bb3db2c45101c21e6af8d1",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Video text extraction is a core technique for multimedia applications such as news-on-demand (NOD) and digital libraries, and research about video text extraction have been conducted vigorously. In this paper, we propose an efficient method for extracting texts in MPEG compressed videos for content-based indexing. The proposed method makes the best use of 2-level DCT coefficients and macroblock type information in MPEG compressed video, and this method can be organized into three stages to increase overall performance; text frame detection, text region extraction, and character extraction. The main advantage of the proposed method is that it can avoid the overhead of decompressing video into individual frames in the pixel domain. We evaluated this method using various types of news video data."
            },
            "slug": "Text-extraction-in-MPEG-compressed-video-for-Lim-Choi",
            "title": {
                "fragments": [],
                "text": "Text extraction in MPEG compressed video for content-based indexing"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The main advantage of the proposed method is that it can avoid the overhead of decompressing video into individual frames in the pixel domain and make the best use of 2-level DCT coefficients and macroblock type information in MPEG compressed video."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 15th International Conference on Pattern Recognition. ICPR-2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48189908"
                        ],
                        "name": "Yasuhiko Watanabe",
                        "slug": "Yasuhiko-Watanabe",
                        "structuredName": {
                            "firstName": "Yasuhiko",
                            "lastName": "Watanabe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yasuhiko Watanabe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2090132835"
                        ],
                        "name": "Yoshihiro Okada",
                        "slug": "Yoshihiro-Okada",
                        "structuredName": {
                            "firstName": "Yoshihiro",
                            "lastName": "Okada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshihiro Okada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146300157"
                        ],
                        "name": "Yeun-Bae Kim",
                        "slug": "Yeun-Bae-Kim",
                        "structuredName": {
                            "firstName": "Yeun-Bae",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yeun-Bae Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105963409"
                        ],
                        "name": "Tetsuya Takeda",
                        "slug": "Tetsuya-Takeda",
                        "structuredName": {
                            "firstName": "Tetsuya",
                            "lastName": "Takeda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tetsuya Takeda"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 118
                            }
                        ],
                        "text": "The early work was focused mainly on the concept itself and required manual selection of the area containing the sign [22], [29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10167499,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "61ae735749334af00d99a78a71df2595913ea5b4",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a camera system which translates Japanese texts in a scene. The system is portable and consists of four components: digital camera, character image extraction process, character recognition process, and translation process. The system extracts character strings from a region which a user specifies, and translates them into English."
            },
            "slug": "Translation-camera-Watanabe-Okada",
            "title": {
                "fragments": [],
                "text": "Translation camera"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "A camera system which translates Japanese texts in a scene using a digital camera, which extracts character strings from a region which a user specifies, and translates them into English."
            },
            "venue": {
                "fragments": [],
                "text": "MTSUMMIT"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143955418"
                        ],
                        "name": "M. S. Brown",
                        "slug": "M.-S.-Brown",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Brown",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. S. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9324035"
                        ],
                        "name": "W. B. Seales",
                        "slug": "W.-B.-Seales",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Seales",
                            "middleNames": [
                                "Brent"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. B. Seales"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 99
                            }
                        ],
                        "text": "2) Affine Parameter Estimation: Several methods exist for obtaining affine parameters from texture [2], [23], [26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18903638,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c6f989451bac14f973b131059fc51ed8d0aa6fa3",
            "isKey": false,
            "numCitedBy": 115,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a framework for restoring arbitrarily warped and deformed documents to their original planar shape. The impetus for this work is the need for tools and techniques to help digitally preserve and restore fragile manuscripts. Current digitization is performed under the assumption that the documents are flat, with subsequent image-processing and restoration algorithms either relying on this assumption or attempting to overcome it without shape information. Although most manuscripts were originally flat, many become deformed from damage and deterioration. Physical flattening is not possible without risking further, possibly irreversible, damage. Our framework addresses this restoration problem with two primary contributions. First, we present a working 3D digitization setup that acquires a 3D model with accurate shape-to-texture registration under multiple lighting conditions. Second, we show how the 3D model and a mass-spring particle system can be used together as a framework for digital flattening. We show that this restoration process can correct document deformations and can significantly improve subsequent document analysis."
            },
            "slug": "Document-restoration-using-3D-shape:-a-general-for-Brown-Seales",
            "title": {
                "fragments": [],
                "text": "Document restoration using 3D shape: a general deskewing algorithm for arbitrarily warped documents"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A framework for restoring arbitrarily warped and deformed documents to their original planar shape is presented and it is shown how the 3D model and a mass-spring particle system can be used together as a framework for digital flattening."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121188750"
                        ],
                        "name": "A. U.S.",
                        "slug": "A.-U.S.",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "U.S.",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. U.S."
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 110
                            }
                        ],
                        "text": "2) Affine Parameter Estimation: Several methods exist for obtaining affine parameters from texture [2], [23], [26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13501500,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "bfe48e07bab87898aeb7622876bceb79ccd1d61a",
            "isKey": false,
            "numCitedBy": 193,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Texture provides an important source of information about the three-dimensional structure of visible surfaces, particularly for stationary monocular views. To recover 3d structure, the distorting effects of projection must be distinguished from properties of the texture on which the distortion acts. This requires that assumptions must be made about the texture, yet the unpredictability of natural textures precludes the use of highly restrictive assumptions. The recovery method reported in this paper exploits the minimal assumption that textures do not mimic projective effects. This assumption determines the strategy of attributing as much as possible of the variation observed in the image to projection. Equivalently, the interpretation is chosen for which the texture, prior to projection, is made as uniform as possible. This strategy was implemented using statistical methods, first for the restricted case of planar surfaces and then, by extension, for curved surfaces. The technique was applied successfully to natural images."
            },
            "slug": "Recovering-Surface-Shape-and-Orientation-from-U.S.",
            "title": {
                "fragments": [],
                "text": "Recovering Surface Shape and Orientation from Texture"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716904"
                        ],
                        "name": "J. Koenderink",
                        "slug": "J.-Koenderink",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Koenderink",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Koenderink"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7977369"
                        ],
                        "name": "A. Doorn",
                        "slug": "A.-Doorn",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Doorn",
                            "middleNames": [
                                "J.",
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Doorn"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "We call this vector a jet[10], which is used to represent the position of local features."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 24284500,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "00aa5220d49f3fcf357c1b64ac14f24cd8afb76d",
            "isKey": false,
            "numCitedBy": 626,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "It is shown that a convolution with certain reasonable receptive field (RF) profiles yields the exact partial derivatives of the retinal illuminance blurred to a specified degree. Arbitrary concatenations of such RF profiles yield again similar ones of higher order and for a greater degree of blurring.By replacing the illuminance with its third order jet extension we obtain position dependent geometries. It is shown how such a representation can function as the substrate for \u201cpoint processors\u201d computing geometrical features such as edge curvature. We obtain a clear dichotomy between local and multilocal visual routines. The terms of the truncated Taylor series representing the jets are partial derivatives whose corresponding RF profiles closely mimic the well known units in the primary visual cortex. Hence this description provides a novel means to understand and classify these units.Taking the receptive field outputs as the basic input data one may devise visual routines that compute geometric features on the basis of standard differential geometry exploiting the equivalence with the local jets (partial derivatives with respect to the space coordinates)."
            },
            "slug": "Representation-of-local-geometry-in-the-visual-Koenderink-Doorn",
            "title": {
                "fragments": [],
                "text": "Representation of local geometry in the visual system"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "It is shown that a convolution with certain reasonable receptive field (RF) profiles yields the exact partial derivatives of the retinal illuminance blurred to a specified degree and how this representation can function as the substrate for \u201cpoint processors\u201d computing geometrical features such as edge curvature."
            },
            "venue": {
                "fragments": [],
                "text": "Biological Cybernetics"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118579343"
                        ],
                        "name": "Jie Yang",
                        "slug": "Jie-Yang",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117039990"
                        ],
                        "name": "Weiyi Yang",
                        "slug": "Weiyi-Yang",
                        "structuredName": {
                            "firstName": "Weiyi",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weiyi Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35142518"
                        ],
                        "name": "Matthias Denecke",
                        "slug": "Matthias-Denecke",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Denecke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthias Denecke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5422965,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "50e02937de851c9ad5a6d4103f87d6fa029ad6b9",
            "isKey": false,
            "numCitedBy": 126,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present our efforts towards developing an intelligent tourist system. The system is equipped with a unique combination of sensors and software. The hardware includes two computers, a GPS receiver, a lapel microphone plus an earphone, a video camera and a head-mounted display. This combination includes a multimodal interface to take advantage of speech and gesture input to provide assistance for a tourist. The software supports natural language processing, speech recognition, machine translation, handwriting recognition and multimodal fusion. A vision module is trained to locate and read written language, is able to adapt to to new environments, and is able to interpret intentions offered by the user such as a spoken clarification or pointing gesture. We illustrate the applications of the system using two examples."
            },
            "slug": "Smart-Sight:-a-tourist-assistant-system-Yang-Yang",
            "title": {
                "fragments": [],
                "text": "Smart Sight: a tourist assistant system"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The system is equipped with a unique combination of sensors and software that supports natural language processing, speech recognition, machine translation, handwriting recognition and multimodal fusion."
            },
            "venue": {
                "fragments": [],
                "text": "Digest of Papers. Third International Symposium on Wearable Computers"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Because of its superior mathematic properties, Gabor wavelet has been widely used for data compression [21], face recognition [ 25 ], texture analysis [16], handwriting recognition [4] and other image processing tasks in recent years."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 30523165,
            "fieldsOfStudy": [],
            "id": "6f01963039d0a921b1930b4563d1601ff36b971f",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Face Recognition by Elastic Bunch Graph Matching"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 126
                            }
                        ],
                        "text": "Because of its superior mathematic properties, Gabor wavelet has been widely used for data compression [21], face recognition [25], texture analysis [16], handwriting recognition [4] and other image processing tasks in recent years."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 28189911,
            "fieldsOfStudy": [],
            "id": "ecf76b772c1650d7ccf2b8bdb7eef599eaf34858",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Face Recognition by Elastic Bunch Graph Matching"
            },
            "venue": {
                "fragments": [],
                "text": "CAIP"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "The other method, edge analysis, is based on more stable edge features [34], and is thus more suitable for text detection from natural scenes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 35696139,
            "fieldsOfStudy": [],
            "id": "d68b95534860e2bddd17d17ef7f362d16c550bde",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Locating text in complex color images"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 97
                            }
                        ],
                        "text": "Some other researchers published their efforts on container text detection and recognition later [1], [3], [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Image recognition for shipping container tracking and I.D"
            },
            "venue": {
                "fragments": [],
                "text": "Adv. Imag., vol. 10, no. 1, pp. 61\u201362, 1995."
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 104
                            }
                        ],
                        "text": "2) Affine Parameter Estimation: Several methods exist for obtaining affine parameters from texture [2], [23], [26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Shape Recovering of a Solid of Resolution from Apparent Distortions of Patterns"
            },
            "venue": {
                "fragments": [],
                "text": "Carnegie-Mellon Univ., Pittsburgh, PA, vol. CMU-CS-80\u2013133, 1984."
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "The work is related to existing research in text detection from general backgrounds [9], [18], [28], video OCR [20], and recognition of text on special objects such as license plates and containers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 151
                            }
                        ],
                        "text": "As signs exist in three-dimensional space, text on signs in scene images can be distorted by slant, tilt, and shape of objects on which they are found [18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recognition of characters in scene images"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal Machine Intell., vol. 16, no. 2, pp. 214\u2013220, 1994."
            },
            "year": 1994
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 20,
            "methodology": 13
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 38,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Automatic-detection-and-recognition-of-signs-from-Chen-Yang/5295b6770ebbbc27a4651ed44b4b7e184d884f8e?sort=total-citations"
}