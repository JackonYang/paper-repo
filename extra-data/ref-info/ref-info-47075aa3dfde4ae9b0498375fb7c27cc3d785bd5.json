{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2433269"
                        ],
                        "name": "Derek Hoiem",
                        "slug": "Derek-Hoiem",
                        "structuredName": {
                            "firstName": "Derek",
                            "lastName": "Hoiem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Derek Hoiem"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 80
                            }
                        ],
                        "text": "An alternative to such a rule-based generative approach is statistical learning [11, 6, 12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 37206,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5276705d71e3dac961ab5d06b86a7b806cc9af64",
            "isKey": false,
            "numCitedBy": 718,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": "Humans have an amazing ability to instantly grasp the overall 3D structure of a scene\u2014ground orientation, relative positions of major landmarks, etc.\u2014even from a single image. This ability is completely missing in most popular recognition algorithms, which pretend that the world is flat and/or view it through a patch-sized peephole. Yet it seems very likely that having a grasp of this \u201csurface layout\u201d of a scene should be of great assistance for many tasks, including recognition, navigation, and novel view synthesis.In this paper, we take the first step towards constructing the surface layout, a labeling of the image intogeometric classes. Our main insight is to learn appearance-based models of these geometric classes, which coarsely describe the 3D scene orientation of each image region. Our multiple segmentation framework provides robust spatial support, allowing a wide variety of cues (e.g., color, texture, and perspective) to contribute to the confidence in each geometric label. In experiments on a large set of outdoor images, we evaluate the impact of the individual cues and design choices in our algorithm. We further demonstrate the applicability of our method to indoor images, describe potential applications, and discuss extensions to a more complete notion of surface layout."
            },
            "slug": "Recovering-Surface-Layout-from-an-Image-Hoiem-Efros",
            "title": {
                "fragments": [],
                "text": "Recovering Surface Layout from an Image"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper takes the first step towards constructing the surface layout, a labeling of the image intogeometric classes, to learn appearance-based models of these geometric classes, which coarsely describe the 3D scene orientation of each image region."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2433269"
                        ],
                        "name": "Derek Hoiem",
                        "slug": "Derek-Hoiem",
                        "structuredName": {
                            "firstName": "Derek",
                            "lastName": "Hoiem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Derek Hoiem"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 80
                            }
                        ],
                        "text": "An alternative to such a rule-based generative approach is statistical learning [11, 6, 12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206769405,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "ae89592317675c9c7642a3976c3a064cef736f92",
            "isKey": false,
            "numCitedBy": 757,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Many computer vision algorithms limit their performance by ignoring the underlying 3D geometric structure in the image. We show that we can estimate the coarse geometric properties of a scene by learning appearance-based models of geometric classes, even in cluttered natural scenes. Geometric classes describe the 3D orientation of an image region with respect to the camera. We provide a multiple-hypothesis framework for robustly estimating scene structure from a single image and obtaining confidences for each geometric label. These confidences can then be used to improve the performance of many other applications. We provide a thorough quantitative evaluation of our algorithm on a set of outdoor images and demonstrate its usefulness in two applications: object detection and automatic single-view reconstruction."
            },
            "slug": "Geometric-context-from-a-single-image-Hoiem-Efros",
            "title": {
                "fragments": [],
                "text": "Geometric context from a single image"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "This work shows that it can estimate the coarse geometric properties of a scene by learning appearance-based models of geometric classes, even in cluttered natural scenes, and provides a multiple-hypothesis framework for robustly estimating scene structure from a single image and obtaining confidences for each geometric label."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3203751"
                        ],
                        "name": "E. Delage",
                        "slug": "E.-Delage",
                        "structuredName": {
                            "firstName": "Erick",
                            "lastName": "Delage",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Delage"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697141"
                        ],
                        "name": "Honglak Lee",
                        "slug": "Honglak-Lee",
                        "structuredName": {
                            "firstName": "Honglak",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Honglak Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 80
                            }
                        ],
                        "text": "An alternative to such a rule-based generative approach is statistical learning [11, 6, 12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14075351,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f4789a2effea966c8fd10491fe859cfc7607137",
            "isKey": false,
            "numCitedBy": 237,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "When we look at a picture, our prior knowledge about the world allows us to resolve some of the ambiguities that are inherent to monocular vision, and thereby infer 3d information about the scene. We also recognize different objects, decide on their orientations, and identify how they are connected to their environment. Focusing on the problem of autonomous 3d reconstruction of indoor scenes, in this paper we present a dynamic Bayesian network model capable of resolving some of these ambiguities and recovering 3d information for many images. Our model assumes a \"floorwall\" geometry on the scene and is trained to recognize the floor-wall boundary in each column of the image. When the image is produced under perspective geometry, we show that this model can be used for 3d reconstruction from a single image. To our knowledge, this was the first monocular approach to automatically recover 3d reconstructions from single indoor images."
            },
            "slug": "A-Dynamic-Bayesian-Network-Model-for-Autonomous-3D-Delage-Lee",
            "title": {
                "fragments": [],
                "text": "A Dynamic Bayesian Network Model for Autonomous 3D Reconstruction from a Single Indoor Image"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This paper presents a dynamic Bayesian network model capable of resolving some of the ambiguities of monocular vision and recovering 3d information for many images and shows that this model can be used for 3d reconstruction from a single image."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696931"
                        ],
                        "name": "H. Barrow",
                        "slug": "H.-Barrow",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Barrow",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Barrow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144592244"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Jay",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 92
                            }
                        ],
                        "text": "shape-from-X modules, where X could be shading [19, 14], texture [8, 16], or line junctions [15, 2, 18])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52817639,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2643759cac24ceb45470e731e808d7ecf0b85bce",
            "isKey": false,
            "numCitedBy": 381,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Interpreting-Line-Drawings-as-Three-Dimensional-Barrow-Tenenbaum",
            "title": {
                "fragments": [],
                "text": "Interpreting Line Drawings as Three-Dimensional Surfaces"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145266088"
                        ],
                        "name": "T. Leung",
                        "slug": "T.-Leung",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Leung",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Leung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 65
                            }
                        ],
                        "text": "shape-from-X modules, where X could be shading [19, 14], texture [8, 16], or line junctions [15, 2, 18])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2377107,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7ab37ee19f7ff2df34c62d1eaa6c4cb9eb14a771",
            "isKey": false,
            "numCitedBy": 150,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an algorithm for detecting, localizing and grouping instances of repeated scene elements. The grouping is represented by a graph where nodes correspond to individual elements and arcs join spatially neighboring elements. Associated with each arc is an affine map that best transforms the image patch at one location to the other. The approach we propose consists of 4 steps: (1) detecting \u201cinteresting\u201d elements in the image; (2) matching elements with their neighbors and estimating the affine transform between them; (3) growing the element to form a more distinctive unit; and (4) grouping the elements. The idea is analogous to tracking in dynamic imagery. In our context, we \u201ctrack\u201d an element to spatially neighboring locations in one image, while in temporal tracking, one would perform the search in neighboring image frames."
            },
            "slug": "Detecting,-localizing-and-grouping-repeated-scene-Leung-Malik",
            "title": {
                "fragments": [],
                "text": "Detecting, localizing and grouping repeated scene elements from an image"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "This paper presents an algorithm for detecting, localizing and grouping instances of repeated scene elements, represented by a graph where nodes correspond to individual elements and arcs join spatially neighboring elements."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145878635"
                        ],
                        "name": "Tom\u00e1\u0161 Werner",
                        "slug": "Tom\u00e1\u0161-Werner",
                        "structuredName": {
                            "firstName": "Tom\u00e1\u0161",
                            "lastName": "Werner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom\u00e1\u0161 Werner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 156
                            }
                        ],
                        "text": "From Line Segments to Line Clusters To classify lines according to their alignment with the spatial frame, one could estimate vanishing points using RANSAC [1, 22]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "We estimate one vanishing point for each of the three line clusters [22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 113
                            }
                        ],
                        "text": "To classify lines according to their alignment with the spatial frame, one could estimate vanishing points using RANSAC [1, 22]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 134
                            }
                        ],
                        "text": "If camera calibration and manual cue inputs can be assumed, remarkable 3D reconstruction can be achieved using geometrical approaches [5, 13, 1, 22], whereas if the type of shape-from-X can be known a priori, then subtly curved surfaces can be precisely reconstructed using statistical approaches [17, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9321214,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "14bbcb48d4d00fc96aee831593bca207d9f72c9c",
            "isKey": true,
            "numCitedBy": 269,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate a strategy for reconstructing of buildings from multiple (uncalibrated) images. In a similar manner to the Facade approach we first generate a coarse piecewise planar model of the principal scene planes and their delineations, and then use these facets to guide the search for indentations and protrusions such as windows and doors. However, unlike the Facade approach which involves manual selection and alignment of the geometric primitives, the strategy here is fully automatic.There are several points of novelty: first we demonstrate that the use of quite generic models together with particular scene constraints (the availability of several principal directions) is sufficiently powerful to enable successful reconstruction of the targeted scenes. Second, we develop and refine a technique for piecewise planar model fitting involving sweeping polygonal primitives, and assess the performance of this technique. Third, lines at infinity are constructed from image correspondences and used to sweep planes in the principal directions.The strategy is illustrated on several image triplets of College buildings. It is demonstrated that convincing texture mapped models are generated which include the main walls and roofs, together with inset windows and also protruding (dormer) roof windows."
            },
            "slug": "New-Techniques-for-Automated-Architectural-from-Werner-Zisserman",
            "title": {
                "fragments": [],
                "text": "New Techniques for Automated Architectural Reconstruction from Photographs"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is demonstrated that convincing texture mapped models are generated which include the main walls and roofs, together with inset windows and also protruding (dormer) roof windows, and the performance of this technique is assessed."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 92
                            }
                        ],
                        "text": "shape-from-X modules, where X could be shading [19, 14], texture [8, 16], or line junctions [15, 2, 18])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6117451,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5732ad4b2e63b4bc322d2893188fba8728fd61b4",
            "isKey": false,
            "numCitedBy": 167,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Theory-of-Origami-World-Kanade",
            "title": {
                "fragments": [],
                "text": "A Theory of Origami World"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778676"
                        ],
                        "name": "P. Debevec",
                        "slug": "P.-Debevec",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Debevec",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Debevec"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31589308"
                        ],
                        "name": "C. J. Taylor",
                        "slug": "C.-J.-Taylor",
                        "structuredName": {
                            "firstName": "Camillo",
                            "lastName": "Taylor",
                            "middleNames": [
                                "Jose"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. J. Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 134
                            }
                        ],
                        "text": "If camera calibration and manual cue inputs can be assumed, remarkable 3D reconstruction can be achieved using geometrical approaches [5, 13, 1, 22], whereas if the type of shape-from-X can be known a priori, then subtly curved surfaces can be precisely reconstructed using statistical approaches [17, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2609415,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "37b0da92da8796835383f85c65cc81a386052a99",
            "isKey": false,
            "numCitedBy": 1992,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new approach for modeling and rendering existing architectural scenes from a sparse set of still photographs. Our modeling approach, which combines both geometry-based and imagebased techniques, has two components. The first component is a photogrammetricmodeling method which facilitates the recovery of the basic geometry of the photographed scene. Our photogrammetric modeling approach is effective, convenient, and robust because it exploits the constraints that are characteristic of architectural scenes. The second component is a model-based stereo algorithm, which recovers how the real scene deviates from the basic model. By making use of the model, our stereo technique robustly recovers accurate depth from widely-spaced image pairs. Consequently, our approach can model large architectural environments with far fewer photographs than current image-based modeling approaches. For producing renderings, we present view-dependent texture mapping, a method of compositing multiple views of a scene that better simulates geometric detail on basic models. Our approach can be used to recover models for use in either geometry-based or image-based rendering systems. We present results that demonstrate our approach\u2019s ability to create realistic renderings of architectural scenes from viewpoints far from the original photographs. CR Descriptors: I.2.10 [Artificial Intelligence]: Vision and Scene Understanding Modeling and recovery of physical attributes; I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism Color, shading, shadowing, and texture I.4.8 [Image Processing]: Scene Analysis Stereo; J.6 [Computer-Aided Engineering]: Computer-aided design (CAD)."
            },
            "slug": "Modeling-and-rendering-architecture-from-a-hybrid-Debevec-Taylor",
            "title": {
                "fragments": [],
                "text": "Modeling and rendering architecture from photographs: a hybrid geometry- and image-based approach"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This work presents a new approach for modeling and rendering existing architectural scenes from a sparse set of still photographs, which combines both geometry-based and imagebased techniques, and presents view-dependent texture mapping, a method of compositing multiple views of a scene that better simulates geometric detail on basic models."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 92
                            }
                        ],
                        "text": "shape-from-X modules, where X could be shading [19, 14], texture [8, 16], or line junctions [15, 2, 18])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11045251,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a4f8599c040dfd4876d9a5d99545b35b6a4e7a57",
            "isKey": false,
            "numCitedBy": 199,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we study the problem of interpreting line drawings of scenes composed of opaque regular solid objects bounded by piecewise smooth surfaces with no markings or texture on them. It is assumed that the line drawing has been formed by orthographic projection of such a scene under general viewpoint, that the line drawing is error free, and that there are no lines due to shadows or specularities. Our definition implicitly excludes laminae, wires, and the apices of cones.A major component of the interpretation of line drawings is line labelling. By line labelling we mean (a) classification of each image curve as corresponding to either a depth or orientation discontinuity in the scene, and (b) further subclassification of each kind of discontinuity. For a depth discontinuity we determine whether it is a limb\u2014a locus of points on the surface where the line of sight is tangent to the surface\u2014or an occluding edge\u2014a tangent plane discontinuity of the surface. For an orientation discontinuity, we determine whether it corresponds to a convex or concave edge. This paper presents the first mathematically rigorous scheme for labelling line drawings of the class of scenes described. Previous schemes for labelling line drawings of scenes containing curved objects were heuristic, incomplete, and lacked proper mathematical justification.By analyzing the projection of the neighborhoods of different kinds of points on a piecewise smooth surface, we are able to catalog all local labelling possibilities for the different types of junctions in a line drawing. An algorithm is developed which utilizes this catalog to determine all legal labellings of the line drawing. A local minimum complexity rule\u2014at each vertex select those labellings which correspond to the minimum number of faces meeting at the vertex\u2014is used in order to prune highly counter-intuitive interpretations. The labelling scheme was implemented and tested on a number of line drawings. The labellings obtained are few and by and large in accordance with human interpretations."
            },
            "slug": "Interpreting-line-drawings-of-curved-objects-Malik",
            "title": {
                "fragments": [],
                "text": "Interpreting line drawings of curved objects"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper presents the first mathematically rigorous scheme for labelling line drawings of the class of scenes described, which is able to catalog all local labelling possibilities for the different types of junctions in a line drawing."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107881808"
                        ],
                        "name": "Stella X. Yu",
                        "slug": "Stella-X.-Yu",
                        "structuredName": {
                            "firstName": "Stella",
                            "lastName": "Yu",
                            "middleNames": [
                                "X."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stella X. Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46865129"
                        ],
                        "name": "Jianbo Shi",
                        "slug": "Jianbo-Shi",
                        "structuredName": {
                            "firstName": "Jianbo",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianbo Shi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "Treating them as pairwise attraction and directional repulsion in a graph cuts framework [23], we integrate both types of cues simultaneously to reach a global depth ordering of quadrilaterals."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5615832,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bed0e992f8570f99961e41b924457b5109a6f022",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a method of image segmentation by integrating pairwise attraction and directional repulsion derived from local grouping and figure-ground cues. These two kinds of pairwise relationships are encoded in the real and imaginary parts of an Hermitian graph weight matrix, through which we can directly generalize the normalized cuts criterion. With bi-graph constructions, this method can be readily extended to handle nondirectional repulsion that captures dissimilarity. We demonstrate the use of repulsion in image segmentation with relative depth cues, which allows segmentation and figure-ground segregation to be computed simultaneously. As a general mechanism to represent the dual measures of attraction and repulsion, this method can also be employed to solve other constraint satisfaction and optimization problems."
            },
            "slug": "Segmentation-with-pairwise-attraction-and-repulsion-Yu-Shi",
            "title": {
                "fragments": [],
                "text": "Segmentation with Pairwise Attraction and Repulsion"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A method of image segmentation by integrating pairwise attraction and directional repulsion derived from local grouping and figure-ground cues, which allows segmentation andfigure-ground segregation to be computed simultaneously."
            },
            "venue": {
                "fragments": [],
                "text": "ICCV"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716777"
                        ],
                        "name": "A. Criminisi",
                        "slug": "A.-Criminisi",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Criminisi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Criminisi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145950884"
                        ],
                        "name": "I. Reid",
                        "slug": "I.-Reid",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Reid",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Reid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 82
                            }
                        ],
                        "text": "With some heuristics to deduce proportional depth relationships from a real image [4], we can visualize relative depth by back-projecting the texture in the 2D image onto planes in the 3D space (Fig."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2499410,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9e96483e264f5a063cadfa232fa325a56d146a0b",
            "isKey": false,
            "numCitedBy": 750,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe how 3D affine measurements may be computed from a single perspective view of a scene given only minimal geometric information determined from the image. This minimal information is typically the vanishing line of a reference plane, and a vanishing point for a direction not parallel to the plane. It is shown that affine scene structure may then be determined from the image, without knowledge of the camera's internal calibration (e.g. focal length), nor of the explicit relation between camera and world (pose).In particular, we show how to (i) compute the distance between planes parallel to the reference plane (up to a common scale factor); (ii) compute area and length ratios on any plane parallel to the reference plane; (iii) determine the camera's location. Simple geometric derivations are given for these results. We also develop an algebraic representation which unifies the three types of measurement and, amongst other advantages, permits a first order error propagation analysis to be performed, associating an uncertainty with each measurement.We demonstrate the technique for a variety of applications, including height measurements in forensic images and 3D graphical modelling from single images."
            },
            "slug": "Single-View-Metrology-Criminisi-Reid",
            "title": {
                "fragments": [],
                "text": "Single View Metrology"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An algebraic representation is developed which unifies the three types of measurement and permits a first order error propagation analysis to be performed, associating an uncertainty with each measurement."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064563493"
                        ],
                        "name": "Feng Han",
                        "slug": "Feng-Han",
                        "structuredName": {
                            "firstName": "Feng",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Feng Han"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[10] presents an attribute graph grammar with 6 production rules for parsing objects, surfaces, rectangles and their spatial relations in man-made scenes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5751287,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "338b3f99d1dda11252f27d34e09261a7dedc4cd3",
            "isKey": false,
            "numCitedBy": 153,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present an attribute graph grammar for image parsing on scenes with man-made objects, such as buildings, hallways, kitchens, and living moms. We choose one class of primitives - 3D planar rectangles projected on images and six graph grammar production rules. Each production rule not only expands a node into its components, but also includes a number of equations that constrain the attributes of a parent node and those of its children. Thus our graph grammar is context sensitive. The grammar rules are used recursively to produce a large number of objects and patterns in images and thus the whole graph grammar is a type of generative model. The inference algorithm integrates bottom-up rectangle detection which activates top-down prediction using the grammar rules. The final results are validated in a Bayesian framework. The output of the inference is a hierarchical parsing graph with objects, surfaces, rectangles, and their spatial relations. In the inference, the acceptance of a grammar rule means recognition of an object, and actions are taken to pass the attributes between a node and its parent through the constraint equations associated with this production rule. When an attribute is passed from a child node to a parent node, it is called bottom-up, and the opposite is called top-down"
            },
            "slug": "Bottom-up/top-down-image-parsing-by-attribute-graph-Han-Zhu",
            "title": {
                "fragments": [],
                "text": "Bottom-up/top-down image parsing by attribute graph grammar"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "An attribute graph grammar for image parsing on scenes with man-made objects, such as buildings, hallways, kitchens, and living moms is presented and the inference algorithm integrates bottom-up rectangle detection which activates top-down prediction using the grammar rules."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739995"
                        ],
                        "name": "Y. Horry",
                        "slug": "Y.-Horry",
                        "structuredName": {
                            "firstName": "Youichi",
                            "lastName": "Horry",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Horry"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794380"
                        ],
                        "name": "K. Anjyo",
                        "slug": "K.-Anjyo",
                        "structuredName": {
                            "firstName": "Ken-ichi",
                            "lastName": "Anjyo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Anjyo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144050755"
                        ],
                        "name": "K. Arai",
                        "slug": "K.-Arai",
                        "structuredName": {
                            "firstName": "Kiyoshi",
                            "lastName": "Arai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Arai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 134
                            }
                        ],
                        "text": "If camera calibration and manual cue inputs can be assumed, remarkable 3D reconstruction can be achieved using geometrical approaches [5, 13, 1, 22], whereas if the type of shape-from-X can be known a priori, then subtly curved surfaces can be precisely reconstructed using statistical approaches [17, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6914801,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "d30c530bfebfa6e6d31ccb2bd503ebb17845aab6",
            "isKey": false,
            "numCitedBy": 452,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method called TIP (Tour Into the Picture) is presented for easily making animations from one 2D picture or photograph of a scene. In TIP, animation is created from the viewpoint of a camera which can be three-dimensionally \"walked or flownthrough\" the 2D picture or photograph. To make such animation, conventional computer vision techniques cannot be applied in the 3D modeling process for the scene, using only a single 2D image. Instead a spidery mesh is employed in our method to obtain a simple scene model from the 2D image of the scene using a graphical user interface. Animation is thus easily generated without the need of multiple 2D images. Unlike existing methods, our method is not intended to construct a precise 3D scene model. The scene model is rather simple, and not fully 3D-structured. The modeling process starts by specifying the vanishing point in the 2D image. The background in the scene model then consists of at most five rectangles, whereas hierarchical polygons are used as a model for each foreground object. Furthermore a virtual camera is moved around the 3D scene model, with the viewing angle being freely controlled. This process is easily and effectively performed using the spidery mesh interface. We have obtained a wide variety of animated scenes which demonstrate the efficiency of TIP. CR"
            },
            "slug": "Tour-into-the-picture:-using-a-spidery-mesh-to-make-Horry-Anjyo",
            "title": {
                "fragments": [],
                "text": "Tour into the picture: using a spidery mesh interface to make animation from a single image"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A new method called TIP (Tour Into the Picture) is presented for easily making animations from one 2D picture or photograph of a scene using a graphical user interface, which is not intended to construct a precise 3D scene model."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3260234"
                        ],
                        "name": "J. G\u00e5rding",
                        "slug": "J.-G\u00e5rding",
                        "structuredName": {
                            "firstName": "Jonas",
                            "lastName": "G\u00e5rding",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. G\u00e5rding"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 65
                            }
                        ],
                        "text": "shape-from-X modules, where X could be shading [19, 14], texture [8, 16], or line junctions [15, 2, 18])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5646127,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "fe60bac0c4c20642d7b5ef2eec1340e69cc04967",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Witkin (1981) has proposed a maximum likelihood (ML) estimator of surface orientation based on the observed directional bias of projected texture elements. However, a drawback of this procedure is that the estimate is only defined indirectly in terms of a set of nonlinear equations. An alternative method is proposed, which allows an estimate of the surface orientation to be computed directly in a single step from certain simple statistics of the image data. We also show that this direct estimate allows Witkin's ML estimate to be computed to within 0.05 degrees in only two or three iterative steps. The performance of the new estimator is demonstrated experimentally and compared to that of the ML estimator, using both synthetic data and real gray-level images. >"
            },
            "slug": "Direct-Estimation-of-Shape-from-Texture-G\u00e5rding",
            "title": {
                "fragments": [],
                "text": "Direct Estimation of Shape from Texture"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "This work shows that a direct estimate of the surface orientation of surface orientation based on the observed directional bias of projected texture elements can be computed to within 0.05 degrees in only two or three iterative steps."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2277678"
                        ],
                        "name": "Anthony Lobay",
                        "slug": "Anthony-Lobay",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Lobay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anthony Lobay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 305,
                                "start": 297
                            }
                        ],
                        "text": "If camera calibration and manual cue inputs can be assumed, remarkable 3D reconstruction can be achieved using geometrical approaches [5, 13, 1, 22], whereas if the type of shape-from-X can be known a priori, then subtly curved surfaces can be precisely reconstructed using statistical approaches [17, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11311073,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c2b2b21face2b85cdd2b69f8b7f88ed07ae3d733",
            "isKey": false,
            "numCitedBy": 127,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a shape from texture method that constructs an estimate of surface geometry using only the deformation of individual texture elements. Our method does not need to use either the boundary of the observed surface or any assumption about the overall distribution of elements.The method assumes that surface texture elements are drawn from a number of different types, each of fixed shape. Neither the shape of the elements nor the number of types need be known in advance. We show that, with this assumption and assuming a generic, scaled orthographic view and texture, each type of texture element can be reconstructed in a frontal coordinate system from image instances. Interest-point methods supply a method of simultaneously obtaining instances of each texture element automatically and defining each type of element. Furthermore, image instances that have been marked in error can be identified and ignored using the Expectation-Maximization algorithm. A further EM procedure yields a surface reconstruction and a relative irradiance map from the data. We provide numerous examples of reconstructions for images of real scenes, show a comparison between our reconstruction and range maps, and demonstrate that the reconstructions display geometric and irradiance phenomena that can be observed in the original image."
            },
            "slug": "Shape-from-Texture-without-Boundaries-Lobay-Forsyth",
            "title": {
                "fragments": [],
                "text": "Shape from Texture without Boundaries"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A shape from texture method that constructs an estimate of surface geometry using only the deformation of individual texture elements using the assumption that surface texture elements are drawn from a number of different types, each of fixed shape."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739452"
                        ],
                        "name": "K. Ikeuchi",
                        "slug": "K.-Ikeuchi",
                        "structuredName": {
                            "firstName": "Katsushi",
                            "lastName": "Ikeuchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Ikeuchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143814637"
                        ],
                        "name": "Berthold K. P. Horn",
                        "slug": "Berthold-K.-P.-Horn",
                        "structuredName": {
                            "firstName": "Berthold",
                            "lastName": "Horn",
                            "middleNames": [
                                "K.",
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Berthold K. P. Horn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 47
                            }
                        ],
                        "text": "shape-from-X modules, where X could be shading [19, 14], texture [8, 16], or line junctions [15, 2, 18])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5738968,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f9a9153f15655ea8a07186d42307d6d43032d7d",
            "isKey": false,
            "numCitedBy": 803,
            "numCiting": 91,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Numerical-Shape-from-Shading-and-Occluding-Ikeuchi-Horn",
            "title": {
                "fragments": [],
                "text": "Numerical Shape from Shading and Occluding Boundaries"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1888069"
                        ],
                        "name": "V. Ramachandran",
                        "slug": "V.-Ramachandran",
                        "structuredName": {
                            "firstName": "Vilayanur",
                            "lastName": "Ramachandran",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Ramachandran"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 47
                            }
                        ],
                        "text": "shape-from-X modules, where X could be shading [19, 14], texture [8, 16], or line junctions [15, 2, 18])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15519525,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "3e44d3cd823b1a4df847786eabe56549213bef40",
            "isKey": false,
            "numCitedBy": 358,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Our visual experience of the world is based on two-dimensional images: nat patterns of varying light intensity and color faIling on a single plane of cells in the retina. Yet we come to perceive solidity and depth. We can do this because a number of cues about depth are available in the retinal image: shading, perspective, occlusion of one object by another and stereoscopic disparity. In some mysterious way the brain is able to exploit these cues to recover the three-dimensional shapes of objects. Of the many mechanisms employed by the visual system to recover the third dimension, the ability to exploit shading is probably the most primitive. One reason for believing this is that in the natural world many animals have evolved pale undersides, presumably to make themselves less visible to predators. \"Countershading\" compensates for the shading effects caused by the sun shining from above and has at least two benefits: it reduces the contrast with the background and it \"flattens\" the animal's perceived shape. The prevalence of countershading in a variety of species, including many fishes, suggests that shading may be a crucial source of information about three-dimensional shape. Painters, of course, have long ex-"
            },
            "slug": "Perceiving-shape-from-shading.-Ramachandran",
            "title": {
                "fragments": [],
                "text": "Perceiving shape from shading."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The prevalence of countershading in a variety of species, including many fishes, suggests that shading may be a crucial source of information about three-dimensional shape."
            },
            "venue": {
                "fragments": [],
                "text": "Scientific American"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809823"
                        ],
                        "name": "Matthew E. Antone",
                        "slug": "Matthew-E.-Antone",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Antone",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew E. Antone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720894"
                        ],
                        "name": "S. Teller",
                        "slug": "S.-Teller",
                        "structuredName": {
                            "firstName": "Seth",
                            "lastName": "Teller",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Teller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 156
                            }
                        ],
                        "text": "From Line Segments to Line Clusters To classify lines according to their alignment with the spatial frame, one could estimate vanishing points using RANSAC [1, 22]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 113
                            }
                        ],
                        "text": "To classify lines according to their alignment with the spatial frame, one could estimate vanishing points using RANSAC [1, 22]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 134
                            }
                        ],
                        "text": "If camera calibration and manual cue inputs can be assumed, remarkable 3D reconstruction can be achieved using geometrical approaches [5, 13, 1, 22], whereas if the type of shape-from-X can be known a priori, then subtly curved surfaces can be precisely reconstructed using statistical approaches [17, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13579741,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01fa61db4df3e363f4bce9f528c0b8e935b6195a",
            "isKey": false,
            "numCitedBy": 195,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we describe a formulation of extrinsic camera calibration that decouples rotation from translation by exploiting properties inherent in urban scenes. We then present an algorithm which uses edge features to robustly and accurately estimate relative rotations among multiple cameras given intrinsic calibration and approximate initial pose. The algorithm is linear both in the number of images and the number of features. We estimate the number and directions of vanishing points (VPs) with respect to each camera using a hybrid approach that combines the robustness of the Hough transform with the accuracy of expectation maximization. Matching and labeling methods identify unique VPs and correspond them across all cameras. Finally, a technique akin to bundle adjustment produces globally optimal estimates of relative camera rotations by bringing all VPs into optimal alignment. Uncertainty is modeled and used at every stage to improve accuracy. We assess the algorithm's performance on both synthetic and real data, and compare our results to those of semi-automated photogrammetric methods for a large set of real hemispherical images, using several consistency and error metrics."
            },
            "slug": "Automatic-recovery-of-relative-camera-rotations-for-Antone-Teller",
            "title": {
                "fragments": [],
                "text": "Automatic recovery of relative camera rotations for urban scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A formulation of extrinsic camera calibration that decouples rotation from translation by exploiting properties inherent in urban scenes is described and an algorithm which uses edge features to robustly and accurately estimate relative rotations among multiple cameras given intrinsic calibration and approximate initial pose is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No.PR00662)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8436115"
                        ],
                        "name": "J. Coughlan",
                        "slug": "J.-Coughlan",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Coughlan",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Coughlan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 52
                            }
                        ],
                        "text": "Such a statistical correlation has been utilized in [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207585013,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ba62db6d84a8aaa55f7b43390bb6c7e187e232f",
            "isKey": false,
            "numCitedBy": 128,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "This letter argues that many visual scenes are based on a Manhattan three-dimensional grid that imposes regularities on the image statistics. We construct a Bayesian model that implements this assumption and estimates the viewer orientation relative to the Manhattan grid. For many images, these estimates are good approximations to the viewer orientation (as estimated manually by the authors). These estimates also make it easy to detect outlier structures that are unaligned to the grid. To determine the applicability of the Manhattan world model, we implement a null hypothesis model that assumes that the image statistics are independent of any three-dimensional scene structure. We then use the log-likelihood ratio test to determine whether an image satisfies the Manhattan world assumption. Our results show that if an image is estimated to be Manhattan, then the Bayesian model's estimates of viewer direction are almost always accurate (according to our manual estimates), and vice versa."
            },
            "slug": "Manhattan-World:-Orientation-and-Outlier-Detection-Coughlan-Yuille",
            "title": {
                "fragments": [],
                "text": "Manhattan World: Orientation and Outlier Detection by Bayesian Inference"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "This letter argues that many visual scenes are based on a Manhattan three-dimensional grid that imposes regularities on the image statistics, and constructs a Bayesian model that implements this assumption and estimates the viewer orientation relative to the Manhattan grid."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46887763"
                        ],
                        "name": "C. Rasmussen",
                        "slug": "C.-Rasmussen",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Rasmussen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rasmussen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 305,
                                "start": 297
                            }
                        ],
                        "text": "If camera calibration and manual cue inputs can be assumed, remarkable 3D reconstruction can be achieved using geometrical approaches [5, 13, 1, 22], whereas if the type of shape-from-X can be known a priori, then subtly curved surfaces can be precisely reconstructed using statistical approaches [17, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9811838,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bf6be85b345f5f2d19fa7027d72d816491e20045",
            "isKey": false,
            "numCitedBy": 88,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Many rural roads lack sharp, smoothly curving edges and a homogeneous surface appearance, hampering traditional vision-based road-following methods. However, they often have strong texture cues parallel to the road direction in the form of ruts and tracks left by other vehicles. This paper describes an unsupervised algorithm for following ill-structured roads in which dominant texture orientations computed with Gabor wavelet filters vote for a consensus road vanishing point location. The technique is first described for estimating the direction of straight-road segments, then extended to curved and undulating roads by tracking the vanishing point indicated by a dierential \u201cstrip\u201d of voters moving up toward the nominal vanishing line. Finally, the vanishing point is used to constrain a search for the road boundaries by maximizing texture- and color-based region discriminant functions. Results are shown for a variety of road scenes including gravel roads, dirt trails, and highways."
            },
            "slug": "Texture-Based-Vanishing-Point-Voting-for-Road-Shape-Rasmussen",
            "title": {
                "fragments": [],
                "text": "Texture-Based Vanishing Point Voting for Road Shape Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "An unsupervised algorithm for following ill-structured roads in which dominant texture orientations computed with Gabor wavelet filters vote for a consensus road vanishing point location is described."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145306925"
                        ],
                        "name": "Sudeep Sarkar",
                        "slug": "Sudeep-Sarkar",
                        "structuredName": {
                            "firstName": "Sudeep",
                            "lastName": "Sarkar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sudeep Sarkar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734012"
                        ],
                        "name": "K. Boyer",
                        "slug": "K.-Boyer",
                        "structuredName": {
                            "firstName": "Kim",
                            "lastName": "Boyer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Boyer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 214
                            }
                        ],
                        "text": "We show that: 1) Despite the role of shading and texture gradients at revealing depth in certain scenarios, edges are more ubiquitous and sometimes sufficient for global depth analysis; 2) Gestalt laws of grouping [21] can be used to reliably evaluate perspectivity; 3) Compared to traditional shape-from-X methods, our depth from intersection point test is more sensitive to subtle depth differences and more universal in a single image."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 39596838,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "d31e783126a0943f888216b724cc7f449fd02495",
            "isKey": false,
            "numCitedBy": 229,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "The role of perceptual organization in computer vision systems is explored. This is done from four vantage points. A brief history of perceptual organization research in both humans and computer vision is offered. A classificatory structure in which to cast perceptual organization research to clarify both the nomenclature and the relationships among the many contributions is proposed. The perceptual organization work in computer vision in the context of this classificatory structure is reviewed. The array of computational techniques applied to perceptual organization problems in computer vision is surveyed. >"
            },
            "slug": "Perceptual-organization-in-computer-vision:-a-and-a-Sarkar-Boyer",
            "title": {
                "fragments": [],
                "text": "Perceptual organization in computer vision: a review and a proposal for a classificatory structure"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "The role of perceptual organization in computer vision systems is explored and a classificatory structure in which to cast perceptual organization research to clarify both the nomenclature and the relationships among the many contributions is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Man Cybern."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107881808"
                        ],
                        "name": "Stella X. Yu",
                        "slug": "Stella-X.-Yu",
                        "structuredName": {
                            "firstName": "Stella",
                            "lastName": "Yu",
                            "middleNames": [
                                "X."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stella X. Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46865129"
                        ],
                        "name": "Jianbo Shi",
                        "slug": "Jianbo-Shi",
                        "structuredName": {
                            "firstName": "Jianbo",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianbo Shi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 98
                            }
                        ],
                        "text": "We treat them as pairwise attraction and repulsion respectively, and use the graph cuts method in [24, 25] to find 3 clusters (Fig."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 119677,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "844e8b978dd256cb8a28a2d5374d5fa6b23cb08e",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Perceptual popout is defined by both feature similarity and local feature contrast. We identify these two measures with attraction and repulsion, and unify the dual processes of association by attraction and segregation by repulsion in a single grouping framework. We generalize normalized cuts to multi-way partitioning with these dual measures. We expand graph partitioning approaches to weight matrices with negative entries, and provide a theoretical basis for solution regularization in such algorithms. We show that attraction, repulsion and regularization each contributes in a unique way to popout. Their roles are demonstrated in various salience detection and visual search scenarios. This work opens up the possibilities of encoding negative correlations in constraint satisfaction problems, where solutions by simple and robust eigendecomposition become possible."
            },
            "slug": "Understanding-popout-through-repulsion-Yu-Shi",
            "title": {
                "fragments": [],
                "text": "Understanding popout through repulsion"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that attraction, repulsion and regularization each contributes in a unique way to popout, and opens up the possibilities of encoding negative correlations in constraint satisfaction problems, where solutions by simple and robust eigendecomposition become possible."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40393949"
                        ],
                        "name": "R. Hetherington",
                        "slug": "R.-Hetherington",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Hetherington",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hetherington"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 153
                            }
                        ],
                        "text": "vanishing points, homography [7]), while the latter studies what types of cues in the image are most correlated with the perception of the 3D real world [9] (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 140748236,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "bf47f891e889c39edd799eb1802a033bf7445412",
            "isKey": false,
            "numCitedBy": 2633,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Let's read! We will often find out this sentence everywhere. When still being a kid, mom used to order us to always read, so did the teacher. Some books are fully read in a week and we need the obligation to support reading. What about now? Do you still love reading? Is reading only for you who have obligation? Absolutely not! We here offer you a new book enPDFd the perception of the visual world to read."
            },
            "slug": "The-Perception-of-the-Visual-World-Hetherington",
            "title": {
                "fragments": [],
                "text": "The Perception of the Visual World"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1952
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107881808"
                        ],
                        "name": "Stella X. Yu",
                        "slug": "Stella-X.-Yu",
                        "structuredName": {
                            "firstName": "Stella",
                            "lastName": "Yu",
                            "middleNames": [
                                "X."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stella X. Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46865129"
                        ],
                        "name": "Jianbo Shi",
                        "slug": "Jianbo-Shi",
                        "structuredName": {
                            "firstName": "Jianbo",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianbo Shi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 98
                            }
                        ],
                        "text": "We treat them as pairwise attraction and repulsion respectively, and use the graph cuts method in [24, 25] to find 3 clusters (Fig."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2196967,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d08824dd86424f7a81f3e8f463f68a817b43aabe",
            "isKey": false,
            "numCitedBy": 975,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a principled account on multiclass spectral clustering. Given a discrete clustering formulation, we first solve a relaxed continuous optimization problem by eigen-decomposition. We clarify the role of eigenvectors as a generator of all optimal solutions through orthonormal transforms. We then solve an optimal discretization problem, which seeks a discrete solution closest to the continuous optima. The discretization is efficiently computed in an iterative fashion using singular value decomposition and nonmaximum suppression. The resulting discrete solutions are nearly global-optimal. Our method is robust to random initialization and converges faster than other clustering methods. Experiments on real image segmentation are reported."
            },
            "slug": "Multiclass-spectral-clustering-Yu-Shi",
            "title": {
                "fragments": [],
                "text": "Multiclass spectral clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "This work proposes a principled account on multiclass spectral clustering by solving a relaxed continuous optimization problem by eigen-decomposition and clarifying the role of eigenvectors as a generator of all optimal solutions through orthonormal transforms."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30457331"
                        ],
                        "name": "J. Gibson",
                        "slug": "J.-Gibson",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Gibson",
                            "middleNames": [
                                "Jerome"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gibson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "vanishing points, homography [7]), while the latter studies what types of cues in the image are most correlated with the perception of the 3D real world [ 9 ] (e.g."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 195034367,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "fc0d34992ba4c2c68806eb52c096749f847f5424",
            "isKey": false,
            "numCitedBy": 2204,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-perception-of-the-visual-world-Gibson",
            "title": {
                "fragments": [],
                "text": "The perception of the visual world"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1950
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2263388"
                        ],
                        "name": "P. K. Ghosh",
                        "slug": "P.-K.-Ghosh",
                        "structuredName": {
                            "firstName": "Pijush",
                            "lastName": "Ghosh",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. K. Ghosh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3250452"
                        ],
                        "name": "S. Mudur",
                        "slug": "S.-Mudur",
                        "structuredName": {
                            "firstName": "Sudhir",
                            "lastName": "Mudur",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mudur"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 29
                            }
                        ],
                        "text": "vanishing points, homography [7]), while the latter studies what types of cues in the image are most correlated with the perception of the 3D real world [9] (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61523068,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9b5cfa32dc382c468fd7410a28783b4b85712844",
            "isKey": false,
            "numCitedBy": 1464,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Three-Dimensional-Computer-Vision:-A-Geometric-Ghosh-Mudur",
            "title": {
                "fragments": [],
                "text": "Three-Dimensional Computer Vision: A Geometric Viewpoint"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Shape-from-X with Applicable Cues"
            },
            "venue": {
                "fragments": [],
                "text": "Shape-from-X with Applicable Cues"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Depth Ordering between 3D Quadrilaterals 15"
            },
            "venue": {
                "fragments": [],
                "text": "Depth Ordering between 3D Quadrilaterals 15"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 22,
            "methodology": 6
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 28,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Inferring-spatial-layout-from-a-single-image-via-Yu-Zhang/47075aa3dfde4ae9b0498375fb7c27cc3d785bd5?sort=total-citations"
}