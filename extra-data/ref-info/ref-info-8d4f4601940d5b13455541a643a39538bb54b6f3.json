{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144570279"
                        ],
                        "name": "F. Bach",
                        "slug": "F.-Bach",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Bach",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Bach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 30
                            }
                        ],
                        "text": "In particular, in recent work [4], we have generalized ICA m N F-ica Jade Imax Kcca Kgv"
                    },
                    "intents": []
                }
            ],
            "corpusId": 11598843,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "9e800b185d038422d358748f30de7b23bc421dc1",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a generalization of independent component analysis (ICA), where instead of looking for a linear transform that makes the data components independent, we look for a transform that makes the data components well fit by a tree-structured graphical model. Treating the problem as a semiparametric statistical problem, we show that the optimal transform is found by minimizing a contrast function based on mutual information, a function that directly extends the contrast function used for classical ICA. We provide two approximations of this contrast function, one using kernel density estimation, and another using kernel generalized variance. This tree-dependent component analysis framework leads naturally to an efficient general multivariate density estimation technique where only bivariate density estimation needs to be performed."
            },
            "slug": "Tree-dependent-Component-Analysis-Bach-Jordan",
            "title": {
                "fragments": [],
                "text": "Tree-dependent Component Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This paper presents a generalization of independent component analysis (ICA), where instead of looking for a linear transform that makes the data components independent, it is shown that the optimal transform is found by minimizing a contrast function based on mutual information."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783831"
                        ],
                        "name": "P. Comon",
                        "slug": "P.-Comon",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Comon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Comon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18340548,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96a1effa4be3f8caa88270d6d258de418993d2e7",
            "isKey": false,
            "numCitedBy": 8327,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Independent-component-analysis,-A-new-concept-Comon",
            "title": {
                "fragments": [],
                "text": "Independent component analysis, A new concept?"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16135158,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "8d946c3eb1d1db376a89ad9342282163b5ae0930",
            "isKey": false,
            "numCitedBy": 5791,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "Independent component analysis (ICA) is a statistical method for transforming an observed multidimensional random vector into components that are statistically as independent from each other as possible. In this paper, we use a combination of two different approaches for linear ICA: Comon's information-theoretic approach and the projection pursuit approach. Using maximum entropy approximations of differential entropy, we introduce a family of new contrast (objective) functions for ICA. These contrast functions enable both the estimation of the whole decomposition by minimizing mutual information, and estimation of individual independent components as projection pursuit directions. The statistical properties of the estimators based on such contrast functions are analyzed under the assumption of the linear mixture model, and it is shown how to choose contrast functions that are robust and/or of minimum variance. Finally, we introduce simple fixed-point algorithms for practical optimization of the contrast functions. These algorithms optimize the contrast functions very fast and reliably."
            },
            "slug": "Fast-and-robust-fixed-point-algorithms-for-analysis-Hyv\u00e4rinen",
            "title": {
                "fragments": [],
                "text": "Fast and robust fixed-point algorithms for independent component analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Using maximum entropy approximations of differential entropy, a family of new contrast (objective) functions for ICA enable both the estimation of the whole decomposition by minimizing mutual information, and estimation of individual independent components as projection pursuit directions."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678311"
                        ],
                        "name": "M. Welling",
                        "slug": "M.-Welling",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Welling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Welling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110604335"
                        ],
                        "name": "Markus Weber",
                        "slug": "Markus-Weber",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Weber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Markus Weber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13960919,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4093e53d0a6c0600d16adac5d9c545b01b90dce0",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a novel way of performing independent component analysis using a constrained version of the expectation-maximization (EM) algorithm. The source distributions are modeled as D one-dimensional mixtures of gaussians. The observed data are modeled as linear mixtures of the sources with additive, isotropic noise. This generative model is fit to the data using constrained EM. The simpler soft-switching approach is introduced, which uses only one parameter to decide on the sub- or supergaussian nature of the sources. We explain how our approach relates to independent factor analysis."
            },
            "slug": "A-Constrained-EM-Algorithm-for-Independent-Analysis-Welling-Weber",
            "title": {
                "fragments": [],
                "text": "A Constrained EM Algorithm for Independent Component Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A novel way of performing independent component analysis using a constrained version of the expectation-maximization (EM) algorithm and the simpler soft-switching approach is introduced, which uses only one parameter to decide on the sub- or supergaussian nature of the sources."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786990"
                        ],
                        "name": "H. Attias",
                        "slug": "H.-Attias",
                        "structuredName": {
                            "firstName": "Hagai",
                            "lastName": "Attias",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Attias"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 746481,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2307fd6058ab4f7554a0b1f188507150ddb5b9a2",
            "isKey": false,
            "numCitedBy": 596,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce the independent factor analysis (IFA) method for recovering independent hidden sources from their observed mixtures. IFA generalizes and unifies ordinary factor analysis (FA), principal component analysis (PCA), and independent component analysis (ICA), and can handle not only square noiseless mixing but also the general case where the number of mixtures differs from the number of sources and the data are noisy. IFA is a two-step procedure. In the first step, the source densities, mixing matrix, and noise covariance are estimated from the observed data by maximum likelihood. For this purpose we present an expectation-maximization (EM) algorithm, which performs unsupervised learning of an associated probabilistic model of the mixing situation. Each source in our model is described by a mixture of gaussians; thus, all the probabilistic calculations can be performed analytically. In the second step, the sources are reconstructed from the observed data by an optimal nonlinear estimator. A variational approximation of this algorithm is derived for cases with a large number of sources, where the exact algorithm becomes intractable. Our IFA algorithm reduces to the one for ordinary FA when the sources become gaussian, and to an EM algorithm for PCA in the zero-noise limit. We derive an additional EM algorithm specifically for noiseless IFA. This algorithm is shown to be superior to ICA since it can learn arbitrary source densities from the data. Beyond blind separation, IFA can be used for modeling multidimensional data by a highly constrained mixture of gaussians and as a tool for nonlinear signal encoding."
            },
            "slug": "Independent-Factor-Analysis-Attias",
            "title": {
                "fragments": [],
                "text": "Independent Factor Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An expectation-maximization (EM) algorithm is presented, which performs unsupervised learning of an associated probabilistic model of the mixing situation and is shown to be superior to ICA since it can learn arbitrary source densities from the data."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 49
                            }
                        ],
                        "text": "Following the spirit of the derivation of kernel PCA [10], it is straightforward to derive a \u201ckernelization\u201d of CCA, which turns out to involve substituting products of Gram matrices for the covariance matrices in Eq."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 17
                            }
                        ],
                        "text": "As in kernel PCA (Sch\u00f6lkopf et al., 1998), the key point to notice is that we only need to consider the subspace of F that contains the span of the data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "While PCA works with a single random vector and maximizes the variance of projections of the data, CCA works with a pair of random vectors (or in general with a set of m random vectors) and\nmaximizes correlation between sets of projections."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "While PCA leads to an eigenvector problem, CCA leads to a generalized eigenvector problem."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 99
                            }
                        ],
                        "text": "Finally, just as PCA can be carried out efficiently in an RKHS by making use of the \u201ckernel trick\u201d (Sch\u00f6lkopf et al., 1998), so too can CCA (as we show in Section 3."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 128
                            }
                        ],
                        "text": "Canonical correlation analysis (CCA) is a multivariate statistical technique similar in spirit to principal component analysis (PCA)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 179
                            }
                        ],
                        "text": "If the points \u03a6(xi ) are not centered, then although it is impossible to actually center them in feature space, it is possible to find the Gram matrix of the centered data points (Sch\u00f6lkopf et al., 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6674407,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3f600e6c6cf93e78c9e6e690443d6d22c4bf18b9",
            "isKey": true,
            "numCitedBy": 7883,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method for performing a nonlinear form of principal component analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in high-dimensional feature spaces, related to input space by some nonlinear mapfor instance, the space of all possible five-pixel products in 16 16 images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition."
            },
            "slug": "Nonlinear-Component-Analysis-as-a-Kernel-Eigenvalue-Sch\u00f6lkopf-Smola",
            "title": {
                "fragments": [],
                "text": "Nonlinear Component Analysis as a Kernel Eigenvalue Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A new method for performing a nonlinear form of principal component analysis by the use of integral operator kernel functions is proposed and experimental results on polynomial feature extraction for pattern recognition are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144815314"
                        ],
                        "name": "J. Cardoso",
                        "slug": "J.-Cardoso",
                        "structuredName": {
                            "firstName": "Jean-Fran\u00e7ois",
                            "lastName": "Cardoso",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cardoso"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "First, our approach generalizes in a straightforward manner to multidimensional ICA ( Cardoso, 1998 ), which is a variant of ICA with multivariate components."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9824633,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8676573fb87797b0e744f1fd62d230c3fb9903ad",
            "isKey": false,
            "numCitedBy": 456,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes to generalize the notion of independent component analysis (ICA) to the notion of multidimensional independent component analysis (MICA). We start from the ICA or blind source separation (BSS) model and show that it can be uniquely identified provided it is properly parameterized in terms of one-dimensional subspaces. From this standpoint, the BSS/ICA model is generalized to multidimensional components. We discuss how ICA standard algorithms can be adapted to MICA decomposition. The relevance of these ideas is illustrated by a MICA decomposition of ECG signals."
            },
            "slug": "Multidimensional-independent-component-analysis-Cardoso",
            "title": {
                "fragments": [],
                "text": "Multidimensional independent component analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper starts from the ICA or blind source separation model and shows that it can be uniquely identified provided it is properly parameterized in terms of one-dimensional subspaces and generalized to multidimensional components."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP '98 (Cat. No.98CH36181)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1963591"
                        ],
                        "name": "A. Buja",
                        "slug": "A.-Buja",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Buja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Buja"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 120429697,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "0032cf1d8f00a68a425bed4c63d85a2df3bbd8dc",
            "isKey": false,
            "numCitedBy": 119,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We discuss properties of some data-analytic methods which are intimately related to each other: alternating least squares (ALS), correspondence analysis and more recently Breiman and Friedman's ACE algorithm. The application of these methods to regression produces nonparametric estimators of nonlinear transformations, both of the response and the predictors. We point out some anomalies as well as some curiosities in the mathematics of these methods, and we relate them to some areas in computer-aided tomography, projection pursuit regression and nonlinear devices in the theory of noise"
            },
            "slug": "Remarks-on-Functional-Canonical-Variates,-Least-and-Buja",
            "title": {
                "fragments": [],
                "text": "Remarks on Functional Canonical Variates, Alternating Least Squares Methods and Ace"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31651045"
                        ],
                        "name": "N. Vlassis",
                        "slug": "N.-Vlassis",
                        "structuredName": {
                            "firstName": "Nikos",
                            "lastName": "Vlassis",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Vlassis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056420"
                        ],
                        "name": "Y. Motomura",
                        "slug": "Y.-Motomura",
                        "structuredName": {
                            "firstName": "Yoichi",
                            "lastName": "Motomura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Motomura"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10681958,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "62e388d3b0a1a167dacedaa65829937b2bf7d174",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "A basic element in most independent component analysis (ICA) algorithms is the choice of a model for the score functions of the unknown sources. While this is usually based on approximations, for large data sets it is possible to achieve \"source adaptivity\" by directly estimating from the data the \"true\" score functions of the sources. We describe an efficient scheme for achieving this by extending the fast density estimation method of Silverman (1982). We show with a real and a synthetic experiment that our method can provide more accurate solutions than state-of-the-art methods when optimization is carried out in the vicinity of the global minimum of the contrast function."
            },
            "slug": "Efficient-source-adaptivity-in-independent-analysis-Vlassis-Motomura",
            "title": {
                "fragments": [],
                "text": "Efficient source adaptivity in independent component analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown with a real and a synthetic experiment that this method can provide more accurate solutions than state-of-the-art methods when optimization is carried out in the vicinity of the global minimum of the contrast function."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34377369"
                        ],
                        "name": "Riccardo Boscolo",
                        "slug": "Riccardo-Boscolo",
                        "structuredName": {
                            "firstName": "Riccardo",
                            "lastName": "Boscolo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Riccardo Boscolo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49349283"
                        ],
                        "name": "Hong Pan",
                        "slug": "Hong-Pan",
                        "structuredName": {
                            "firstName": "Hong",
                            "lastName": "Pan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hong Pan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686063"
                        ],
                        "name": "V. Roychowdhury",
                        "slug": "V.-Roychowdhury",
                        "structuredName": {
                            "firstName": "Vwani",
                            "lastName": "Roychowdhury",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Roychowdhury"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14175917,
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "id": "ef562d40c001ae0e8e6a828917c27c056ac10fae",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a novel approach to the blind signal separation (BSS) problem that is capable of jointly estimating the probability density function (pdf) of the source signals and the unmixing matrix. We demonstrate that, using a kernel density estimation based Projection Pursuit (PP) algorithm, it is possible to extract, from instantaneous mixtures, independent sources that are arbitrarily distributed. The proposed algorithm is non-parametric, and unlike conventional Independent Component Analysis (ICA) frameworks, it requires neither the definition of a contrast function, nor the minimization of the high-order cross-cumulants of the reconstructed signals. We derive a new method for solving the resulting constrained optimization problem that is capable of accurately and efficiently estimating the unmixing matrix, and which does not require the selection of any tuning parameters. Our simulations demonstrate that the proposed method can accurately separate sources with arbitrary marginal pdfs with significant performance gain when compared to existing ICA algorithms. In particular, we are successful in separating mixtures of skewed, almost zero-kurtotic signals, which other ICA algorithms fail to separate."
            },
            "slug": "NON-PARAMETRIC-ICA-Boscolo-Pan",
            "title": {
                "fragments": [],
                "text": "NON-PARAMETRIC ICA"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A new method is derived for solving the resulting constrained optimization problem that is capable of accurately and efficiently estimating the unmixing matrix, and which does not require the selection of any tuning parameters."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145290346"
                        ],
                        "name": "M. Borga",
                        "slug": "M.-Borga",
                        "structuredName": {
                            "firstName": "Magnus",
                            "lastName": "Borga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Borga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3117916"
                        ],
                        "name": "H. Knutsson",
                        "slug": "H.-Knutsson",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Knutsson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Knutsson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2976013"
                        ],
                        "name": "T. Landelius",
                        "slug": "T.-Landelius",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Landelius",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Landelius"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 43
                            }
                        ],
                        "text": "While PCA works with a single random vector and maximizes the variance of projections of the data, CCA works with a pair of random vectors (or in general with a set of m random vectors) and\nmaximizes correlation between sets of projections."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15002629,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7e591e07e7a2d836ee28e2fb594207ebd1c34eb2",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a novel learning algorithm that finds the linear combination of one set of multi-dimensional variates that is the best predictor, and at the same time finds the linear combination of another set which is the most predictable. This relation is known as the canonical correlation and has the property of being invariant with respect to affine transformations of the two sets of variates. The algorithm successively finds all the canonical correlations beginning with the largest one. It is shown that canonical correlations can be used in computer vision to find feature detectors by giving examples of the desired features. When used on the pixel level, the method finds quadrature filters and when used on a higher level, the method finds combinations of filter output that are less sensitive to noise compared to vector averaging."
            },
            "slug": "Learning-Canonical-Correlations-Borga-Knutsson",
            "title": {
                "fragments": [],
                "text": "Learning Canonical Correlations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103917250"
                        ],
                        "name": "Te-Won Lee",
                        "slug": "Te-Won-Lee",
                        "structuredName": {
                            "firstName": "Te-Won",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Te-Won Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50859190"
                        ],
                        "name": "M. Girolami",
                        "slug": "M.-Girolami",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Girolami",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Girolami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Comparisons were made with three existing ICA algorithms: the FastICA algorithm [8], the lade algorithm [7], and the extended Infomax algorithm [ 9 ]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 207739442,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "642f41bc18b36dead2b85e45a93bcfb8379224a2",
            "isKey": false,
            "numCitedBy": 1710,
            "numCiting": 122,
            "paperAbstract": {
                "fragments": [],
                "text": "An extension of the infomax algorithm of Bell and Sejnowski (1995) is presented that is able blindly to separate mixed signals with sub- and supergaussian source distributions. This was achieved by using a simple type of learning rule first derived by Girolami (1997) by choosing negentropy as a projection pursuit index. Parameterized probability distributions that have sub- and supergaussian regimes were used to derive a general learning rule that preserves the simple architecture proposed by Bell and Sejnowski (1995), is optimized using the natural gradient by Amari (1998), and uses the stability analysis of Cardoso and Laheld (1996) to switch between sub- and supergaussian regimes. We demonstrate that the extended infomax algorithm is able to separate 20 sources with a variety of source distributions easily. Applied to high-dimensional data from electroencephalographic recordings, it is effective at separating artifacts such as eye blinks and line noise from weaker electrical signals that arise from sources in the brain."
            },
            "slug": "Independent-Component-Analysis-Using-an-Extended-Lee-Girolami",
            "title": {
                "fragments": [],
                "text": "Independent Component Analysis Using an Extended Infomax Algorithm for Mixed Subgaussian and Supergaussian Sources"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "An extension of the infomax algorithm of Bell and Sejnowski (1995) is presented that is able blindly to separate mixed signals with sub- and supergaussian source distributions and is effective at separating artifacts such as eye blinks and line noise from weaker electrical signals that arise from sources in the brain."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143842629"
                        ],
                        "name": "T. Melzer",
                        "slug": "T.-Melzer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Melzer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Melzer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060020641"
                        ],
                        "name": "M. Reiter",
                        "slug": "M.-Reiter",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Reiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Reiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144746444"
                        ],
                        "name": "H. Bischof",
                        "slug": "H.-Bischof",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Bischof",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bischof"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "2  Melzer et al. (2001)  have independently derived the kernelized CCA algorithm for two variables that"
                    },
                    "intents": []
                }
            ],
            "corpusId": 33570889,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "260e6ff494c26202bcd2929de40cf7d1c732b4f9",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a new non-linear feature extraction technique based on Canonical Correlation Analysis (CCA) with applications in regression and object recognition. The non-linear transformation of the input data is performed using kernel-methods. Although, in this respect, our approach is similar to other generalized linear methods like kernel-PCA, our method is especially well suited for relating two sets of measurements. The benefits of our method compared to standard feature extraction methods based on PCA will be illustrated with several experiments from the field of object recognition and pose estimation."
            },
            "slug": "Nonlinear-Feature-Extraction-Using-Generalized-Melzer-Reiter",
            "title": {
                "fragments": [],
                "text": "Nonlinear Feature Extraction Using Generalized Canonical Correlation Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A new non-linear feature extraction technique based on Canonical Correlation Analysis (CCA), which is especially well suited for relating two sets of measurements, with applications in regression and object recognition."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734990"
                        ],
                        "name": "S. Harmeling",
                        "slug": "S.-Harmeling",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Harmeling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Harmeling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2522238"
                        ],
                        "name": "A. Ziehe",
                        "slug": "A.-Ziehe",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Ziehe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ziehe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716788"
                        ],
                        "name": "M. Kawanabe",
                        "slug": "M.-Kawanabe",
                        "structuredName": {
                            "firstName": "Motoaki",
                            "lastName": "Kawanabe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kawanabe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7875856,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "080d3bccb37eb85334c17b2f3871131ca6862ccd",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "In kernel based learning the data is mapped to a kernel feature space of a dimension that corresponds to the number of training data points. In practice, however, the data forms a smaller submanifold in feature space, a fact that has been used e.g. by reduced set techniques for SVMs. We propose a new mathematical construction that permits to adapt to the intrinsic dimension and to find an orthonormal basis of this submanifold. In doing so, computations get much simpler and more important our theoretical framework allows to derive elegant kernelized blind source separation (BSS) algorithms for arbitrary invertible nonlinear mixings. Experiments demonstrate the good performance and high computational efficiency of our kTDSEP algorithm for the problem of nonlinear BSS."
            },
            "slug": "Kernel-Feature-Spaces-and-Nonlinear-Blind-Souce-Harmeling-Ziehe",
            "title": {
                "fragments": [],
                "text": "Kernel Feature Spaces and Nonlinear Blind Souce Separation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new mathematical construction is proposed that permits to adapt to the intrinsic dimension of the data and to find an orthonormal basis of this submanifold and allows to derive elegant kernelized blind source separation (BSS) algorithms for arbitrary invertible nonlinear mixings."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 601110,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "8985a9637540daa0b7b8295f8a5bbda3a3be1dea",
            "isKey": false,
            "numCitedBy": 673,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-connection-between-regularization-operators-and-Smola-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "The connection between regularization operators and support vector kernels"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696508"
                        ],
                        "name": "C. Jutten",
                        "slug": "C.-Jutten",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Jutten",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jutten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798563"
                        ],
                        "name": "J. H\u00e9rault",
                        "slug": "J.-H\u00e9rault",
                        "structuredName": {
                            "firstName": "Jeanny",
                            "lastName": "H\u00e9rault",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H\u00e9rault"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The earliest ICA algorithms were (in retrospect) based on contrast functions dened in terms of expectations of a single xed nonlinear function, chosen in an ad-hoc manner ( Jutten and Herault, 1991 )."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "While in principle one could form an empirical mutual information or empirical likelihood, which is subsequently optimized with respect to W , the more common approach to ICA is to work with approximations to the mutual information (Amari et al., 1996, Comon, 1994, Hyv\u00a8 arinen, 1999), or to use alternative contrast functions ( Jutten and Herault, 1991 )."
                    },
                    "intents": []
                }
            ],
            "corpusId": 33162734,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e73081ed096c62c073b3faa1b3b80aab89998c5",
            "isKey": false,
            "numCitedBy": 2689,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Blind-separation-of-sources,-part-I:-An-adaptive-on-Jutten-H\u00e9rault",
            "title": {
                "fragments": [],
                "text": "Blind separation of sources, part I: An adaptive algorithm based on neuromimetic architecture"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145683892"
                        ],
                        "name": "A. Cichocki",
                        "slug": "A.-Cichocki",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Cichocki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cichocki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8896870"
                        ],
                        "name": "H. Yang",
                        "slug": "H.-Yang",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Yang",
                            "middleNames": [
                                "Hua"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 209
                            }
                        ],
                        "text": "Unfortunately, the mutual information for real-valued variables is difficult to approximate and optimize on the basis of a finite sample, and much research on ICA has focused on alternative contrast functions [8, 7, 1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 131
                            }
                        ],
                        "text": "We measure the performance of the algorithm in terms of the difference between W and W0, via the standard ICA metric introduced by [1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7941673,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fac0e753905d1498e0b3debf01431696e1f0c645",
            "isKey": false,
            "numCitedBy": 2220,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A new on-line learning algorithm which minimizes a statistical dependency among outputs is derived for blind separation of mixed signals. The dependency is measured by the average mutual information (MI) of the outputs. The source signals and the mixing matrix are unknown except for the number of the sources. The Gram-Charlier expansion instead of the Edgeworth expansion is used in evaluating the MI. The natural gradient approach is used to minimize the MI. A novel activation function is proposed for the on-line learning algorithm which has an equivariant property and is easily implemented on a neural network like model. The validity of the new learning algorithm are verified by computer simulations."
            },
            "slug": "A-New-Learning-Algorithm-for-Blind-Signal-Amari-Cichocki",
            "title": {
                "fragments": [],
                "text": "A New Learning Algorithm for Blind Signal Separation"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A new on-line learning algorithm which minimizes a statistical dependency among outputs is derived for blind separation of mixed signals and has an equivariant property and is easily implemented on a neural network like model."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144616256"
                        ],
                        "name": "D. Pham",
                        "slug": "D.-Pham",
                        "structuredName": {
                            "firstName": "Dinh",
                            "lastName": "Pham",
                            "middleNames": [
                                "Tuan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46355559"
                        ],
                        "name": "Philippe Garat",
                        "slug": "Philippe-Garat",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Garat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philippe Garat"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 32042337,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "f93fed0bac99e37acf30ea0c1356725f74ec2b78",
            "isKey": false,
            "numCitedBy": 494,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose two methods for separating mixture of independent sources without any precise knowledge of their probability distribution. They are obtained by considering a maximum likelihood (ML) solution corresponding to some given distributions of the sources and relaxing this assumption afterward. The first method is specially adapted to temporally independent non-Gaussian sources and is based on the use of nonlinear separating functions. The second method is specially adapted to correlated sources with distinct spectra and is based on the use of linear separating filters. A theoretical analysis of the performance of the methods has been made. A simple procedure for optimally choosing the separating functions is proposed. Further, in the second method, a simple implementation based on the simultaneous diagonalization of two symmetric matrices is provided. Finally, some numerical and simulation results are given, illustrating the performance of the method and the good agreement between the experiments and the theory."
            },
            "slug": "Blind-separation-of-mixture-of-independent-sources-Pham-Garat",
            "title": {
                "fragments": [],
                "text": "Blind separation of mixture of independent sources through a quasi-maximum likelihood approach"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "Two methods for separating mixture of independent sources without any precise knowledge of their probability distribution are proposed by considering a maximum likelihood (ML) solution corresponding to some given distributions of the sources and relaxing this assumption afterward."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187218"
                        ],
                        "name": "A. J. Bell",
                        "slug": "A.-J.-Bell",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Bell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 128
                            }
                        ],
                        "text": "By specifying distributions for the components xi, one obtains a parametric model that can be estimated via maximum li kelihood [5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1701422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d7d0e8c4791700defd4b0df82a26b50055346e0",
            "isKey": false,
            "numCitedBy": 8757,
            "numCiting": 121,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive a new self-organizing learning algorithm that maximizes the information transferred in a network of nonlinear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximization has extra properties not found in the linear case (Linsker 1989). The nonlinearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalization of principal components analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to 10 speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal). Finally, we derive dependencies of information transfer on time delays. We suggest that information maximization provides a unifying framework for problems in \"blind\" signal processing."
            },
            "slug": "An-Information-Maximization-Approach-to-Blind-and-Bell-Sejnowski",
            "title": {
                "fragments": [],
                "text": "An Information-Maximization Approach to Blind Separation and Blind Deconvolution"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is suggested that information maximization provides a unifying framework for problems in \"blind\" signal processing and dependencies of information transfer on time delays are derived."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145319478"
                        ],
                        "name": "Michael J. Jones",
                        "slug": "Michael-J.-Jones",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jones",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For concreteness we restrict ourselves to translation-invariant kernels in this paper; that is, to kernel functions of the form K(x; y )= k(x y). In this case the RKHS can be described succinctly using Fourier theory ( Girosi et al., 1995,  Smola et al., 1998)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 49743910,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "eae2430d9a984120bf511655a03c15089b007499",
            "isKey": false,
            "numCitedBy": 1366,
            "numCiting": 198,
            "paperAbstract": {
                "fragments": [],
                "text": "We had previously shown that regularization principles lead to approximation schemes that are equivalent to networks with one layer of hidden units, called regularization networks. In particular, standard smoothness functionals lead to a subclass of regularization networks, the well known radial basis functions approximation schemes. This paper shows that regularization networks encompass a much broader range of approximation schemes, including many of the popular general additive models and some of the neural networks. In particular, we introduce new classes of smoothness functionals that lead to different classes of basis functions. Additive splines as well as some tensor product splines can be obtained from appropriate classes of smoothness functionals. Furthermore, the same generalization that extends radial basis functions (RBF) to hyper basis functions (HBF) also leads from additive models to ridge approximation models, containing as special cases Breiman's hinge functions, some forms of projection pursuit regression, and several types of neural networks. We propose to use the term generalized regularization networks for this broad class of approximation schemes that follow from an extension of regularization. In the probabilistic interpretation of regularization, the different classes of basis functions correspond to different classes of prior probabilities on the approximating function spaces, and therefore to different types of smoothness assumptions. In summary, different multilayer networks with one hidden layer, which we collectively call generalized regularization networks, correspond to different classes of priors and associated smoothness functionals in a classical regularization principle. Three broad classes are (1) radial basis functions that can be generalized to hyper basis functions, (2) some tensor product splines, and (3) additive splines that can be generalized to schemes of the type of ridge approximation, hinge functions, and several perceptron-like neural networks with one hidden layer."
            },
            "slug": "Regularization-Theory-and-Neural-Networks-Girosi-Jones",
            "title": {
                "fragments": [],
                "text": "Regularization Theory and Neural Networks Architectures"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "This paper shows that regularization networks encompass a much broader range of approximation schemes, including many of the popular general additive models and some of the neural networks, and introduces new classes of smoothness functionals that lead to different classes of basis functions."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48893922"
                        ],
                        "name": "S. Leurgans",
                        "slug": "S.-Leurgans",
                        "structuredName": {
                            "firstName": "Sue",
                            "lastName": "Leurgans",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Leurgans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2080793"
                        ],
                        "name": "R. Moyeed",
                        "slug": "R.-Moyeed",
                        "structuredName": {
                            "firstName": "Rana",
                            "lastName": "Moyeed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Moyeed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49800308"
                        ],
                        "name": "B. Silverman",
                        "slug": "B.-Silverman",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Silverman",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Silverman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 52
                            }
                        ],
                        "text": "It is also possible to use crossvalidation to set \u03ba (Leurgans et al., 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 119032630,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f460ab210eda29cff479bf561d70d18abef70db7",
            "isKey": false,
            "numCitedBy": 305,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY It is not immediately straightforward to extend canonical correlation analysis to the context of functional data analysis, where the data are themselves curves or functions. The obvious approach breaks down, and it is necessary to use a method involving smoothing in some way. Such a method is introduced and discussed with reference to a data set on human gait. The breakdown of the unsmoothed method is illustrated in a practical context and is demonstrated theoretically. A consistency theorem for the smoothed method is proved. In an increasing number of problems in a wide range of fields, the data observed are not the univariate or multivariate observations of classical statistics, but are func- tions observed continuously. Ramsay and Dalzell (1991) give the name functional data analysis to the analysis of data of this kind. In most cases, the observations will be functions of time, or a closely related variable, but there are clearly applications where the functions are surfaces observed over two- or three-dimensional space. The motivating example for the present paper is a set of data on children's gait collected by the Motion Analysis Laboratory at Children's Hospital, San Diego, California; see Olshen et al. (1989) for full details. For each of a number of children, several angles made by the child's joints (knee, hip, etc.) are observed during the child's gait cycle. One aim of the study is to gain understanding of the gait cycle of a 'normal' child to make comparisons with children suffering from walking diffi- culties. These data motivated Rice and Silverman (1991) to discuss the extension of principal component analysis to the functional setting, and to explain how smoothing can be incorporated into the analysis in a natural way. In a substantial complemen- tary paper, Ramsay and Dalzell (1991) discuss different approaches to regression and principal component analysis in functional data analysis, illustrating their work by a meteorological example. The insights of functional data analysis may be helpful in chemometrics; for instance a standard chemometric problem is the analysis of spectra observed in chromatography, and these are to all intents and purposes functional observations. Another obvious area of potential relevance is the analysis of growth curve data. The focus in the present paper will be on canonical correlation analysis (CCA). In the gait example, one might ask how variability in the knee angle cycle is related to"
            },
            "slug": "Canonical-correlation-analysis-when-the-data-are-Leurgans-Moyeed",
            "title": {
                "fragments": [],
                "text": "Canonical correlation analysis when the data are curves."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144397527"
                        ],
                        "name": "A. Edelman",
                        "slug": "A.-Edelman",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Edelman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Edelman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35933266"
                        ],
                        "name": "T. Arias",
                        "slug": "T.-Arias",
                        "structuredName": {
                            "firstName": "Tom\u00e1s",
                            "lastName": "Arias",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Arias"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120869470"
                        ],
                        "name": "S.T. Smith",
                        "slug": "S.T.-Smith",
                        "structuredName": {
                            "firstName": "S.T.",
                            "lastName": "Smith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S.T. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 89
                            }
                        ],
                        "text": "The set of all m \u00d7 m matrices W such that W T W = I is an instance of a Stiefel manifold (Edelman et al., 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16632320,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "11ca955f8d42dcb24b48b94f5faed41f673bd0f1",
            "isKey": false,
            "numCitedBy": 2414,
            "numCiting": 127,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we develop new Newton and conjugate gradient algorithms on the Grassmann and Stiefel manifolds. These manifolds represent the constraints that arise in such areas as the symmetric eigenvalue problem, nonlinear eigenvalue problems, electronic structures computations, and signal processing. In addition to the new algorithms, we show how the geometrical framework gives penetrating new insights allowing us to create, understand, and compare algorithms. The theory proposed here provides a taxonomy for numerical linear algebra algorithms that provide a top level mathematical view of previously unrelated algorithms. It is our hope that developers of new algorithms and perturbation theories will benefit from the theory, methods, and examples in this paper."
            },
            "slug": "The-Geometry-of-Algorithms-with-Orthogonality-Edelman-Arias",
            "title": {
                "fragments": [],
                "text": "The Geometry of Algorithms with Orthogonality Constraints"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The theory proposed here provides a taxonomy for numerical linear algebra algorithms that provide a top level mathematical view of previously unrelated algorithms and developers of new algorithms and perturbation theories will benefit from the theory."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Matrix Anal. Appl."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 207585383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a767a341364de1f75bea85e0b12ba7d3586a461",
            "isKey": false,
            "numCitedBy": 2730,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "When a parameter space has a certain underlying structure, the ordinary gradient of a function does not represent its steepest direction, but the natural gradient does. Information geometry is used for calculating the natural gradients in the parameter space of perceptrons, the space of matrices (for blind source separation), and the space of linear dynamical systems (for blind source deconvolution). The dynamical behavior of natural gradient online learning is analyzed and is proved to be Fisher efficient, implying that it has asymptotically the same performance as the optimal batch estimation of parameters. This suggests that the plateau phenomenon, which appears in the backpropagation learning algorithm of multilayer perceptrons, might disappear or might not be so serious when the natural gradient is used. An adaptive method of updating the learning rate is proposed and analyzed."
            },
            "slug": "Natural-Gradient-Works-Efficiently-in-Learning-Amari",
            "title": {
                "fragments": [],
                "text": "Natural Gradient Works Efficiently in Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The dynamical behavior of natural gradient online learning is analyzed and is proved to be Fisher efficient, implying that it has asymptotically the same performance as the optimal batch estimation of parameters."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684722"
                        ],
                        "name": "S. Fine",
                        "slug": "S.-Fine",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Fine",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2005127"
                        ],
                        "name": "K. Scheinberg",
                        "slug": "K.-Scheinberg",
                        "structuredName": {
                            "firstName": "Katya",
                            "lastName": "Scheinberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Scheinberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13899309,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0db7af02be7cbadc029f9104a8c784d02de42df7",
            "isKey": false,
            "numCitedBy": 662,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "SVM training is a convex optimization problem which scales with the training set size rather than the feature space dimension. While this is usually considered to be a desired quality, in large scale problems it may cause training to be impractical. The common techniques to handle this difficulty basically build a solution by solving a sequence of small scale subproblems. Our current effort is concentrated on the rank of the kernel matrix as a source for further enhancement of the training procedure. We first show that for a low rank kernel matrix it is possible to design a better interior point method (IPM) in terms of storage requirements as well as computational complexity. We then suggest an efficient use of a known factorization technique to approximate a given kernel matrix by a low rank matrix, which in turn will be used to feed the optimizer. Finally, we derive an upper bound on the change in the objective function value based on the approximation error and the number of active constraints (support vectors). This bound is general in the sense that it holds regardless of the approximation method."
            },
            "slug": "Efficient-SVM-Training-Using-Low-Rank-Kernel-Fine-Scheinberg",
            "title": {
                "fragments": [],
                "text": "Efficient SVM Training Using Low-Rank Kernel Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work shows that for a low rank kernel matrix it is possible to design a better interior point method (IPM) in terms of storage requirements as well as computational complexity and derives an upper bound on the change in the objective function value based on the approximation error and the number of active constraints (support vectors)."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727076"
                        ],
                        "name": "H. Lodhi",
                        "slug": "H.-Lodhi",
                        "structuredName": {
                            "firstName": "Huma",
                            "lastName": "Lodhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Lodhi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5516891,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "942ed8a81716c1f1038ed091ca6149bc3ae22c8f",
            "isKey": false,
            "numCitedBy": 319,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Kernel methods like support vector machines have successfully been used for text categorization. A standard choice of kernel function has been the inner product between the vector-space representation of two documents, in analogy with classical information retrieval (IR) approaches.Latent semantic indexing (LSI) has been successfully used for IR purposes as a technique for capturing semantic relations between terms and inserting them into the similarity measure between two documents. One of its main drawbacks, in IR, is its computational cost.In this paper we describe how the LSI approach can be implemented in a kernel-defined feature space.We provide experimental results demonstrating that the approach can significantly improve performance, and that it does not impair it."
            },
            "slug": "Latent-Semantic-Kernels-Cristianini-Shawe-Taylor",
            "title": {
                "fragments": [],
                "text": "Latent Semantic Kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper describes how the LSI approach can be implemented in a kernel-defined feature space and provides experimental results demonstrating that the approach can significantly improve performance, and that it does not impair it."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Intelligent Information Systems"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2242773"
                        ],
                        "name": "H. Widom",
                        "slug": "H.-Widom",
                        "structuredName": {
                            "firstName": "Harold",
                            "lastName": "Widom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Widom"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 121113493,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2b6f86d8e33e98798c3bbce8ed0b740dd531632f",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "then To is just convolution by k. However we shall not insist that this be the case. Let V(x) be a bounded non-negative function with bounded support and denote by MVI/2 the operator on L2(Ed) which is multiplication by V(x)1/2. We shall denote by 21 > A2 _> ? the positive eigenvalues of the positive semi-definite operator MVl/2ToMvI/2. In case K is the Fourier transform of an L1 function k then this is just the integral operator in equation (1).The result (Theorem II) is as"
            },
            "slug": "Asymptotic-behavior-of-the-eigenvalues-of-certain-Widom",
            "title": {
                "fragments": [],
                "text": "Asymptotic behavior of the eigenvalues of certain integral equations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1963
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2156394833"
                        ],
                        "name": "Jean-Franois Cardoso",
                        "slug": "Jean-Franois-Cardoso",
                        "structuredName": {
                            "firstName": "Jean-Franois",
                            "lastName": "Cardoso",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Franois Cardoso"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 209
                            }
                        ],
                        "text": "Unfortunately, the mutual information for real-valued variables is difficult to approximate and optimize on the basis of a finite sample, and much research on ICA has focused on alternative contrast functions [8, 7, 1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 104
                            }
                        ],
                        "text": "Comparisons were made with three existing ICA algorithms: the FastICA algorithm [8], the Jade algorithm [7], and the extended Infomax algorithm [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14842756,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ac14ba0722f22f0f316be7eda19f14e96cd557fb",
            "isKey": false,
            "numCitedBy": 1254,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "This article considers high-order measures of independence for the independent component analysis problem and discusses the class of Jacobi algorithms for their optimization. Several implementations are discussed. We compare the proposed approaches with gradient-based techniques from the algorithmic point of view and also on a set of biomedical data."
            },
            "slug": "High-Order-Contrasts-for-Independent-Component-Cardoso",
            "title": {
                "fragments": [],
                "text": "High-Order Contrasts for Independent Component Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "This article considers high-order measures of independence for the independent component analysis problem and discusses the class of Jacobi algorithms for their optimization and compares the proposed approaches with gradient-based techniques from the algorithmic point of view and also on a set of biomedical data."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 153
                            }
                        ],
                        "text": "This is equivalent to choosing the feature space of functions on Rm\u22121 to be the tensor product of the m \u2212 1 (identical) feature spaces of functions on R (Vapnik, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 67
                            }
                        ],
                        "text": "Instead, in our simulations, we use the Hermite polynomial kernels (Vapnik, 1998), which correspond to a more natural norm in a space of polynomials: If we let hk(x) denote the k-th Hermite polynomials (Szeg\u00f6, 1975), we define the following Hermite polynomial kernel of order d, K(x, y) = \u2211d k=0 e \u2212x2/2\u03c32e\u2212y (2)/2\u03c3(2) hk(x/\u03c3)hk(y/\u03c3) 2kk! ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 28637672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "385197d4c02593e2823c71e4f90a0993b703620e",
            "isKey": false,
            "numCitedBy": 26320,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "slug": "Statistical-learning-theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "Statistical learning theory"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46969875"
                        ],
                        "name": "J. Kettenring",
                        "slug": "J.-Kettenring",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Kettenring",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kettenring"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "There are several ways to generalize CCA to more than two sets of variables ( Kettenring, 1971 )."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "(For alternative generalizations of CCA, see  Kettenring, 1971 )."
                    },
                    "intents": []
                }
            ],
            "corpusId": 18043644,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "259a7282ac6fc2f4ea996a7d5065c56ecb1a6e8f",
            "isKey": false,
            "numCitedBy": 563,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY Five extensions of the classical two-set theory of canonical correlation analysis to three or more sets are considered. For each one, a model of the general principal component type is constructed to aid in motivating, comparing and understanding the methods. Procedures are developed for finding the canonical variables associated with the different approaches. Some practical considerations and an example are also included."
            },
            "slug": "Canonical-analysis-of-several-sets-of-variables-Kettenring",
            "title": {
                "fragments": [],
                "text": "Canonical analysis of several sets of variables"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2242773"
                        ],
                        "name": "H. Widom",
                        "slug": "H.-Widom",
                        "structuredName": {
                            "firstName": "Harold",
                            "lastName": "Widom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Widom"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Widom (1963, 1964)  provides some useful results regarding the spectra of the operator T dened in Eq. (25) for translation-invariant kernels of the form k(x y). He shows that the rate of decay of the spectrum depends only on the rate of decay of the Fourier tranform (!) of k, and of the rate of decay of the probability density function of the underlying input variable x. Moreover, he provides asymptotic equivalents for many cases of ..."
                    },
                    "intents": []
                }
            ],
            "corpusId": 17784620,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "31c0369fdfa8fe0435a416a5358e61b321094625",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "then T0 is just convolution by fe. However we shall not insist that this be the case. Let F(x) be a bounded non-negative function with bounded support and denote by MyVi the operator on L2(Ed) which is multiplication by F(x)1/2. We shall denote by XX^.X2^.--the positive eigenvalues of the positive semi-definite operator Mv'^T0Mv'\u00e0. In case K is the Fourier transform of an Lt function k then this is just the integral operator in equation (l).The result (Theorem II) is as"
            },
            "slug": "Asymptotic-behavior-of-the-eigenvalues-of-certain-Widom",
            "title": {
                "fragments": [],
                "text": "Asymptotic behavior of the eigenvalues of certain integral equations. II"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727076"
                        ],
                        "name": "H. Lodhi",
                        "slug": "H.-Lodhi",
                        "structuredName": {
                            "firstName": "Huma",
                            "lastName": "Lodhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Lodhi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144884649"
                        ],
                        "name": "C. Saunders",
                        "slug": "C.-Saunders",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Saunders",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Saunders"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4562073"
                        ],
                        "name": "C. Watkins",
                        "slug": "C.-Watkins",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Watkins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Watkins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Second, kernels can be dened on data that are not necessarily numerical (e.g., the \\string kernels\" of  Lodhi et al., 2001 ), and it is interesting to explore the possibility that"
                    },
                    "intents": []
                }
            ],
            "corpusId": 669209,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f330f1f472f860212b980bb9be81eff884f7f0e1",
            "isKey": false,
            "numCitedBy": 1643,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a novel kernel for comparing two text documents. The kernel is an inner product in the feature space consisting of all subsequences of length k. A subsequence is any ordered sequence of k characters occurring in the text though not necessarily contiguously. The subsequences are weighted by an exponentially decaying factor of their full length in the text, hence emphasising those occurrences which are close to contiguous. A direct computation of this feature vector would involve a prohibitive amount of computation even for modest values of k, since the dimension of the feature space grows exponentially with k. The paper describes how despite this fact the inner product can be efficiently evaluated by a dynamic programming technique. A preliminary experimental comparison of the performance of the kernel compared with a standard word feature space kernel [6] is made showing encouraging results."
            },
            "slug": "Text-Classification-using-String-Kernels-Lodhi-Saunders",
            "title": {
                "fragments": [],
                "text": "Text Classification using String Kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A novel kernel is introduced for comparing two text documents consisting of an inner product in the feature space consisting of all subsequences of length k, which can be efficiently evaluated by a dynamic programming technique."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47652868"
                        ],
                        "name": "H. Hotelling",
                        "slug": "H.-Hotelling",
                        "structuredName": {
                            "firstName": "Harold",
                            "lastName": "Hotelling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hotelling"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 90
                            }
                        ],
                        "text": "This is exactly the definition of the first canonical correlation between \u03a6(x1) and \u03a6(x2) (Hotelling, 1936)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 122166830,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "45db76270416a42517a21c63a77e9c4260fa979a",
            "isKey": false,
            "numCitedBy": 5596,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Concepts of correlation and regression may be applied not only to ordinary one-dimensional variates but also to variates of two or more dimensions. Marksmen side by side firing simultaneous shots at targets, so that the deviations are in part due to independent individual errors and in part to common causes such as wind, provide a familiar introduction to the theory of correlation; but only the correlation of the horizontal components is ordinarily discussed, whereas the complex consisting of horizontal and vertical deviations may be even more interesting. The wind at two places may be compared, using both components of the velocity in each place. A fluctuating vector is thus matched at each moment with another fluctuating vector. The study of individual differences in mental and physical traits calls for a detailed study of the relations between sets of correlated variates. For example the scores on a number of mental tests may be compared with physical measurements on the same persons. The questions then arise of determining the number and nature of the independent relations of mind and body shown by these data to exist, and of extracting from the multiplicity of correlations in the system suitable characterizations of these independent relations. As another example, the inheritance of intelligence in rats might be studied by applying not one but s different mental tests to N mothers and to a daughter of each"
            },
            "slug": "Relations-Between-Two-Sets-of-Variates-Hotelling",
            "title": {
                "fragments": [],
                "text": "Relations Between Two Sets of Variates"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1936
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144731788"
                        ],
                        "name": "Stephen J. Wright",
                        "slug": "Stephen-J.-Wright",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Wright",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen J. Wright"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 202
                            }
                        ],
                        "text": "Following Fine and Scheinberg (2001), the particular tool that we employ here is the incomplete Cholesky decomposition, commonly used in implementations of interior point methods for linear programming (Wright, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2947306,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0eca0a4af617b33c2665bf9582894478dc98ad96",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate a modified Cholesky algorithm typical of those used in most interior-point codes for linear programming. Cholesky-based interior-point codes are popular for three reasons: their implementation requires only minimal changes to standard sparse Cholesky algorithms (allowing us to take full advantage of software written by specialists in that area); they tend to be more efficient than competing approaches that use alternative factorizations; and they perform robustly on most practical problems, yielding good interior-point steps even when the coefficient matrix of the main linear system to be solved for the step components is ill conditioned. We investigate this surprisingly robust performance by using analytical tools from matrix perturbation theory and error analysis, illustrating our results with computational experiments. Finally, we point out the potential limitations of this approach."
            },
            "slug": "Modified-Cholesky-Factorizations-in-Interior-Point-Wright",
            "title": {
                "fragments": [],
                "text": "Modified Cholesky Factorizations in Interior-Point Algorithms for Linear Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A modified Cholesky algorithm typical of those used in most interior-point codes for linear programming is investigated, yielding good interior- point steps even when the coefficient matrix of the main linear system to be solved for the step components is ill conditioned."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Optim."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2135764"
                        ],
                        "name": "A. Kolmogorov",
                        "slug": "A.-Kolmogorov",
                        "structuredName": {
                            "firstName": "Andrei",
                            "lastName": "Kolmogorov",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kolmogorov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "mation between all discretizations of x and y ( Kolmogorov, 1956 )."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1185837,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0e02b535c582f43140d34c5322d117f89be25799",
            "isKey": false,
            "numCitedBy": 347,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In addition to the scheduled program, the following two papers, by A. N. Kolmogorov and V. I. Siforov, were presented at the 1956 Symposium on Information Theory. However, the manuscripts were received too late for inclusion in the September (Symposium) issue of these TRANSACTIONS. The papers were submitted in response to our invitation to these distinguished Russian scientists, and the following translations were distributed to those attending the Symposium.--The Editor."
            },
            "slug": "On-the-Shannon-theory-of-information-transmission-Kolmogorov",
            "title": {
                "fragments": [],
                "text": "On the Shannon theory of information transmission in the case of continuous signals"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "In addition to the scheduled program, the following two papers, by A. N. Kolmogorov and V. I. Siforov, were presented at the 1956 Symposium on Information Theory."
            },
            "venue": {
                "fragments": [],
                "text": "IRE Trans. Inf. Theory"
            },
            "year": 1956
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678771"
                        ],
                        "name": "P. Bickel",
                        "slug": "P.-Bickel",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bickel",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bickel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 247,
                                "start": 244
                            }
                        ],
                        "text": "In practical applications, however, one does not generally know the distributions of the components xi, and it is preferable to view the ICA model as a semiparametric model in which the distributions of the components of x are left unspecified [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 119754792,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2ea931fb55701a5f5b82aa491a64c0e431fb4b38",
            "isKey": false,
            "numCitedBy": 1391,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction.- Asymptotic Inference for (Finite-Dimensional) Parametric Models.- Information Bounds for Euclidean Parameters in Infinite-Dimensional Models.- Euclidean Parameters: Further Examples.- Information Bounds for Infinite-Dimensional Parameters.- Infinite-Dimensional Parameters: Further Examples: Construction of Examples."
            },
            "slug": "Efficient-and-Adaptive-Estimation-for-Models-Bickel",
            "title": {
                "fragments": [],
                "text": "Efficient and Adaptive Estimation for Semiparametric Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15652822,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7de372cae64dea5263076b5139c6b79df9e3157b",
            "isKey": false,
            "numCitedBy": 1647,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract In regression analysis the response variable Y and the predictor variables X 1 \u2026, Xp are often replaced by functions \u03b8(Y) and O1(X 1), \u2026, O p (Xp ). We discuss a procedure for estimating those functions \u03b8 and O1, \u2026, O p that minimize e 2 = E{[\u03b8(Y) \u2014 \u03a3 O j (Xj )]2}/var[\u03b8(Y)], given only a sample {(yk , xk1 , \u2026, xkp ), 1 \u2a7d k \u2a7d N} and making minimal assumptions concerning the data distribution or the form of the solution functions. For the bivariate case, p = 1, \u03b8 and O satisfy \u03c1 = p(\u03b8, O) = max\u03b8,O\u03c1[\u03b8(Y), O(X)], where \u03c1 is the product moment correlation coefficient and \u03c1 is the maximal correlation between X and Y. Our procedure thus also provides a method for estimating the maximal correlation between two variables."
            },
            "slug": "Estimating-Optimal-Transformations-for-Multiple-and-Breiman-Friedman",
            "title": {
                "fragments": [],
                "text": "Estimating Optimal Transformations for Multiple Regression and Correlation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144522912"
                        ],
                        "name": "R. Durrett",
                        "slug": "R.-Durrett",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Durrett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Durrett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Unfortunately, the mutual information is dicult to approximate and optimize on the basis of a nite sample, and much research on ICA has focused on alternative contrast functions (Amari et al., 1996,  Comon, 1994,  Hyv\u00a8 arinen and Oja, 1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "While in principle one could form an empirical mutual information or empirical likelihood, which is subsequently optimized with respect to W , the more common approach to ICA is to work with approximations to the mutual information (Amari et al., 1996,  Comon, 1994,  Hyv\u00a8 arinen, 1999), or to use alternative contrast functions (Jutten and Herault, 1991)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "which implies that x1 and x2 are independent ( Durrett, 1996 )."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Moreover, given that the mutual information of a random vector is nonnegative, and zero if and only if the components of the vector are independent, the use of mutual information as a function to be minimized is well motivated, quite apart from the link to maximum likelihood ( Comon, 1994 )."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "1 The identiability of the ICA model has been discussed by  Comon (1994) ."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16740132,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "aaaae6628c9c33e165e5cd5538b27d9275c24347",
            "isKey": true,
            "numCitedBy": 5163,
            "numCiting": 167,
            "paperAbstract": {
                "fragments": [],
                "text": "This book is an introduction to probability theory covering laws of large numbers, central limit theorems, random walks, martingales, Markov chains, ergodic theorems, and Brownian motion. It is a comprehensive treatment concentrating on the results that are the most useful for applications. Its philosophy is that the best way to learn probability is to see it in action, so there are 200 examples and 450 problems."
            },
            "slug": "Probability:-Theory-and-Examples-Durrett",
            "title": {
                "fragments": [],
                "text": "Probability: Theory and Examples"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725180"
                        ],
                        "name": "T. Cormen",
                        "slug": "T.-Cormen",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cormen",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cormen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145372049"
                        ],
                        "name": "C. Leiserson",
                        "slug": "C.-Leiserson",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Leiserson",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Leiserson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113911099"
                        ],
                        "name": "R. Rivest",
                        "slug": "R.-Rivest",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rivest",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rivest"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 222237163,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f01c4e51cb33f4bed8d37832dc1325ec5dedf49d",
            "isKey": false,
            "numCitedBy": 12423,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nThe updated new edition of the classic Introduction to Algorithms is intended primarily for use in undergraduate or graduate courses in algorithms or data structures. Like the first edition,this text can also be used for self-study by technical professionals since it discusses engineering issues in algorithm design as well as the mathematical aspects. \nIn its new edition,Introduction to Algorithms continues to provide a comprehensive introduction to the modern study of algorithms. The revision has been updated to reflect changes in the years since the book's original publication. New chapters on the role of algorithms in computing and on probabilistic analysis and randomized algorithms have been included. Sections throughout the book have been rewritten for increased clarity,and material has been added wherever a fuller explanation has seemed useful or new information warrants expanded coverage. \nAs in the classic first edition,this new edition of Introduction to Algorithms presents a rich variety of algorithms and covers them in considerable depth while making their design and analysis accessible to all levels of readers. Further,the algorithms are presented in pseudocode to make the book easily accessible to students from all programming language backgrounds. \nEach chapter presents an algorithm,a design technique,an application area,or a related topic. The chapters are not dependent on one another,so the instructor can organize his or her use of the book in the way that best suits the course's needs. Additionally,the new edition offers a 25% increase over the first edition in the number of problems,giving the book 155 problems and over 900 exercises thatreinforcethe concepts the students are learning."
            },
            "slug": "Introduction-to-Algorithms-Cormen-Leiserson",
            "title": {
                "fragments": [],
                "text": "Introduction to Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "The updated new edition of the classic Introduction to Algorithms is intended primarily for use in undergraduate or graduate courses in algorithms or data structures and presents a rich variety of algorithms and covers them in considerable depth while making their design and analysis accessible to all levels of readers."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "Following the spirit of the derivation of kernel PCA [10], it is straightforward to derive a \u201ckernelization\u201d of CCA, which turns out to involve substituting products of Gram matrices for the covariance matrices in Eq."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "While PCA works with a single random vector and maximizes the variance of projections of the data, CCA works with a pair of random vectors (or in general with a set of m random vectors) and\nmaximizes correlation between sets of projections."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "While PCA leads to an eigenvector problem, CCA leads to a generalized eigenvector problem."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "The centered Gram matrices [10] K1 and K2 are defined as the Gram matrices of the centered (in feature space) data points and are equal to"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 158
                            }
                        ],
                        "text": "Recent research on kernel methods has yielded important new computational tools for solving large-scale, nonparametric classification and regression problems [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 205
                            }
                        ],
                        "text": "However, Gram matrices have a spectrum that tends to show rapid decay, and low-rank approximations of Gram matrices can therefore often provide sufficient fidelity for the needs of kernel-based algorithms [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 151
                            }
                        ],
                        "text": "In this paper, our focus is the Gaussian kernel, K(x, y) = exp(\u2212(x\u2212y)(2)/2\u03c3(2)), which corresponds to an infinite-dimensional RKHS of smooth functions [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 128
                            }
                        ],
                        "text": "Canonical correlation analysis (CCA) is a multivariate statistical technique similar in spirit to principal component analysis (PCA)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 29871328,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5051890e501117097eeffbd8ded87694f0d8063",
            "isKey": true,
            "numCitedBy": 6578,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "All rights reserved. No part of this book may be reproduced in any form by any electronic or mechanical means (including photocopying, recording, or information storage and retrieval) without permission in writing from the publisher."
            },
            "slug": "Learning-with-kernels-Smola",
            "title": {
                "fragments": [],
                "text": "Learning with kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "This book is intended to be a guide to the art of self-consistency and should not be used as a substitute for a comprehensive guide to self-confidence."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703769"
                        ],
                        "name": "J. Karhunen",
                        "slug": "J.-Karhunen",
                        "structuredName": {
                            "firstName": "Juha",
                            "lastName": "Karhunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Karhunen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Unfortunately, the mutual information for real-valued var iables is difficult to approximate and optimize on the basis of a finit e sample, and much research on ICA has focused on alternative contrast functions [ 8 , 7, 1]. These have either been derived as expansionbased approximations to the mutual information, or have had a looser relationship to the mutual information, essentiall y borrowing its key property of being equal to zero if and ..."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Comparisons were made with three existing ICA algorithms: the FastICA algorithm [ 8 ], the Jade algorithm [7], and the extended Infomax algorithm [9]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This is done by whitening the data and subsequently restricting the minimization to orthogonal matrices W [ 8 ]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "W and provides estimates of the latent components via \u02c6 x = \u02c6 Wy [ 8 ]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Independent component analysis (ICA)[ 8 ] is an interesting unsupervised learning problem in which to explore these issues."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9558257,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "573dfd36d62d4619196888e27beb946b3747716b",
            "isKey": true,
            "numCitedBy": 1251,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "In this chapter, we discuss a statistical generative model called independent component analysis. It is basically a proper probabilistic formulation of the ideas underpinning sparse coding. It shows how sparse coding can be interpreted as providing a Bayesian prior, and answers some questions which were not properly answered in the sparse coding framework."
            },
            "slug": "Independent-Component-Analysis-Hyv\u00e4rinen-Karhunen",
            "title": {
                "fragments": [],
                "text": "Independent Component Analysis"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Neural Networks"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1911861"
                        ],
                        "name": "V. Totik",
                        "slug": "V.-Totik",
                        "structuredName": {
                            "firstName": "Vilmos",
                            "lastName": "Totik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Totik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 202
                            }
                        ],
                        "text": "Instead, in our simulations, we use the Hermite polynomial kernels (Vapnik, 1998), which correspond to a more natural norm in a space of polynomials: If we let hk(x) denote the k-th Hermite polynomials (Szeg\u00f6, 1975), we define the following Hermite polynomial kernel of order d, K(x, y) = \u2211d k=0 e \u2212x2/2\u03c32e\u2212y (2)/2\u03c3(2) hk(x/\u03c3)hk(y/\u03c3) 2kk! ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15399994,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ee4204e5c0d58006735508e13762e7a6f3491138",
            "isKey": false,
            "numCitedBy": 6065,
            "numCiting": 116,
            "paperAbstract": {
                "fragments": [],
                "text": "In this survey, different aspects of the theory of orthogonal polynomials of one (real or complex) variable are reviewed. Orthogonal polynomials on the unit circle are not discussed."
            },
            "slug": "Orthogonal-Polynomials-Totik",
            "title": {
                "fragments": [],
                "text": "Orthogonal Polynomials"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703769"
                        ],
                        "name": "J. Karhunen",
                        "slug": "J.-Karhunen",
                        "structuredName": {
                            "firstName": "Juha",
                            "lastName": "Karhunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Karhunen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Unfortunately, the mutual information for real-valued var iables is difficult to approximate and optimize on the basis of a finit e sample, and much research on ICA has focused on alternative contrast functions [ 8 , 7, 1]. These have either been derived as expansionbased approximations to the mutual information, or have had a looser relationship to the mutual information, essentiall y borrowing its key property of being equal to zero if and ..."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Comparisons were made with three existing ICA algorithms: the FastICA algorithm [ 8 ], the Jade algorithm [7], and the extended Infomax algorithm [9]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This is done by whitening the data and subsequently restricting the minimization to orthogonal matrices W [ 8 ]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "W and provides estimates of the latent components via \u02c6 x = \u02c6 Wy [ 8 ]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Independent component analysis (ICA)[ 8 ] is an interesting unsupervised learning problem in which to explore these issues."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9558257,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "573dfd36d62d4619196888e27beb946b3747716b",
            "isKey": true,
            "numCitedBy": 1251,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "In this chapter, we discuss a statistical generative model called independent component analysis. It is basically a proper probabilistic formulation of the ideas underpinning sparse coding. It shows how sparse coding can be interpreted as providing a Bayesian prior, and answers some questions which were not properly answered in the sparse coding framework."
            },
            "slug": "Independent-Component-Analysis-Hyv\u00e4rinen-Karhunen",
            "title": {
                "fragments": [],
                "text": "Independent Component Analysis"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Neural Networks"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 5
                            }
                        ],
                        "text": "1See [3] for a detailed explanation of why we use the smallestgeneralized eigenvalue in our general definition, and how this acc ords with our earlier definition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "In [3], we show that the converse is also true fo r the reproducing kernel Hilbert spaces based on Gaussian kernel s."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "In [3], we show theoretically that for a regul arization parameter \u03ba that is linear inN , we require low-rank approximations of sizeM , whereM is a constant that is independent of the number N of samples."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 189
                            }
                        ],
                        "text": "We also varied the number of compo nents, from 2 to 16, the number of training samples, from 250 t o 4000, and studied the robustness of the algorithms to varyin g numbers of outliers (see [3] for details)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 121
                            }
                        ],
                        "text": ", xm) that is an approximation of the mutual information between the original non-Gaussian variables in t he input space [3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 97
                            }
                        ],
                        "text": "It is straightforward to extend CCA, and its kernelized coun terpart, to the case of m variables [3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Kernel independent component analysis.J"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning Research  ,"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2734323"
                        ],
                        "name": "Y. Sawano",
                        "slug": "Y.-Sawano",
                        "structuredName": {
                            "firstName": "Yoshihiro",
                            "lastName": "Sawano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Sawano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087748"
                        ],
                        "name": "S. Saitoh",
                        "slug": "S.-Saitoh",
                        "structuredName": {
                            "firstName": "Saburou",
                            "lastName": "Saitoh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Saitoh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 49
                            }
                        ],
                        "text": "We then have the well-known reproducing property (Saitoh, 1988): f(x) = \u3008\u03a6(x), f\u3009, \u2200f \u2208 F , \u2200x \u2208 X ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 81
                            }
                        ],
                        "text": "Such a function space is unique and can always be completed into a Hilbert space (Saitoh, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 31
                            }
                        ],
                        "text": "Let K(x, y) be a Mercer kernel (Saitoh, 1988) on X = <p, that is, a function for which the Gram matrix Kij = K(xi, xj) is positive semidefinite for any collection {xi}i=1,."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 117252811,
            "fieldsOfStudy": [],
            "id": "636b46471adea4916ec1b2e38c8e8265218f6952",
            "isKey": true,
            "numCitedBy": 616,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Theory-of-Reproducing-Kernels-and-Its-Applications-Sawano-Saitoh",
            "title": {
                "fragments": [],
                "text": "Theory of Reproducing Kernels and Its Applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69577099"
                        ],
                        "name": "T. W. Anderson",
                        "slug": "T.-W.-Anderson",
                        "structuredName": {
                            "firstName": "Theodore",
                            "lastName": "Anderson",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. W. Anderson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Indeed, there is a classical relations hip between the full CCA spectrum and the mutual information of Gaussian variables x1 and x2 [ 2 ]: the mutual information I(x1,x2) is equal to \u2212 1 2 log Q i(1 \u2212 \ufffd 2 i). The product Q i(1 \u2212 \ufffd 2 i) is usually"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This is exactly the definition of the first canonical correlation [ 2 ] between \ufffd(x1) and \ufffd(x2)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "By taking derivatives with respect to \ufffd1 and \ufffd2, this problem is easily seen to reduce to the following generalized eigenvalue problem [ 2 ]: \ufffd 0 C12"
                    },
                    "intents": []
                }
            ],
            "corpusId": 121297223,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "02f96e21e8f87f9e3cd59ccbe5eaee5f8357e766",
            "isKey": true,
            "numCitedBy": 6345,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface to the Third Edition.Preface to the Second Edition.Preface to the First Edition.1. Introduction.2. The Multivariate Normal Distribution.3. Estimation of the Mean Vector and the Covariance Matrix.4. The Distributions and Uses of Sample Correlation Coefficients.5. The Generalized T2-Statistic.6. Classification of Observations.7. The Distribution of the Sample Covariance Matrix and the Sample Generalized Variance.8. Testing the General Linear Hypothesis: Multivariate Analysis of Variance9. Testing Independence of Sets of Variates.10. Testing Hypotheses of Equality of Covariance Matrices and Equality of Mean Vectors and Covariance Matrices.11. Principal Components.12. Cononical Correlations and Cononical Variables.13. The Distributions of Characteristic Roots and Vectors.14. Factor Analysis.15. Pattern of Dependence Graphical Models.Appendix A: Matrix Theory.Appendix B: Tables.References.Index."
            },
            "slug": "An-Introduction-to-Multivariate-Statistical-Anderson",
            "title": {
                "fragments": [],
                "text": "An Introduction to Multivariate Statistical Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1959
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "2.2 Reproducing kernel Hilbert spaces Let K(x; y) be a Mercer kernel ( Saitoh, 1988 ) on X =  Saitoh, 1988 )."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Let F be an RKHS on  Saitoh, 1988 ):"
                    },
                    "intents": []
                }
            ],
            "corpusId": 41680909,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9ded923a192ffbf13e4466c6b7d2ede55724b716",
            "isKey": true,
            "numCitedBy": 726,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Sparse-Greedy-Matrix-Approximation-for-Machine-Smola-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "Sparse Greedy Matrix Approximation for Machine Learning"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 131
                            }
                        ],
                        "text": "Indeed, there is a classical relations h p between the full CCA spectrum and the mutual information of Gau ssian variablesx1 andx2 [2]: the mutual informationI(x1, x2) is equal to\u2212 1 2 log \u220f i(1 \u2212 \u03c1 2 i )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 134
                            }
                        ],
                        "text": "By taking derivatives with respect to \u03be1 and\u03be2, this problem is easily seen to reduce to the following generalized eigenvalue problem [2] : ( 0 C12 C21 0 ) ( \u03be1 \u03be2 )"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 64
                            }
                        ],
                        "text": "This is exactly the definition of the firstcanonical correlation[2] between\u03a6(x1) and\u03a6(x2)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Anderson.An Introduction to Multivariate Statistical Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712211"
                        ],
                        "name": "G. Golub",
                        "slug": "G.-Golub",
                        "structuredName": {
                            "firstName": "Gene",
                            "lastName": "Golub",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Golub"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 132
                            }
                        ],
                        "text": "Symmetric permutations of rows and columns are necessary during the factorization if we require the rank to be as small as possible (Golub and Loan, 1983)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 126299280,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "225ca57add3b3fb12ef01cc97c4683350dc93fe4",
            "isKey": false,
            "numCitedBy": 27000,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Matrix-computations-Golub",
            "title": {
                "fragments": [],
                "text": "Matrix computations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153129968"
                        ],
                        "name": "K. Do",
                        "slug": "K.-Do",
                        "structuredName": {
                            "firstName": "Kim-Anh",
                            "lastName": "Do",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Do"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678771"
                        ],
                        "name": "P. Bickel",
                        "slug": "P.-Bickel",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bickel",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bickel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2857518"
                        ],
                        "name": "C. Klaassen",
                        "slug": "C.-Klaassen",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Klaassen",
                            "middleNames": [
                                "A.",
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Klaassen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "100532061"
                        ],
                        "name": "Y. Ritov",
                        "slug": "Y.-Ritov",
                        "structuredName": {
                            "firstName": "Yaacov",
                            "lastName": "Ritov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Ritov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144952368"
                        ],
                        "name": "J. Wellner",
                        "slug": "J.-Wellner",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Wellner",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Wellner"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 125283279,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b8160ac3f34352b4f1fa99cb49bd72acef31240e",
            "isKey": false,
            "numCitedBy": 1398,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Efficient-and-Adaptive-Estimation-for-Models.-Do-Bickel",
            "title": {
                "fragments": [],
                "text": "Efficient and Adaptive Estimation for Semiparametric Models."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144885041"
                        ],
                        "name": "C. Baker",
                        "slug": "C.-Baker",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Baker",
                            "middleNames": [
                                "T.",
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Baker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107597903"
                        ],
                        "name": "R. Taylor",
                        "slug": "R.-Taylor",
                        "structuredName": {
                            "firstName": "Robert L.",
                            "lastName": "Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Taylor"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 213
                            }
                        ],
                        "text": "The study of the spectrum of Gram matrices calculated from a kernel K(x, y) is usually carried out by studying the spectrum of an associated integral operator, and using the Nystr\u00f6m method to relate these spectra (Baker, 1977)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 123967005,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "fecf8f9b8f09c3c87def62a66485558e92065737",
            "isKey": false,
            "numCitedBy": 798,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Numerical-Treatment-of-Integral-Equations-Baker-Taylor",
            "title": {
                "fragments": [],
                "text": "The Numerical Treatment of Integral Equations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 41
                            }
                        ],
                        "text": ", xm), is readily computed in terms of C (Kullback, 1959):"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 101
                            }
                        ],
                        "text": "The mutual information, M(x1, x2) = \u222b p(x1, x2) log[p(x1, x2)/p(x1)p(x2)]dx1dx2, is readily computed (Kullback, 1959):"
                    },
                    "intents": []
                }
            ],
            "corpusId": 125523249,
            "fieldsOfStudy": [],
            "id": "11fbf06e4c1c4eddc91a68e434433a4fc5f7cfc4",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information Theory and Statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1959
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2748598"
                        ],
                        "name": "S. Kullback",
                        "slug": "S.-Kullback",
                        "structuredName": {
                            "firstName": "Solomon",
                            "lastName": "Kullback",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kullback"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The mutual information, M(x1 ;x 2 )= R p(x1 ;x 2 )l og[p(x1 ;x 2)=p(x1)p(x2)]dx1dx2, is readily computed ( Kullback, 1959 ):"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The mutual information, M(x1 ;:::;x m), is readily computed in terms of C ( Kullback, 1959 ):"
                    },
                    "intents": []
                }
            ],
            "corpusId": 86412308,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "51739712c9b795f9533131122698cd5d01699f9d",
            "isKey": false,
            "numCitedBy": 82,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "information theory and statistics. Book lovers, when you need a new book to read, find the book here. Never worry not to find what you need. Is the information theory and statistics your needed book now? That's true; you are really a good reader. This is a perfect book that comes from great author to share with you. The book offers the best experience and lesson to take, not only take, but also learn."
            },
            "slug": "Information-Theory-and-Statistics-Kullback",
            "title": {
                "fragments": [],
                "text": "Information Theory and Statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1959
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680574"
                        ],
                        "name": "M. Seeger",
                        "slug": "M.-Seeger",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Seeger",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Seeger"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 42041158,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b6fff8b8ea77f157913986e7af53951d9fc1128e",
            "isKey": false,
            "numCitedBy": 2170,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "A major problem for kernel-based predictors (such as Support Vector Machines and Gaussian processes) is that the amount of computation required to find the solution scales as O(n3), where n is the number of training examples. We show that an approximation to the eigendecomposition of the Gram matrix can be computed by the Nystrom method (which is used for the numerical solution of eigenproblems). This is achieved by carrying out an eigendecomposition on a smaller system of size m < n, and then expanding the results back up to n dimensions. The computational complexity of a predictor using this approximation is O(m2n). We report experiments on the USPS and abalone data sets and show that we can set m \u226a n without any significant decrease in the accuracy of the solution."
            },
            "slug": "Using-the-Nystr\u00f6m-Method-to-Speed-Up-Kernel-Williams-Seeger",
            "title": {
                "fragments": [],
                "text": "Using the Nystr\u00f6m Method to Speed Up Kernel Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that an approximation to the eigendecomposition of the Gram matrix can be computed by the Nystrom method (which is used for the numerical solution of eigenproblems) and the computational complexity of a predictor using this approximation is O(m2n)."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742997"
                        ],
                        "name": "C. Fyfe",
                        "slug": "C.-Fyfe",
                        "structuredName": {
                            "firstName": "Colin",
                            "lastName": "Fyfe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Fyfe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9370258"
                        ],
                        "name": "P. L. Lai",
                        "slug": "P.-L.-Lai",
                        "structuredName": {
                            "firstName": "Pei",
                            "lastName": "Lai",
                            "middleNames": [
                                "Ling"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. L. Lai"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117602571,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a520ef9928cf3cb71f137ff9a36294a81bf4630",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Ica-Using-Kernel-Canonical-Correlation-Analysis-Fyfe-Lai",
            "title": {
                "fragments": [],
                "text": "Ica Using Kernel Canonical Correlation Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784682"
                        ],
                        "name": "T. Hastie",
                        "slug": "T.-Hastie",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Hastie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hastie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761784"
                        ],
                        "name": "R. Tibshirani",
                        "slug": "R.-Tibshirani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tibshirani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tibshirani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 100
                            }
                        ],
                        "text": "4 transfers readily to the general setting of generalized additive models based on kernel smoothers (Hastie and Tibshirani, 1990), thus enabling a fast implementation of the fitting procedure for such models."
                    },
                    "intents": []
                }
            ],
            "corpusId": 236049,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5fa2ace8450629a53eea82e2e4174b16bce6182a",
            "isKey": false,
            "numCitedBy": 507,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "This article reviews flexible statistical methods that are useful for characterizing the effect of potential prognostic factors on disease endpoints. Applications to survival models and binary outcome models are illustrated."
            },
            "slug": "Generalized-additive-models-for-medical-research-Hastie-Tibshirani",
            "title": {
                "fragments": [],
                "text": "Generalized additive models for medical research"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "Flexible statistical methods that are useful for characterizing the effect of potential prognostic factors on disease endpoints are reviewed."
            },
            "venue": {
                "fragments": [],
                "text": "Statistical methods in medical research"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 184
                            }
                        ],
                        "text": "In particular, in the case of the FastICA algorithm, the hyperbolic tangent and Gaussian nonlinearities are recommended in place of the default polynomial when robustness is a concern (Hyv\u00e4rinen and Oja, 1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 204
                            }
                        ],
                        "text": "More sophisticated algorithms have been obtained by careful choice of a single fixed nonlinear function, such that the expectations of this function yield a robust approximation to the mutual information (Hyv\u00e4rinen and Oja, 1997)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 207661551,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0899a6b62251ebb4af1ed35f0c6f9d63bed8c8e9",
            "isKey": false,
            "numCitedBy": 1643,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a novel fast algorithm for independent component analysis, which can be used for blind source separation and feature extraction. We show how a neural network learning rule can be transformed into a fixedpoint iteration, which provides an algorithm that is very simple, does not depend on any user-defined parameters, and is fast to converge to the most accurate solution allowed by the data. The algorithm finds, one at a time, all nongaussian independent components, regardless of their probability distributions. The computations can be performed in either batch mode or a semiadaptive manner. The convergence of the algorithm is rigorously proved, and the convergence speed is shown to be cubic. Some comparisons to gradient-based algorithms are made, showing that the new algorithm is usually 10 to 100 times faster, sometimes giving the solution in just a few iterations."
            },
            "slug": "A-Fast-Fixed-Point-Algorithm-for-Independent-Hyv\u00e4rinen-Oja",
            "title": {
                "fragments": [],
                "text": "A Fast Fixed-Point Algorithm for Independent Component Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A novel fast algorithm for independent component analysis is introduced, which can be used for blind source separation and feature extraction, and the convergence speed is shown to be cubic."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680574"
                        ],
                        "name": "M. Seeger",
                        "slug": "M.-Seeger",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Seeger",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Seeger"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "As pointed out by  Williams and Seeger (2000) , for one-dimensional input spaces the eigenvalues decay geometrically if the input density is Gaussian."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8107066,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "763f4b3c0e830b92a2d6af3c547fcc4e52b5225e",
            "isKey": false,
            "numCitedBy": 181,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Keywords: Gaussian process ; Nystroem approximation Reference EPFL-CONF-161323 Record created on 2010-12-02, modified on 2016-08-09"
            },
            "slug": "The-Effect-of-the-Input-Density-Distribution-on-Williams-Seeger",
            "title": {
                "fragments": [],
                "text": "The Effect of the Input Density Distribution on Kernel-based Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "Gaussian process ; Nystroem approximation Reference EPFL-CONF-161323 Record created on 2010-12-02, modified on 2016-08-09."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103917250"
                        ],
                        "name": "Te-Won Lee",
                        "slug": "Te-Won-Lee",
                        "structuredName": {
                            "firstName": "Te-Won",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Te-Won Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50859190"
                        ],
                        "name": "M. Girolami",
                        "slug": "M.-Girolami",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Girolami",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Girolami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 221401721,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "219ee1c7eee3142c8a4156733fbaacd5cc753ac6",
            "isKey": false,
            "numCitedBy": 302,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Independent-Component-Analysis-Using-an-Extended-Lee-Girolami",
            "title": {
                "fragments": [],
                "text": "Independent Component Analysis Using an Extended Infomax Algorithm for Mixed Sub-Gaussian and Super-Gaussian Sources"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Comput."
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The FastICA MATLAB toolbox. Available at http://www.cis.hut./projects/ica/fastica"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Independent Componenrilnalysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The FastICA MATLAB toolbox Available at http://www.cis.hut./projects/ica/fastica"
            },
            "venue": {
                "fragments": [],
                "text": "The FastICA MATLAB toolbox Available at http://www.cis.hut./projects/ica/fastica"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 50
                            }
                        ],
                        "text": "It is important to emphasize at the outset that KERNELICA is not the\n\u201ckernelization\u201d of an extant ICA algorithm."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 35
                            }
                        ],
                        "text": "In Section 3, we define a kernel-based contrast function in terms of the first eigenvalue of a certain generalized eigenvector problem, and show how this function relates to probabilistic independence."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Independent Component Analysis, a new concept? Signal Processing"
            },
            "venue": {
                "fragments": [],
                "text": "Independent Component Analysis, a new concept? Signal Processing"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 39
                            }
                        ],
                        "text": "T is called a Hilbert-Schmidt operator (Brezis, 1980)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Analyse Fonctionelle"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 132
                            }
                        ],
                        "text": ", 2001) and Gaussian mixtures (Attias, 1999, Welling and Weber, 2001), and methods that minimize asymptotic variances of estimators (Pham and Garat, 1997)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Blind separation of mixtures of independent sources through a quasi-maximum likelihood approach"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Signal Processing,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 104
                            }
                        ],
                        "text": "Comparisons were made with three existing ICA algorithms: the FastICA algorithm [8], the Jade algorithm [7], and the ex tended Infomax algorithm [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 221,
                                "start": 212
                            }
                        ],
                        "text": "Unfortunately, the mutual information for real-valued var iables is difficult to approximate and optimize on the basis of a finit e sample, and much research on ICA has focused on alternative cont rast functions [8, 7, 1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "High-order contrasts for independent co  mp nent analysis.Neural Computation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 289,
                                "start": 276
                            }
                        ],
                        "text": "Moreover, given that the mutual information of a random vector is nonnegative, and zero if and only if the components of the vector are independent, the use of mutual information as a function to be minimized is well motivated, quite apart from the link to maximum likelihood (Comon, 1994)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Independent Component Analysis, a new concept? Signal Processing, 36(3):287\u2013314"
            },
            "venue": {
                "fragments": [],
                "text": "Special issue on Higher-Order Statistics"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An Introduction to Multivariate Statistical"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 12,
            "methodology": 12
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 67,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/Kernel-independent-component-analysis-Bach-Jordan/8d4f4601940d5b13455541a643a39538bb54b6f3?sort=total-citations"
}