{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35685584"
                        ],
                        "name": "Guillaume Jaume",
                        "slug": "Guillaume-Jaume",
                        "structuredName": {
                            "firstName": "Guillaume",
                            "lastName": "Jaume",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guillaume Jaume"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3025777"
                        ],
                        "name": "H. K. Ekenel",
                        "slug": "H.-K.-Ekenel",
                        "structuredName": {
                            "firstName": "Hazim",
                            "lastName": "Ekenel",
                            "middleNames": [
                                "Kemal"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. K. Ekenel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710257"
                        ],
                        "name": "J. Thiran",
                        "slug": "J.-Thiran",
                        "structuredName": {
                            "firstName": "Jean-Philippe",
                            "lastName": "Thiran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Thiran"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 6
                            }
                        ],
                        "text": "A.1.6 FUNSD for general form understanding FUNSD form understanding task consists of two sub tasks: entity labeling (ELB) and entity linking (ELK)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 114
                            }
                        ],
                        "text": "In the FUNSD form understanding task, we measure entity labeling (ELB) and entity linking (ELK) scores following (Jaume et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 149
                            }
                        ],
                        "text": "\u2026et al., 2019; Zhao et al., 2019; Denk and Reisswig, 2019; Hwang et al., 2019; Park et al., 2019; Xu\n1https://github.com/clovaai/spade\net al., 2019; Jaume et al., 2019; Zhong et al., 2019; Rausch et al., 2019; Yu et al., 2020; Wei et al., 2020; Majumder et al., 2020; Lockard et al., 2020;\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 130
                            }
                        ],
                        "text": "We thank Geewook Kim for the critical comments on the manuscript and Teakgyu Hong and Sungrae Park for the helpful discussions on FUNSD experiments."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 141
                            }
                        ],
                        "text": "Handling general documents In order to see if SPADEs can handle more general kinds of documents, we use the FUNSD form understanding dataset (Jaume et al., 2019) where document IE is performed under a more abstract setting by finding general key-value pairs and their inter-grouping (Section A."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 142
                            }
                        ],
                        "text": "Handling general documents In order to see if SPADEs can handle more general kinds of documents, we use the FUNSD form understanding dataset (Jaume et al., 2019) where document IE is performed under a more abstract setting by finding general key-value pairs and their inter-grouping (Section A.1.6)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 104
                            }
                        ],
                        "text": "For FUNSD dataset, entity labeling and entity linking scores are computed following the original paper (Jaume et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 52
                            }
                        ],
                        "text": "The dev sets are used to pick the best model except FUNSD task in which the model is trained in two steps."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 61
                            }
                        ],
                        "text": "The performance is measured on two OCR-independent subtasks (Jaume et al., 2019): (1) \u201centity-labeling (ELB)\u201d which predicts the information category of the serialized words, and (2) \u201centity-linking (ELK)\u201d which measures the score for key-value pair link prediction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 133
                            }
                        ],
                        "text": "Finally, we show that SPADEs can handle even more types of documents by evaluating the model on a form understanding dataset, FUNSD (Jaume et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 173188931,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "58c793e278cdbf669a615b2c2479cd69ff785d63",
            "isKey": true,
            "numCitedBy": 72,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new dataset for form understanding in noisy scanned documents (FUNSD) that aims at extracting and structuring the textual content of forms. The dataset comprises 199 real, fully annotated, scanned forms. The documents are noisy and vary widely in appearance, making form understanding (FoUn) a challenging task. The proposed dataset can be used for various tasks, including text detection, optical character recognition, spatial layout analysis, and entity labeling/linking. To the best of our knowledge, this is the first publicly available dataset with comprehensive annotations to address FoUn task. We also present a set of baselines and introduce metrics to evaluate performance on the FUNSD dataset, which can be downloaded at https://guillaumejaume.github.io/FUNSD."
            },
            "slug": "FUNSD:-A-Dataset-for-Form-Understanding-in-Noisy-Jaume-Ekenel",
            "title": {
                "fragments": [],
                "text": "FUNSD: A Dataset for Form Understanding in Noisy Scanned Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This work presents a new dataset for form understanding in noisy scanned documents (FUNSD) that aims at extracting and structuring the textual content of forms, and is the first publicly available dataset with comprehensive annotations to address FoUn task."
            },
            "venue": {
                "fragments": [],
                "text": "2019 International Conference on Document Analysis and Recognition Workshops (ICDARW)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40348417"
                        ],
                        "name": "Ashish Vaswani",
                        "slug": "Ashish-Vaswani",
                        "structuredName": {
                            "firstName": "Ashish",
                            "lastName": "Vaswani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ashish Vaswani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1846258"
                        ],
                        "name": "Noam M. Shazeer",
                        "slug": "Noam-M.-Shazeer",
                        "structuredName": {
                            "firstName": "Noam",
                            "lastName": "Shazeer",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noam M. Shazeer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3877127"
                        ],
                        "name": "Niki Parmar",
                        "slug": "Niki-Parmar",
                        "structuredName": {
                            "firstName": "Niki",
                            "lastName": "Parmar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Niki Parmar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39328010"
                        ],
                        "name": "Jakob Uszkoreit",
                        "slug": "Jakob-Uszkoreit",
                        "structuredName": {
                            "firstName": "Jakob",
                            "lastName": "Uszkoreit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jakob Uszkoreit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145024664"
                        ],
                        "name": "Llion Jones",
                        "slug": "Llion-Jones",
                        "structuredName": {
                            "firstName": "Llion",
                            "lastName": "Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Llion Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "19177000"
                        ],
                        "name": "Aidan N. Gomez",
                        "slug": "Aidan-N.-Gomez",
                        "structuredName": {
                            "firstName": "Aidan",
                            "lastName": "Gomez",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aidan N. Gomez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40527594"
                        ],
                        "name": "Lukasz Kaiser",
                        "slug": "Lukasz-Kaiser",
                        "structuredName": {
                            "firstName": "Lukasz",
                            "lastName": "Kaiser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lukasz Kaiser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3443442"
                        ],
                        "name": "Illia Polosukhin",
                        "slug": "Illia-Polosukhin",
                        "structuredName": {
                            "firstName": "Illia",
                            "lastName": "Polosukhin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Illia Polosukhin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 33
                            }
                        ],
                        "text": "Unlike the original Transformer (Vaswani et al., 2017), there is no order among the input tokens, making the model invariant under the permutation of the input tokens."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 14
                            }
                        ],
                        "text": "In (original) Transformer, only the first term of Equation 1 is used."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 146
                            }
                        ],
                        "text": "Hwang et al. (2019) and Denk and Reisswig (2019) combine a manually engineered text serializer that turn the OCR text boxes into a sequence and a Transformer-based encoder, BERT (Devlin et al., 2018), that performs IOB tagging on the sequence or semantic segmentation from images."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 167
                            }
                        ],
                        "text": "First, the relative coordinates be-\ntween each pair of tokens are computed.2 Next, the coordinates are quantized into integers and embedded using sin and cos functions (Vaswani et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 12
                            }
                        ],
                        "text": "Inspired by Transformer XL (Dai et al., 2019), the attention weights (between each key and query vector) is computed by\nqTi kj + q T i rij + (b key i ) Tkj + (breli ) T rij (1)\nwhere qi is the query vector of the i-th input token, kj is the key vector of the j-th input token, rij is the relative spatial vector of the j-th token with respect to the i-th token, and bkey|reli is a bias vector."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 23
                            }
                        ],
                        "text": "We use 12 layers of 2D Transformer encoder (Section 4.1)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 36
                            }
                        ],
                        "text": "Spatial text encoder is based on 2D Transformer architecture."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 93
                            }
                        ],
                        "text": "2 Next, the coordinates are quantized into integers and embedded using sin and cos functions (Vaswani et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13756489,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "isKey": true,
            "numCitedBy": 35148,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
            },
            "slug": "Attention-is-All-you-Need-Vaswani-Shazeer",
            "title": {
                "fragments": [],
                "text": "Attention is All you Need"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely is proposed, which generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054135353"
                        ],
                        "name": "Wonseok Hwang",
                        "slug": "Wonseok-Hwang",
                        "structuredName": {
                            "firstName": "Wonseok",
                            "lastName": "Hwang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wonseok Hwang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109603647"
                        ],
                        "name": "Seonghyeon Kim",
                        "slug": "Seonghyeon-Kim",
                        "structuredName": {
                            "firstName": "Seonghyeon",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seonghyeon Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4418074"
                        ],
                        "name": "Minjoon Seo",
                        "slug": "Minjoon-Seo",
                        "structuredName": {
                            "firstName": "Minjoon",
                            "lastName": "Seo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minjoon Seo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49841374"
                        ],
                        "name": "Jinyeong Yim",
                        "slug": "Jinyeong-Yim",
                        "structuredName": {
                            "firstName": "Jinyeong",
                            "lastName": "Yim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jinyeong Yim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7882243"
                        ],
                        "name": "Seunghyun Park",
                        "slug": "Seunghyun-Park",
                        "structuredName": {
                            "firstName": "Seunghyun",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seunghyun Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71537829"
                        ],
                        "name": "Sungrae Park",
                        "slug": "Sungrae-Park",
                        "structuredName": {
                            "firstName": "Sungrae",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sungrae Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39924323"
                        ],
                        "name": "Junyeop Lee",
                        "slug": "Junyeop-Lee",
                        "structuredName": {
                            "firstName": "Junyeop",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junyeop Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2722273"
                        ],
                        "name": "Bado Lee",
                        "slug": "Bado-Lee",
                        "structuredName": {
                            "firstName": "Bado",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bado Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72152162"
                        ],
                        "name": "Hwalsuk Lee",
                        "slug": "Hwalsuk-Lee",
                        "structuredName": {
                            "firstName": "Hwalsuk",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hwalsuk Lee"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", database schema), which has become an increasingly important task in both research community and industry (Palm, Winther, and Laws 2017; Katti et al. 2018; Qian et al. 2019; Liu et al. 2019; Zhao, Wu, and Wang 2019; Denk and Reisswig 2019; Hwang et al. 2019; Park et al. 2019; Xu et al. 2019; Jaume, Ekenel, and Thiran 2019; Yu et al. 2020; Wei, He, and Zhang 2020; Lockard et al. 2020; Lin et al. 2020)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The scores we report for CORD cannot be directly compared to those reported in Hwang et al. (2019), because the ontology changed in the publicized dataset as the authors mentioned in the update section of https://github."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Hwang et al. (2019) and Denk and Reisswig (2019) combine a manually engineered text serializer that turn the OCR text boxes into a sequence and a Transformer-based encoder, BERT (Devlin et al., 2018), that performs IOB tagging on the sequence or semantic segmentation from images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In this setup, the text segments in the document (either obtained through an OCR engine or trivially parsed from a web page or pdf) are first serialized, and then an independent tagging model classifies each of the flattened lists into one of the pre-defined IOB categories (Ramshaw and Marcus 1995; Palm, Winther, and Laws 2017; Hwang et al. 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ", receipt), Hwang et al. (2019) augment each IOB tag with higher-layer information."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 145
                            }
                        ],
                        "text": "\u2026from both research community and industry (Katti et al., 2018; Qian et al., 2019; Liu et al., 2019; Zhao et al., 2019; Denk and Reisswig, 2019; Hwang et al., 2019; Park et al., 2019; Xu\n1https://github.com/clovaai/spade\net al., 2019; Jaume et al., 2019; Zhong et al., 2019; Rausch et al.,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 186
                            }
                        ],
                        "text": "A.1 Dataset A.1.1 Dataset collection The internal datasets Receipt-idn, namecard and Invoice are annotated by the crowd through an in-house web application following (Park et al., 2019; Hwang et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207910450,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "da5d93e2931c12b81774a6857db0175875fdf71a",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Parsing textual information embedded in images is important for various downstream tasks. However, many previously developed parsers are limited to handling the information presented in one dimensional sequence format. Here, we present POST OCR TAGGING BASED PARSER (POT), a simple and robust parser that can parse visually embedded texts by BIO-tagging the output of optical character recognition (OCR) task. Our shallow parsing approach enables building robust neural parser with less than a thousand labeled data. POT is validated on receipt and namecard parsing tasks."
            },
            "slug": "Post-OCR-parsing:-building-simple-and-robust-parser-Hwang-Kim",
            "title": {
                "fragments": [],
                "text": "Post-OCR parsing: building simple and robust parser via BIO tagging"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work presents POST OCR TAGGING BASED PARSER (POT), a simple and robust parser that can parse visually embedded texts by BIO-tagging the output of optical character recognition (OCR) task."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7882243"
                        ],
                        "name": "Seunghyun Park",
                        "slug": "Seunghyun-Park",
                        "structuredName": {
                            "firstName": "Seunghyun",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seunghyun Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111068603"
                        ],
                        "name": "Seung Shin",
                        "slug": "Seung-Shin",
                        "structuredName": {
                            "firstName": "Seung",
                            "lastName": "Shin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seung Shin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2722273"
                        ],
                        "name": "Bado Lee",
                        "slug": "Bado-Lee",
                        "structuredName": {
                            "firstName": "Bado",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bado Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39924323"
                        ],
                        "name": "Junyeop Lee",
                        "slug": "Junyeop-Lee",
                        "structuredName": {
                            "firstName": "Junyeop",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junyeop Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10787779"
                        ],
                        "name": "Jaeheung Surh",
                        "slug": "Jaeheung-Surh",
                        "structuredName": {
                            "firstName": "Jaeheung",
                            "lastName": "Surh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jaeheung Surh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4418074"
                        ],
                        "name": "Minjoon Seo",
                        "slug": "Minjoon-Seo",
                        "structuredName": {
                            "firstName": "Minjoon",
                            "lastName": "Seo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minjoon Seo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72152162"
                        ],
                        "name": "Hwalsuk Lee",
                        "slug": "Hwalsuk-Lee",
                        "structuredName": {
                            "firstName": "Hwalsuk",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hwalsuk Lee"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": "SPADEs significantly outperforms s+bert+iob2 (+13.4% F1 in CORD+, +31.1% F1.b in CORD++)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In CORD, s+bert+iob2 shows comparable performance with SPADEs with the oracle (-0.1 F1) but shows +1.9 F1 on the test set (2nd and 3rd rows, co)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ", database schema), which has become an increasingly important task in both research community and industry (Palm, Winther, and Laws 2017; Katti et al. 2018; Qian et al. 2019; Liu et al. 2019; Zhao, Wu, and Wang 2019; Denk and Reisswig 2019; Hwang et al. 2019; Park et al. 2019; Xu et al. 2019; Jaume, Ekenel, and Thiran 2019; Yu et al. 2020; Wei, He, and Zhang 2020; Lockard et al. 2020; Lin et al. 2020)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "Handling hierarchical structure in documents CORD consists of receipt images without creases or warping."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": "A.1.3 Receipt-idn for receipt IE Receipt-idn is similar to CORD but includes more diverse information categories (50) such as store name, store address, and payment time (Table 6)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 90
                            }
                        ],
                        "text": "Inflexibility of tagging model in handling complex spatial relationships Next, we prepare CORD+ and CORD++, which are more challenging setups where the images are warped or tilted as often seen in real-world applications (Fig."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 122
                            }
                        ],
                        "text": "We first show that our model can handle hierarchical structure in documents by evaluating the model on two datasets CORD (Park et al., 2019) and Receipt-idn that consist of (Indonesian) receipt images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 58
                            }
                        ],
                        "text": "The result shows although there is a large improvement in CORD+ and CORD++ task compared to s+bert+iob2, SPADEs still shows the better performance (+2.0% in CORD+, +18.3% in CORD++, 1st and 4th rows)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "First, in a modified version of Consolidated Receipt Dataset (CORD) (Park et al. 2019) that simulates real-world receipt distortions (we dub it CORD+), our model outperforms the strong baseline by a large margin of 30."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "A.1.2 CORD, CORD+, CORD++, and CORD-M for receipt IE CORD and their variant consist of 30 information categories such as menu name, count, unit price, price, and total price (Table 6)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 146
                            }
                        ],
                        "text": "\u2026community and industry (Katti et al., 2018; Qian et al., 2019; Liu et al., 2019; Zhao et al., 2019; Denk and Reisswig, 2019; Hwang et al., 2019; Park et al., 2019; Xu\n1https://github.com/clovaai/spade\net al., 2019; Jaume et al., 2019; Zhong et al., 2019; Rausch et al., 2019; Yu et al., 2020;\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "The competent performance of SPADEs on CORD-M, a dataset generated by concatenating two receipt images from CORD into a single image (Fig."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We first show that our model can handle hierarchical structure in documents by evaluating the model on datasets CORD (Park et al. 2019), CORD+ and CORD-M that consist of (Indonesian) receipt images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 167
                            }
                        ],
                        "text": "A.1 Dataset A.1.1 Dataset collection The internal datasets Receipt-idn, namecard and Invoice are annotated by the crowd through an in-house web application following (Park et al., 2019; Hwang et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207900784,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c69942bf1b4f75e53cb62d0c5126c1cb4a5aa7bc",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "OCR is inevitably linked to NLP since its final output is in text. Advances in document intelligence are driving the need for a unified technology that integrates OCR with various NLP tasks, especially semantic parsing. Since OCR and semantic parsing have been studied as separate tasks so far, the datasets for each task on their own are rich, while those for the integrated post-OCR parsing tasks are relatively insufficient. In this study, we publish a consolidated dataset for receipt parsing as the first step towards post-OCR parsing tasks. The dataset consists of thousands of Indonesian receipts, which contains images and box/text annotations for OCR, and multi-level semantic labels for parsing. The proposed dataset can be used to address various OCR and parsing tasks."
            },
            "slug": "CORD:-A-Consolidated-Receipt-Dataset-for-Post-OCR-Park-Shin",
            "title": {
                "fragments": [],
                "text": "CORD: A Consolidated Receipt Dataset for Post-OCR Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A consolidated dataset for receipt parsing is published, which consists of thousands of Indonesian receipts, which contains images and box/text annotations for OCR, and multi-level semantic labels for parsing."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110965354"
                        ],
                        "name": "Xiaojing Liu",
                        "slug": "Xiaojing-Liu",
                        "structuredName": {
                            "firstName": "Xiaojing",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojing Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112256"
                        ],
                        "name": "Feiyu Gao",
                        "slug": "Feiyu-Gao",
                        "structuredName": {
                            "firstName": "Feiyu",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Feiyu Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112207346"
                        ],
                        "name": "Qiong Zhang",
                        "slug": "Qiong-Zhang",
                        "structuredName": {
                            "firstName": "Qiong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiong Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36530509"
                        ],
                        "name": "Huasha Zhao",
                        "slug": "Huasha-Zhao",
                        "structuredName": {
                            "firstName": "Huasha",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huasha Zhao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 150
                            }
                        ],
                        "text": "\u2026are well reflected in their increased number of publication record from both research community and industry (Katti et al., 2018; Qian et al., 2019; Liu et al., 2019; Zhao et al., 2019; Denk and Reisswig, 2019; Hwang et al., 2019; Park et al., 2019; Xu\n1https://github.com/clovaai/spade\net al.,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Graph-based IE Liu et al. (2019); Qian et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ", database schema), which has become an increasingly important task in both research community and industry (Palm, Winther, and Laws 2017; Katti et al. 2018; Qian et al. 2019; Liu et al. 2019; Zhao, Wu, and Wang 2019; Denk and Reisswig 2019; Hwang et al. 2019; Park et al. 2019; Xu et al. 2019; Jaume, Ekenel, and Thiran 2019; Yu et al. 2020; Wei, He, and Zhang 2020; Lockard et al. 2020; Lin et al. 2020)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 15
                            }
                        ],
                        "text": "Graph-based IE Liu et al. (2019); Qian et al. (2019); Wei et al. (2020); Yu et al. (2020) utilize a graph convolution network to contextualize the tokens in a document and a bidirectional LSTM with CRF to predict the IOB tags."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "UB-FLAT represents the maximum achievable scores for the models generating flat outputs Liu et al. (2019); Qian et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 85528598,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04df8c70257b5280b9d303502c9d7ddf946f181b",
            "isKey": true,
            "numCitedBy": 86,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Visually rich documents (VRDs) are ubiquitous in daily business and life. Examples are purchase receipts, insurance policy documents, custom declaration forms and so on. In VRDs, visual and layout information is critical for document understanding, and texts in such documents cannot be serialized into the one-dimensional sequence without losing information. Classic information extraction models such as BiLSTM-CRF typically operate on text sequences and do not incorporate visual features. In this paper, we introduce a graph convolution based model to combine textual and visual information presented in VRDs. Graph embeddings are trained to summarize the context of a text segment in the document, and further combined with text embeddings for entity extraction. Extensive experiments have been conducted to show that our method outperforms BiLSTM-CRF baselines by significant margins, on two real-world datasets. Additionally, ablation studies are also performed to evaluate the effectiveness of each component of our model."
            },
            "slug": "Graph-Convolution-for-Multimodal-Information-from-Liu-Gao",
            "title": {
                "fragments": [],
                "text": "Graph Convolution for Multimodal Information Extraction from Visually Rich Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper introduces a graph convolution based model to combine textual and visual information presented in VRDs and outperforms BiLSTM-CRF baselines by significant margins, on two real-world datasets."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2019
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 271,
                                "start": 254
                            }
                        ],
                        "text": "\u2026al., 2019; Jaume et al., 2019; Zhong et al., 2019; Rausch et al., 2019; Yu et al., 2020; Wei et al., 2020; Majumder et al., 2020; Lockard et al., 2020; Garncarek et al., 2020; Lin et al., 2020; Xu et al., 2020; Powalski et al., 2021; Wang et al., 2021; Hong et al., 2021; Hwang et al., 2021)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 236923196,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aa111c8920e963195968360f59c9de271ae470c2",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Understanding document from their visual snapshots is an emerging and challenging problem that requires both advanced computer vision and NLP methods. Although the recent advance in OCR enables the accurate extraction of text segments, it is still challenging to extract key information from documents due to the diversity of layouts. To compensate for the difficulties, this paper introduces a pre-trained language model, BERT Relying On Spatiality (BROS), that represents and understands the semantics of spatially distributed texts. Different from previous pre-training methods on 1D text, BROS is pre-trained on large-scale semistructured documents with a novel area-masking strategy while efficiently including the spatial layout information of input documents. Also, to generate structured outputs in various document understanding tasks, BROS utilizes a powerful graphbased decoder that can capture the relation between text segments. BROS achieves state-of-the-art results on four benchmark tasks: FUNSD, SROIE*, CORD, and SciTSR. Our experimental settings and implementation codes will be publicly available."
            },
            "slug": "BROS:-A-PRE-TRAINED-LANGUAGE-MODEL",
            "title": {
                "fragments": [],
                "text": "BROS: A PRE-TRAINED LANGUAGE MODEL"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A pre-trained language model, BERT Relying On Spatiality (BROS), that represents and understands the semantics of spatially distributed texts and achieves state-of-the-art results on four benchmark tasks: FUNSD, SROIE*, CORD, and SciTSR."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054135353"
                        ],
                        "name": "Wonseok Hwang",
                        "slug": "Wonseok-Hwang",
                        "structuredName": {
                            "firstName": "Wonseok",
                            "lastName": "Hwang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wonseok Hwang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2140191673"
                        ],
                        "name": "Hyun Dong Lee",
                        "slug": "Hyun-Dong-Lee",
                        "structuredName": {
                            "firstName": "Hyun",
                            "lastName": "Lee",
                            "middleNames": [
                                "Dong"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hyun Dong Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49841374"
                        ],
                        "name": "Jinyeong Yim",
                        "slug": "Jinyeong-Yim",
                        "structuredName": {
                            "firstName": "Jinyeong",
                            "lastName": "Yim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jinyeong Yim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115988519"
                        ],
                        "name": "Geewook Kim",
                        "slug": "Geewook-Kim",
                        "structuredName": {
                            "firstName": "Geewook",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geewook Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4418074"
                        ],
                        "name": "Minjoon Seo",
                        "slug": "Minjoon-Seo",
                        "structuredName": {
                            "firstName": "Minjoon",
                            "lastName": "Seo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minjoon Seo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 273
                            }
                        ],
                        "text": "\u2026al., 2019; Jaume et al., 2019; Zhong et al., 2019; Rausch et al., 2019; Yu et al., 2020; Wei et al., 2020; Majumder et al., 2020; Lockard et al., 2020; Garncarek et al., 2020; Lin et al., 2020; Xu et al., 2020; Powalski et al., 2021; Wang et al., 2021; Hong et al., 2021; Hwang et al., 2021)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 233289426,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "39d23cc236dd5057df7ff1949fa4fe8e87559f6e",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "A real-world information extraction (IE) system for semi-structured document images often involves a long pipeline of multiple modules, whose complexity dramatically increases its development and maintenance cost. One can instead consider an end-to-end model that directly maps the input to the target output and simplify the entire process. However, such generation approach is known to lead to unstable performance if not designed carefully. Here we present our recent effort on transitioning from our existing pipeline-based IE system to an end-to-end system focusing on practical challenges that are associated with replacing and deploying the system in real, large-scale production. By carefully formulating document IE as a sequence generation task, we show that a single end-to-end IE system can be built and still achieve competent performance."
            },
            "slug": "Cost-effective-End-to-end-Information-Extraction-Hwang-Lee",
            "title": {
                "fragments": [],
                "text": "Cost-effective End-to-end Information Extraction for Semi-structured Document Images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "By carefully formulating document IE as a sequence generation task, it is shown that a single end-to-end IE system can be built and still achieve competent performance."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2021
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1498640514"
                        ],
                        "name": "Rafal Powalski",
                        "slug": "Rafal-Powalski",
                        "structuredName": {
                            "firstName": "Rafal",
                            "lastName": "Powalski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rafal Powalski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3448729"
                        ],
                        "name": "\u0141ukasz Borchmann",
                        "slug": "\u0141ukasz-Borchmann",
                        "structuredName": {
                            "firstName": "\u0141ukasz",
                            "lastName": "Borchmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u0141ukasz Borchmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400414734"
                        ],
                        "name": "Dawid Jurkiewicz",
                        "slug": "Dawid-Jurkiewicz",
                        "structuredName": {
                            "firstName": "Dawid",
                            "lastName": "Jurkiewicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dawid Jurkiewicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3407177"
                        ],
                        "name": "Tomasz Dwojak",
                        "slug": "Tomasz-Dwojak",
                        "structuredName": {
                            "firstName": "Tomasz",
                            "lastName": "Dwojak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomasz Dwojak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749652564"
                        ],
                        "name": "Michal Pietruszka",
                        "slug": "Michal-Pietruszka",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Pietruszka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michal Pietruszka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400414730"
                        ],
                        "name": "Gabriela Pa\u0142ka",
                        "slug": "Gabriela-Pa\u0142ka",
                        "structuredName": {
                            "firstName": "Gabriela",
                            "lastName": "Pa\u0142ka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gabriela Pa\u0142ka"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 233,
                                "start": 212
                            }
                        ],
                        "text": "\u2026al., 2019; Jaume et al., 2019; Zhong et al., 2019; Rausch et al., 2019; Yu et al., 2020; Wei et al., 2020; Majumder et al., 2020; Lockard et al., 2020; Garncarek et al., 2020; Lin et al., 2020; Xu et al., 2020; Powalski et al., 2021; Wang et al., 2021; Hong et al., 2021; Hwang et al., 2021)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 231951453,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4bd0e72a6e41e34ae4d8d01f72aee0a3ce2d646a",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the challenging problem of Natural Language Comprehension beyond plain-text documents by introducing the TILT neural network architecture which simultaneously learns layout information, visual features, and textual semantics. Contrary to previous approaches, we rely on a decoder capable of unifying a variety of problems involving natural language. The layout is represented as an attention bias and complemented with contextualized visual information, while the core of our model is a pretrained encoder-decoder Transformer. Our novel approach achieves state-of-the-art results in extracting information from documents and answering questions which demand layout understanding (DocVQA, CORD, SROIE). At the same time, we simplify the process by employing an end-to-end model."
            },
            "slug": "Going-Full-TILT-Boogie-on-Document-Understanding-Powalski-Borchmann",
            "title": {
                "fragments": [],
                "text": "Going Full-TILT Boogie on Document Understanding with Text-Image-Layout Transformer"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This work addresses the challenging problem of Natural Language Comprehension beyond plain-text documents by introducing the TILT neural network architecture which simultaneously learns layout information, visual features, and textual semantics."
            },
            "venue": {
                "fragments": [],
                "text": "ICDAR"
            },
            "year": 2021
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48094112"
                        ],
                        "name": "Jiapeng Wang",
                        "slug": "Jiapeng-Wang",
                        "structuredName": {
                            "firstName": "Jiapeng",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiapeng Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9401586"
                        ],
                        "name": "Chongyu Liu",
                        "slug": "Chongyu-Liu",
                        "structuredName": {
                            "firstName": "Chongyu",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chongyu Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144838978"
                        ],
                        "name": "Lianwen Jin",
                        "slug": "Lianwen-Jin",
                        "structuredName": {
                            "firstName": "Lianwen",
                            "lastName": "Jin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lianwen Jin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153073573"
                        ],
                        "name": "Guozhi Tang",
                        "slug": "Guozhi-Tang",
                        "structuredName": {
                            "firstName": "Guozhi",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guozhi Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118131879"
                        ],
                        "name": "Jiaxin Zhang",
                        "slug": "Jiaxin-Zhang",
                        "structuredName": {
                            "firstName": "Jiaxin",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiaxin Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108432365"
                        ],
                        "name": "Shuaitao Zhang",
                        "slug": "Shuaitao-Zhang",
                        "structuredName": {
                            "firstName": "Shuaitao",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuaitao Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109025998"
                        ],
                        "name": "Qianying Wang",
                        "slug": "Qianying-Wang",
                        "structuredName": {
                            "firstName": "Qianying",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qianying Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29908089"
                        ],
                        "name": "Y. Wu",
                        "slug": "Y.-Wu",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052051801"
                        ],
                        "name": "Mingxiang Cai",
                        "slug": "Mingxiang-Cai",
                        "structuredName": {
                            "firstName": "Mingxiang",
                            "lastName": "Cai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mingxiang Cai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 235
                            }
                        ],
                        "text": "\u2026al., 2019; Jaume et al., 2019; Zhong et al., 2019; Rausch et al., 2019; Yu et al., 2020; Wei et al., 2020; Majumder et al., 2020; Lockard et al., 2020; Garncarek et al., 2020; Lin et al., 2020; Xu et al., 2020; Powalski et al., 2021; Wang et al., 2021; Hong et al., 2021; Hwang et al., 2021)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 231924690,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43c3ccb02ed34b6f38872bc7d75a85d812ac2746",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Visual information extraction (VIE) has attracted considerable attention recently owing to its various advanced applications such as document understanding, automatic marking and intelligent education. Most existing works decoupled this problem into several independent sub-tasks of text spotting (text detection and recognition) and information extraction, which completely ignored the high correlation among them during optimization. In this paper, we propose a robust visual information extraction system (VIES) towards real-world scenarios, which is an unified end-to-end trainable framework for simultaneous text detection, recognition and information extraction by taking a single document image as input and outputting the structured information. Specifically, the information extraction branch collects abundant visual and semantic representations from text spotting for multimodal feature fusion and conversely, provides higherlevel semantic clues to contribute to the optimization of text spotting. Moreover, regarding the shortage of public benchmarks, we construct a fully-annotated dataset called EPHOIE (https://github.com/HCIILAB/EPHOIE), which is the first Chinese benchmark for both text spotting and visual information extraction. EPHOIE consists of 1,494 images of examination paper head with complex layouts and background, including a total of 15,771 Chinese handwritten or printed text instances. Compared with the state-of-the-art methods, our VIES shows significant superior performance on the EPHOIE dataset and achieves a 9.01% F-score gain on the widely used SROIE dataset under the end-to-end scenario."
            },
            "slug": "Towards-Robust-Visual-Information-Extraction-in-New-Wang-Liu",
            "title": {
                "fragments": [],
                "text": "Towards Robust Visual Information Extraction in Real World: New Dataset and Novel Solution"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper proposes a robust visual information extraction system (VIES) towards real-world scenarios, which is an unified end-to-end trainable framework for simultaneous text detection, recognition and information extraction by taking a single document image as input and outputting the structured information."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2021
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115683276"
                        ],
                        "name": "Yang Xu",
                        "slug": "Yang-Xu",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yang Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3032611"
                        ],
                        "name": "Yiheng Xu",
                        "slug": "Yiheng-Xu",
                        "structuredName": {
                            "firstName": "Yiheng",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiheng Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1379581011"
                        ],
                        "name": "Tengchao Lv",
                        "slug": "Tengchao-Lv",
                        "structuredName": {
                            "firstName": "Tengchao",
                            "lastName": "Lv",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tengchao Lv"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114843952"
                        ],
                        "name": "Lei Cui",
                        "slug": "Lei-Cui",
                        "structuredName": {
                            "firstName": "Lei",
                            "lastName": "Cui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lei Cui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49807919"
                        ],
                        "name": "Furu Wei",
                        "slug": "Furu-Wei",
                        "structuredName": {
                            "firstName": "Furu",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Furu Wei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41211249"
                        ],
                        "name": "Guoxin Wang",
                        "slug": "Guoxin-Wang",
                        "structuredName": {
                            "firstName": "Guoxin",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guoxin Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38534822"
                        ],
                        "name": "Yijuan Lu",
                        "slug": "Yijuan-Lu",
                        "structuredName": {
                            "firstName": "Yijuan",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yijuan Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1882479"
                        ],
                        "name": "D. Flor\u00eancio",
                        "slug": "D.-Flor\u00eancio",
                        "structuredName": {
                            "firstName": "Dinei",
                            "lastName": "Flor\u00eancio",
                            "middleNames": [
                                "A.",
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Flor\u00eancio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109292017"
                        ],
                        "name": "Cha Zhang",
                        "slug": "Cha-Zhang",
                        "structuredName": {
                            "firstName": "Cha",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cha Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2256319"
                        ],
                        "name": "Wanxiang Che",
                        "slug": "Wanxiang-Che",
                        "structuredName": {
                            "firstName": "Wanxiang",
                            "lastName": "Che",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wanxiang Che"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2156053262"
                        ],
                        "name": "Min Zhang",
                        "slug": "Min-Zhang",
                        "structuredName": {
                            "firstName": "Min",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Min Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2143359114"
                        ],
                        "name": "Lidong Zhou",
                        "slug": "Lidong-Zhou",
                        "structuredName": {
                            "firstName": "Lidong",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lidong Zhou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 195
                            }
                        ],
                        "text": "\u2026al., 2019; Jaume et al., 2019; Zhong et al., 2019; Rausch et al., 2019; Yu et al., 2020; Wei et al., 2020; Majumder et al., 2020; Lockard et al., 2020; Garncarek et al., 2020; Lin et al., 2020; Xu et al., 2020; Powalski et al., 2021; Wang et al., 2021; Hong et al., 2021; Hwang et al., 2021)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 229923949,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0197abda042e6de87b5f716caa708a6a459f078c",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Pre-training of text and layout has proved effective in a variety of visually-rich document understanding tasks due to its effective model architecture and the advantage of large-scale unlabeled scanned/digital-born documents. We propose LayoutLMv2 architecture with new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework. Specifically, with a two-stream multi-modal Transformer encoder, LayoutLMv2 uses not only the existing masked visual-language modeling task but also the new text-image alignment and text-image matching tasks, which make it better capture the cross-modality interaction in the pre-training stage. Meanwhile, it also integrates a spatial-aware self-attention mechanism into the Transformer architecture so that the model can fully understand the relative positional relationship among different text blocks. Experiment results show that LayoutLMv2 outperforms LayoutLM by a large margin and achieves new state-of-the-art results on a wide variety of downstream visually-rich document understanding tasks, including FUNSD (0.7895 to 0.8420), CORD (0.9493 to 0.9601), SROIE (0.9524 to 0.9781), Kleister-NDA (0.8340 to 0.8520), RVL-CDIP (0.9443 to 0.9564), and DocVQA (0.7295 to 0.8672)."
            },
            "slug": "LayoutLMv2:-Multi-modal-Pre-training-for-Document-Xu-Xu",
            "title": {
                "fragments": [],
                "text": "LayoutLMv2: Multi-modal Pre-training for Visually-rich Document Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "LayoutLMv2 architecture with new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework and achieves new state-of-the-art results on a wide variety of downstream visually-rich document understanding tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2021
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51583409"
                        ],
                        "name": "Bill Yuchen Lin",
                        "slug": "Bill-Yuchen-Lin",
                        "structuredName": {
                            "firstName": "Bill Yuchen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bill Yuchen Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053802028"
                        ],
                        "name": "Ying Sheng",
                        "slug": "Ying-Sheng",
                        "structuredName": {
                            "firstName": "Ying",
                            "lastName": "Sheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ying Sheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40331081"
                        ],
                        "name": "N. Vo",
                        "slug": "N.-Vo",
                        "structuredName": {
                            "firstName": "Nguyen",
                            "lastName": "Vo",
                            "middleNames": [
                                "Ha"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Vo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2519906"
                        ],
                        "name": "Sandeep Tata",
                        "slug": "Sandeep-Tata",
                        "structuredName": {
                            "firstName": "Sandeep",
                            "lastName": "Tata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sandeep Tata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 221191345,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "832bdc41e74c4e5e5274b8e57107e857861f7bdf",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": "Extracting structured data from HTML documents is a long-studied problem with a broad range of applications like augmenting knowledge bases, supporting faceted search, and providing domain-specific experiences for key verticals like shopping and movies. Previous approaches have either required a small number of examples for each target site or relied on carefully handcrafted heuristics built over visual renderings of websites. In this paper, we present a novel two-stage neural approach, named FreeDOM, which overcomes both these limitations. The first stage learns a representation for each DOM node in the page by combining both the text and markup information. The second stage captures longer range distance and semantic relatedness using a relational neural network. By combining these stages, FreeDOM is able to generalize to unseen sites after training on a small number of seed sites from that vertical without requiring expensive hand-crafted features over visual renderings of the page. Through experiments on a public dataset with 8 different verticals, we show that FreeDOM beats the previous state of the art by nearly 3.7 F1 points on average without requiring features over rendered pages or expensive hand-crafted features."
            },
            "slug": "FreeDOM:-A-Transferable-Neural-Architecture-for-on-Lin-Sheng",
            "title": {
                "fragments": [],
                "text": "FreeDOM: A Transferable Neural Architecture for Structured Information Extraction on Web Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper presents a novel two-stage neural approach, named FreeDOM, which is able to generalize to unseen sites after training on a small number of seed sites from that vertical without requiring expensive hand-crafted features over visual renderings of the page."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3165738"
                        ],
                        "name": "Bodhisattwa Prasad Majumder",
                        "slug": "Bodhisattwa-Prasad-Majumder",
                        "structuredName": {
                            "firstName": "Bodhisattwa",
                            "lastName": "Majumder",
                            "middleNames": [
                                "Prasad"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bodhisattwa Prasad Majumder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2406599"
                        ],
                        "name": "Navneet Potti",
                        "slug": "Navneet-Potti",
                        "structuredName": {
                            "firstName": "Navneet",
                            "lastName": "Potti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Navneet Potti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2519906"
                        ],
                        "name": "Sandeep Tata",
                        "slug": "Sandeep-Tata",
                        "structuredName": {
                            "firstName": "Sandeep",
                            "lastName": "Tata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sandeep Tata"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796372"
                        ],
                        "name": "James Bradley Wendt",
                        "slug": "James-Bradley-Wendt",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Wendt",
                            "middleNames": [
                                "Bradley"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Bradley Wendt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110560772"
                        ],
                        "name": "Qi Zhao",
                        "slug": "Qi-Zhao",
                        "structuredName": {
                            "firstName": "Qi",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qi Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763978"
                        ],
                        "name": "Marc Najork",
                        "slug": "Marc-Najork",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Najork",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc Najork"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 219732851,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "58877aa9aa2d09585a4ff4881b02cb1c7ff9bc28",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel approach using representation learning for tackling the problem of extracting structured information from form-like document images. We propose an extraction system that uses knowledge of the types of the target fields to generate extraction candidates and a neural network architecture that learns a dense representation of each candidate based on neighboring words in the document. These learned representations are not only useful in solving the extraction task for unseen document templates from two different domains but are also interpretable, as we show using loss cases."
            },
            "slug": "Representation-Learning-for-Information-Extraction-Majumder-Potti",
            "title": {
                "fragments": [],
                "text": "Representation Learning for Information Extraction from Form-like Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An extraction system that uses knowledge of the types of the target fields to generate extraction candidates and a neural network architecture that learns a dense representation of each candidate based on neighboring words in the document is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39980566"
                        ],
                        "name": "Mengxi Wei",
                        "slug": "Mengxi-Wei",
                        "structuredName": {
                            "firstName": "Mengxi",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mengxi Wei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38425357"
                        ],
                        "name": "Yifan He",
                        "slug": "Yifan-He",
                        "structuredName": {
                            "firstName": "Yifan",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yifan He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115485"
                        ],
                        "name": "Qiong Zhang",
                        "slug": "Qiong-Zhang",
                        "structuredName": {
                            "firstName": "Qiong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiong Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 149
                            }
                        ],
                        "text": "\u2026Park et al., 2019; Xu\n1https://github.com/clovaai/spade\net al., 2019; Jaume et al., 2019; Zhong et al., 2019; Rausch et al., 2019; Yu et al., 2020; Wei et al., 2020; Majumder et al., 2020; Lockard et al., 2020; Garncarek et al., 2020; Lin et al., 2020; Xu et al., 2020; Powalski et al., 2021; Wang\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 54
                            }
                        ],
                        "text": "Graph-based IE Liu et al. (2019); Qian et al. (2019); Wei et al. (2020); Yu et al. (2020) utilize a graph convolution network to contextualize the tokens in a document and a bidirectional LSTM with CRF to predict the IOB tags."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 218862990,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a03407e7e8a4530d9bb96672e425cfa067f92b76",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Many business documents processed in modern NLP and IR pipelines are visually rich: in addition to text, their semantics can also be captured by visual traits such as layout, format, and fonts. We study the problem of information extraction from visually rich documents (VRDs) and present a model that combines the power of large pre-trained language models and graph neural networks to efficiently encode both textual and visual information in business documents. We further introduce new fine-tuning objectives to improve in-domain unsupervised fine-tuning to better utilize large amount of unlabeled in-domain data. We experiment on real world invoice and resume data sets and show that the proposed method outperforms strong text-based RoBERTa baselines by 6.3% absolute F1 on invoices and 4.7% absolute F1 on resumes. When evaluated in a few-shot setting, our method requires up to 30x less annotation data than the baseline to achieve the same level of performance at ~90% F1."
            },
            "slug": "Robust-Layout-aware-IE-for-Visually-Rich-Documents-Wei-He",
            "title": {
                "fragments": [],
                "text": "Robust Layout-aware IE for Visually Rich Documents with Pre-trained Language Models"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work studies the problem of information extraction from visually rich documents (VRDs) and presents a model that combines the power of large pre-trained language models and graph neural networks to efficiently encode both textual and visual information in business documents."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144182018"
                        ],
                        "name": "Colin Lockard",
                        "slug": "Colin-Lockard",
                        "structuredName": {
                            "firstName": "Colin",
                            "lastName": "Lockard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Colin Lockard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3310534"
                        ],
                        "name": "Prashant Shiralkar",
                        "slug": "Prashant-Shiralkar",
                        "structuredName": {
                            "firstName": "Prashant",
                            "lastName": "Shiralkar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Prashant Shiralkar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2143917898"
                        ],
                        "name": "Xin Dong",
                        "slug": "Xin-Dong",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Dong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xin Dong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2548384"
                        ],
                        "name": "Hannaneh Hajishirzi",
                        "slug": "Hannaneh-Hajishirzi",
                        "structuredName": {
                            "firstName": "Hannaneh",
                            "lastName": "Hajishirzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hannaneh Hajishirzi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 131
                            }
                        ],
                        "text": "\u2026al., 2019; Jaume et al., 2019; Zhong et al., 2019; Rausch et al., 2019; Yu et al., 2020; Wei et al., 2020; Majumder et al., 2020; Lockard et al., 2020; Garncarek et al., 2020; Lin et al., 2020; Xu et al., 2020; Powalski et al., 2021; Wang et al., 2021; Hong et al., 2021; Hwang\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ", database schema), which has become an increasingly important task in both research community and industry (Palm, Winther, and Laws 2017; Katti et al. 2018; Qian et al. 2019; Liu et al. 2019; Zhao, Wu, and Wang 2019; Denk and Reisswig 2019; Hwang et al. 2019; Park et al. 2019; Xu et al. 2019; Jaume, Ekenel, and Thiran 2019; Yu et al. 2020; Wei, He, and Zhang 2020; Lockard et al. 2020; Lin et al. 2020)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 0
                            }
                        ],
                        "text": "Lockard et al. (2019, 2020) also utilize a graph to extract semantic relation from semi-structrued web-page."
                    },
                    "intents": []
                }
            ],
            "corpusId": 218628743,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dcf7fb34ae0be4eb7a838679b6bef0736375bbac",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "In many documents, such as semi-structured webpages, textual semantics are augmented with additional information conveyed using visual elements including layout, font size, and color. Prior work on information extraction from semi-structured websites has required learning an extraction model specific to a given template via either manually labeled or distantly supervised data from that template. In this work, we propose a solution for \u201czero-shot\u201d open-domain relation extraction from webpages with a previously unseen template, including from websites with little overlap with existing sources of knowledge for distant supervision and websites in entirely new subject verticals. Our model uses a graph neural network-based approach to build a rich representation of text fields on a webpage and the relationships between them, enabling generalization to new templates. Experiments show this approach provides a 31% F1 gain over a baseline for zero-shot extraction in a new subject vertical."
            },
            "slug": "ZeroShotCeres:-Zero-Shot-Relation-Extraction-from-Lockard-Shiralkar",
            "title": {
                "fragments": [],
                "text": "ZeroShotCeres: Zero-Shot Relation Extraction from Semi-Structured Webpages"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes a solution for \u201czero-shot\u201d open-domain relation extraction from webpages with a previously unseen template, including from websites with little overlap with existing sources of knowledge for distant supervision and websites in entirely new subject verticals."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2048569409"
                        ],
                        "name": "Wenwen Yu",
                        "slug": "Wenwen-Yu",
                        "structuredName": {
                            "firstName": "Wenwen",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenwen Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054645836"
                        ],
                        "name": "Ning Lu",
                        "slug": "Ning-Lu",
                        "structuredName": {
                            "firstName": "Ning",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ning Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2689287"
                        ],
                        "name": "Xianbiao Qi",
                        "slug": "Xianbiao-Qi",
                        "structuredName": {
                            "firstName": "Xianbiao",
                            "lastName": "Qi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xianbiao Qi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2124651409"
                        ],
                        "name": "Ping Gong",
                        "slug": "Ping-Gong",
                        "structuredName": {
                            "firstName": "Ping",
                            "lastName": "Gong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ping Gong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069928447"
                        ],
                        "name": "Rong Xiao",
                        "slug": "Rong-Xiao",
                        "structuredName": {
                            "firstName": "Rong",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rong Xiao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 73
                            }
                        ],
                        "text": "Graph-based IE Liu et al. (2019); Qian et al. (2019); Wei et al. (2020); Yu et al. (2020) utilize a graph convolution network to contextualize the tokens in a document and a bidirectional LSTM with CRF to predict the IOB tags."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 146
                            }
                        ],
                        "text": "\u2026et al., 2019; Park et al., 2019; Xu\n1https://github.com/clovaai/spade\net al., 2019; Jaume et al., 2019; Zhong et al., 2019; Rausch et al., 2019; Yu et al., 2020; Wei et al., 2020; Majumder et al., 2020; Lockard et al., 2020; Garncarek et al., 2020; Lin et al., 2020; Xu et al., 2020; Powalski\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 215786577,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ba5ed98c4546fada5c732bced4a1c1615f1a4c16",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Computer vision with state-of-the-art deep learning models has achieved huge success in the field of Optical Character Recognition (OCR) including text detection and recognition tasks recently. However, Key Information Extraction (KIE) from documents as the downstream task of OCR, having a large number of use scenarios in real-world, remains a challenge because documents not only have textual features extracting from OCR systems but also have semantic visual features that are not fully exploited and play a critical role in KIE. Too little work has been devoted to efficiently make full use of both textual and visual features of the documents. In this paper, we introduce PICK, a framework that is effective and robust in handling complex documents layout for KIE by combining graph learning with graph convolution operation, yielding a richer semantic representation containing the textual and visual features and global layout without ambiguity. Extensive experiments on realworld datasets have been conducted to show that our method outperforms baselines methods by significant margins. Our code is available at https://github.com/wenwenyu/PICK-pytorch."
            },
            "slug": "PICK:-Processing-Key-Information-Extraction-from-Yu-Lu",
            "title": {
                "fragments": [],
                "text": "PICK: Processing Key Information Extraction from Documents using Improved Graph Learning-Convolutional Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "PICK is introduced, a framework that is effective and robust in handling complex documents layout for KIE by combining graph learning with graph convolution operation, yielding a richer semantic representation containing the textual and visual features and global layout without ambiguity."
            },
            "venue": {
                "fragments": [],
                "text": "2020 25th International Conference on Pattern Recognition (ICPR)"
            },
            "year": 2021
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "79665491"
                        ],
                        "name": "Lukasz Garncarek",
                        "slug": "Lukasz-Garncarek",
                        "structuredName": {
                            "firstName": "Lukasz",
                            "lastName": "Garncarek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lukasz Garncarek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1498640514"
                        ],
                        "name": "Rafal Powalski",
                        "slug": "Rafal-Powalski",
                        "structuredName": {
                            "firstName": "Rafal",
                            "lastName": "Powalski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rafal Powalski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1822665"
                        ],
                        "name": "Tomasz Stanislawek",
                        "slug": "Tomasz-Stanislawek",
                        "structuredName": {
                            "firstName": "Tomasz",
                            "lastName": "Stanislawek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomasz Stanislawek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11016367"
                        ],
                        "name": "Bartosz Topolski",
                        "slug": "Bartosz-Topolski",
                        "structuredName": {
                            "firstName": "Bartosz",
                            "lastName": "Topolski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bartosz Topolski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "108821285"
                        ],
                        "name": "Piotr Halama",
                        "slug": "Piotr-Halama",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Halama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Halama"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064873282"
                        ],
                        "name": "Filip Grali'nski",
                        "slug": "Filip-Grali'nski",
                        "structuredName": {
                            "firstName": "Filip",
                            "lastName": "Grali'nski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Filip Grali'nski"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 153
                            }
                        ],
                        "text": "\u2026al., 2019; Jaume et al., 2019; Zhong et al., 2019; Rausch et al., 2019; Yu et al., 2020; Wei et al., 2020; Majumder et al., 2020; Lockard et al., 2020; Garncarek et al., 2020; Lin et al., 2020; Xu et al., 2020; Powalski et al., 2021; Wang et al., 2021; Hong et al., 2021; Hwang et al., 2021)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 211171875,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e551546e51c570fe796b67ae23d97297b0717921",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we introduce a novel approach to the problem of understanding documents where the local semantics is influenced by non-trivial layout. Namely, we modify the Transformer architecture in a way that allows it to use the graphical features defined by the layout, without the need to re-learn the language semantics from scratch, thanks to starting the training process from a model pretrained on classical language modeling tasks."
            },
            "slug": "LAMBERT:-Layout-Aware-language-Modeling-using-BERT-Garncarek-Powalski",
            "title": {
                "fragments": [],
                "text": "LAMBERT: Layout-Aware language Modeling using BERT for information extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The Transformer architecture is modified in a way that allows it to use the graphical features defined by the layout, without the need to re-learn the language semantics from scratch, thanks to starting the training process from a model pretrained on classical language modeling tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3032611"
                        ],
                        "name": "Yiheng Xu",
                        "slug": "Yiheng-Xu",
                        "structuredName": {
                            "firstName": "Yiheng",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiheng Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123545597"
                        ],
                        "name": "Minghao Li",
                        "slug": "Minghao-Li",
                        "structuredName": {
                            "firstName": "Minghao",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minghao Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145500855"
                        ],
                        "name": "Lei Cui",
                        "slug": "Lei-Cui",
                        "structuredName": {
                            "firstName": "Lei",
                            "lastName": "Cui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lei Cui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110003"
                        ],
                        "name": "Shaohan Huang",
                        "slug": "Shaohan-Huang",
                        "structuredName": {
                            "firstName": "Shaohan",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaohan Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49807919"
                        ],
                        "name": "Furu Wei",
                        "slug": "Furu-Wei",
                        "structuredName": {
                            "firstName": "Furu",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Furu Wei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "92660691"
                        ],
                        "name": "Ming Zhou",
                        "slug": "Ming-Zhou",
                        "structuredName": {
                            "firstName": "Ming",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming Zhou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", database schema), which has become an increasingly important task in both research community and industry (Palm, Winther, and Laws 2017; Katti et al. 2018; Qian et al. 2019; Liu et al. 2019; Zhao, Wu, and Wang 2019; Denk and Reisswig 2019; Hwang et al. 2019; Park et al. 2019; Xu et al. 2019; Jaume, Ekenel, and Thiran 2019; Yu et al. 2020; Wei, He, and Zhang 2020; Lockard et al. 2020; Lin et al. 2020)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We evaluate our model on FUNSD data and compare the result with the scores reported in the previous works of Jaume, Ekenel, and Thiran (2019) and Xu et al. (2019). The performance is measured on two OCRindependent subtasks following the original paper (Jaume, Ekenel, and Thiran 2019): (1) \u201centity-labeling\u201d which predicts the information category of the serialized words, and (2) \u201centity-linking\u201d which measures the score for key-value pair link prediction."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 195
                            }
                        ],
                        "text": "\u2026al., 2019; Jaume et al., 2019; Zhong et al., 2019; Rausch et al., 2019; Yu et al., 2020; Wei et al., 2020; Majumder et al., 2020; Lockard et al., 2020; Garncarek et al., 2020; Lin et al., 2020; Xu et al., 2020; Powalski et al., 2021; Wang et al., 2021; Hong et al., 2021; Hwang et al., 2021)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 209515395,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3465c06c872d8c48d628c5fc2d484087719351b6",
            "isKey": false,
            "numCitedBy": 164,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Pre-training techniques have been verified successfully in a variety of NLP tasks in recent years. Despite the widespread use of pre-training models for NLP applications, they almost exclusively focus on text-level manipulation, while neglecting layout and style information that is vital for document image understanding. In this paper, we propose the LayoutLM to jointly model interactions between text and layout information across scanned document images, which is beneficial for a great number of real-world document image understanding tasks such as information extraction from scanned documents. Furthermore, we also leverage image features to incorporate words' visual information into LayoutLM. To the best of our knowledge, this is the first time that text and layout are jointly learned in a single framework for document-level pre-training. It achieves new state-of-the-art results in several downstream tasks, including form understanding (from 70.72 to 79.27), receipt understanding (from 94.02 to 95.24) and document image classification (from 93.07 to 94.42). The code and pre-trained LayoutLM models are publicly available at https://aka.ms/layoutlm."
            },
            "slug": "LayoutLM:-Pre-training-of-Text-and-Layout-for-Image-Xu-Li",
            "title": {
                "fragments": [],
                "text": "LayoutLM: Pre-training of Text and Layout for Document Image Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The LayoutLM is proposed to jointly model interactions between text and layout information across scanned document images, which is beneficial for a great number of real-world document image understanding tasks such as information extraction from scanned documents."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113309578"
                        ],
                        "name": "Xu Zhong",
                        "slug": "Xu-Zhong",
                        "structuredName": {
                            "firstName": "Xu",
                            "lastName": "Zhong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xu Zhong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9585722"
                        ],
                        "name": "Elaheh Shafieibavani",
                        "slug": "Elaheh-Shafieibavani",
                        "structuredName": {
                            "firstName": "Elaheh",
                            "lastName": "Shafieibavani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Elaheh Shafieibavani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399097376"
                        ],
                        "name": "Antonio Jimeno-Yepes",
                        "slug": "Antonio-Jimeno-Yepes",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Jimeno-Yepes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antonio Jimeno-Yepes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 150
                            }
                        ],
                        "text": "\u2026et al., 2019; Denk and Reisswig, 2019; Hwang et al., 2019; Park et al., 2019; Xu\n1https://github.com/clovaai/spade\net al., 2019; Jaume et al., 2019; Zhong et al., 2019; Rausch et al., 2019; Yu et al., 2020; Wei et al., 2020; Majumder et al., 2020; Lockard et al., 2020; Garncarek et al., 2020; Lin\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 208267858,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "99b2a05177c17dd86627863608ac45dc4cfe0446",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Important information that relates to a specific topic in a document is often organized in tabular format to assist readers with information retrieval and comparison, which may be difficult to provide in natural language. However, tabular data in unstructured digital documents, e.g., Portable Document Format (PDF) and images, are difficult to parse into structured machine-readable format, due to complexity and diversity in their structure and style. To facilitate image-based table recognition with deep learning, we develop the largest publicly available table recognition dataset PubTabNet (this https URL), containing 568k table images with corresponding structured HTML representation. PubTabNet is automatically generated by matching the XML and PDF representations of the scientific articles in PubMed Central Open Access Subset (PMCOA). We also propose a novel attention-based encoder-dual-decoder (EDD) architecture that converts images of tables into HTML code. The model has a structure decoder which reconstructs the table structure and helps the cell decoder to recognize cell content. In addition, we propose a new Tree-Edit-Distance-based Similarity (TEDS) metric for table recognition, which more appropriately captures multi-hop cell misalignment and OCR errors than the pre-established metric. The experiments demonstrate that the EDD model can accurately recognize complex tables solely relying on the image representation, outperforming the state-of-the-art by 9.7% absolute TEDS score."
            },
            "slug": "Image-based-table-recognition:-data,-model,-and-Zhong-Shafieibavani",
            "title": {
                "fragments": [],
                "text": "Image-based table recognition: data, model, and evaluation"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The largest publicly available table recognition dataset PubTabNet is developed, containing 568k table images with corresponding structured HTML representation, and a new Tree-Edit-Distance-based Similarity (TEDS) metric for table recognition is proposed, which more appropriately captures multi-hop cell misalignment and OCR errors than the pre-established metric."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5185738"
                        ],
                        "name": "Johannes Rausch",
                        "slug": "Johannes-Rausch",
                        "structuredName": {
                            "firstName": "Johannes",
                            "lastName": "Rausch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Johannes Rausch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7729843"
                        ],
                        "name": "Octavio Mart\u00ednez",
                        "slug": "Octavio-Mart\u00ednez",
                        "structuredName": {
                            "firstName": "Octavio",
                            "lastName": "Mart\u00ednez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Octavio Mart\u00ednez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1402912519"
                        ],
                        "name": "Fabian Bissig",
                        "slug": "Fabian-Bissig",
                        "structuredName": {
                            "firstName": "Fabian",
                            "lastName": "Bissig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fabian Bissig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1776014"
                        ],
                        "name": "Ce Zhang",
                        "slug": "Ce-Zhang",
                        "structuredName": {
                            "firstName": "Ce",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ce Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3207649"
                        ],
                        "name": "S. Feuerriegel",
                        "slug": "S.-Feuerriegel",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Feuerriegel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Feuerriegel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 147
                            }
                        ],
                        "text": "\u2026Reisswig, 2019; Hwang et al., 2019; Park et al., 2019; Xu\n1https://github.com/clovaai/spade\net al., 2019; Jaume et al., 2019; Zhong et al., 2019; Rausch et al., 2019; Yu et al., 2020; Wei et al., 2020; Majumder et al., 2020; Lockard et al., 2020; Garncarek et al., 2020; Lin et al., 2020; Xu et\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 207870462,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c49d3e214055e176c47b3358e77acbbea7d74ecc",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Translating document renderings (e.g. PDFs, scans) into hierarchical structures is extensively demanded in the daily routines of many real-world applications, and is often a prerequisite step of many downstream NLP tasks. Earlier attempts focused on different but simpler tasks such as the detection of table or cell locations within documents; however, a holistic, principled approach to inferring the complete hierarchical structure in documents is missing. As a remedy, we developed \"DocParser\": an end-to-end system for parsing the complete document structure - including all text elements, figures, tables, and table cell structures. To the best of our knowledge, DocParser is the first system that derives the full hierarchical document compositions. Given the complexity of the task, annotating appropriate datasets is costly. Therefore, our second contribution is to provide a dataset for evaluating hierarchical document structure parsing. Our third contribution is to propose a scalable learning framework for settings where domain-specific data is scarce, which we address by a novel approach to weak supervision. Our computational experiments confirm the effectiveness of our proposed weak supervision: Compared to the baseline without weak supervision, it improves the mean average precision for detecting document entities by 37.1%. When classifying hierarchical relations between entity pairs, it improves the F1 score by 27.6%."
            },
            "slug": "DocParser:-Hierarchical-Structure-Parsing-of-Rausch-Mart\u00ednez",
            "title": {
                "fragments": [],
                "text": "DocParser: Hierarchical Structure Parsing of Document Renderings"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work developed \"DocParser\": an end-to-end system for parsing the complete document structure - including all text elements, figures, tables, and table cell structures, which is the first system that derives the full hierarchical document compositions."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153368526"
                        ],
                        "name": "Timo I. Denk",
                        "slug": "Timo-I.-Denk",
                        "structuredName": {
                            "firstName": "Timo",
                            "lastName": "Denk",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timo I. Denk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9992847"
                        ],
                        "name": "C. Reisswig",
                        "slug": "C.-Reisswig",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Reisswig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Reisswig"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 202558968,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2fbda89395f993040b7665730c64182ade3be195",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "For understanding generic documents, information like font sizes, column layout, and generally the positioning of words may carry semantic information that is crucial for solving a downstream document intelligence task. Our novel BERTgrid, which is based on Chargrid by Katti et al. (2018), represents a document as a grid of contextualized word piece embedding vectors, thereby making its spatial structure and semantics accessible to the processing neural network. The contextualized embedding vectors are retrieved from a BERT language model. We use BERTgrid in combination with a fully convolutional network on a semantic instance segmentation task for extracting fields from invoices. We demonstrate its performance on tabulated line item and document header field extraction."
            },
            "slug": "BERTgrid:-Contextualized-Embedding-for-2D-Document-Denk-Reisswig",
            "title": {
                "fragments": [],
                "text": "BERTgrid: Contextualized Embedding for 2D Document Representation and Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The novel BERTgrid, which is based on Chargrid, represents a document as a grid of contextualized word piece embedding vectors, thereby making its spatial structure and semantics accessible to the processing neural network."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144182018"
                        ],
                        "name": "Colin Lockard",
                        "slug": "Colin-Lockard",
                        "structuredName": {
                            "firstName": "Colin",
                            "lastName": "Lockard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Colin Lockard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3310534"
                        ],
                        "name": "Prashant Shiralkar",
                        "slug": "Prashant-Shiralkar",
                        "structuredName": {
                            "firstName": "Prashant",
                            "lastName": "Shiralkar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Prashant Shiralkar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2143917898"
                        ],
                        "name": "Xin Dong",
                        "slug": "Xin-Dong",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Dong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xin Dong"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 174799980,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b31eef8d9263b02f7d0c1ab55b26012550a2e95a",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Open Information Extraction (OpenIE), the problem of harvesting triples from natural language text whose predicate relations are not aligned to any pre-defined ontology, has been a popular subject of research for the last decade. However, this research has largely ignored the vast quantity of facts available in semi-structured webpages. In this paper, we define the problem of OpenIE from semi-structured websites to extract such facts, and present an approach for solving it. We also introduce a labeled evaluation dataset to motivate research in this area. Given a semi-structured website and a set of seed facts for some relations existing on its pages, we employ a semi-supervised label propagation technique to automatically create training data for the relations present on the site. We then use this training data to learn a classifier for relation extraction. Experimental results of this method on our new benchmark dataset obtained a precision of over 70%. A larger scale extraction experiment on 31 websites in the movie vertical resulted in the extraction of over 2 million triples."
            },
            "slug": "OpenCeres:-When-Open-Information-Extraction-Meets-Lockard-Shiralkar",
            "title": {
                "fragments": [],
                "text": "OpenCeres: When Open Information Extraction Meets the Semi-Structured Web"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper defines the problem of OpenIE from semi-structured websites to extract such facts, and presents an approach for solving it, and introduces a labeled evaluation dataset to motivate research in this area."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057310228"
                        ],
                        "name": "Youngmin Baek",
                        "slug": "Youngmin-Baek",
                        "structuredName": {
                            "firstName": "Youngmin",
                            "lastName": "Baek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Youngmin Baek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2722273"
                        ],
                        "name": "Bado Lee",
                        "slug": "Bado-Lee",
                        "structuredName": {
                            "firstName": "Bado",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bado Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2086576"
                        ],
                        "name": "Dongyoon Han",
                        "slug": "Dongyoon-Han",
                        "structuredName": {
                            "firstName": "Dongyoon",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dongyoon Han"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2151587"
                        ],
                        "name": "Sangdoo Yun",
                        "slug": "Sangdoo-Yun",
                        "structuredName": {
                            "firstName": "Sangdoo",
                            "lastName": "Yun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sangdoo Yun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72152162"
                        ],
                        "name": "Hwalsuk Lee",
                        "slug": "Hwalsuk-Lee",
                        "structuredName": {
                            "firstName": "Hwalsuk",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hwalsuk Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 123
                            }
                        ],
                        "text": "To extract the visually embedded texts from an image, we use our in-house OCR system that consists of CRAFT text detector (Baek et al., 2019b) and Comb.best text recognizer (Baek et al., 2019a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 122
                            }
                        ],
                        "text": "To extract the visually embedded texts from an image, we use our in-house OCR system that consists of CRAFT text detector (Baek et al., 2019b) and Comb."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 102480461,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "206b2aeb81b29463968b8deb1efce51941f18208",
            "isKey": false,
            "numCitedBy": 290,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Scene text detection methods based on neural networks have emerged recently and have shown promising results. Previous methods trained with rigid word-level bounding boxes exhibit limitations in representing the text region in an arbitrary shape. In this paper, we propose a new scene text detection method to effectively detect text area by exploring each character and affinity between characters. To overcome the lack of individual character level annotations, our proposed framework exploits both the given character-level annotations for synthetic images and the estimated character-level ground-truths for real images acquired by the learned interim model. In order to estimate affinity between characters, the network is trained with the newly proposed representation for affinity. Extensive experiments on six benchmarks, including the TotalText and CTW-1500 datasets which contain highly curved texts in natural images, demonstrate that our character-level text detection significantly outperforms the state-of-the-art detectors. According to the results, our proposed method guarantees high flexibility in detecting complicated scene text images, such as arbitrarily-oriented, curved, or deformed texts."
            },
            "slug": "Character-Region-Awareness-for-Text-Detection-Baek-Lee",
            "title": {
                "fragments": [],
                "text": "Character Region Awareness for Text Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A new scene text detection method to effectively detect text area by exploring each character and affinity between characters by exploiting both the given character- level annotations for synthetic images and the estimated character-level ground-truths for real images acquired by the learned interim model."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34133335"
                        ],
                        "name": "Jeonghun Baek",
                        "slug": "Jeonghun-Baek",
                        "structuredName": {
                            "firstName": "Jeonghun",
                            "lastName": "Baek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeonghun Baek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115988519"
                        ],
                        "name": "Geewook Kim",
                        "slug": "Geewook-Kim",
                        "structuredName": {
                            "firstName": "Geewook",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geewook Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39924323"
                        ],
                        "name": "Junyeop Lee",
                        "slug": "Junyeop-Lee",
                        "structuredName": {
                            "firstName": "Junyeop",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junyeop Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71537829"
                        ],
                        "name": "Sungrae Park",
                        "slug": "Sungrae-Park",
                        "structuredName": {
                            "firstName": "Sungrae",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sungrae Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2086576"
                        ],
                        "name": "Dongyoon Han",
                        "slug": "Dongyoon-Han",
                        "structuredName": {
                            "firstName": "Dongyoon",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dongyoon Han"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2151587"
                        ],
                        "name": "Sangdoo Yun",
                        "slug": "Sangdoo-Yun",
                        "structuredName": {
                            "firstName": "Sangdoo",
                            "lastName": "Yun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sangdoo Yun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2390510"
                        ],
                        "name": "Seong Joon Oh",
                        "slug": "Seong-Joon-Oh",
                        "structuredName": {
                            "firstName": "Seong Joon",
                            "lastName": "Oh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seong Joon Oh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72152162"
                        ],
                        "name": "Hwalsuk Lee",
                        "slug": "Hwalsuk-Lee",
                        "structuredName": {
                            "firstName": "Hwalsuk",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hwalsuk Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 123
                            }
                        ],
                        "text": "To extract the visually embedded texts from an image, we use our in-house OCR system that consists of CRAFT text detector (Baek et al., 2019b) and Comb.best text recognizer (Baek et al., 2019a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 102481180,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9387ffc038bc744e5eb335fe54b4f3f184202d77",
            "isKey": false,
            "numCitedBy": 201,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Many new proposals for scene text recognition (STR) models have been introduced in recent years. While each claim to have pushed the boundary of the technology, a holistic and fair comparison has been largely missing in the field due to the inconsistent choices of training and evaluation datasets. This paper addresses this difficulty with three major contributions. First, we examine the inconsistencies of training and evaluation datasets, and the performance gap results from inconsistencies. Second, we introduce a unified four-stage STR framework that most existing STR models fit into. Using this framework allows for the extensive evaluation of previously proposed STR modules and the discovery of previously unexplored module combinations. Third, we analyze the module-wise contributions to performance in terms of accuracy, speed, and memory demand, under one consistent set of training and evaluation datasets. Such analyses clean up the hindrance on the current comparisons to understand the performance gain of the existing modules. Our code is publicly available."
            },
            "slug": "What-Is-Wrong-With-Scene-Text-Recognition-Model-and-Baek-Kim",
            "title": {
                "fragments": [],
                "text": "What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A unified four-stage STR framework is introduced that most existing STR models fit into and allows for the extensive evaluation of previously proposed STR modules and the discovery of previously unexplored module combinations."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108812052"
                        ],
                        "name": "Xiaohui Zhao",
                        "slug": "Xiaohui-Zhao",
                        "structuredName": {
                            "firstName": "Xiaohui",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaohui Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109569771"
                        ],
                        "name": "Zhuo Wu",
                        "slug": "Zhuo-Wu",
                        "structuredName": {
                            "firstName": "Zhuo",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhuo Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108004518"
                        ],
                        "name": "Xiaoguang Wang",
                        "slug": "Xiaoguang-Wang",
                        "structuredName": {
                            "firstName": "Xiaoguang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoguang Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 149
                            }
                        ],
                        "text": "\u2026in their increased number of publication record from both research community and industry (Katti et al., 2018; Qian et al., 2019; Liu et al., 2019; Zhao et al., 2019; Denk and Reisswig, 2019; Hwang et al., 2019; Park et al., 2019; Xu\n1https://github.com/clovaai/spade\net al., 2019; Jaume et al.,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 88518818,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "024aea24cc4d0949d6fe1482591d2429a9d8bbeb",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Extracting key information from documents, such as receipts or invoices, and preserving the interested texts to structured data is crucial in the document-intensive streamline processes of office automation in areas that includes but not limited to accounting, financial, and taxation areas. To avoid designing expert rules for each specific type of document, some published works attempt to tackle the problem by learning a model to explore the semantic context in text sequences based on the Named Entity Recognition (NER) method in the NLP field. In this paper, we propose to harness the effective information from both semantic meaning and spatial distribution of texts in documents. Specifically, our proposed model, Convolutional Universal Text Information Extractor (CUTIE), applies convolutional neural networks on gridded texts where texts are embedded as features with semantical connotations. We further explore the effect of employing different structures of convolutional neural network and propose a fast and portable structure. We demonstrate the effectiveness of the proposed method on a dataset with up to $4,484$ labelled receipts, without any pre-training or post-processing, achieving state of the art performance that is much better than the NER based methods in terms of either speed and accuracy. Experimental results also demonstrate that the proposed CUTIE model being able to achieve good performance with a much smaller amount of training data."
            },
            "slug": "CUTIE:-Learning-to-Understand-Documents-with-Text-Zhao-Wu",
            "title": {
                "fragments": [],
                "text": "CUTIE: Learning to Understand Documents with Convolutional Universal Text Information Extractor"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The proposed model, Convolutional Universal Text Information Extractor (CUTIE), applies convolutional neural networks on gridded texts where texts are embedded as features with semantical connotations and aims to harness the effective information from both semantic meaning and spatial distribution of texts in documents."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3422912"
                        ],
                        "name": "Zihang Dai",
                        "slug": "Zihang-Dai",
                        "structuredName": {
                            "firstName": "Zihang",
                            "lastName": "Dai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zihang Dai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109512754"
                        ],
                        "name": "Zhilin Yang",
                        "slug": "Zhilin-Yang",
                        "structuredName": {
                            "firstName": "Zhilin",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhilin Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35729970"
                        ],
                        "name": "Yiming Yang",
                        "slug": "Yiming-Yang",
                        "structuredName": {
                            "firstName": "Yiming",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiming Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143712374"
                        ],
                        "name": "J. Carbonell",
                        "slug": "J.-Carbonell",
                        "structuredName": {
                            "firstName": "Jaime",
                            "lastName": "Carbonell",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Carbonell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 27
                            }
                        ],
                        "text": "Inspired by Transformer XL (Dai et al., 2019), the attention weights (between each key and query vector) is computed by"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 28
                            }
                        ],
                        "text": "Inspired by Transformer XL (Dai et al., 2019), the attention weights (between each key and query vector) is computed by\nqTi kj + q T i rij + (b key i ) Tkj + (breli ) T rij (1)\nwhere qi is the query vector of the i-th input token, kj is the key vector of the j-th input token, rij is the relative\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 57759363,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6",
            "isKey": false,
            "numCitedBy": 1771,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch."
            },
            "slug": "Transformer-XL:-Attentive-Language-Models-beyond-a-Dai-Yang",
            "title": {
                "fragments": [],
                "text": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work proposes a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence, which consists of a segment-level recurrence mechanism and a novel positional encoding scheme."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5606742"
                        ],
                        "name": "Yujie Qian",
                        "slug": "Yujie-Qian",
                        "structuredName": {
                            "firstName": "Yujie",
                            "lastName": "Qian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yujie Qian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2628786"
                        ],
                        "name": "Enrico Santus",
                        "slug": "Enrico-Santus",
                        "structuredName": {
                            "firstName": "Enrico",
                            "lastName": "Santus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Enrico Santus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8752221"
                        ],
                        "name": "Zhijing Jin",
                        "slug": "Zhijing-Jin",
                        "structuredName": {
                            "firstName": "Zhijing",
                            "lastName": "Jin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhijing Jin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144084849"
                        ],
                        "name": "Jiang Guo",
                        "slug": "Jiang-Guo",
                        "structuredName": {
                            "firstName": "Jiang",
                            "lastName": "Guo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiang Guo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741283"
                        ],
                        "name": "R. Barzilay",
                        "slug": "R.-Barzilay",
                        "structuredName": {
                            "firstName": "Regina",
                            "lastName": "Barzilay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Barzilay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 141
                            }
                        ],
                        "text": "\u2026documents are well reflected in their increased number of publication record from both research community and industry (Katti et al., 2018; Qian et al., 2019; Liu et al., 2019; Zhao et al., 2019; Denk and Reisswig, 2019; Hwang et al., 2019; Park et al., 2019;\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ", database schema), which has become an increasingly important task in both research community and industry (Palm, Winther, and Laws 2017; Katti et al. 2018; Qian et al. 2019; Liu et al. 2019; Zhao, Wu, and Wang 2019; Denk and Reisswig 2019; Hwang et al. 2019; Park et al. 2019; Xu et al. 2019; Jaume, Ekenel, and Thiran 2019; Yu et al. 2020; Wei, He, and Zhang 2020; Lockard et al. 2020; Lin et al. 2020)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 34
                            }
                        ],
                        "text": "Graph-based IE Liu et al. (2019); Qian et al. (2019); Wei et al. (2020); Yu et al. (2020) utilize a graph convolution network to contextualize the tokens in a document and a bidirectional LSTM with CRF to predict the IOB tags."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 53109320,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1da8e1ad1814d81f69433ac877ef70caa950e4e6",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Most modern Information Extraction (IE) systems are implemented as sequential taggers and only model local dependencies. Non-local and non-sequential context is, however, a valuable source of information to improve predictions. In this paper, we introduce GraphIE, a framework that operates over a graph representing a broad set of dependencies between textual units (i.e. words or sentences). The algorithm propagates information between connected nodes through graph convolutions, generating a richer representation that can be exploited to improve word-level predictions. Evaluation on three different tasks \u2014 namely textual, social media and visual information extraction \u2014 shows that GraphIE consistently outperforms the state-of-the-art sequence tagging model by a significant margin."
            },
            "slug": "GraphIE:-A-Graph-Based-Framework-for-Information-Qian-Santus",
            "title": {
                "fragments": [],
                "text": "GraphIE: A Graph-Based Framework for Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Evaluation on three different tasks shows that GraphIE consistently outperforms the state-of-the-art sequence tagging model by a significant margin, and generates a richer representation that can be exploited to improve word-level predictions."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39172707"
                        ],
                        "name": "Jacob Devlin",
                        "slug": "Jacob-Devlin",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Devlin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jacob Devlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744179"
                        ],
                        "name": "Ming-Wei Chang",
                        "slug": "Ming-Wei-Chang",
                        "structuredName": {
                            "firstName": "Ming-Wei",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Wei Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2544107"
                        ],
                        "name": "Kenton Lee",
                        "slug": "Kenton-Lee",
                        "structuredName": {
                            "firstName": "Kenton",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenton Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3259253"
                        ],
                        "name": "Kristina Toutanova",
                        "slug": "Kristina-Toutanova",
                        "structuredName": {
                            "firstName": "Kristina",
                            "lastName": "Toutanova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kristina Toutanova"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 165
                            }
                        ],
                        "text": "(2019) and Denk and Reisswig (2019) combine a manually engineered text serializer that turn the OCR text boxes into a sequence and a Transformer-based encoder, BERT (Devlin et al., 2018), that performs IOB tagging on the sequence or semantic segmentation from images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 179
                            }
                        ],
                        "text": "Hwang et al. (2019) and Denk and Reisswig (2019) combine a manually engineered text serializer that turn the OCR text boxes into a sequence and a Transformer-based encoder, BERT (Devlin et al., 2018), that performs IOB tagging on the sequence or semantic segmentation from images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 55
                            }
                        ],
                        "text": "The parameters are initialized from bert-multilingual (Devlin et al., 2018) 4."
                    },
                    "intents": []
                }
            ],
            "corpusId": 52967399,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "isKey": false,
            "numCitedBy": 33744,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
            },
            "slug": "BERT:-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang",
            "title": {
                "fragments": [],
                "text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A new language representation model, BERT, designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers, which can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719068"
                        ],
                        "name": "Anoop R. Katti",
                        "slug": "Anoop-R.-Katti",
                        "structuredName": {
                            "firstName": "Anoop",
                            "lastName": "Katti",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anoop R. Katti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9992847"
                        ],
                        "name": "C. Reisswig",
                        "slug": "C.-Reisswig",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Reisswig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Reisswig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39387393"
                        ],
                        "name": "Cordula Guder",
                        "slug": "Cordula-Guder",
                        "structuredName": {
                            "firstName": "Cordula",
                            "lastName": "Guder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cordula Guder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14334250"
                        ],
                        "name": "Sebastian Brarda",
                        "slug": "Sebastian-Brarda",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Brarda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Brarda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2704747"
                        ],
                        "name": "S. Bickel",
                        "slug": "S.-Bickel",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Bickel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bickel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2216963"
                        ],
                        "name": "J. H\u00f6hne",
                        "slug": "J.-H\u00f6hne",
                        "structuredName": {
                            "firstName": "Johannes",
                            "lastName": "H\u00f6hne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H\u00f6hne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2803968"
                        ],
                        "name": "J. Faddoul",
                        "slug": "J.-Faddoul",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Faddoul",
                            "middleNames": [
                                "Baptiste"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Faddoul"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 10
                            }
                        ],
                        "text": "Chargrid (Katti et al., 2018) performs semantic segmentation on invoice images to extract target key-value pairs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 141
                            }
                        ],
                        "text": "\u2026from semi-structued documents are well reflected in their increased number of publication record from both research community and industry (Katti et al., 2018; Qian et al., 2019; Liu et al., 2019; Zhao et al., 2019; Denk and Reisswig, 2019; Hwang et al., 2019; Park et al., 2019;\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52815006,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15aae08159856cdbf0ce539357d473a04dcbb7f3",
            "isKey": false,
            "numCitedBy": 100,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a novel type of text representation that preserves the 2D layout of a document. This is achieved by encoding each document page as a two-dimensional grid of characters. Based on this representation, we present a generic document understanding pipeline for structured documents. This pipeline makes use of a fully convolutional encoder-decoder network that predicts a segmentation mask and bounding boxes. We demonstrate its capabilities on an information extraction task from invoices and show that it significantly outperforms approaches based on sequential text or document images."
            },
            "slug": "Chargrid:-Towards-Understanding-2D-Documents-Katti-Reisswig",
            "title": {
                "fragments": [],
                "text": "Chargrid: Towards Understanding 2D Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A novel type of text representation is introduced that preserves the 2D layout of a document by encoding each document page as a two-dimensional grid of characters and it is shown that it significantly outperforms approaches based on sequential text or document images."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2277385"
                        ],
                        "name": "Timothy Dozat",
                        "slug": "Timothy-Dozat",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Dozat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timothy Dozat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 177
                            }
                        ],
                        "text": "Dependency parsing Dependency parsing is the task of obtaining the syntactic or semantic structure of a sentence by defining the relationships between the words in the sentence (Zettlemoyer and Collins, 2012; Peng et al., 2017; Dozat and Manning, 2018)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 49578052,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c267b4a64066b56c8eef053de391c3cbe58c9eb3",
            "isKey": false,
            "numCitedBy": 114,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "While syntactic dependency annotations concentrate on the surface or functional structure of a sentence, semantic dependency annotations aim to capture between-word relationships that are more closely related to the meaning of a sentence, using graph-structured representations. We extend the LSTM-based syntactic parser of Dozat and Manning (2017) to train on and generate these graph structures. The resulting system on its own achieves state-of-the-art performance, beating the previous, substantially more complex state-of-the-art system by 0.6% labeled F1. Adding linguistically richer input representations pushes the margin even higher, allowing us to beat it by 1.9% labeled F1."
            },
            "slug": "Simpler-but-More-Accurate-Semantic-Dependency-Dozat-Manning",
            "title": {
                "fragments": [],
                "text": "Simpler but More Accurate Semantic Dependency Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "The LSTM-based syntactic parser of Dozat and Manning (2017) is extended to train on and generate graph structures that aim to capture between-word relationships that are more closely related to the meaning of a sentence, using graph-structured representations."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067170682"
                        ],
                        "name": "Rasmus Berg Palm",
                        "slug": "Rasmus-Berg-Palm",
                        "structuredName": {
                            "firstName": "Rasmus",
                            "lastName": "Palm",
                            "middleNames": [
                                "Berg"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rasmus Berg Palm"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724252"
                        ],
                        "name": "O. Winther",
                        "slug": "O.-Winther",
                        "structuredName": {
                            "firstName": "Ole",
                            "lastName": "Winther",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Winther"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805808"
                        ],
                        "name": "Florian Laws",
                        "slug": "Florian-Laws",
                        "structuredName": {
                            "firstName": "Florian",
                            "lastName": "Laws",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Florian Laws"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 312,
                                "start": 267
                            }
                        ],
                        "text": "In this setup, the tokens in the document (either obtained through an OCR engine or trivially parsed from a web page or pdf) are first serialized, and then an independent tagging model classifies each of the flattened lists into one of the pre-defined IOB categories (Ramshaw and Marcus, 1995; Palm et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 280
                            }
                        ],
                        "text": "\u2026the tokens in the document (either obtained through an OCR engine or trivially parsed from a web page or pdf) are first serialized, and then an independent tagging model classifies each of the flattened lists into one of the pre-defined IOB categories (Ramshaw and Marcus, 1995; Palm et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 30210556,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01a50b9662a59070c4ff53873453d8854c15ade1",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We present CloudScan; an invoice analysis system that requires zero configuration or upfront annotation. In contrast to previous work, CloudScan does not rely on templates of invoice layout, instead it learns a single global model of invoices that naturally generalizes to unseen invoice layouts. The model is trained using data automatically extracted from end-user provided feedback. This automatic training data extraction removes the requirement for users to annotate the data precisely. We describe a recurrent neural network model that can capture long range context and compare it to a baseline logistic regression model corresponding to the current CloudScan production system. We train and evaluate the system on 8 important fields using a dataset of 326,471 invoices. The recurrent neural network and baseline model achieve 0.891 and 0.887 average F1 scores respectively on seen invoice layouts. For the harder task of unseen invoice layouts, the recurrent neural network model outperforms the baseline with 0.840 average F1 compared to 0.788."
            },
            "slug": "CloudScan-A-Configuration-Free-Invoice-Analysis-Palm-Winther",
            "title": {
                "fragments": [],
                "text": "CloudScan - A Configuration-Free Invoice Analysis System Using Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A recurrent neural network model that can capture long range context and compare it to a baseline logistic regression model corresponding to the current CloudScan production system are described."
            },
            "venue": {
                "fragments": [],
                "text": "2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3157053"
                        ],
                        "name": "Nanyun Peng",
                        "slug": "Nanyun-Peng",
                        "structuredName": {
                            "firstName": "Nanyun",
                            "lastName": "Peng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nanyun Peng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759772"
                        ],
                        "name": "Hoifung Poon",
                        "slug": "Hoifung-Poon",
                        "structuredName": {
                            "firstName": "Hoifung",
                            "lastName": "Poon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hoifung Poon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2596310"
                        ],
                        "name": "Chris Quirk",
                        "slug": "Chris-Quirk",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Quirk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Quirk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3259253"
                        ],
                        "name": "Kristina Toutanova",
                        "slug": "Kristina-Toutanova",
                        "structuredName": {
                            "firstName": "Kristina",
                            "lastName": "Toutanova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kristina Toutanova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144105277"
                        ],
                        "name": "Wen-tau Yih",
                        "slug": "Wen-tau-Yih",
                        "structuredName": {
                            "firstName": "Wen-tau",
                            "lastName": "Yih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wen-tau Yih"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 209
                            }
                        ],
                        "text": "Dependency parsing Dependency parsing is the task of obtaining the syntactic or semantic structure of a sentence by defining the relationships between the words in the sentence (Zettlemoyer and Collins, 2012; Peng et al., 2017; Dozat and Manning, 2018)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2797612,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54b8aadb7c2576665ce26caf59464b6449ac9ccf",
            "isKey": false,
            "numCitedBy": 346,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Past work in relation extraction has focused on binary relations in single sentences. Recent NLP inroads in high-value domains have sparked interest in the more general setting of extracting n-ary relations that span multiple sentences. In this paper, we explore a general relation extraction framework based on graph long short-term memory networks (graph LSTMs) that can be easily extended to cross-sentence n-ary relation extraction. The graph formulation provides a unified way of exploring different LSTM approaches and incorporating various intra-sentential and inter-sentential dependencies, such as sequential, syntactic, and discourse relations. A robust contextual representation is learned for the entities, which serves as input to the relation classifier. This simplifies handling of relations with arbitrary arity, and enables multi-task learning with related relations. We evaluate this framework in two important precision medicine settings, demonstrating its effectiveness with both conventional supervised learning and distant supervision. Cross-sentence extraction produced larger knowledge bases. and multi-task learning significantly improved extraction accuracy. A thorough analysis of various LSTM approaches yielded useful insight the impact of linguistic analysis on extraction accuracy."
            },
            "slug": "Cross-Sentence-N-ary-Relation-Extraction-with-Graph-Peng-Poon",
            "title": {
                "fragments": [],
                "text": "Cross-Sentence N-ary Relation Extraction with Graph LSTMs"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A general relation extraction framework based on graph long short-term memory networks (graph LSTMs) that can be easily extended to cross-sentence n-ary relation extraction is explored, demonstrating its effectiveness with both conventional supervised learning and distant supervision."
            },
            "venue": {
                "fragments": [],
                "text": "TACL"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2378954"
                        ],
                        "name": "Xuezhe Ma",
                        "slug": "Xuezhe-Ma",
                        "structuredName": {
                            "firstName": "Xuezhe",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuezhe Ma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144547315"
                        ],
                        "name": "E. Hovy",
                        "slug": "E.-Hovy",
                        "structuredName": {
                            "firstName": "Eduard",
                            "lastName": "Hovy",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Hovy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Previous formulation: Sequence tagging As mentioned, IOB sequence tagging is appropriate for document IE when the layout and the information structure are simple (Ramshaw and Marcus 1995; Lample et al. 2016; Ma and Hovy 2020; Chiu and Nichols 2016)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10489017,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4",
            "isKey": false,
            "numCitedBy": 1994,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data pre-processing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks --- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both the two data --- 97.55\\% accuracy for POS tagging and 91.21\\% F1 for NER."
            },
            "slug": "End-to-end-Sequence-Labeling-via-Bi-directional-Ma-Hovy",
            "title": {
                "fragments": [],
                "text": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel neutral network architecture is introduced that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF, thus making it applicable to a wide range of sequence labeling tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830914"
                        ],
                        "name": "Guillaume Lample",
                        "slug": "Guillaume-Lample",
                        "structuredName": {
                            "firstName": "Guillaume",
                            "lastName": "Lample",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guillaume Lample"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143668305"
                        ],
                        "name": "Miguel Ballesteros",
                        "slug": "Miguel-Ballesteros",
                        "structuredName": {
                            "firstName": "Miguel",
                            "lastName": "Ballesteros",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Miguel Ballesteros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50324141"
                        ],
                        "name": "Sandeep Subramanian",
                        "slug": "Sandeep-Subramanian",
                        "structuredName": {
                            "firstName": "Sandeep",
                            "lastName": "Subramanian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sandeep Subramanian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2189948"
                        ],
                        "name": "Kazuya Kawakami",
                        "slug": "Kazuya-Kawakami",
                        "structuredName": {
                            "firstName": "Kazuya",
                            "lastName": "Kawakami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kazuya Kawakami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745899"
                        ],
                        "name": "Chris Dyer",
                        "slug": "Chris-Dyer",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Dyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Dyer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 150
                            }
                        ],
                        "text": "As mentioned, IOB sequence tagging is appropriate for document IE when the layout and the information structure are simple (Ramshaw and Marcus, 1995; Lample et al., 2016; Chiu and Nichols, 2016; Ma and Hovy, 2020)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6042994,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "24158c9fc293c8a998ac552b1188404a877da292",
            "isKey": false,
            "numCitedBy": 2898,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "Comunicacio presentada a la 2016 Conference of the North American Chapter of the Association for Computational Linguistics, celebrada a San Diego (CA, EUA) els dies 12 a 17 de juny 2016."
            },
            "slug": "Neural-Architectures-for-Named-Entity-Recognition-Lample-Ballesteros",
            "title": {
                "fragments": [],
                "text": "Neural Architectures for Named Entity Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "Comunicacio presentada a la 2016 Conference of the North American Chapter of the Association for Computational Linguistics, celebrada a San Diego (CA, EUA) els dies 12 a 17 of juny 2016."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052473561"
                        ],
                        "name": "Jason P. C. Chiu",
                        "slug": "Jason-P.-C.-Chiu",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Chiu",
                            "middleNames": [
                                "P.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason P. C. Chiu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143662759"
                        ],
                        "name": "Eric Nichols",
                        "slug": "Eric-Nichols",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Nichols",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Nichols"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 123
                            }
                        ],
                        "text": "As mentioned, IOB sequence tagging is appropriate for document IE when the layout and the information structure are simple (Ramshaw and Marcus, 1995; Lample et al., 2016; Chiu and Nichols, 2016; Ma and Hovy, 2020)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6300165,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10a4db59e81d26b2e0e896d3186ef81b4458b93f",
            "isKey": false,
            "numCitedBy": 1335,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. In this paper, we present a novel neural network architecture that automatically detects word- and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering. We also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches. Extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the CoNLL-2003 dataset and surpasses the previously reported state of the art performance on the OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed from publicly-available sources, we establish new state of the art performance with an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information."
            },
            "slug": "Named-Entity-Recognition-with-Bidirectional-Chiu-Nichols",
            "title": {
                "fragments": [],
                "text": "Named Entity Recognition with Bidirectional LSTM-CNNs"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel neural network architecture is presented that automatically detects word- and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering."
            },
            "venue": {
                "fragments": [],
                "text": "TACL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726807"
                        ],
                        "name": "Diederik P. Kingma",
                        "slug": "Diederik-P.-Kingma",
                        "structuredName": {
                            "firstName": "Diederik",
                            "lastName": "Kingma",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diederik P. Kingma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2503659"
                        ],
                        "name": "Jimmy Ba",
                        "slug": "Jimmy-Ba",
                        "structuredName": {
                            "firstName": "Jimmy",
                            "lastName": "Ba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jimmy Ba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 15
                            }
                        ],
                        "text": "ADAM optimizer (Kingma and Ba, 2015) is used with the following learning rates: 1e-5 for the encoder, 1e-4 for the graph generator, and 2e-5 for"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6628106,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "isKey": false,
            "numCitedBy": 90052,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm."
            },
            "slug": "Adam:-A-Method-for-Stochastic-Optimization-Kingma-Ba",
            "title": {
                "fragments": [],
                "text": "Adam: A Method for Stochastic Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This work introduces Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments, and provides a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1982950"
                        ],
                        "name": "Luke Zettlemoyer",
                        "slug": "Luke-Zettlemoyer",
                        "structuredName": {
                            "firstName": "Luke",
                            "lastName": "Zettlemoyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luke Zettlemoyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 178
                            }
                        ],
                        "text": "Dependency parsing Dependency parsing is the task of obtaining the syntactic or semantic structure of a sentence by defining the relationships between the words in the sentence (Zettlemoyer and Collins, 2012; Peng et al., 2017; Dozat and Manning, 2018)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 449252,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "74fe7ec751cd50295b15cfd46389a8fefb37c414",
            "isKey": false,
            "numCitedBy": 874,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of mapping natural language sentences to lambda\u2013calculus encodings of their meaning. We describe a learning algorithm that takes as input a training set of sentences labeled with expressions in the lambda calculus. The algorithm induces a grammar for the problem, along with a log-linear model that represents a distribution over syntactic and semantic analyses conditioned on the input sentence. We apply the method to the task of learning natural language interfaces to databases and show that the learned parsers outperform previous methods in two benchmark database domains."
            },
            "slug": "Learning-to-Map-Sentences-to-Logical-Form:-with-Zettlemoyer-Collins",
            "title": {
                "fragments": [],
                "text": "Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "A learning algorithm is described that takes as input a training set of sentences labeled with expressions in the lambda calculus and induces a grammar for the problem, along with a log-linear model that represents a distribution over syntactic and semantic analyses conditioned on the input sentence."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744313"
                        ],
                        "name": "L. Ramshaw",
                        "slug": "L.-Ramshaw",
                        "structuredName": {
                            "firstName": "Lance",
                            "lastName": "Ramshaw",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ramshaw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 312,
                                "start": 267
                            }
                        ],
                        "text": "In this setup, the tokens in the document (either obtained through an OCR engine or trivially parsed from a web page or pdf) are first serialized, and then an independent tagging model classifies each of the flattened lists into one of the pre-defined IOB categories (Ramshaw and Marcus, 1995; Palm et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 123
                            }
                        ],
                        "text": "As mentioned, IOB sequence tagging is appropriate for document IE when the layout and the information structure are simple (Ramshaw and Marcus, 1995; Lample et al., 2016; Chiu and Nichols, 2016; Ma and Hovy, 2020)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 725590,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d9c71db75046473f0e3d3229950d7c84c09afd5e",
            "isKey": false,
            "numCitedBy": 1530,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Transformation-based learning, a technique introduced by Eric Brill (1993b), has been shown to do part-of-speech tagging with fairly high accuracy. This same method can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive \u201cbaseNP\u201d chunks. For this purpose, it is convenient to view chunking as a tagging problem by encoding the chunk structure in new tags attached to each word. In automatic tests using Treebank-derived data, this technique achieved recall and precision rates of roughly 93% for baseNP chunks (trained on 950K words) and 88% for somewhat more complex chunks that partition the sentence (trained on 200K words). Working in this new application and with larger template and training sets has also required some interesting adaptations to the transformation-based learning approach."
            },
            "slug": "Text-Chunking-using-Transformation-Based-Learning-Ramshaw-Marcus",
            "title": {
                "fragments": [],
                "text": "Text Chunking using Transformation-Based Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work has shown that the transformation-based learning approach can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive \u201cbaseNP\u201d chunks."
            },
            "venue": {
                "fragments": [],
                "text": "VLC@ACL"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "bust Layout - aware IE for Visually Rich Documents with Pre - trained Language Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 0
                            }
                        ],
                        "text": "Xu et al. (2019) propose LayoutLM that jointly embeds the\nimage segments, text tokens, and positions of the tokens in an image to make a pretrained model for document understanding."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "bust Layout - aware IE for Visually Rich Documents with Pre - trained Language Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2019
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 14,
            "methodology": 17,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 39,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Spatial-Dependency-Parsing-for-Semi-Structured-Hwang-Yim/20d0564fd3fdbc24f266ca2076826a2271c3ea08"
}