{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144461837"
                        ],
                        "name": "C. Shannon",
                        "slug": "C.-Shannon",
                        "structuredName": {
                            "firstName": "Claude",
                            "lastName": "Shannon",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Shannon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9101213,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c1e3f2d537e50e0d5263e4731ab6c7983acd6687",
            "isKey": false,
            "numCitedBy": 2530,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method of estimating the entropy and redundancy of a language is described. This method exploits the knowledge of the language statistics possessed by those who speak the language, and depends on experimental results in prediction of the next letter when the preceding text is known. Results of experiments in prediction are given, and some properties of an ideal predictor are developed."
            },
            "slug": "Prediction-and-entropy-of-printed-English-Shannon",
            "title": {
                "fragments": [],
                "text": "Prediction and entropy of printed English"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "A new method of estimating the entropy and redundancy of a language is described, which exploits the knowledge of the language statistics possessed by those who speak the language, and depends on experimental results in prediction of the next letter when the preceding text is known."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1951
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2696176"
                        ],
                        "name": "L. Bahl",
                        "slug": "L.-Bahl",
                        "structuredName": {
                            "firstName": "Lalit",
                            "lastName": "Bahl",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14789841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58",
            "isKey": false,
            "numCitedBy": 1403,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Speech recognition is formulated as a problem of maximum likelihood decoding. This formulation requires statistical models of the speech production process. In this paper, we describe a number of statistical models for use in speech recognition. We give special attention to determining the parameters for such models from sparse data. We also describe two decoding methods, one appropriate for constrained artificial languages and one appropriate for more realistic decoding tasks. To illustrate the usefulness of the methods described, we review a number of decoding results that have been obtained with them."
            },
            "slug": "A-Maximum-Likelihood-Approach-to-Continuous-Speech-Bahl-Jelinek",
            "title": {
                "fragments": [],
                "text": "A Maximum Likelihood Approach to Continuous Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper describes a number of statistical models for use in speech recognition, with special attention to determining the parameters for such models from sparse data, and describes two decoding methods appropriate for constrained artificial languages and one appropriate for more realistic decoding tasks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150023694"
                        ],
                        "name": "A. N\u00e1das",
                        "slug": "A.-N\u00e1das",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "N\u00e1das",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. N\u00e1das"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Some related statistical topics are discussed in Niidas [ 7 ] and in Niidas [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 119559365,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5b163877b1c9e19b9d1bfa11ad667564a2e9678f",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "The objects listed in the title have proven to be useful and practical modeling tools in continuous speech recognition work and elsewhere. Nevertheless, there are natural and simple situations in which the forward-backward algorithm will be inadequate for its intended purpose of finding useful maximum likelihood estimates of the parameters of the distribution of a probabilistic function of a Markov chain (a \"hidden Markov model\" or \"Markov source model\"). We observe some difficulties that arise in the case of common (e.g., Gaussian) families of conditional distributions for the observables. These difficulties are due not to the algorithm itself, but to modeling assumptions which introduce singularities into the likelihood function. We also comment on the fact that the parameters of a hidden Markov model cannot, in general, be determined, even if the distribution of the observables is completely known. We close with remarks about some effects of these modeling and estimating difficulties on practical speech recognition, and about the role of initial statistics."
            },
            "slug": "Hidden-Markov-chains,-the-forward-backward-and-N\u00e1das",
            "title": {
                "fragments": [],
                "text": "Hidden Markov chains, the forward-backward algorithm, and initial statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150023694"
                        ],
                        "name": "A. N\u00e1das",
                        "slug": "A.-N\u00e1das",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "N\u00e1das",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. N\u00e1das"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120638127,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "79eb272eaf061cf4e65b8e61c9f02c027b3b6933",
            "isKey": false,
            "numCitedBy": 140,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "The choice of method for training a speech recognizer is posed as an optimization problem. The currently used method of maximum likelihood, while heuristic, is shown to be superior under certain assumptions to another heuristic: the method of conditional maximum likelihood."
            },
            "slug": "A-decision-theorectic-formulation-of-a-training-in-N\u00e1das",
            "title": {
                "fragments": [],
                "text": "A decision theorectic formulation of a training problem in speech recognition and a comparison of training by unconditional versus conditional maximum likelihood"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The currently used method of maximum likelihood, while heuristic, is shown to be superior under certain assumptions to another heuristic: the method of conditional maximum likelihood."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145648751"
                        ],
                        "name": "H. Robbins",
                        "slug": "H.-Robbins",
                        "structuredName": {
                            "firstName": "Herbert",
                            "lastName": "Robbins",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Robbins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 122432455,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c9876704eeaf676f2825e63af570545aa6861361",
            "isKey": false,
            "numCitedBy": 606,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The empirical Bayes approach to statistical decision problems is applicable when the same decision problem presents itself repeatedly and independently with a fixed but unknown a priori distribution of the parameter. Not all decision problems in practice come to us imbedded in such a sequence, but when they do the empirical Bayes approach offers certain advantages over any approach which ignores the fact that the parameter is itself a random variable, as well as over any approach which assumes a personal or a conventional distribution of the parameter not subject to change with experience. My own interest in the empirical Bayes approach was renewed by recent work of E. Samuel [10], [11] and J. Neyman [6], to both of whom I am very much indebted. In keeping with the purpose of the Rietz Lecture I shall not confine myself to presenting new results and shall try to make the argument explicit at the risk of being tedious. In the current controversy between the Bayesian school and their opponents it is obvious that any theory of statistical inference will find itself in and out of fashion as the winds of doctrine blow. Here, then, are some remarks and references for further reading which I hope will interest my audience in thinking the matter through for themselves. Considerations of space have confined mention of the non-parametric case, and of the closely related \u201ccompound\u201d approach in which no a priori distribution of the parameter is assumed, to the references at the end of the article."
            },
            "slug": "The-Empirical-Bayes-Approach-to-Statistical-Robbins",
            "title": {
                "fragments": [],
                "text": "The Empirical Bayes Approach to Statistical Decision Problems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102463755"
                        ],
                        "name": "B. Griffin",
                        "slug": "B.-Griffin",
                        "structuredName": {
                            "firstName": "Barry",
                            "lastName": "Griffin",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Griffin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47494853"
                        ],
                        "name": "R. G. Krutchkoff",
                        "slug": "R.-G.-Krutchkoff",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Krutchkoff",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. G. Krutchkoff"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120172819,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "81fabcaa91ff3eb0bf8203039b0547d5fb1e8177",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY An empirical Bayes estimator is one which estimates the posterior mean by making use of past data. For certain conditional distributions though, no empirical Bayes estimator can be found which converges to the posterior mean as past data are accumulated. However, an optimal linear estimator for a parameter, say 0, can often be found. This optimal linear estimator depends upon the first two prior moments, both of which can often be estimated. The resulting estimator has been simulated under the assumption that the conditional distribution is binomial and these simulations have shown its risk substantially smaller than the risk of the maximum likelihood estimator."
            },
            "slug": "Optimal-linear-estimators:-an-empirical-Bayes-with-Griffin-Krutchkoff",
            "title": {
                "fragments": [],
                "text": "Optimal linear estimators: an empirical Bayes version with application to the binomial distribution"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145648751"
                        ],
                        "name": "H. Robbins",
                        "slug": "H.-Robbins",
                        "structuredName": {
                            "firstName": "Herbert",
                            "lastName": "Robbins",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Robbins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For a thorough discussion of conjugate distributions, see Raiffa and Schlaifer [ 9 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 26161481,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2c0cdd08a8d3cb834da33515037eafa4bdb5faff",
            "isKey": false,
            "numCitedBy": 992,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Let X be a random variable which for simplicity we shall assume to have discrete values x and which has a probability distribution depending in a known way on an unknown real parameter A, \n \n$$ p\\left( {x|\\lambda } \\right) = Pr[X = x|\\Lambda = \\lambda ], $$ \n \n(1) \n \nA itself being a random variable with a priori distribution function \n \n$$ G\\left( \\lambda \\right) = \\operatorname{P} r[\\Lambda {\\text{ }}\\underline \\leqslant {\\text{ }}\\lambda ]. $$ \n \n(2)"
            },
            "slug": "An-Empirical-Bayes-Approach-to-Statistics-Robbins",
            "title": {
                "fragments": [],
                "text": "An Empirical Bayes Approach to Statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1956
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145179124"
                        ],
                        "name": "I. Good",
                        "slug": "I.-Good",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Good",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Good"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 125385534,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "777bdf51061a66947b51d0712c58704930fb121f",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-Estimation-of-Small-Frequencies-in-Tables-Good",
            "title": {
                "fragments": [],
                "text": "On the Estimation of Small Frequencies in Contingency Tables"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1956
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145179124"
                        ],
                        "name": "I. Good",
                        "slug": "I.-Good",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Good",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Good"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "... existence of G, we cannot know what it is. The empirical Bayes estimate of 8, so named by Robbins [lo] and developed in Robbins [ll], is sometimes called nonparametric to emphasize the fact that it does not require the specification of a parametric family of probability distribution functions that must contain the distribution function G. Robbins' empirical Bayes estimator for the binomial problem strongly resembles the Turing-Good [ 2 ] ..."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The Turing-Good estimator of small probabilities was first published by Good in [ 2 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11945361,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b2986b25f50babd536dd0ecf2237d9eabf5843c2",
            "isKey": false,
            "numCitedBy": 3274,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "THE-POPULATION-FREQUENCIES-OF-SPECIES-AND-THE-OF-Good",
            "title": {
                "fragments": [],
                "text": "THE POPULATION FREQUENCIES OF SPECIES AND THE ESTIMATION OF POPULATION PARAMETERS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1953
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 2,
            "methodology": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 9,
        "totalPages": 1
    },
    "page_url": "https://www.semanticscholar.org/paper/Estimation-of-probabilities-in-the-language-model-N\u00e1das/226a6dff9ccc1c2db9f09db644b13eb9d04322e7?sort=total-citations"
}