{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895771"
                        ],
                        "name": "D. Zipser",
                        "slug": "D.-Zipser",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zipser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zipser"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14711886,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce9a21b93ba29d4145a8ef6bf401e77f261848de",
            "isKey": false,
            "numCitedBy": 3833,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks. These algorithms have (1) the advantage that they do not require a precisely defined training interval, operating while the network runs; and (2) the disadvantage that they require nonlocal communication in the network being trained and are computationally expensive. These algorithms allow networks having recurrent connections to learn complex tasks that require the retention of information over time periods having either fixed or indefinite length."
            },
            "slug": "A-Learning-Algorithm-for-Continually-Running-Fully-Williams-Zipser",
            "title": {
                "fragments": [],
                "text": "A Learning Algorithm for Continually Running Fully Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2884373"
                        ],
                        "name": "J. Elman",
                        "slug": "J.-Elman",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Elman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Elman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1281,
                                "start": 62
                            }
                        ],
                        "text": "We have shown that the network architecture first proposed by Elman (1988) is capable of mastering an infinite coIpus of strings generated from a finite-state grammar after training on a finite set of exemplars with a learning algorithm that is local in time. The network develops internal representations that correspond to the nodes of the grammar and closely approximates the corresponding minimal finite-state recognizer. We have also shown that the simple recurrent network is able to encode information about contingencies that are not local to a given letter and its immediate predecessor, such as those implied by a length constraint on the strings. Encoding of sequential structure in the patterns of activation over the hidden layers proceeds in stages. The network first develops stable hidden-layer representations for individual letters, and then for individual arcs in the grammar. Finally, the network is able to exploit slight differences in the patterns of activation which denote a specific path through the grammar. Our current work is exploring the relevance of this architecture to the processing of embedded sequences typical of natural language. The results of some preliminary experiments are available in Servan-Schreiber, Cleeremans and McClelland (1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 62
                            }
                        ],
                        "text": "We have shown that the network architecture first proposed by Elman (1988) is capable of mastering an infinite coIpus of strings generated from a finite-state grammar after training on a finite set of exemplars with a learning algorithm that is local in time."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Elman (1988) has introduced a simple recurrent network (SRN) that has the potential to master an infinite corpus of sequences with the limited means of a learning procedure that is completely local in time (see Figure I."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 48
                            }
                        ],
                        "text": "We explore a network architecture introduced by Elman (1988) for predicting successive elements of a sequence."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2763403,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "668087f0ae7ce1de6e0bd0965dbb480c08103260",
            "isKey": true,
            "numCitedBy": 9863,
            "numCiting": 111,
            "paperAbstract": {
                "fragments": [],
                "text": "Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction."
            },
            "slug": "Finding-Structure-in-Time-Elman",
            "title": {
                "fragments": [],
                "text": "Finding Structure in Time"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory and suggests a method for representing lexical categories and the type/token distinction is developed."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3140752"
                        ],
                        "name": "Charles R. Rosenberg",
                        "slug": "Charles-R.-Rosenberg",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Rosenberg",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles R. Rosenberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13921532,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "406033f22b6a671b94bcbdfaf63070b7ce6f3e48",
            "isKey": false,
            "numCitedBy": 761,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Unrestricted English text can be converted to speech by applying phonological rules and handling exceptions with a look-up table. However, this approach is highly labor intensive since each entry and rule must be hand-crafted. NETtalk is an alternative approach that is based on an automated learning procedure for a parallel network of deterministic processing units. ~ f t e r ' training on a corpus of informal continuous speech, it achieves good performance and generalizes to novel words. The distributed internal representations of the phonological regularities discovered by the network are damage resistant."
            },
            "slug": "NETtalk:-a-parallel-network-that-learns-to-read-Sejnowski-Rosenberg",
            "title": {
                "fragments": [],
                "text": "NETtalk: a parallel network that learns to read aloud"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "NETtalk is an alternative approach that is based on an automated learning procedure for a parallel network of deterministic processing units that achieves good performance and generalizes to novel words."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35414283"
                        ],
                        "name": "A. Reber",
                        "slug": "A.-Reber",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Reber",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Reber"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 31
                            }
                        ],
                        "text": "The small fmite-state grammar (Reber."
                    },
                    "intents": []
                }
            ],
            "corpusId": 145282756,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "43485c50f9f51d13e9759a831c835861e0b29600",
            "isKey": false,
            "numCitedBy": 1720,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Implicit-learning-of-artificial-grammars-Reber",
            "title": {
                "fragments": [],
                "text": "Implicit learning of artificial grammars"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "118969901"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46831169"
                        ],
                        "name": "G. Hinton",
                        "slug": "G.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145993114"
                        ],
                        "name": "R. William",
                        "slug": "R.-William",
                        "structuredName": {
                            "firstName": "R",
                            "lastName": "William",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. William"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 151380665,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "749ce8ccd9453d1b34901143cddf5f9bee2977cf",
            "isKey": false,
            "numCitedBy": 1334,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-representations-by-back-propagation-nature-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning representations by back-propagation errors, nature"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "118969901"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59859558,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d453386011ef21285fa81fb4f87fdf811c6ad7a",
            "isKey": false,
            "numCitedBy": 524,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-internal-representations-by-errors-Rumelhart",
            "title": {
                "fragments": [],
                "text": "Learning internal representations by back-propagating errors"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 62
                            }
                        ],
                        "text": "We have shown that the network architecture first proposed by Elman (1988) is capable of mastering an infinite coIpus of strings generated from a finite-state grammar after training on a finite set of exemplars with a learning algorithm that is local in time."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Finding structure in time. CRL Technical report 9901"
            },
            "venue": {
                "fragments": [],
                "text": "Finding structure in time. CRL Technical report 9901"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Encoding sequential structure in simple recurrent networks"
            },
            "venue": {
                "fragments": [],
                "text": "Encoding sequential structure in simple recurrent networks"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Finding structure in time . CRL Technical report 9901 . Center for Research in Language"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 2,
            "methodology": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 9,
        "totalPages": 1
    },
    "page_url": "https://www.semanticscholar.org/paper/Learning-Subsequential-Structure-in-Simple-Networks-Servan-Schreiber-Cleeremans/d0d7f8b5d54e3d68fd45a70d4a0d13f42e8d71ff?sort=total-citations"
}