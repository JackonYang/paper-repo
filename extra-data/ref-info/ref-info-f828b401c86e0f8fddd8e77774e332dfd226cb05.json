{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2088368"
                        ],
                        "name": "F. Gers",
                        "slug": "F.-Gers",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Gers",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Gers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780235"
                        ],
                        "name": "Fred Cummins",
                        "slug": "Fred-Cummins",
                        "structuredName": {
                            "firstName": "Fred",
                            "lastName": "Cummins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fred Cummins"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Appendix A lists pseudocode for the entire algorithm; see [ 5 ] for a full derivation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Forget gates were shown to be essential for problems involving continual or very long input strings [ 5 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "RNN algorithms on numerous tasks involving real-valued or discrete inputs and targets [ 5 ], [7], including tasks that require to learn the rules of regular languages (RLs) describable by deterministic finite-state automata (DFA) [1], [2], [8], [21], [31]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We also tried different bias configurations; the results were qualitatively the same, which supports our claim that precise initialization is not critical\u2014see also [ 5 ] and [7] for additional evidence in this vein."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Essentially, LSTMs backward pass is an efficient fusion of slightly modified, truncated BPTT, and a customized version of RTRL (for details see [ 5 ] and [7])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Although CFLs like those studied in this paper may also be learnable by certain discrete SGLAs [9], [12], [24], the latter exhibit much more task-specific bias, and are not designed to solve numerous other sequence processing tasks involving noise, realvalued inputs/internal states, and continuous output trajectories, which LSTM solves easily [ 5 ], [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11598600,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "11540131eae85b2e11d53df7f1360eeb6476e7f4",
            "isKey": false,
            "numCitedBy": 3221,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Long short-term memory (LSTM; Hochreiter & Schmidhuber, 1997) can solve numerous tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset. Without resets, the state may grow indefinitely and eventually cause the network to break down. Our remedy is a novel, adaptive forget gate that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review illustrative benchmark problems on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve continual versions of these problems. LSTM with forget gates, however, easily solves them, and in an elegant way."
            },
            "slug": "Learning-to-Forget:-Continual-Prediction-with-LSTM-Gers-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Learning to Forget: Continual Prediction with LSTM"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This work identifies a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset, and proposes a novel, adaptive forget gate that enables an LSTm cell to learn to reset itself at appropriate times, thus releasing internal resources."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688235"
                        ],
                        "name": "P. Frasconi",
                        "slug": "P.-Frasconi",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Frasconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frasconi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Until recently, however, standard RNNs [13] have been plagued by a major practical problem: the gradient of the total output error with respect to previous inputs quickly vanishes as the time lags between relevant inputs and errors increase [6], [7], [ 32 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206457500,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0be39ee052d246ae99c082a565aba25b811be2d",
            "isKey": false,
            "numCitedBy": 6144,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered."
            },
            "slug": "Learning-long-term-dependencies-with-gradient-is-Bengio-Simard",
            "title": {
                "fragments": [],
                "text": "Learning long-term dependencies with gradient descent is difficult"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work shows why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases, and exposes a trade-off between efficient learning by gradient descent and latching on information for long periods."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308557"
                        ],
                        "name": "S. Hochreiter",
                        "slug": "S.-Hochreiter",
                        "structuredName": {
                            "firstName": "Sepp",
                            "lastName": "Hochreiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hochreiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We also tried different bias configurations; the results were qualitatively the same, which supports our claim that precise initialization is not critical\u2014see also [5] and [ 7 ] for additional evidence in this vein."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "RNN algorithms on numerous tasks involving real-valued or discrete inputs and targets [5], [ 7 ], including tasks that require to learn the rules of regular languages (RLs) describable by deterministic finite-state automata (DFA) [1], [2], [8], [21], [31]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Essentially, LSTMs backward pass is an efficient fusion of slightly modified, truncated BPTT, and a customized version of RTRL (for details see [5] and [ 7 ])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Until recently, however, standard RNNs [13] have been plagued by a major practical problem: the gradient of the total output error with respect to previous inputs quickly vanishes as the time lags between relevant inputs and errors increase [6], [ 7 ], [32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Although CFLs like those studied in this paper may also be learnable by certain discrete SGLAs [9], [12], [24], the latter exhibit much more task-specific bias, and are not designed to solve numerous other sequence processing tasks involving noise, realvalued inputs/internal states, and continuous output trajectories, which LSTM solves easily [5], [ 7 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The recent long short-term memory (LSTM) method [ 7 ], however, is not affected by this problem."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "to bridge minimal time lags in excess of 1000 discrete-time steps [ 7 ] by enforcing constant error flow through constant error carousels (CECs) within special units, without loss of short-time lag capabilities."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1915014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44d2abe2175df8153f465f6c39b68b76a0d40ab9",
            "isKey": false,
            "numCitedBy": 51711,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms."
            },
            "slug": "Long-Short-Term-Memory-Hochreiter-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Long Short-Term Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A novel, efficient, gradient based method called long short-term memory (LSTM) is introduced, which can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145155628"
                        ],
                        "name": "P. Rodr\u00edguez",
                        "slug": "P.-Rodr\u00edguez",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Rodr\u00edguez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Rodr\u00edguez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716264"
                        ],
                        "name": "Janet Wiles",
                        "slug": "Janet-Wiles",
                        "structuredName": {
                            "firstName": "Janet",
                            "lastName": "Wiles",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Janet Wiles"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Unlike the previous approach of [ 16 ], LSTM easily learns the complete training set and reliably finds solutions that generalize well."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Sun et al. [24] used a highly specialized architecture, the \u201cneural pushdown automaton,\u201d which also did not generalize well [3], [24]. CFL . Rodriguez and Wiles [ 16 ] used"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "CFL [ 16 ]. The second half of a string from this palindrome or mirror language is completely predictable from the first half."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Until now, however, it has remained unclear whether LSTMs superiority carries over to tasks involving context-free languages (CFLs), such as those discussed in the RNN literature [ 16 ], [17], [23]\u2010[25], [28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "CFL . We use two training sets: 1) The same set as used by [ 16 ]: , with (sequences of length 24)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "2 Applying brute force search to the weights of the best network of [ 16 ] further improves performance to acceptance up to . GERS AND SCHMIDHUBER: LSTM RECURRENT NETWORKS 1337"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9031409,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a34d3ccf53d20851f8154376e8b2faebcf99832f",
            "isKey": true,
            "numCitedBy": 30,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently researchers have derived formal complexity analysis of analog computation in the setting of discrete-time dynamical systems. As an empirical constrast, training recurrent neural networks (RNNs) produces self-organized systems that are realizations of analog mechanisms. Previous work showed that a RNN can learn to process a simple context-free language (CFL) by counting. Herein, we extend that work to show that a RNN can learn a harder CFL, a simple palindrome, by organizing its resources into a symbol-sensitive counting solution, and we provide a dynamical systems analysis which demonstrates how the network: can not only count, but also copy and store counting information."
            },
            "slug": "Recurrent-Neural-Networks-Can-Learn-to-Implement-Rodr\u00edguez-Wiles",
            "title": {
                "fragments": [],
                "text": "Recurrent Neural Networks Can Learn to Implement Symbol-Sensitive Counting"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work shows that a RNN can learn a harder CFL, a simple palindrome, by organizing its resources into a symbol-sensitive counting solution, and provides a dynamical systems analysis which demonstrates how the network can not only count, but also copy and store counting information."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2088368"
                        ],
                        "name": "F. Gers",
                        "slug": "F.-Gers",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Gers",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Gers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This task is equivalent to the \u201ccontinual spike timing\u201d task [ 4 ] learned by LSTM for with . A hand-made, hardwired solution (no learning) of a second-order RNN worked for values of until 120 [23]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We are using LSTM with forget gates and the recently introduced peephole connections [ 4 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This proved to be essential for numerous symbolic and real-valued counting tasks [ 4 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Here we will focus on our most recent LSTM variant [ 4 ], which provides the gates with direct access to the CEC states, can learn to selectively reset its own memory contents, and can produce stable results in presence of never-ending input streams."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 36867983,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "545a4e23bf00ddbc1d3325324b4c61f57cf45081",
            "isKey": false,
            "numCitedBy": 505,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The size of the time intervals between events conveys information essential for numerous sequential tasks such as motor control and rhythm detection. While hidden Markov models tend to ignore this information, recurrent neural networks (RNN) can in principle learn to make use of it. We focus on long short-term memory (LSTM) because it usually outperforms other RNN. Surprisingly, LSTM augmented by \"peephole connections\" from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes separated by either 50 or 49 discrete time steps, without the help of any short training exemplars. Without external resets or teacher forcing or loss of performance on tasks reported earlier, our LSTM variant also learns to generate very stable sequences of highly nonlinear, precisely timed spikes. This makes LSTM a promising approach for real-world tasks that require to time and count."
            },
            "slug": "Recurrent-nets-that-time-and-count-Gers-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Recurrent nets that time and count"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Surprisingly, LSTM augmented by \"peephole connections\" from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes separated by either 50 or 49 discrete time steps, without the help of any short training exemplars."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145155628"
                        ],
                        "name": "P. Rodr\u00edguez",
                        "slug": "P.-Rodr\u00edguez",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Rodr\u00edguez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Rodr\u00edguez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Further improvements: Even better results can be obtained through increased training time and stepwise reduction of the learning rate, as done in [ 17 ]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Until now, however, it has remained unclear whether LSTMs superiority carries over to tasks involving context-free languages (CFLs), such as those discussed in the RNN literature [16], [ 17 ], [23]\u2010[25], [28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "CFL . Published results on the language are summarized in Table I. 1 2 RNNs trained with plain BPTT tend to learn to just reproduce the input [ 17 ], [25], [28])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7080922,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "57dbba9281cbd1b4dd5d1932f0dd605b8f498322",
            "isKey": true,
            "numCitedBy": 226,
            "numCiting": 69,
            "paperAbstract": {
                "fragments": [],
                "text": "Parallel distributed processing (PDP) architectures demonstrate a potentially radical alternative to the traditional theories of language processing that are based on serial computational models. However, learning complex structural relationships in temporal data presents a serious challenge to PDP systems. For example, automata theory dictates that processing strings from a context-free language (CFL) requires a stack or counter memory device. While some PDP models have been hand-crafted to emulate such a device, it is not clear how a neural network might develop such a device when learning a CFL. This research employs standard backpropagation training techniques for a recurrent neural network (RNN) in the task of learning to predict the next character in a simple deterministic CFL (DCFL). We show that an RNN can learn to recognize the structure of a simple DCFL. We use dynamical systems theory to identify how network states reflect that structure by building counters in phase space. The work is an empirical investigation which is complementary to theoretical analyses of network capabilities, yet original in its specific configuration of dynamics involved. The application of dynamical systems theory helps us relate the simulation results to theoretical results, and the learning task enables us to highlight some issues for understanding dynamical systems that process language with counters."
            },
            "slug": "A-Recurrent-Neural-Network-that-Learns-to-Count-Rodr\u00edguez",
            "title": {
                "fragments": [],
                "text": "A Recurrent Neural Network that Learns to Count"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This research employs standard backpropagation training techniques for a recurrent neural network in the task of learning to predict the next character in a simple deterministic CFL (DCFL), and shows that an RNN can learn to recognize the structure of a simple DCFL."
            },
            "venue": {
                "fragments": [],
                "text": "Connect. Sci."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075413715"
                        ],
                        "name": "Zheng Zeng",
                        "slug": "Zheng-Zeng",
                        "structuredName": {
                            "firstName": "Zheng",
                            "lastName": "Zeng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zheng Zeng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145135018"
                        ],
                        "name": "R. Goodman",
                        "slug": "R.-Goodman",
                        "structuredName": {
                            "firstName": "Rodney",
                            "lastName": "Goodman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Goodman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50860274"
                        ],
                        "name": "Padhraic Smyth",
                        "slug": "Padhraic-Smyth",
                        "structuredName": {
                            "firstName": "Padhraic",
                            "lastName": "Smyth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Padhraic Smyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "RNN algorithms on numerous tasks involving real-valued or discrete inputs and targets [5], [7], including tasks that require to learn the rules of regular languages (RLs) describable by deterministic finite-state automata (DFA) [1], [2], [8], [21], [ 31 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 518523,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bff3ea999978e8c9503b62510bba11c2a5f24e51",
            "isKey": false,
            "numCitedBy": 90,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Describes a novel neural architecture for learning deterministic context-free grammars, or equivalently, deterministic pushdown automata. The unique feature of the proposed network is that it forms stable state representations during learning-previous work has shown that conventional analog recurrent networks can be inherently unstable in that they cannot retain their state memory for long input strings. The authors have previously introduced the discrete recurrent network architecture for learning finite-state automata. Here they extend this model to include a discrete external stack with discrete symbols. A composite error function is described to handle the different situations encountered in learning. The pseudo-gradient learning method (introduced in previous work) is in turn extended for the minimization of these error functions. Empirical trials validating the effectiveness of the pseudo-gradient learning method are presented, for networks both with and without an external stack. Experimental results show that the new networks are successful in learning some simple pushdown automata, though overfitting and non-convergent learning can also occur. Once learned, the internal representation of the network is provably stable; i.e., it classifies unseen strings of arbitrary length with 100% accuracy."
            },
            "slug": "Discrete-recurrent-neural-networks-for-grammatical-Zeng-Goodman",
            "title": {
                "fragments": [],
                "text": "Discrete recurrent neural networks for grammatical inference"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A novel neural architecture for learning deterministic context-free grammars, or equivalently, deterministic pushdown automata is described, and a composite error function is described to handle the different situations encountered in learning."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18271205,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "50c770b425a5bb25c77387f687a9910a9d130722",
            "isKey": false,
            "numCitedBy": 428,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous neural network learning algorithms for sequence processing are computationally expensive and perform poorly when it comes to long time lags. This paper first introduces a simple principle for reducing the descriptions of event sequences without loss of information. A consequence of this principle is that only unexpected inputs can be relevant. This insight leads to the construction of neural architectures that learn to divide and conquer by recursively decomposing sequences. I describe two architectures. The first functions as a self-organizing multilevel hierarchy of recurrent networks. The second, involving only two recurrent networks, tries to collapse a multilevel predictor hierarchy into a single recurrent net. Experiments show that the system can require less computation per time step and many fewer training sequences than conventional training algorithms for recurrent nets."
            },
            "slug": "Learning-Complex,-Extended-Sequences-Using-the-of-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Learning Complex, Extended Sequences Using the Principle of History Compression"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A simple principle for reducing the descriptions of event sequences without loss of information is introduced and this insight leads to the construction of neural architectures that learn to divide and conquer by recursively decomposing sequences."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061481139"
                        ],
                        "name": "Michael Casey",
                        "slug": "Michael-Casey",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Casey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Casey"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Small training sets were already sufficient for perfect generalization up to the tested maximum: . Note that long sequences of this kind require very stable, finely tuned control of the network\u2019s internal counters [ 2 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "RNN algorithms on numerous tasks involving real-valued or discrete inputs and targets [5], [7], including tasks that require to learn the rules of regular languages (RLs) describable by deterministic finite-state automata (DFA) [1], [ 2 ], [8], [21], [31]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1601719,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "225cfcc2bdb0fd6ab1b831257009032232688c2f",
            "isKey": false,
            "numCitedBy": 209,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent neural networks (RNNs) can learn to perform finite state computations. It is shown that an RNN performing a finite state computation must organize its state space to mimic the states in the minimal deterministic finite state machine that can perform that computation, and a precise description of the attractor structure of such systems is given. This knowledge effectively predicts activation space dynamics, which allows one to understand RNN computation dynamics in spite of complexity in activation dynamics. This theory provides a theoretical framework for understanding finite state machine (FSM) extraction techniques and can be used to improve training methods for RNNs performing FSM computations. This provides an example of a successful approach to understanding a general class of complex systems that has not been explicitly designed, e.g., systems that have evolved or learned their internal structure."
            },
            "slug": "The-Dynamics-of-Discrete-Time-Computation,-with-to-Casey",
            "title": {
                "fragments": [],
                "text": "The Dynamics of Discrete-Time Computation, with Application to Recurrent Neural Networks and Finite State Machine Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "It is shown that an RNN performing a finite state computation must organize its state space to mimic the states in the minimal deterministic finite state machine that can perform that computation, and a precise description of the attractor structure of such systems is given."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1806864"
                        ],
                        "name": "A. Blair",
                        "slug": "A.-Blair",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Blair",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blair"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145539951"
                        ],
                        "name": "J. Pollack",
                        "slug": "J.-Pollack",
                        "structuredName": {
                            "firstName": "Jordan",
                            "lastName": "Pollack",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pollack"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For the gates, , is a logistic sigmoid with range [0,  1 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "RNN algorithms on numerous tasks involving real-valued or discrete inputs and targets [5], [7], including tasks that require to learn the rules of regular languages (RLs) describable by deterministic finite-state automata (DFA) [ 1 ], [2], [8], [21], [31]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 30218561,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f1d563993f8b61a6e592bf24fa38be9576fb2892",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Pollack (1991) demonstrated that second-order recurrent neural networks can act as dynamical recognizers for formal languages when trained on positive and negative examples, and observed both phase transitions in learning and interacted function system-like fractal state sets. Follow on work focused mainly on the extraction and minimization of a finite state automaton (FSA) from the trained network. However, such networks are capable of inducing languages that are not regular and therefore not equivalent to any FSA. Indeed, it may be simpler for a small network to fit its training data by inducing such a nonregular language. But when is the network's language not regular? In this article, using a low-dimensional network capable of learning all the Tomita data sets, we present an empirical method for testing whether the language induced by the network is regular. We also provide a detailed \"-machine analysis of trained networks for both regular and nonregular languages."
            },
            "slug": "Analysis-of-Dynamical-Recognizers-Blair-Pollack",
            "title": {
                "fragments": [],
                "text": "Analysis of Dynamical Recognizers"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This article presents an empirical method for testing whether the language induced by the network is regular, and provides a detailed \"-machine analysis of trained networks for both regular and nonregular languages\"."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145247053"
                        ],
                        "name": "W. Maass",
                        "slug": "W.-Maass",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Maass",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Maass"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790264"
                        ],
                        "name": "Eduardo Sontag",
                        "slug": "Eduardo-Sontag",
                        "structuredName": {
                            "firstName": "Eduardo",
                            "lastName": "Sontag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eduardo Sontag"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "RNNs, however, perform gradient descent in a very general, continuous space of potentially noise-resistant algorithms, using distributed internal memories for mapping real-valued input sequences to real-valued output sequences (noise tends to make learning harder though\u2014compare, e.g., [10] and [ 11 ])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6540959,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8c56c5ff6e213dc7dbfed5c0c86a0106d0d28b1f",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider recurrent analog neural nets where the output of each gate is subject to gaussian noise or any other common noise distribution that is nonzero on a sufficiently large part of the state-space. We show that many regular languages cannot be recognized by networks of this type, and we give a precise characterization of languages that can be recognized. This result implies severe constraints on possibilities for constructing recurrent analog neural nets that are robust against realistic types of analog noise. On the other hand, we present a method for constructing feedfor-ward analog neural nets that are robust with regard to analog noise of this type."
            },
            "slug": "Analog-Neural-Nets-with-Gaussian-or-Other-Common-Maass-Sontag",
            "title": {
                "fragments": [],
                "text": "Analog Neural Nets with Gaussian or Other Common Noise Distributions Cannot Recognize Arbitrary Regular Languages"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is shown that many regular languages cannot be recognized by networks of this type, and a precise characterization of languages that can be recognized is given."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2447220"
                        ],
                        "name": "Y. Kalinke",
                        "slug": "Y.-Kalinke",
                        "structuredName": {
                            "firstName": "Yvonne",
                            "lastName": "Kalinke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Kalinke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066089797"
                        ],
                        "name": "Helko Lehmann",
                        "slug": "Helko-Lehmann",
                        "structuredName": {
                            "firstName": "Helko",
                            "lastName": "Lehmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Helko Lehmann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "RNN algorithms on numerous tasks involving real-valued or discrete inputs and targets [5], [7], including tasks that require to learn the rules of regular languages (RLs) describable by deterministic finite-state automata (DFA) [1], [2], [ 8 ], [21], [31]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6994754,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "daea8726709ac048093f42ba32801bc22b237bf9",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "In the paper we address the problem of computation in recurrent neural networks (RNN). In the first part we provide a formal analysis of the dynamical behavior of a RNN with a single self-recurrent unit in the hidden layer, show how such a RNN may be designed to perform an (unrestricted) counting task and describe a generalization of the counter network that performs binary stack operations."
            },
            "slug": "Computation-in-Recurrent-Neural-Networks:-From-to-Kalinke-Lehmann",
            "title": {
                "fragments": [],
                "text": "Computation in Recurrent Neural Networks: From Counters to Iterated Function Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "This paper provides a formal analysis of the dynamical behavior of a RNN with a single self-recurrent unit in the hidden layer, shows how such an RNN may be designed to perform an (unrestricted) counting task and describes a generalization of the counter network that performs binary stack operations."
            },
            "venue": {
                "fragments": [],
                "text": "Australian Joint Conference on Artificial Intelligence"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18721007,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d72a0e83e772468c6084ae7c79e43a4f5989feb",
            "isKey": false,
            "numCitedBy": 99,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Most known learning algorithms for dynamic neural networks in non-stationary environments need global computations to perform credit assignment. These algorithms either are not local in time or not local in space. Those algorithms which are local in both time and space usually cannot deal sensibly with \u2018hidden units\u2019. In contrast, as far as we can judge, learning rules in biological systems with many \u2018hidden units\u2019 are local in both space and time. In this paper we propose a parallel on-line learning algorithms which performs local computations only, yet still is designed to deal with hidden units and with units whose past activations are \u2018hidden in time\u2019. The approach is inspired by Holland's idea of the bucket brigade for classifier systems, which is transformed to run on a neural network with fixed topology. The result is a feedforward or recurrent \u2018neural\u2019 dissipative system which is consuming \u2018weight-substance\u2019 and permanently trying to distribute this substance onto its connections in an ap..."
            },
            "slug": "A-Local-Learning-Algorithm-for-Dynamic-Feedforward-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "A Local Learning Algorithm for Dynamic Feedforward and Recurrent Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper proposes a parallel on-line learning algorithms which performs local computations only, yet still is designed to deal with hidden units and with units whose past activations are \u2018hidden in time\u2019."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47918185"
                        ],
                        "name": "Jing Peng",
                        "slug": "Jing-Peng",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Peng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Peng"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "is typically a small constant\u2014in this paper it is in fact always equal to one, so that the computational complexity is . BPTT [27], [ 29 ] has the same computational complexity (whereas RTRL is much worse) but is not local in time."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Moreover, LSTMs learning algorithm is more efficient than previous RNN algorithms such as real-time recurrent learning (RTRL) [15], [30] and backpropagation through time (BPTT) [27], [ 29 ]: it is local in space and time, with computational complexity per time step and weight."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12979634,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "26bc0449360d7016f684eafae5b5d2feded32041",
            "isKey": false,
            "numCitedBy": 634,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel variant of the familiar backpropagation-through-time approach to training recurrent networks is described. This algorithm is intended to be used on arbitrary recurrent networks that run continually without ever being reset to an initial state, and it is specifically designed for computationally efficient computer implementation. This algorithm can be viewed as a cross between epochwise backpropagation through time, which is not appropriate for continually running networks, and the widely used on-line gradient approximation technique of truncated backpropagation through time."
            },
            "slug": "An-Efficient-Gradient-Based-Algorithm-for-On-Line-Williams-Peng",
            "title": {
                "fragments": [],
                "text": "An Efficient Gradient-Based Algorithm for On-Line Training of Recurrent Network Trajectories"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A novel variant of the familiar backpropagation-through-time approach to training recurrent networks is described, intended to be used on arbitrary recurrent networks that run continually without ever being reset to an initial state."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700974"
                        ],
                        "name": "Barak A. Pearlmutter",
                        "slug": "Barak-A.-Pearlmutter",
                        "structuredName": {
                            "firstName": "Barak",
                            "lastName": "Pearlmutter",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barak A. Pearlmutter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Until recently, however, standard RNNs [ 13 ] have been plagued by a major practical problem: the gradient of the total output error with respect to previous inputs quickly vanishes as the time lags between relevant inputs and errors increase [6], [7], [32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1400872,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "32e97eef94beacace020e79322cef0e1e5a76ee0",
            "isKey": false,
            "numCitedBy": 608,
            "numCiting": 302,
            "paperAbstract": {
                "fragments": [],
                "text": "Surveys learning algorithms for recurrent neural networks with hidden units and puts the various techniques into a common framework. The authors discuss fixed point learning algorithms, namely recurrent backpropagation and deterministic Boltzmann machines, and nonfixed point algorithms, namely backpropagation through time, Elman's history cutoff, and Jordan's output feedback architecture. Forward propagation, an on-line technique that uses adjoint equations, and variations thereof, are also discussed. In many cases, the unified presentation leads to generalizations of various sorts. The author discusses advantages and disadvantages of temporally continuous neural networks in contrast to clocked ones continues with some \"tricks of the trade\" for training, using, and simulating continuous time and recurrent neural networks. The author presents some simulations, and at the end, addresses issues of computational complexity and learning speed."
            },
            "slug": "Gradient-calculations-for-dynamic-recurrent-neural-Pearlmutter",
            "title": {
                "fragments": [],
                "text": "Gradient calculations for dynamic recurrent neural networks: a survey"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The author discusses advantages and disadvantages of temporally continuous neural networks in contrast to clocked ones and presents some \"tricks of the trade\" for training, using, and simulating continuous time and recurrent neural networks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47055692"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "is typically a small constant\u2014in this paper it is in fact always equal to one, so that the computational complexity is . BPTT [ 27 ], [29] has the same computational complexity (whereas RTRL is much worse) but is not local in time."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Moreover, LSTMs learning algorithm is more efficient than previous RNN algorithms such as real-time recurrent learning (RTRL) [15], [30] and backpropagation through time (BPTT) [ 27 ], [29]: it is local in space and time, with computational complexity per time step and weight."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 205118721,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "266e07d0dd9a75b61e3632e9469993dbaf063f1c",
            "isKey": false,
            "numCitedBy": 882,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Generalization-of-backpropagation-with-application-Werbos",
            "title": {
                "fragments": [],
                "text": "Generalization of backpropagation with application to a recurrent gas market model"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057788"
                        ],
                        "name": "M. Osborne",
                        "slug": "M.-Osborne",
                        "structuredName": {
                            "firstName": "Miles",
                            "lastName": "Osborne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Osborne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145693410"
                        ],
                        "name": "Ted Briscoe",
                        "slug": "Ted-Briscoe",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Briscoe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ted Briscoe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Although CFLs like those studied in this paper may also be learnable by certain discrete SGLAs [9], [ 12 ], [24], the latter exhibit much more task-specific bias, and are not designed to solve numerous other sequence processing tasks involving noise, realvalued inputs/internal states, and continuous output trajectories, which LSTM solves easily [5], [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Unlike RNNs, SGLAs cannot deal well with noisy input sequences[ 12 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 747480,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6744bfa03ae3195d3db6c5558221aaaa0761ed4b",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Stochastic categorial grammars (SCGs) are introduced as a more appropriate formalism for statistical language learners to est imate than stochastic context free grammars. As a vehicle for demonstrating SCG estimation, we show, in terms of crossing rates and in coverage, that when training material is limited, SCG estimation using the Minimum Description Length Principle is preferable to SCG estimation using an indifferent prior."
            },
            "slug": "Learning-Stochastic-Categorial-Grammars-Osborne-Briscoe",
            "title": {
                "fragments": [],
                "text": "Learning Stochastic Categorial Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown, in terms of crossing rates and in coverage, that when training material is limited, SCG estimation using the Minimum Description Length Principle is preferable to SCGs estimation using an indifferent prior."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705051"
                        ],
                        "name": "Y. Sakakibara",
                        "slug": "Y.-Sakakibara",
                        "structuredName": {
                            "firstName": "Yasubumi",
                            "lastName": "Sakakibara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Sakakibara"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In principle they are applicable to tasks beyond the reach of hidden Markov models (HMMs) or discrete symbolic grammar learning algorithms (SGLAs) [9], [ 18 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15154520,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46135fdeed34366802a2999e6477095d593326ed",
            "isKey": false,
            "numCitedBy": 198,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Recent-Advances-of-Grammatical-Inference-Sakakibara",
            "title": {
                "fragments": [],
                "text": "Recent Advances of Grammatical Inference"
            },
            "venue": {
                "fragments": [],
                "text": "Theor. Comput. Sci."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400820110"
                        ],
                        "name": "K. Vijay-Shanker",
                        "slug": "K.-Vijay-Shanker",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Vijay-Shanker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Vijay-Shanker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The language is one of the simplest CSLs; it can be generated by a tree-adjoined grammar and recognized using a so-called embedded push-down automaton [ 26 ] or a finite-state automaton with access to two counters that can be incremented or decremented."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2375919,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10c3d4008665e82e15b63fb95401c1c0e3877518",
            "isKey": false,
            "numCitedBy": 164,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a new interpretation of Tree Adjoining Grammars (TAG) that allows the embedding of TAG in the unification framework in a manner consistent with the declarative approach taken in this framework. In the new interpretation we present in this paper, the objects manipulated by a TAG are considered to be descriptions of trees. This is in contrast to the traditional view that in a TAG the composition operations of adjoining and substitution combine trees. Borrowing ideas from Description Theory, we propose quasi-trees as a means to represent partial descriptions of trees. Using quasi-trees, we are able to justify the definition of feature structure-based Tree Adjoining Grammars (FTAG) that was first given in Vijay-Shanker (1987) and Vijay-Shanker and Joshi (1988). In the definition of the FTAG formalism given here, we argue that a grammar manipulates descriptions of trees (i.e., quasi-trees); whereas the structures derived by a grammar are trees that are obtained by taking the minimal readings of such descriptions. We then build on and refine the earlier version of FTAG, give examples that illustrate the usefulness of embedding TAG in the unification framework, and present a logical formulation (and its associated semantics) of FTAG that shows the separation between descriptions of well-formed structures and the actual structures that are derived, a theme that is central to this work. Finally, we discuss some questions that are raised by our new interpretation of the TAG formalism: questions dealing with the nature and definition of the adjoining operation (in contrast to substitution), its relation to multi-component adjoining, and the distinctions between auxiliary and initial structures."
            },
            "slug": "Using-Descriptions-of-Trees-in-a-Tree-Adjoining-Vijay-Shanker",
            "title": {
                "fragments": [],
                "text": "Using Descriptions of Trees in a Tree Adjoining Grammar"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A new interpretation of Tree Adjoining Grammars (TAG) is described that allows the embedding of TAG in the unification framework in a manner consistent with the declarative approach taken in this framework and proposes quasi-trees as a means to represent partial descriptions of trees."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145247053"
                        ],
                        "name": "W. Maass",
                        "slug": "W.-Maass",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Maass",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Maass"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2557668"
                        ],
                        "name": "P. Orponen",
                        "slug": "P.-Orponen",
                        "structuredName": {
                            "firstName": "Pekka",
                            "lastName": "Orponen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Orponen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "RNNs, however, perform gradient descent in a very general, continuous space of potentially noise-resistant algorithms, using distributed internal memories for mapping real-valued input sequences to real-valued output sequences (noise tends to make learning harder though\u2014compare, e.g., [ 10 ] and [11])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11597139,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cad840434232731c3e87df0e463e09c0ab0bf757",
            "isKey": false,
            "numCitedBy": 104,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a model for analog computation with discrete time in the presence of analog noise that is flexible enough to cover the most important concrete cases, such as noisy analog neural nets and networks of spiking neurons. This model subsumes the classical model for digital computation in the presence of noise. We show that the presence of arbitrarily small amounts of analog noise reduces the power of analog computational models to that of finite automata, and we also prove a new type of upper bound for the VC-dimension of computational models with analog noise."
            },
            "slug": "On-the-Effect-of-Analog-Noise-in-Discrete-Time-Maass-Orponen",
            "title": {
                "fragments": [],
                "text": "On the Effect of Analog Noise in Discrete-Time Analog Computations"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that the presence of arbitrarily small amounts of analog noise reduces the power of analog computational models to that of finite automata, and a new type of upper bound for the VC-dimension of computational models with analog noise is proved."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2797623"
                        ],
                        "name": "H. Siegelmann",
                        "slug": "H.-Siegelmann",
                        "structuredName": {
                            "firstName": "Hava",
                            "lastName": "Siegelmann",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Siegelmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790264"
                        ],
                        "name": "Eduardo Sontag",
                        "slug": "Eduardo-Sontag",
                        "structuredName": {
                            "firstName": "Eduardo",
                            "lastName": "Sontag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eduardo Sontag"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ECURRENT neural networks (RNNs) are remarkably general sequence processing devices [ 22 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5909565,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7509b472cbe7b1fe71a8fccf60f34cc873d1ab63",
            "isKey": false,
            "numCitedBy": 365,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Turing-computability-with-neural-nets-Siegelmann-Sontag",
            "title": {
                "fragments": [],
                "text": "Turing computability with neural nets"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895771"
                        ],
                        "name": "D. Zipser",
                        "slug": "D.-Zipser",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zipser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zipser"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Moreover, LSTM is local in space and time [19], and it is more efficient than other RNN algorithms such as RTRL [15], [ 30 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Essentially, LSTM has the complexity of BPTT [ 30 ], which truncates errors after steps, and which works as well as BPTT, according to our experience\u2014if we did not have LSTM we would in fact use BPTT ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Moreover, LSTMs learning algorithm is more efficient than previous RNN algorithms such as real-time recurrent learning (RTRL) [15], [ 30 ] and backpropagation through time (BPTT) [27], [29]: it is local in space and time, with computational complexity per time step and weight."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14792754,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10dae7fca6b65b61d155a622f0c6ca2bc3922251",
            "isKey": true,
            "numCitedBy": 567,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Gradient-based-learning-algorithms-for-recurrent-Williams-Zipser",
            "title": {
                "fragments": [],
                "text": "Gradient-based learning algorithms for recurrent networks and their computational complexity"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066667188"
                        ],
                        "name": "Sepp Hochreiter",
                        "slug": "Sepp-Hochreiter",
                        "structuredName": {
                            "firstName": "Sepp",
                            "lastName": "Hochreiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sepp Hochreiter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Hence standard RNNs fail to learn in the presence of time lags exceeding as few as five to ten discrete-time steps between relevant input events and target signals [ 6 ], [20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Until recently, however, standard RNNs [13] have been plagued by a major practical problem: the gradient of the total output error with respect to previous inputs quickly vanishes as the time lags between relevant inputs and errors increase [ 6 ], [7], [32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60091947,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3f3d13e95c25a8f6a753e38dfce88885097cbd43",
            "isKey": false,
            "numCitedBy": 603,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Untersuchungen-zu-dynamischen-neuronalen-Netzen-Hochreiter",
            "title": {
                "fragments": [],
                "text": "Untersuchungen zu dynamischen neuronalen Netzen"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 19,
            "methodology": 9,
            "result": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 23,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/LSTM-recurrent-networks-learn-simple-context-free-Gers-Schmidhuber/f828b401c86e0f8fddd8e77774e332dfd226cb05?sort=total-citations"
}