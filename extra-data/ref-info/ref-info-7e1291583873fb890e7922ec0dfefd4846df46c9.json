{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696678"
                        ],
                        "name": "D. Wolpert",
                        "slug": "D.-Wolpert",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Wolpert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Wolpert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 5
                            }
                        ],
                        "text": "(In (Wolpert 1990d), the output of this feedback net of generalizers is fed through yet another generalizer to get the final guess."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 61
                            }
                        ],
                        "text": "For example, in the use of metric-based HERBIEs reported in (Wolpert 1990b), the input space was 7 dimensional, and each of the 7 coordinates of any input value were scaled by a distinct weighting factor, \u03c1 i , 1 \u2264 i \u2264 7, before the conventional metric was applied."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 17
                            }
                        ],
                        "text": "This metricbased HERBIE is one of the simplest generalizers there are."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 110
                            }
                        ],
                        "text": "The data set used for the experiment reported here was standard Carterette and Jones (1974), modified (as in (Wolpert 1990b)) to force consistency amongst the several speakers record- ed."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 292,
                                "start": 279
                            }
                        ],
                        "text": "\u2026a countably infinite set of functions {g i }, 1 \u2264 i < \u221e, one function for each possible value of m. g 1 takes three arguments (the learning set input x 1 , the learning set output y 1 , and the question q); g 2 takes five arguments (x 1 , y 1 , x 2 , y 2 , and q); and so on (see (Wolpert 1990c))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 38
                            }
                        ],
                        "text": "(As usual, there were in fact 21 such HERBIEs, making a 21-dimensional guess which in turn specified the phoneme guessed by the entire stacked generalizer.)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 89
                            }
                        ],
                        "text": "Unless otherwise specified, in the rest of this section, whenever the term \"metric-based HERBIE\" is used, what is really meant is a set of 21 such HER- BIEs combining in the manner discussed here to guess a legal phoneme."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 201
                            }
                        ],
                        "text": "The second is using it to improve upon the individual performance of several generalizers for a modified version of the text-to-phoneme data set that went into making NETtalk (Stanfill and Waltz 1986, Wolpert 1990b, Carterette and Jones 1974, Sejnowski and Rosenberg 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 96
                            }
                        ],
                        "text": "8 Other examples of this kind of implementation of stacked generalization are fan generalizers (Wolpert 1990e), the extension of non-linear time-series analysis to multiple input dimensions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 53
                            }
                        ],
                        "text": "To use metric-based HERBIEs for this problem (as in (Wolpert 1990b)), 21 such HERBIEs have to be used, one for each component of the phoneme vector space."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 145
                            }
                        ],
                        "text": "\u2026with Alan Lapedes and Rob Farber suggests that using stacked generalization to combine ID3, perceptrons , and the author's metric-based HERBIE (Wolpert 1990b) for the problem of predicting splice junctions in DNA sequences gives accuracy better than any of the techniques by itself (i.e.,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 134
                            }
                        ],
                        "text": "For example, one might require that the generalizer be invariant under Euclidean symmetry operations in the level 0 space R n+1 (see (Wolpert 1990c))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 94
                            }
                        ],
                        "text": "All these level 0 generalizers could then use a conventional generalizer (e.g. a metric-based HERBIE) along with the entire ("
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 76
                            }
                        ],
                        "text": "As with the output neurons of NETtalk, the guesses of these 21 metric-based HERBIEs are then passed through a post-processor which combines them to form a 21-dimensional guess, which in turn specifies a phoneme guess."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 66
                            }
                        ],
                        "text": "The level 1 generalizer was the metric-based HERBIE described in (Wolpert 1990b, Wolpert 1990c)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 9
                            }
                        ],
                        "text": "In both (Wolpert 1990b) and (Sejnowski and Rosenberg 1988) generalizers never guess directly from 7-letter fields to phonemes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 92
                            }
                        ],
                        "text": "The purpose of this text-to-phoneme experiment wasn't to beat the performance (reported in (Wolpert 1990b)) of a metric-based HERBIE having access to all 7 input letters, nor even to beat the performance of back-propagation (i.e., NETtalk) on this data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 164
                            }
                        ],
                        "text": "5 Finally, work in progress with Alan Lapedes and Rob Farber suggests that using stacked generalization to combine ID3, perceptrons , and the author's metric-based HERBIE (Wolpert 1990b) for the problem of predicting splice junctions in DNA sequences gives accuracy better than any of the techniques by itself (i.e., preliminary evidence indicates that this implementation of stacked generalization is the best known generalization method for this problem)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 41
                            }
                        ],
                        "text": "[7] This is essentially what is done in (Wolpert 1990d), where a genetic evolution process is used to create a feedback net of generalizers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 284
                            }
                        ],
                        "text": "\u2026generalizers are back-propagated neural nets (Rumelhart and McClelland 1986), Holland's classifier system (Holland 1975), and Rissanen's minimum description length principle (Rissanen 1986) (which, along with all other schemes which attempt to exploit Occam's razor, is analyzed in (Wolpert 1990a))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 49
                            }
                        ],
                        "text": "Each such level 0 generalizer was a metric-based HERBIE, where 4 nearest neighbors were used."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 109
                            }
                        ],
                        "text": "(Since the space of possible matrices is so large however, rather than the trial and error approach used in (Wolpert 1990b) one would probably want to employ something like gradient descent in the space of crossvalidation error to find the \"optimal\" weighting matrix.)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 156
                            }
                        ],
                        "text": "6 In addition, along with (for example) Farmer's local linear technique, metric-based HERBIEs necessarily always reproduce their learning set exactly (see (Wolpert 1990b) and (Wolpert 1990c))."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 31
                            }
                        ],
                        "text": "(See (Stanfill and Waltz 1986, Wolpert 1990b, Carterette and Jones 1974, Sejnowski and Rosenberg 1988).)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 43
                            }
                        ],
                        "text": "The level 1 generalizer was a metric-based HERBIE using a full Hamming metric over the 3-dimensional level 1 input space."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 5
                            }
                        ],
                        "text": "See (Wolpert 1990d) for details."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 82
                            }
                        ],
                        "text": "(To make the construction unique, one must impose other constraints as well -see (Wolpert 1990d).)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 109
                            }
                        ],
                        "text": "For example, back-propagation behaves somewhat locally (see (Lapedes and Farber 1988) and the discussion in (Wolpert 1990e) on using backpropagation to try to generalize the parity input-output function)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 48
                            }
                        ],
                        "text": "[6] More sophisticated versions of metric-based HERBIEs replace a pre-fixed metric with something less restrictive."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 88
                            }
                        ],
                        "text": "This jump from use of a weighting vector to use of a weighting matrix in a metric-based HERBIE is loosely equivalent to the jump from using a perceptron (with its vector of synaptic weights) to using a feedforward neural net with a single hidden layer (where one has matrices of synaptic weights)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 218
                            }
                        ],
                        "text": "\u2026examples are memory-based reasoning schemes (Stanfill and Waltz 1986), regularization theory (Poggio et al. 1988), and similar schemes for overt surface fitting of a parent function to the learning set (Wolpert 1989, Wolpert 1990a, Wolpert 1990b, Farmer and Sidorowich 1988, Omohundro 1987)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 4
                            }
                        ],
                        "text": "See(Wolpert 1990b).)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 1596189,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "412218ee996c239ff606c669d31a83f9205c56f8",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of how best to generalize from a given learning set of input-output examples is cent ral to the fields of neural nets, statistics, approximation theory, and ar tificial in telligence. T his series of papers inv estigates this pr oblem from within an abstract and modelindep endent fr am ework and then tests some of the resulting concepts in realworld sit uations. In this abs tract fram ework a gene ralizer is completely specified by a certain countably infinite set of fun ctions , so the mathematics of generali zat ion becomes an investigation into candida te set s of cri teria governing the behavior of that infinite set of fun ctions. In the first pap er of thi s series, t he foundations of this mathematics are spelled out and some relatively simple generalization criteri a are investigated. Elsewhere the real-world generalizing of sys tems construct ed with these generalization crite ria in mind h ave been favorably comp ared to neural nets for several real generali zation problems , including Sejnowski's problem of reading aloud . T his leads to the conclusion that (current) neural nets in fact cons titute a poor means of generali zing. In th e second of this pair of papers, ot her sets of crit eria, mor e sophistic at ed than those crit eri a embodied in thi s first series of paper s, are investi gat ed. Generali zers meeting these more sophisticated criteria can readil y be approximated on computers. Some of th ese approxi mations employ net work structures built via an evolutionar y process. A preliminary and favorable in vestigati on int o the generali zation behavior of these approximations fini shes the second pap er of thi s series . Outline of these papers In section 1 of this p ap er the topic of generalizat ion is dis cussed fr om a very broad perspective . It is argued that it is t he ir ability t o generalize that consti t u tes the pr imary reason for curre nt interest in neural ne ts (even thoug h suc h neural nets in fact gener a lize poorly on average, as is demonstrat ed in [13]) . This section goes on to discuss the b en efits that wo uld come fro m h aving a particularly good generaliz ing algorithm. Section 1 t hen ends wi th a @ 1990 Complex Systems Publications, Inc. 152 David H. Wolpert detailed out line of the rest of these pap ers , presented in terms of the pr eceding discussion of generalization . Here and throughout these pap ers, generalization is assumed to be t ak ing place wit hout any knowledge of what the variables involved \"really mean. \" An abs tract, model-independent formalism is the most rigorous way to deal with this kind of generalizing. Section 2 of this paper begins with a mathematically precise definition of generalizers. It t hen goes on to exp lore some of the more basic properties that can be required of generalizers and elucidates some of the more straightforward mathematical consequences of such requirements . Some of these conse quences (e.g., no linear mathematical model should be used to generalize whe n the underlying syste m being mo delled is known to be geometric in nature) are not intuitively obvious . The par adigm here and throughout these papers is to make general, broad requiremen ts for gen erali zing behavior , and then see what (if any) mathematical solut ions there are for such requirements . This contrasts to the usual (extremely ad hoc) way of dealing with generalizers, which is to take a concrete generalizer and investigate its behavior for assorted tes t problems. In these papers , the behavior defines the architecture, not the ot her way aro und. The other way of trying to build an eng ine which exhibits \"good\" gene ralization is, ultimately, depend ent to a degree on sheer luck. Sections 1 and 2 of the second paper present and explore some sets of restrictions on generalization behavior which are more sophist ica ted than those found in section 2 of the first pap er. The first of these new restrictions , exp lored at length in sect ion 1 of the second pap er , is th e restriction of \"self-guess ing\" in its various formulations. Intuitively, self-guessing refers to the requirement that if taught with a subset of the full t raining set , the generalizer should correctly guess the rest of the training set . One of the more interesti ng results concern ing self-guessing is th at it is impossible to construct generalization crit eria which, along wit h self-guessing, specify unique generalization of a learn ing set . (Any particular set of crite ria will always be either under-restrictive or over-restricti ve.) Sectio n 2 of the second pap er then discusses the restriction of information compactification , which can be viewed as a mathematically precise stat ement of Occam 's razor. Particul ar at tention is drawn to the fac t (and it s consequen ces) that at present there is no known way of making an a priori most reason abl e definition of information measure in a model-independent way. Finally, sect ion 3 of th e second paper is a partial investigation of t he real-worl d efficacy of the tools elaborated in the first paper and in the first two sections of the second paper. References [13J consist of other such investigations and show that these techniques far outperform backpropagation [4, 5J and in particular easily beat NETtaik [6J. T he tests and investigations pr esented in section 3 of this second paper are intended to be an extension and elaboration of these tests presented in [1-3]. A Mathematical Th eory of Generalization : Part I \"T his then is the measure of a man that from the parti culars he can discern the pattern in the greater whole .\" [U. Merre, from Studies on the Nature of ManJ 153"
            },
            "slug": "A-Mathematical-Theory-of-Generalization:-Part-I-Wolpert",
            "title": {
                "fragments": [],
                "text": "A Mathematical Theory of Generalization: Part I"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This series of papers inv estigates this problem of how best to generalize from a given learning set of input-output examples from within an abstract and modelindep endent and then tests some of the resulting concepts in realworld sit uations."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696678"
                        ],
                        "name": "D. Wolpert",
                        "slug": "D.-Wolpert",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Wolpert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Wolpert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 5
                            }
                        ],
                        "text": "(In (Wolpert 1990d), the output of this feedback net of generalizers is fed through yet another generalizer to get the final guess."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 61
                            }
                        ],
                        "text": "For example, in the use of metric-based HERBIEs reported in (Wolpert 1990b), the input space was 7 dimensional, and each of the 7 coordinates of any input value were scaled by a distinct weighting factor, \u03c1 i , 1 \u2264 i \u2264 7, before the conventional metric was applied."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 17
                            }
                        ],
                        "text": "This metricbased HERBIE is one of the simplest generalizers there are."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 110
                            }
                        ],
                        "text": "The data set used for the experiment reported here was standard Carterette and Jones (1974), modified (as in (Wolpert 1990b)) to force consistency amongst the several speakers record- ed."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 292,
                                "start": 279
                            }
                        ],
                        "text": "\u2026a countably infinite set of functions {g i }, 1 \u2264 i < \u221e, one function for each possible value of m. g 1 takes three arguments (the learning set input x 1 , the learning set output y 1 , and the question q); g 2 takes five arguments (x 1 , y 1 , x 2 , y 2 , and q); and so on (see (Wolpert 1990c))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 38
                            }
                        ],
                        "text": "(As usual, there were in fact 21 such HERBIEs, making a 21-dimensional guess which in turn specified the phoneme guessed by the entire stacked generalizer.)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 89
                            }
                        ],
                        "text": "Unless otherwise specified, in the rest of this section, whenever the term \"metric-based HERBIE\" is used, what is really meant is a set of 21 such HER- BIEs combining in the manner discussed here to guess a legal phoneme."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 201
                            }
                        ],
                        "text": "The second is using it to improve upon the individual performance of several generalizers for a modified version of the text-to-phoneme data set that went into making NETtalk (Stanfill and Waltz 1986, Wolpert 1990b, Carterette and Jones 1974, Sejnowski and Rosenberg 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 96
                            }
                        ],
                        "text": "8 Other examples of this kind of implementation of stacked generalization are fan generalizers (Wolpert 1990e), the extension of non-linear time-series analysis to multiple input dimensions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 53
                            }
                        ],
                        "text": "To use metric-based HERBIEs for this problem (as in (Wolpert 1990b)), 21 such HERBIEs have to be used, one for each component of the phoneme vector space."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 145
                            }
                        ],
                        "text": "\u2026with Alan Lapedes and Rob Farber suggests that using stacked generalization to combine ID3, perceptrons , and the author's metric-based HERBIE (Wolpert 1990b) for the problem of predicting splice junctions in DNA sequences gives accuracy better than any of the techniques by itself (i.e.,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 134
                            }
                        ],
                        "text": "For example, one might require that the generalizer be invariant under Euclidean symmetry operations in the level 0 space R n+1 (see (Wolpert 1990c))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 94
                            }
                        ],
                        "text": "All these level 0 generalizers could then use a conventional generalizer (e.g. a metric-based HERBIE) along with the entire ("
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 76
                            }
                        ],
                        "text": "As with the output neurons of NETtalk, the guesses of these 21 metric-based HERBIEs are then passed through a post-processor which combines them to form a 21-dimensional guess, which in turn specifies a phoneme guess."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 66
                            }
                        ],
                        "text": "The level 1 generalizer was the metric-based HERBIE described in (Wolpert 1990b, Wolpert 1990c)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 9
                            }
                        ],
                        "text": "In both (Wolpert 1990b) and (Sejnowski and Rosenberg 1988) generalizers never guess directly from 7-letter fields to phonemes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 92
                            }
                        ],
                        "text": "The purpose of this text-to-phoneme experiment wasn't to beat the performance (reported in (Wolpert 1990b)) of a metric-based HERBIE having access to all 7 input letters, nor even to beat the performance of back-propagation (i.e., NETtalk) on this data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 164
                            }
                        ],
                        "text": "5 Finally, work in progress with Alan Lapedes and Rob Farber suggests that using stacked generalization to combine ID3, perceptrons , and the author's metric-based HERBIE (Wolpert 1990b) for the problem of predicting splice junctions in DNA sequences gives accuracy better than any of the techniques by itself (i.e., preliminary evidence indicates that this implementation of stacked generalization is the best known generalization method for this problem)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 41
                            }
                        ],
                        "text": "[7] This is essentially what is done in (Wolpert 1990d), where a genetic evolution process is used to create a feedback net of generalizers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 284
                            }
                        ],
                        "text": "\u2026generalizers are back-propagated neural nets (Rumelhart and McClelland 1986), Holland's classifier system (Holland 1975), and Rissanen's minimum description length principle (Rissanen 1986) (which, along with all other schemes which attempt to exploit Occam's razor, is analyzed in (Wolpert 1990a))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 49
                            }
                        ],
                        "text": "Each such level 0 generalizer was a metric-based HERBIE, where 4 nearest neighbors were used."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 109
                            }
                        ],
                        "text": "(Since the space of possible matrices is so large however, rather than the trial and error approach used in (Wolpert 1990b) one would probably want to employ something like gradient descent in the space of crossvalidation error to find the \"optimal\" weighting matrix.)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 156
                            }
                        ],
                        "text": "6 In addition, along with (for example) Farmer's local linear technique, metric-based HERBIEs necessarily always reproduce their learning set exactly (see (Wolpert 1990b) and (Wolpert 1990c))."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 31
                            }
                        ],
                        "text": "(See (Stanfill and Waltz 1986, Wolpert 1990b, Carterette and Jones 1974, Sejnowski and Rosenberg 1988).)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 43
                            }
                        ],
                        "text": "The level 1 generalizer was a metric-based HERBIE using a full Hamming metric over the 3-dimensional level 1 input space."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 5
                            }
                        ],
                        "text": "See (Wolpert 1990d) for details."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 82
                            }
                        ],
                        "text": "(To make the construction unique, one must impose other constraints as well -see (Wolpert 1990d).)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 109
                            }
                        ],
                        "text": "For example, back-propagation behaves somewhat locally (see (Lapedes and Farber 1988) and the discussion in (Wolpert 1990e) on using backpropagation to try to generalize the parity input-output function)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 48
                            }
                        ],
                        "text": "[6] More sophisticated versions of metric-based HERBIEs replace a pre-fixed metric with something less restrictive."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 88
                            }
                        ],
                        "text": "This jump from use of a weighting vector to use of a weighting matrix in a metric-based HERBIE is loosely equivalent to the jump from using a perceptron (with its vector of synaptic weights) to using a feedforward neural net with a single hidden layer (where one has matrices of synaptic weights)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 218
                            }
                        ],
                        "text": "\u2026examples are memory-based reasoning schemes (Stanfill and Waltz 1986), regularization theory (Poggio et al. 1988), and similar schemes for overt surface fitting of a parent function to the learning set (Wolpert 1989, Wolpert 1990a, Wolpert 1990b, Farmer and Sidorowich 1988, Omohundro 1987)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 4
                            }
                        ],
                        "text": "See(Wolpert 1990b).)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 15356996,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "cbd786e4f03b7ca6d852ff7d6a7465f20ca90283",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "A b st r a ct. The problem of how best to generalize from a given learning set of input-output examples is cent ral to the fields of neur al nets, statistics, approximation theory, and artificial intelligence. This series of pap ers inve stigates this problem from within an abst ract and mod elindepend ent fr amework and then test s some of the resultant concept s in real-world situations. In thi s abstrac t framework a generalizer is comple tely specified by a certain countably infinite set of functions, so the mathemati cs of generali zati on becomes an investigat ion into can dida te sets of criteria governing the behavior of that infinite set of funct ions . In the first pap er of thi s series the found ati ons of this mathematics are spelled out and some relatively simple generalization criteria are investigated . Elsewhere the real-world generalizing of systems constructed wit h these generalization criteria in mind have been favorably compared to neural nets for several real gene ralization problems, including Sejnowski 's problem of reading aloud. Th is leads to the conclusion that (current) neural nets in fact constitute a poo r means of general izing . In t he second of this pair of papers other sets of criteria, more sophisticated than those criteria embo died in this first series of pap ers, are investigated . Generalizers meeting these more sophis tica te d criteria can readil y be app roximated on computers. Some of these approximations employ network st ructures built via an evolutionary pro cess. A pr elimin ary and favo rable investigation in to the gener alization beh avior of these approxi mations finishes the second paper of this series."
            },
            "slug": "A-Mathematical-Theory-of-Generalization:-Part-II-Wolpert",
            "title": {
                "fragments": [],
                "text": "A Mathematical Theory of Generalization: Part II"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "It is leads to the conclusion that (current) neural nets in fact constitute a poo r means of general izing, and other sets of criteria, more sophisticated than those criteria embo died in this first series of pap ers, are investigated."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696678"
                        ],
                        "name": "D. Wolpert",
                        "slug": "D.-Wolpert",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Wolpert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Wolpert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 5
                            }
                        ],
                        "text": "(In (Wolpert 1990d), the output of this feedback net of generalizers is fed through yet another generalizer to get the final guess."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 61
                            }
                        ],
                        "text": "For example, in the use of metric-based HERBIEs reported in (Wolpert 1990b), the input space was 7 dimensional, and each of the 7 coordinates of any input value were scaled by a distinct weighting factor, \u03c1 i , 1 \u2264 i \u2264 7, before the conventional metric was applied."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 17
                            }
                        ],
                        "text": "This metricbased HERBIE is one of the simplest generalizers there are."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 110
                            }
                        ],
                        "text": "The data set used for the experiment reported here was standard Carterette and Jones (1974), modified (as in (Wolpert 1990b)) to force consistency amongst the several speakers record- ed."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 292,
                                "start": 279
                            }
                        ],
                        "text": "\u2026a countably infinite set of functions {g i }, 1 \u2264 i < \u221e, one function for each possible value of m. g 1 takes three arguments (the learning set input x 1 , the learning set output y 1 , and the question q); g 2 takes five arguments (x 1 , y 1 , x 2 , y 2 , and q); and so on (see (Wolpert 1990c))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 38
                            }
                        ],
                        "text": "(As usual, there were in fact 21 such HERBIEs, making a 21-dimensional guess which in turn specified the phoneme guessed by the entire stacked generalizer.)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 89
                            }
                        ],
                        "text": "Unless otherwise specified, in the rest of this section, whenever the term \"metric-based HERBIE\" is used, what is really meant is a set of 21 such HER- BIEs combining in the manner discussed here to guess a legal phoneme."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 201
                            }
                        ],
                        "text": "The second is using it to improve upon the individual performance of several generalizers for a modified version of the text-to-phoneme data set that went into making NETtalk (Stanfill and Waltz 1986, Wolpert 1990b, Carterette and Jones 1974, Sejnowski and Rosenberg 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 96
                            }
                        ],
                        "text": "8 Other examples of this kind of implementation of stacked generalization are fan generalizers (Wolpert 1990e), the extension of non-linear time-series analysis to multiple input dimensions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 53
                            }
                        ],
                        "text": "To use metric-based HERBIEs for this problem (as in (Wolpert 1990b)), 21 such HERBIEs have to be used, one for each component of the phoneme vector space."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 145
                            }
                        ],
                        "text": "\u2026with Alan Lapedes and Rob Farber suggests that using stacked generalization to combine ID3, perceptrons , and the author's metric-based HERBIE (Wolpert 1990b) for the problem of predicting splice junctions in DNA sequences gives accuracy better than any of the techniques by itself (i.e.,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 134
                            }
                        ],
                        "text": "For example, one might require that the generalizer be invariant under Euclidean symmetry operations in the level 0 space R n+1 (see (Wolpert 1990c))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 94
                            }
                        ],
                        "text": "All these level 0 generalizers could then use a conventional generalizer (e.g. a metric-based HERBIE) along with the entire ("
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 76
                            }
                        ],
                        "text": "As with the output neurons of NETtalk, the guesses of these 21 metric-based HERBIEs are then passed through a post-processor which combines them to form a 21-dimensional guess, which in turn specifies a phoneme guess."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 66
                            }
                        ],
                        "text": "The level 1 generalizer was the metric-based HERBIE described in (Wolpert 1990b, Wolpert 1990c)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 9
                            }
                        ],
                        "text": "In both (Wolpert 1990b) and (Sejnowski and Rosenberg 1988) generalizers never guess directly from 7-letter fields to phonemes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 92
                            }
                        ],
                        "text": "The purpose of this text-to-phoneme experiment wasn't to beat the performance (reported in (Wolpert 1990b)) of a metric-based HERBIE having access to all 7 input letters, nor even to beat the performance of back-propagation (i.e., NETtalk) on this data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 164
                            }
                        ],
                        "text": "5 Finally, work in progress with Alan Lapedes and Rob Farber suggests that using stacked generalization to combine ID3, perceptrons , and the author's metric-based HERBIE (Wolpert 1990b) for the problem of predicting splice junctions in DNA sequences gives accuracy better than any of the techniques by itself (i.e., preliminary evidence indicates that this implementation of stacked generalization is the best known generalization method for this problem)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 41
                            }
                        ],
                        "text": "[7] This is essentially what is done in (Wolpert 1990d), where a genetic evolution process is used to create a feedback net of generalizers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 284
                            }
                        ],
                        "text": "\u2026generalizers are back-propagated neural nets (Rumelhart and McClelland 1986), Holland's classifier system (Holland 1975), and Rissanen's minimum description length principle (Rissanen 1986) (which, along with all other schemes which attempt to exploit Occam's razor, is analyzed in (Wolpert 1990a))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 49
                            }
                        ],
                        "text": "Each such level 0 generalizer was a metric-based HERBIE, where 4 nearest neighbors were used."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 109
                            }
                        ],
                        "text": "(Since the space of possible matrices is so large however, rather than the trial and error approach used in (Wolpert 1990b) one would probably want to employ something like gradient descent in the space of crossvalidation error to find the \"optimal\" weighting matrix.)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 156
                            }
                        ],
                        "text": "6 In addition, along with (for example) Farmer's local linear technique, metric-based HERBIEs necessarily always reproduce their learning set exactly (see (Wolpert 1990b) and (Wolpert 1990c))."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 31
                            }
                        ],
                        "text": "(See (Stanfill and Waltz 1986, Wolpert 1990b, Carterette and Jones 1974, Sejnowski and Rosenberg 1988).)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 43
                            }
                        ],
                        "text": "The level 1 generalizer was a metric-based HERBIE using a full Hamming metric over the 3-dimensional level 1 input space."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 5
                            }
                        ],
                        "text": "See (Wolpert 1990d) for details."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 82
                            }
                        ],
                        "text": "(To make the construction unique, one must impose other constraints as well -see (Wolpert 1990d).)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 109
                            }
                        ],
                        "text": "For example, back-propagation behaves somewhat locally (see (Lapedes and Farber 1988) and the discussion in (Wolpert 1990e) on using backpropagation to try to generalize the parity input-output function)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 48
                            }
                        ],
                        "text": "[6] More sophisticated versions of metric-based HERBIEs replace a pre-fixed metric with something less restrictive."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 88
                            }
                        ],
                        "text": "This jump from use of a weighting vector to use of a weighting matrix in a metric-based HERBIE is loosely equivalent to the jump from using a perceptron (with its vector of synaptic weights) to using a feedforward neural net with a single hidden layer (where one has matrices of synaptic weights)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 218
                            }
                        ],
                        "text": "\u2026examples are memory-based reasoning schemes (Stanfill and Waltz 1986), regularization theory (Poggio et al. 1988), and similar schemes for overt surface fitting of a parent function to the learning set (Wolpert 1989, Wolpert 1990a, Wolpert 1990b, Farmer and Sidorowich 1988, Omohundro 1987)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 4
                            }
                        ],
                        "text": "See(Wolpert 1990b).)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 7776597,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c58f4d7fc1fb6ab0420e3c1a5623394cd43952dc",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Occam's razor, the principle of parsimony, is a tool that finds application in many areas of science. Its validity has never been proven in full generality from first principles. Convergent guessing is the property th at as more examples of an input-output mapp ing are provided , one would expect the modelling of that mapping to become mor e and more accurate. It too is widely used and has not been proven from first principles. In thi s paper it is shown that Occam 's razor and convergent guessin g are not independ ent if convergent guessing holds , so does Occam 's raz or. (The converse of thi s statement is also true , providing some extra condi tions are met.) Therefore, if you have reason to believe th at your guesses are getting mor e accurate as you are fed more data, you also have reason to believe that application of Occam's razor will likely result in better guesses. Rather than attri butes concerning how an ar chitecture works (e .g. , it s coding length , or its number of free par ameters) , thi s pap er is concern ed exclusively with how the ar chitecture g uesses (which is, aft er all , what we're really interest ed in). In this contex t Occam's razor mean s that one should guess according to the \"simplicity\" of an archite cture's guessing behavior (as opposed to according to the sim plicity of how the ar chi tecture works). This pap er deduces an optimal measure of the \"simplicity\" of an architecture's guessing beh avior. Given this op timal simplicity measure, this paper th en establishes th e aforementioned relationship between Occam's razor and converg ent guessing. T his paper goes on to elucidate the many oth er advantages, both practical an d theoreti cal, of using the optimal simplicity measure. Finally, this paper end s by exploring the ramifications of th is analysis for the question of how best to measure t he \"complexity\" of a syste m. ' Elect ronic m ail ad dress: dhw@coot.lanl.gov. \u00a9 1990 Complex Systems Pu blications, Inc ."
            },
            "slug": "The-Relationship-Between-Occam's-Razor-and-Guessing-Wolpert",
            "title": {
                "fragments": [],
                "text": "The Relationship Between Occam's Razor and Convergent Guessing"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper establishes the relationship between Occam's razor and convergent guessing, and deduces an optimal measure of the \"simplicity\" of an architecture's guessing beh avior, and goes on to elucidate the many advantages, both practical and theoreti cal, of using the optimal simplicity measure."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741124"
                        ],
                        "name": "L. Valiant",
                        "slug": "L.-Valiant",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Valiant",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Valiant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 191
                            }
                        ],
                        "text": "The central idea is that one can do better than simply list all guesses as to the parent function which are consistent with a learning set (as is done in PAC-style learning (Dietterich 1990, Valiant 1984), for example)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59712,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10ddb646feddc12337b5a755c72e153e37088c02",
            "isKey": false,
            "numCitedBy": 4189,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Humans appear to be able to learn new concepts without needing to be programmed explicitly in any conventional sense. In this paper we regard learning as the phenomenon of knowledge acquisition in the absence of explicit programming. We give a precise methodology for studying this phenomenon from a computational viewpoint. It consists of choosing an appropriate information gathering mechanism, the learning protocol, and exploring the class of concepts that can be learnt using it in a reasonable (polynomial) number of steps. We find that inherent algorithmic complexity appears to set serious limits to the range of concepts that can be so learnt. The methodology and results suggest concrete principles for designing realistic learning systems."
            },
            "slug": "A-theory-of-the-learnable-Valiant",
            "title": {
                "fragments": [],
                "text": "A theory of the learnable"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper regards learning as the phenomenon of knowledge acquisition in the absence of explicit programming, and gives a precise methodology for studying this phenomenon from a computational viewpoint."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '84"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696678"
                        ],
                        "name": "D. Wolpert",
                        "slug": "D.-Wolpert",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Wolpert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Wolpert"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 5
                            }
                        ],
                        "text": "(In (Wolpert 1990d), the output of this feedback net of generalizers is fed through yet another generalizer to get the final guess."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 61
                            }
                        ],
                        "text": "For example, in the use of metric-based HERBIEs reported in (Wolpert 1990b), the input space was 7 dimensional, and each of the 7 coordinates of any input value were scaled by a distinct weighting factor, \u03c1 i , 1 \u2264 i \u2264 7, before the conventional metric was applied."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 17
                            }
                        ],
                        "text": "This metricbased HERBIE is one of the simplest generalizers there are."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 110
                            }
                        ],
                        "text": "The data set used for the experiment reported here was standard Carterette and Jones (1974), modified (as in (Wolpert 1990b)) to force consistency amongst the several speakers record- ed."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 292,
                                "start": 279
                            }
                        ],
                        "text": "\u2026a countably infinite set of functions {g i }, 1 \u2264 i < \u221e, one function for each possible value of m. g 1 takes three arguments (the learning set input x 1 , the learning set output y 1 , and the question q); g 2 takes five arguments (x 1 , y 1 , x 2 , y 2 , and q); and so on (see (Wolpert 1990c))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 38
                            }
                        ],
                        "text": "(As usual, there were in fact 21 such HERBIEs, making a 21-dimensional guess which in turn specified the phoneme guessed by the entire stacked generalizer.)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 89
                            }
                        ],
                        "text": "Unless otherwise specified, in the rest of this section, whenever the term \"metric-based HERBIE\" is used, what is really meant is a set of 21 such HER- BIEs combining in the manner discussed here to guess a legal phoneme."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 201
                            }
                        ],
                        "text": "The second is using it to improve upon the individual performance of several generalizers for a modified version of the text-to-phoneme data set that went into making NETtalk (Stanfill and Waltz 1986, Wolpert 1990b, Carterette and Jones 1974, Sejnowski and Rosenberg 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 96
                            }
                        ],
                        "text": "8 Other examples of this kind of implementation of stacked generalization are fan generalizers (Wolpert 1990e), the extension of non-linear time-series analysis to multiple input dimensions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 53
                            }
                        ],
                        "text": "To use metric-based HERBIEs for this problem (as in (Wolpert 1990b)), 21 such HERBIEs have to be used, one for each component of the phoneme vector space."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 145
                            }
                        ],
                        "text": "\u2026with Alan Lapedes and Rob Farber suggests that using stacked generalization to combine ID3, perceptrons , and the author's metric-based HERBIE (Wolpert 1990b) for the problem of predicting splice junctions in DNA sequences gives accuracy better than any of the techniques by itself (i.e.,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 134
                            }
                        ],
                        "text": "For example, one might require that the generalizer be invariant under Euclidean symmetry operations in the level 0 space R n+1 (see (Wolpert 1990c))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 94
                            }
                        ],
                        "text": "All these level 0 generalizers could then use a conventional generalizer (e.g. a metric-based HERBIE) along with the entire ("
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 76
                            }
                        ],
                        "text": "As with the output neurons of NETtalk, the guesses of these 21 metric-based HERBIEs are then passed through a post-processor which combines them to form a 21-dimensional guess, which in turn specifies a phoneme guess."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 66
                            }
                        ],
                        "text": "The level 1 generalizer was the metric-based HERBIE described in (Wolpert 1990b, Wolpert 1990c)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 9
                            }
                        ],
                        "text": "In both (Wolpert 1990b) and (Sejnowski and Rosenberg 1988) generalizers never guess directly from 7-letter fields to phonemes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 92
                            }
                        ],
                        "text": "The purpose of this text-to-phoneme experiment wasn't to beat the performance (reported in (Wolpert 1990b)) of a metric-based HERBIE having access to all 7 input letters, nor even to beat the performance of back-propagation (i.e., NETtalk) on this data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 164
                            }
                        ],
                        "text": "5 Finally, work in progress with Alan Lapedes and Rob Farber suggests that using stacked generalization to combine ID3, perceptrons , and the author's metric-based HERBIE (Wolpert 1990b) for the problem of predicting splice junctions in DNA sequences gives accuracy better than any of the techniques by itself (i.e., preliminary evidence indicates that this implementation of stacked generalization is the best known generalization method for this problem)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 41
                            }
                        ],
                        "text": "[7] This is essentially what is done in (Wolpert 1990d), where a genetic evolution process is used to create a feedback net of generalizers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 284
                            }
                        ],
                        "text": "\u2026generalizers are back-propagated neural nets (Rumelhart and McClelland 1986), Holland's classifier system (Holland 1975), and Rissanen's minimum description length principle (Rissanen 1986) (which, along with all other schemes which attempt to exploit Occam's razor, is analyzed in (Wolpert 1990a))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 49
                            }
                        ],
                        "text": "Each such level 0 generalizer was a metric-based HERBIE, where 4 nearest neighbors were used."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 109
                            }
                        ],
                        "text": "(Since the space of possible matrices is so large however, rather than the trial and error approach used in (Wolpert 1990b) one would probably want to employ something like gradient descent in the space of crossvalidation error to find the \"optimal\" weighting matrix.)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 156
                            }
                        ],
                        "text": "6 In addition, along with (for example) Farmer's local linear technique, metric-based HERBIEs necessarily always reproduce their learning set exactly (see (Wolpert 1990b) and (Wolpert 1990c))."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 31
                            }
                        ],
                        "text": "(See (Stanfill and Waltz 1986, Wolpert 1990b, Carterette and Jones 1974, Sejnowski and Rosenberg 1988).)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 43
                            }
                        ],
                        "text": "The level 1 generalizer was a metric-based HERBIE using a full Hamming metric over the 3-dimensional level 1 input space."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 5
                            }
                        ],
                        "text": "See (Wolpert 1990d) for details."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 82
                            }
                        ],
                        "text": "(To make the construction unique, one must impose other constraints as well -see (Wolpert 1990d).)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 109
                            }
                        ],
                        "text": "For example, back-propagation behaves somewhat locally (see (Lapedes and Farber 1988) and the discussion in (Wolpert 1990e) on using backpropagation to try to generalize the parity input-output function)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 48
                            }
                        ],
                        "text": "[6] More sophisticated versions of metric-based HERBIEs replace a pre-fixed metric with something less restrictive."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 88
                            }
                        ],
                        "text": "This jump from use of a weighting vector to use of a weighting matrix in a metric-based HERBIE is loosely equivalent to the jump from using a perceptron (with its vector of synaptic weights) to using a feedforward neural net with a single hidden layer (where one has matrices of synaptic weights)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 218
                            }
                        ],
                        "text": "\u2026examples are memory-based reasoning schemes (Stanfill and Waltz 1986), regularization theory (Poggio et al. 1988), and similar schemes for overt surface fitting of a parent function to the learning set (Wolpert 1989, Wolpert 1990a, Wolpert 1990b, Farmer and Sidorowich 1988, Omohundro 1987)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 4
                            }
                        ],
                        "text": "See(Wolpert 1990b).)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 21490888,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2e37bbb4052694c1b3359e574708a0d90789252a",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Constructing-a-generalizer-superior-to-NETtalk-via-Wolpert",
            "title": {
                "fragments": [],
                "text": "Constructing a generalizer superior to NETtalk via a mathematical theory of generalization"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2020074"
                        ],
                        "name": "A. Lapedes",
                        "slug": "A.-Lapedes",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Lapedes",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lapedes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2542113"
                        ],
                        "name": "R. Farber",
                        "slug": "R.-Farber",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Farber",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Farber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 61
                            }
                        ],
                        "text": "For example, back-propagation behaves somewhat locally (see (Lapedes and Farber 1988) and the discussion in (Wolpert 1990e) on using backpropagation to try to generalize the parity input-output function)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18474528,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2d7de94252e5040a38ebaaf535841d3500791c79",
            "isKey": false,
            "numCitedBy": 373,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "There is presently great interest in the abilities of neural networks to mimic \"qualitative reasoning\" by manipulating neural incodings of symbols. Less work has been performed on using neural networks to process floating point numbers and it is sometimes stated that neural networks are somehow inherently inaccurate and therefore best suited for \"fuzzy\" qualitative reasoning. Nevertheless, the potential speed of massively parallel operations make neural net \"number crunching\" an interesting topic to explore. In this paper we discuss some of our work in which we demonstrate that for certain applications neural networks can achieve significantly higher numerical accuracy than more conventional techniques. In particular, prediction of future values of a chaotic time series can be performed with exceptionally high accuracy. We analyze how a neural net is able to do this, and in the process show that a large class of functions from Rn \u2192 Rm may be accurately approximated by a backpropagation neural net with just two \"hidden\" layers. The network uses this functional approximation to perform either interpolation (signal processing applications) or extrapolation (symbol processing applications). Neural nets therefore use quite familiar methods to perform their tasks. The geometrical viewpoint advocated here seems to be a useful approach to analyzing neural network operation and relates neural networks to well studied topics in functional approximation."
            },
            "slug": "How-Neural-Nets-Work-Lapedes-Farber",
            "title": {
                "fragments": [],
                "text": "How Neural Nets Work"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper demonstrates that for certain applications neural networks can achieve significantly higher numerical accuracy than more conventional techniques, and shows that prediction of future values of a chaotic time series can be performed with exceptionally high accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2550392"
                        ],
                        "name": "B. Efron",
                        "slug": "B.-Efron",
                        "structuredName": {
                            "firstName": "Bradley",
                            "lastName": "Efron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Efron"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 138
                            }
                        ],
                        "text": "Most algorithmic schemes for addressing this problem, including in particular non-parametric statistics techniques like cross-validation (Efron 1979, Stone 1977), generalized cross-validation (Li 1985) and bootstrapping (Efron 1979), are winnertakes-all strategies."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 120199221,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7d61ed7f458aba6c563235dabbfe37e4c100e89f",
            "isKey": false,
            "numCitedBy": 754,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "This is a survey article concerning recent advances in certain areas of statistical theory, written for a mathematical audience with no background in statistics. The topics are chosen to illustrate a special point: how the advent of the high-speed computer has affected the development of statistical theory. The topics discussed include nonparametric methods, the jackknife, the bootstrap, cross-validation, error-rate estimation in discriminant analysis, robust estimation, the influence function, censored data, the EM algorithm, and Cox\u2019s likelihood function. The exposition is mainly by example, with only a little offered in the way of theoretical development."
            },
            "slug": "Computers-and-the-Theory-of-Statistics:-Thinking-Efron",
            "title": {
                "fragments": [],
                "text": "Computers and the Theory of Statistics: Thinking the Unthinkable"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120971205"
                        ],
                        "name": "M. Brady",
                        "slug": "M.-Brady",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Brady",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Brady"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 110
                            }
                        ],
                        "text": "Other important examples are memory-based reasoning schemes (Stanfill and Waltz 1986), regularization theory (Poggio et al. 1988), and similar schemes for overt surface fitting of a parent function to the learning set (Wolpert 1989, Wolpert 1990a, Wolpert 1990b, Farmer and Sidorowich 1988,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 58521521,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "164bde6e0a7211148924b5c7ae8310822baeb7ad",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : In this series of Image Understanding Workshop Proceedings, we have stressed the issue of representation. In particular, we have described the development by Horn and his colleagues of the reflectance map, the albedo image, and the Gaussian image, and we have described the work of the group founded by Marr using the primal sketch, the 2 1/2 D sketch, and axis based 3-D models. In the April, 1981 Proceedings, we reviewed work on computing shape from shading and occluding boundaries, including a local parallel algorithm for doing this due to Ikeuchi and Horn 1981; the detection and perception of motion by computing the optical flow and directional selectivity of zero crossings; the interpolation of curves and surfaces; the real-time convolution of images with a difference-of-Gaussians (DOG) operator; and progress toward computing the full primal sketch. Here we review work on stereo to facilitate the computation of depth information and visible surface characteristics, the detction and interpretation of motion, the interpolation and description of visible surfaces, the description of two- and three-dimensional shapes, real-time convolution, and shape from shading. (Author)"
            },
            "slug": "MIT-Progress-in-Understanding-Images-Brady",
            "title": {
                "fragments": [],
                "text": "MIT Progress in Understanding Images"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Work on stereo to facilitate the computation of depth information and visible surface characteristics, the detction and interpretation of motion, the interpolation and description of visible surfaces, the description of two- and three-dimensional shapes, real-time convolution, and shape from shading is reviewed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9047124"
                        ],
                        "name": "J. Deppisch",
                        "slug": "J.-Deppisch",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Deppisch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Deppisch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144866315"
                        ],
                        "name": "H. Bauer",
                        "slug": "H.-Bauer",
                        "structuredName": {
                            "firstName": "Hans-Ulrich",
                            "lastName": "Bauer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710480"
                        ],
                        "name": "T. Geisel",
                        "slug": "T.-Geisel",
                        "structuredName": {
                            "firstName": "Theo",
                            "lastName": "Geisel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Geisel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 48
                            }
                        ],
                        "text": "An example of such a scheme is investigated in (Deppisch et al. 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123643758,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0341b7b5195b4a5a3636f242a2ab4eb6f480ac47",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Hierarchical-training-of-neural-networks-and-of-Deppisch-Bauer",
            "title": {
                "fragments": [],
                "text": "Hierarchical training of neural networks and prediction of chaotic time series"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38751698"
                        ],
                        "name": "V. Morozov",
                        "slug": "V.-Morozov",
                        "structuredName": {
                            "firstName": "Vitali",
                            "lastName": "Morozov",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Morozov"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 234
                            }
                        ],
                        "text": "This non-universality is inevitable, and holds for any generalizing scheme whatsoever, due to the fact that guessing a parent function based on only a finite number of samples of it is an ill-posed problem in the Hadamard sense (see (Morozov 1984))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 118764354,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "b65f5f60618304c2cfa8431488f481888e1cc10c",
            "isKey": false,
            "numCitedBy": 967,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Spend your time even for only few minutes to read a book. Reading a book will never reduce and waste your time to be useless. Reading, for some people become a need that is to do every day such as spending time for eating. Now, what about you? Do you like to read a book? Now, we will show you a new book enPDFd methods for solving incorrectly posed problems that can be a new way to explore the knowledge. When reading this book, you can get one thing to always remember in every reading time, even step by step."
            },
            "slug": "Methods-for-Solving-Incorrectly-Posed-Problems-Morozov",
            "title": {
                "fragments": [],
                "text": "Methods for Solving Incorrectly Posed Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new book enPDFd methods for solving incorrectly posed problems that can be a new way to explore the knowledge and get one thing to always remember in every reading time, even step by step is shown."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3140752"
                        ],
                        "name": "Charles R. Rosenberg",
                        "slug": "Charles-R.-Rosenberg",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Rosenberg",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles R. Rosenberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 271,
                                "start": 243
                            }
                        ],
                        "text": "The second is using it to improve upon the individual performance of several generalizers for a modified version of the text-to-phoneme data set that went into making NETtalk (Stanfill and Waltz 1986, Wolpert 1990b, Carterette and Jones 1974, Sejnowski and Rosenberg 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 29
                            }
                        ],
                        "text": "In both (Wolpert 1990b) and (Sejnowski and Rosenberg 1988) generalizers never guess directly from 7-letter fields to phonemes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 73
                            }
                        ],
                        "text": "(See (Stanfill and Waltz 1986, Wolpert 1990b, Carterette and Jones 1974, Sejnowski and Rosenberg 1988).)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13921532,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "406033f22b6a671b94bcbdfaf63070b7ce6f3e48",
            "isKey": false,
            "numCitedBy": 761,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Unrestricted English text can be converted to speech by applying phonological rules and handling exceptions with a look-up table. However, this approach is highly labor intensive since each entry and rule must be hand-crafted. NETtalk is an alternative approach that is based on an automated learning procedure for a parallel network of deterministic processing units. ~ f t e r ' training on a corpus of informal continuous speech, it achieves good performance and generalizes to novel words. The distributed internal representations of the phonological regularities discovered by the network are damage resistant."
            },
            "slug": "NETtalk:-a-parallel-network-that-learns-to-read-Sejnowski-Rosenberg",
            "title": {
                "fragments": [],
                "text": "NETtalk: a parallel network that learns to read aloud"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "NETtalk is an alternative approach that is based on an automated learning procedure for a parallel network of deterministic processing units that achieves good performance and generalizes to novel words."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144404817"
                        ],
                        "name": "J. Holland",
                        "slug": "J.-Holland",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Holland",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Holland"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 124
                            }
                        ],
                        "text": "Some examples of generalizers are back-propagated neural nets (Rumelhart and McClelland 1986), Holland's classifier system (Holland 1975), and Rissanen's minimum description length principle (Rissanen 1986) (which, along with all other schemes which attempt to exploit Occam's razor, is analyzed in\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 58781161,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4b4279db68b16e20fbc56f9d41980a950191d30a",
            "isKey": false,
            "numCitedBy": 38742,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Name of founding work in the area. Adaptation is key to survival and evolution. Evolution implicitly optimizes organisims. AI wants to mimic biological optimization { Survival of the ttest { Exploration and exploitation { Niche nding { Robust across changing environments (Mammals v. Dinos) { Self-regulation,-repair and-reproduction 2 Artiicial Inteligence Some deenitions { \"Making computers do what they do in the movies\" { \"Making computers do what humans (currently) do best\" { \"Giving computers common sense; letting them make simple deci-sions\" (do as I want, not what I say) { \"Anything too new to be pidgeonholed\" Adaptation and modiication is root of intelligence Some (Non-GA) branches of AI: { Expert Systems (Rule based deduction)"
            },
            "slug": "Adaptation-in-natural-and-artificial-systems-Holland",
            "title": {
                "fragments": [],
                "text": "Adaptation in natural and artificial systems"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "Names of founding work in the area of Adaptation and modiication, which aims to mimic biological optimization, and some (Non-GA) branches of AI."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32640517"
                        ],
                        "name": "S. Gustafson",
                        "slug": "S.-Gustafson",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Gustafson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gustafson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "95238792"
                        ],
                        "name": "G. Little",
                        "slug": "G.-Little",
                        "structuredName": {
                            "firstName": "Gordon",
                            "lastName": "Little",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Little"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144450171"
                        ],
                        "name": "D. Simon",
                        "slug": "D.-Simon",
                        "structuredName": {
                            "firstName": "Darren",
                            "lastName": "Simon",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Simon"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 42
                            }
                        ],
                        "text": "A more elegant scheme has been devised by Gustafson et al. (1990): require that the level 1 surface guessed by the level 1 generalizer contains the line \u03b1e, where \u03b1 runs over the reals and e is a diagonal vector, that is a vector in R k+1 all k+1 of whose coordinate projections are identical and\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 146
                            }
                        ],
                        "text": "For example, Gustafson et al. have reported that (what amounts to) stacked generalization beats back-propagation for some hydrodynamics problems (Gustafson et al. 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62130440,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "af42462805d030e561b0666fec2ff96d9f217476",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "A locally linear neural network for interpolation and extrapolation is described. Desirable characteristics of this network include exact recall of training data optimal linear generalization of testing data and training in a known number of computational steps."
            },
            "slug": "Neural-network-for-interpolation-and-extrapolation-Gustafson-Little",
            "title": {
                "fragments": [],
                "text": "Neural network for interpolation and extrapolation"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "A locally linear neural network for interpolation and extrapolation is described and desirable characteristics of this network include exact recall of training data optimal linear generalization of testing data and training in a known number of computational steps."
            },
            "venue": {
                "fragments": [],
                "text": "Defense, Security, and Sensing"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5216020"
                        ],
                        "name": "M. Casdagli",
                        "slug": "M.-Casdagli",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Casdagli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Casdagli"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 89
                            }
                        ],
                        "text": "Non-linear time-series analysis, with its \"delay embedding\" (Farmer and Sidorowich 1988, Casdagli 1989), is an example of such a use of stacked generalization with a set of similar, dumb, level 0 generalizers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122236599,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "be946457d3f880d9ec836aee3d0d231ffa3bcc9a",
            "isKey": false,
            "numCitedBy": 1398,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Nonlinear-prediction-of-chaotic-time-series-Casdagli",
            "title": {
                "fragments": [],
                "text": "Nonlinear prediction of chaotic time series"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3303302"
                        ],
                        "name": "G. Schulz",
                        "slug": "G.-Schulz",
                        "structuredName": {
                            "firstName": "Georg",
                            "lastName": "Schulz",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Schulz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145077987"
                        ],
                        "name": "C. D. Barry",
                        "slug": "C.-D.-Barry",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Barry",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. D. Barry"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061987706"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Friedman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38395345"
                        ],
                        "name": "P. Y. Chou",
                        "slug": "P.-Y.-Chou",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Chou",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Y. Chou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5437548"
                        ],
                        "name": "G. Fasman",
                        "slug": "G.-Fasman",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Fasman",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Fasman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31442957"
                        ],
                        "name": "A. Finkelstein",
                        "slug": "A.-Finkelstein",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Finkelstein",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Finkelstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34954368"
                        ],
                        "name": "V. Lim",
                        "slug": "V.-Lim",
                        "structuredName": {
                            "firstName": "Valery",
                            "lastName": "Lim",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2629227"
                        ],
                        "name": "O. Ptitsyn",
                        "slug": "O.-Ptitsyn",
                        "structuredName": {
                            "firstName": "O.",
                            "lastName": "Ptitsyn",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Ptitsyn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4499479"
                        ],
                        "name": "E. Kabat",
                        "slug": "E.-Kabat",
                        "structuredName": {
                            "firstName": "Elvin",
                            "lastName": "Kabat",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Kabat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39327730"
                        ],
                        "name": "T. T. Wu",
                        "slug": "T.-T.-Wu",
                        "structuredName": {
                            "firstName": "Tai",
                            "lastName": "Wu",
                            "middleNames": [
                                "Te"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. T. Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145162720"
                        ],
                        "name": "M. Levitt",
                        "slug": "M.-Levitt",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Levitt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Levitt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145809427"
                        ],
                        "name": "B. Robson",
                        "slug": "B.-Robson",
                        "structuredName": {
                            "firstName": "Barry",
                            "lastName": "Robson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Robson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87153926"
                        ],
                        "name": "K. Nagano",
                        "slug": "K.-Nagano",
                        "structuredName": {
                            "firstName": "Kozo",
                            "lastName": "Nagano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Nagano"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 182
                            }
                        ],
                        "text": "4 Similarly, a number of researchers have reported on the efficacy of simply averaging a set of generalizers, for example for aspects of the problem of predicting protein structure (Schulz et al. 1974)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4284334,
            "fieldsOfStudy": [
                "Chemistry"
            ],
            "id": "bc14df89e0b9401a436f0228fe57d73c7d6bbefb",
            "isKey": false,
            "numCitedBy": 146,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "IT is generally accepted that the action of a protein cannot be understood until its three-dimensional structure is known. At present, X-ray analysis of protein crystals is the only method of obtaining such structural information. It is to be feared, however, that many important proteins will never give suitable crystals so that one is obliged to consider other approaches to structure elucidation. Renaturation experiments indicated1\u20134 that the three-dimensional structure of many if not all proteins is a unique function of their amino acid sequence. Therefore, in principle one should be able to determine these structures by using only the information contained in the sequence. A first step in this direction is the prediction of secondary structures (\u03b1 helices, \u03b2 pleated sheets, \u03b2 bends) in globular proteins from amino acid sequences. Several prediction schemes have been devised to this end5\u201323. It is the aim of this paper to demonstrate directly the current standing of such methods."
            },
            "slug": "Comparison-of-predicted-and-experimentally-of-Schulz-Barry",
            "title": {
                "fragments": [],
                "text": "Comparison of predicted and experimentally determined secondary structure of adenyl kinase"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The aim of this paper is to demonstrate directly the current standing of prediction schemes for the prediction of secondary structures in globular proteins from amino acid sequences, and to suggest new approaches to structure elucidation."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "147608487"
                        ],
                        "name": "M. Stone",
                        "slug": "M.-Stone",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Stone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stone"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 150
                            }
                        ],
                        "text": "Most algorithmic schemes for addressing this problem, including in particular non-parametric statistics techniques like cross-validation (Efron 1979, Stone 1977), generalized cross-validation (Li 1985) and bootstrapping (Efron 1979), are winnertakes-all strategies."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 121719087,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "65796753c74771c33d2e0591e912e33ac30420aa",
            "isKey": false,
            "numCitedBy": 290,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY The asymptotic consistency of cross-validatory assessment and the asymptotic efficiency of cross-validatory choice is investigated both in some generality and also in the context of particular applications."
            },
            "slug": "Asymptotics-for-and-against-cross-validation-Stone",
            "title": {
                "fragments": [],
                "text": "Asymptotics for and against cross-validation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 176
                            }
                        ],
                        "text": "\u2026generalizers are back-propagated neural nets (Rumelhart and McClelland 1986), Holland's classifier system (Holland 1975), and Rissanen's minimum description length principle (Rissanen 1986) (which, along with all other schemes which attempt to exploit Occam's razor, is analyzed in (Wolpert 1990a))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120741100,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7e1ca8d081fc07e6190a3bf5e3156569d8e9c96b",
            "isKey": false,
            "numCitedBy": 1036,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "On demontre un theoreme fondamental qui donne une borne inferieure pour la longueur de code et donc, pour les erreurs de prediction. On definit les notions \u00abd'information a priori\u00bb et \u00abd'information utile\u00bb dans les donnees"
            },
            "slug": "Stochastic-Complexity-and-Modeling-Rissanen",
            "title": {
                "fragments": [],
                "text": "Stochastic Complexity and Modeling"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49243178"
                        ],
                        "name": "Ker-Chau Li",
                        "slug": "Ker-Chau-Li",
                        "structuredName": {
                            "firstName": "Ker-Chau",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ker-Chau Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 193
                            }
                        ],
                        "text": "Most algorithmic schemes for addressing this problem, including in particular non-parametric statistics techniques like cross-validation (Efron 1979, Stone 1977), generalized cross-validation (Li 1985) and bootstrapping (Efron 1979), are winnertakes-all strategies."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122234838,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "15d59561ada9c4cc6b22fc58112937f0b68faacd",
            "isKey": false,
            "numCitedBy": 165,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "On considere une methode interessante pour le choix d'estimateurs lineaires et on presente une nouvelle approche basee sur les estimateurs de Stein (1981)"
            },
            "slug": "From-Stein's-Unbiased-Risk-Estimates-to-the-Method-Li",
            "title": {
                "fragments": [],
                "text": "From Stein's Unbiased Risk Estimates to the Method of Generalized Cross Validation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2928927"
                        ],
                        "name": "C. Stanfill",
                        "slug": "C.-Stanfill",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Stanfill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Stanfill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788375"
                        ],
                        "name": "D. Waltz",
                        "slug": "D.-Waltz",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Waltz",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Waltz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 61
                            }
                        ],
                        "text": "Other important examples are memory-based reasoning schemes (Stanfill and Waltz 1986), regularization theory (Poggio et al. 1988), and similar schemes for overt surface fitting of a parent function to the learning set (Wolpert 1989, Wolpert 1990a, Wolpert 1990b, Farmer and Sidorowich 1988,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 176
                            }
                        ],
                        "text": "The second is using it to improve upon the individual performance of several generalizers for a modified version of the text-to-phoneme data set that went into making NETtalk (Stanfill and Waltz 1986, Wolpert 1990b, Carterette and Jones 1974, Sejnowski and Rosenberg 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 10
                            }
                        ],
                        "text": "Therefore NETtalk, for example, is a neural net which takes (a suitable encoding of) a 7-letter input field as its input, and guesses a vector in a 21-dimensional space."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 30
                            }
                        ],
                        "text": "As with the output neurons of NETtalk, the guesses of these 21 metric-based HERBIEs are then passed through a post-processor which combines them to form a 21-dimensional guess, which in turn specifies a phoneme guess."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 48
                            }
                        ],
                        "text": "The other numerical experiment was based on the NETtalk \"reading aloud\" problem."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 231
                            }
                        ],
                        "text": "The purpose of this text-to-phoneme experiment wasn't to beat the performance (reported in (Wolpert 1990b)) of a metric-based HERBIE having access to all 7 input letters, nor even to beat the performance of back-propagation (i.e., NETtalk) on this data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 6
                            }
                        ],
                        "text": "(See (Stanfill and Waltz 1986, Wolpert 1990b, Carterette and Jones 1974, Sejnowski and Rosenberg 1988).)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 42
                            }
                        ],
                        "text": "This is indeed the case, according to the NETtalk-type experiments reported here and according to other experiments reported elsewhere."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16624499,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "123a726f6feb2bce29708b68ab2db5cdf9fcdaf4",
            "isKey": true,
            "numCitedBy": 1436,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "The intensive use of memory to recall specific episodes from the past\u2014rather than rules\u2014should be the foundation of machine reasoning."
            },
            "slug": "Toward-memory-based-reasoning-Stanfill-Waltz",
            "title": {
                "fragments": [],
                "text": "Toward memory-based reasoning"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The intensive use of memory to recall specific episodes from the past\u2014rather than rules\u2014should be the foundation of machine reasoning."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 5
                            }
                        ],
                        "text": "(In (Wolpert 1990d), the output of this feedback net of generalizers is fed through yet another generalizer to get the final guess."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 61
                            }
                        ],
                        "text": "For example, in the use of metric-based HERBIEs reported in (Wolpert 1990b), the input space was 7 dimensional, and each of the 7 coordinates of any input value were scaled by a distinct weighting factor, \u03c1 i , 1 \u2264 i \u2264 7, before the conventional metric was applied."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 17
                            }
                        ],
                        "text": "This metricbased HERBIE is one of the simplest generalizers there are."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 110
                            }
                        ],
                        "text": "The data set used for the experiment reported here was standard Carterette and Jones (1974), modified (as in (Wolpert 1990b)) to force consistency amongst the several speakers record- ed."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 292,
                                "start": 279
                            }
                        ],
                        "text": "\u2026a countably infinite set of functions {g i }, 1 \u2264 i < \u221e, one function for each possible value of m. g 1 takes three arguments (the learning set input x 1 , the learning set output y 1 , and the question q); g 2 takes five arguments (x 1 , y 1 , x 2 , y 2 , and q); and so on (see (Wolpert 1990c))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 38
                            }
                        ],
                        "text": "(As usual, there were in fact 21 such HERBIEs, making a 21-dimensional guess which in turn specified the phoneme guessed by the entire stacked generalizer.)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 89
                            }
                        ],
                        "text": "Unless otherwise specified, in the rest of this section, whenever the term \"metric-based HERBIE\" is used, what is really meant is a set of 21 such HER- BIEs combining in the manner discussed here to guess a legal phoneme."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 201
                            }
                        ],
                        "text": "The second is using it to improve upon the individual performance of several generalizers for a modified version of the text-to-phoneme data set that went into making NETtalk (Stanfill and Waltz 1986, Wolpert 1990b, Carterette and Jones 1974, Sejnowski and Rosenberg 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 96
                            }
                        ],
                        "text": "8 Other examples of this kind of implementation of stacked generalization are fan generalizers (Wolpert 1990e), the extension of non-linear time-series analysis to multiple input dimensions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 53
                            }
                        ],
                        "text": "To use metric-based HERBIEs for this problem (as in (Wolpert 1990b)), 21 such HERBIEs have to be used, one for each component of the phoneme vector space."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 145
                            }
                        ],
                        "text": "\u2026with Alan Lapedes and Rob Farber suggests that using stacked generalization to combine ID3, perceptrons , and the author's metric-based HERBIE (Wolpert 1990b) for the problem of predicting splice junctions in DNA sequences gives accuracy better than any of the techniques by itself (i.e.,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 134
                            }
                        ],
                        "text": "For example, one might require that the generalizer be invariant under Euclidean symmetry operations in the level 0 space R n+1 (see (Wolpert 1990c))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 94
                            }
                        ],
                        "text": "All these level 0 generalizers could then use a conventional generalizer (e.g. a metric-based HERBIE) along with the entire ("
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 76
                            }
                        ],
                        "text": "As with the output neurons of NETtalk, the guesses of these 21 metric-based HERBIEs are then passed through a post-processor which combines them to form a 21-dimensional guess, which in turn specifies a phoneme guess."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 66
                            }
                        ],
                        "text": "The level 1 generalizer was the metric-based HERBIE described in (Wolpert 1990b, Wolpert 1990c)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 9
                            }
                        ],
                        "text": "In both (Wolpert 1990b) and (Sejnowski and Rosenberg 1988) generalizers never guess directly from 7-letter fields to phonemes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 92
                            }
                        ],
                        "text": "The purpose of this text-to-phoneme experiment wasn't to beat the performance (reported in (Wolpert 1990b)) of a metric-based HERBIE having access to all 7 input letters, nor even to beat the performance of back-propagation (i.e., NETtalk) on this data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 164
                            }
                        ],
                        "text": "5 Finally, work in progress with Alan Lapedes and Rob Farber suggests that using stacked generalization to combine ID3, perceptrons , and the author's metric-based HERBIE (Wolpert 1990b) for the problem of predicting splice junctions in DNA sequences gives accuracy better than any of the techniques by itself (i.e., preliminary evidence indicates that this implementation of stacked generalization is the best known generalization method for this problem)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 41
                            }
                        ],
                        "text": "[7] This is essentially what is done in (Wolpert 1990d), where a genetic evolution process is used to create a feedback net of generalizers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 284
                            }
                        ],
                        "text": "\u2026generalizers are back-propagated neural nets (Rumelhart and McClelland 1986), Holland's classifier system (Holland 1975), and Rissanen's minimum description length principle (Rissanen 1986) (which, along with all other schemes which attempt to exploit Occam's razor, is analyzed in (Wolpert 1990a))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 49
                            }
                        ],
                        "text": "Each such level 0 generalizer was a metric-based HERBIE, where 4 nearest neighbors were used."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 109
                            }
                        ],
                        "text": "(Since the space of possible matrices is so large however, rather than the trial and error approach used in (Wolpert 1990b) one would probably want to employ something like gradient descent in the space of crossvalidation error to find the \"optimal\" weighting matrix.)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 156
                            }
                        ],
                        "text": "6 In addition, along with (for example) Farmer's local linear technique, metric-based HERBIEs necessarily always reproduce their learning set exactly (see (Wolpert 1990b) and (Wolpert 1990c))."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 31
                            }
                        ],
                        "text": "(See (Stanfill and Waltz 1986, Wolpert 1990b, Carterette and Jones 1974, Sejnowski and Rosenberg 1988).)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 43
                            }
                        ],
                        "text": "The level 1 generalizer was a metric-based HERBIE using a full Hamming metric over the 3-dimensional level 1 input space."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 5
                            }
                        ],
                        "text": "See (Wolpert 1990d) for details."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 82
                            }
                        ],
                        "text": "(To make the construction unique, one must impose other constraints as well -see (Wolpert 1990d).)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 109
                            }
                        ],
                        "text": "For example, back-propagation behaves somewhat locally (see (Lapedes and Farber 1988) and the discussion in (Wolpert 1990e) on using backpropagation to try to generalize the parity input-output function)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 48
                            }
                        ],
                        "text": "[6] More sophisticated versions of metric-based HERBIEs replace a pre-fixed metric with something less restrictive."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 88
                            }
                        ],
                        "text": "This jump from use of a weighting vector to use of a weighting matrix in a metric-based HERBIE is loosely equivalent to the jump from using a perceptron (with its vector of synaptic weights) to using a feedforward neural net with a single hidden layer (where one has matrices of synaptic weights)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 218
                            }
                        ],
                        "text": "\u2026examples are memory-based reasoning schemes (Stanfill and Waltz 1986), regularization theory (Poggio et al. 1988), and similar schemes for overt surface fitting of a parent function to the learning set (Wolpert 1989, Wolpert 1990a, Wolpert 1990b, Farmer and Sidorowich 1988, Omohundro 1987)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 4
                            }
                        ],
                        "text": "See(Wolpert 1990b).)"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improving the performance of generalizers via time-series-like pre-processing of the learning set"
            },
            "venue": {
                "fragments": [],
                "text": "Improving the performance of generalizers via time-series-like pre-processing of the learning set"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145394227"
                        ],
                        "name": "J. D. Farmer",
                        "slug": "J.-D.-Farmer",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Farmer",
                            "middleNames": [
                                "Doyne"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. D. Farmer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1452639101"
                        ],
                        "name": "John J. Sidorowichl",
                        "slug": "John-J.-Sidorowichl",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Sidorowichl",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John J. Sidorowichl"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 248
                            }
                        ],
                        "text": "\u2026examples are memory-based reasoning schemes (Stanfill and Waltz 1986), regularization theory (Poggio et al. 1988), and similar schemes for overt surface fitting of a parent function to the learning set (Wolpert 1989, Wolpert 1990a, Wolpert 1990b, Farmer and Sidorowich 1988, Omohundro 1987)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 61
                            }
                        ],
                        "text": "Non-linear time-series analysis, with its \"delay embedding\" (Farmer and Sidorowich 1988, Casdagli 1989), is an example of such a use of stacked generalization with a set of similar, dumb, level 0 generalizers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 269
                            }
                        ],
                        "text": "\u2026the parent functions were simple high-school-math-type functions, and the level 0 generalizer was \"linearly connect the dots of the learning set to make an input-output surface which then serves as a guess for the parent function\", i.e., the local linear technique of Farmer and Sidorowich (1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59693817,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "07f3f6d430ffca990dee62cb2649bae40c60b380",
            "isKey": false,
            "numCitedBy": 326,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Exploiting-Chaos-to-Predict-the-Future-and-Reduce-Farmer-Sidorowichl",
            "title": {
                "fragments": [],
                "text": "Exploiting Chaos to Predict the Future and Reduce Noise"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808760"
                        ],
                        "name": "S. Omohundro",
                        "slug": "S.-Omohundro",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Omohundro",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Omohundro"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 276
                            }
                        ],
                        "text": "\u2026examples are memory-based reasoning schemes (Stanfill and Waltz 1986), regularization theory (Poggio et al. 1988), and similar schemes for overt surface fitting of a parent function to the learning set (Wolpert 1989, Wolpert 1990a, Wolpert 1990b, Farmer and Sidorowich 1988, Omohundro 1987)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 44717168,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff80b7820fbc54926946c245e139c382266489ae",
            "isKey": false,
            "numCitedBy": 213,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Efficient-Algorithms-with-Neural-Network-Behavior-Omohundro",
            "title": {
                "fragments": [],
                "text": "Efficient Algorithms with Neural Network Behavior"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 96
                            }
                        ],
                        "text": "The idea is to use them as pedagogical and heuristic tools, much like the toy problems used in (Rumelhart and McClelland 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 63
                            }
                        ],
                        "text": "Some examples of generalizers are back-propagated neural nets (Rumelhart and McClelland 1986), Holland's classifier system (Holland 1975), and Rissanen's minimum description length principle (Rissanen 1986) (which, along with all other schemes which attempt to exploit Occam's razor, is analyzed in\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Explorations in the microstructure of cognition, volumes I and II"
            },
            "venue": {
                "fragments": [],
                "text": "Explorations in the microstructure of cognition, volumes I and II"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 105
                            }
                        ],
                        "text": "Conventional classifiers (e.g. ID3 (Quinlan 1986), Bayesian classifiers like Schlimmer's Stagger system (Dietterich 1990), etc.) don't have this flexibility, although in all other respects they are valid examples of"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 174
                            }
                        ],
                        "text": "The central idea is that one can do better than simply list all guesses as to the parent function which are consistent with a learning set (as is done in PAC-style learning (Dietterich 1990, Valiant 1984), for example)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Machine learning. Annual review of computer science"
            },
            "venue": {
                "fragments": [],
                "text": "Machine learning. Annual review of computer science"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 64
                            }
                        ],
                        "text": "The data set used for the experiment reported here was standard Carterette and Jones (1974), modified (as in (Wolpert 1990b)) to force consistency amongst the several speakers record- ed."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 216
                            }
                        ],
                        "text": "The second is using it to improve upon the individual performance of several generalizers for a modified version of the text-to-phoneme data set that went into making NETtalk (Stanfill and Waltz 1986, Wolpert 1990b, Carterette and Jones 1974, Sejnowski and Rosenberg 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 46
                            }
                        ],
                        "text": "(See (Stanfill and Waltz 1986, Wolpert 1990b, Carterette and Jones 1974, Sejnowski and Rosenberg 1988).)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Informal Speech"
            },
            "venue": {
                "fragments": [],
                "text": "Informal Speech"
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 89
                            }
                        ],
                        "text": "Non-linear time-series analysis, with its \"delay embedding\" (Farmer and Sidorowich 1988, Casdagli 1989), is an example of such a use of stacked generalization with a set of similar, dumb, level 0 generalizers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Non-linear prediction of chaotic time-series"
            },
            "venue": {
                "fragments": [],
                "text": "Physica D"
            },
            "year": 1989
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 21,
            "methodology": 16
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 26,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Stacked-generalization-Wolpert/7e1291583873fb890e7922ec0dfefd4846df46c9?sort=total-citations"
}