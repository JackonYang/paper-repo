{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38340927"
                        ],
                        "name": "Yi-Feng Pan",
                        "slug": "Yi-Feng-Pan",
                        "structuredName": {
                            "firstName": "Yi-Feng",
                            "lastName": "Pan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi-Feng Pan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761961"
                        ],
                        "name": "Xinwen Hou",
                        "slug": "Xinwen-Hou",
                        "structuredName": {
                            "firstName": "Xinwen",
                            "lastName": "Hou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinwen Hou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689269"
                        ],
                        "name": "Cheng-Lin Liu",
                        "slug": "Cheng-Lin-Liu",
                        "structuredName": {
                            "firstName": "Cheng-Lin",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cheng-Lin Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Scene text detection, maximally stable extremal regions, single-link clustering, distance metric learning"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10564829,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a79f43246bed540084ca2d1fcf99a68c69820747",
            "isKey": false,
            "numCitedBy": 410,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Text detection and localization in natural scene images is important for content-based image analysis. This problem is challenging due to the complex background, the non-uniform illumination, the variations of text font, size and line orientation. In this paper, we present a hybrid approach to robustly detect and localize texts in natural scene images. A text region detector is designed to estimate the text existing confidence and scale information in image pyramid, which help segment candidate text components by local binarization. To efficiently filter out the non-text components, a conditional random field (CRF) model considering unary component properties and binary contextual component relationships with supervised parameter learning is proposed. Finally, text components are grouped into text lines/words with a learning-based energy minimization method. Since all the three stages are learning-based, there are very few parameters requiring manual tuning. Experimental results evaluated on the ICDAR 2005 competition dataset show that our approach yields higher precision and recall performance compared with state-of-the-art methods. We also evaluated our approach on a multilingual image dataset with promising results."
            },
            "slug": "A-Hybrid-Approach-to-Detect-and-Localize-Texts-in-Pan-Hou",
            "title": {
                "fragments": [],
                "text": "A Hybrid Approach to Detect and Localize Texts in Natural Scene Images"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A hybrid approach to robustly detect and localize texts in natural scene images using a text region detector, a conditional random field model, and a learning-based energy minimization method are presented."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46772671"
                        ],
                        "name": "Xiangrong Chen",
                        "slug": "Xiangrong-Chen",
                        "structuredName": {
                            "firstName": "Xiangrong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiangrong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 117
                            }
                        ],
                        "text": "Existing methods for scene text detection can roughly be categorized into three groups: sliding window based methods [6]\u2013[8], connected component based methods [9]\u2013[13], and hybrid methods [14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61234963,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "37ba7b9a823e8a400046bd149b7756adf5d698da",
            "isKey": false,
            "numCitedBy": 512,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper gives an algorithm for detecting and reading text in natural images. The algorithm is intended for use by blind and visually impaired subjects walking through city scenes. We first obtain a dataset of city images taken by blind and normally sighted subjects. From this dataset, we manually label and extract the text regions. Next we perform statistical analysis of the text regions to determine which image features are reliable indicators of text and have low entropy (i.e. feature response is similar for all text images). We obtain weak classifiers by using joint probabilities for feature responses on and off text. These weak classifiers are used as input to an AdaBoost machine learning algorithm to train a strong classifier. In practice, we trained a cascade with 4 strong classifiers containing 79 features. An adaptive binarization and extension algorithm is applied to those regions selected by the cascade classifier. Commercial OCR software is used to read the text or reject it as a non-text region. The overall algorithm has a success rate of over 90% (evaluated by complete detection and reading of the text) on the test set and the unread text is typically small and distant from the viewer."
            },
            "slug": "Detecting-and-reading-text-in-natural-scenes-Chen-Yuille",
            "title": {
                "fragments": [],
                "text": "Detecting and reading text in natural scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The overall algorithm has a success rate of over 90% (evaluated by complete detection and reading of the text) on the test set and the unread text is typically small and distant from the viewer."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2004"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8135633"
                        ],
                        "name": "Xuwang Yin",
                        "slug": "Xuwang-Yin",
                        "structuredName": {
                            "firstName": "Xuwang",
                            "lastName": "Yin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuwang Yin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682664"
                        ],
                        "name": "Xu-Cheng Yin",
                        "slug": "Xu-Cheng-Yin",
                        "structuredName": {
                            "firstName": "Xu-Cheng",
                            "lastName": "Yin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xu-Cheng Yin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143680014"
                        ],
                        "name": "Hongwei Hao",
                        "slug": "Hongwei-Hao",
                        "structuredName": {
                            "firstName": "Hongwei",
                            "lastName": "Hao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hongwei Hao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49688168"
                        ],
                        "name": "Khalid Iqbal",
                        "slug": "Khalid-Iqbal",
                        "structuredName": {
                            "firstName": "Khalid",
                            "lastName": "Iqbal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Khalid Iqbal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ompetition dataset, text candidates identi\ufb01ed as text are further partitioned into words by classifying inner character distances into character spacings and word spacings using an AdaBoost classi\ufb01er [26]. The following features are adopted: spacing aspect ratio, relative width difference between left and right neighbors, number of character candidates in the text candidate. 3.1 Letter Candidates Extr"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "s classi\ufb01cation. text candidates corresponding to true text are identi\ufb01ed by the text classi\ufb01er. An AdaBoost classi\ufb01er is trained to decide whether an text candidate corresponding to true text or not [26]. As characters in the same text tend to have similar features, the uniformity of character candidates\u2019 features are used as text candidate\u2019s features to train the classi\ufb01er. In order to measure the p"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16389170,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b41dd7ea777a0b603518366742ffcf10f2581552",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Text localization in natural scene images is an important prerequisite for many content-based image analysis tasks. In this paper, we proposed a novel and effective approach to accurately localize scene texts. Firstly, Maximally stable extremal regions(MSER) are extracted as letter candidates. Secondly, after elimination of non-letter candidates by using geometric information, candidate regions are constructed by grouping similar letter candidates using disjoint set. Candidate region features based on horizontal and vertical variances, stroke width, color and geometry are extracted. An AdaBoost classifier is built from these features and text regions are identified. The overall system is evaluated on the ICDAR 2011 competition dataset and the experimental results show that the proposed algorithm yields high precision and recall compared with the latest published algorithms."
            },
            "slug": "Effective-text-localization-in-natural-scene-images-Yin-Yin",
            "title": {
                "fragments": [],
                "text": "Effective text localization in natural scene images with MSER, geometry-based grouping and AdaBoost"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel and effective approach to accurately localize scene texts in natural scene images by extracting Maximally stable extremal regions (MSER) and constructing candidate regions by grouping similar letter candidates using disjoint set."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145532509"
                        ],
                        "name": "Luk\u00e1s Neumann",
                        "slug": "Luk\u00e1s-Neumann",
                        "structuredName": {
                            "firstName": "Luk\u00e1s",
                            "lastName": "Neumann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luk\u00e1s Neumann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145564537"
                        ],
                        "name": "Jiri Matas",
                        "slug": "Jiri-Matas",
                        "structuredName": {
                            "firstName": "Jiri",
                            "lastName": "Matas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiri Matas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Scene text detection, maximally stable extremal regions, single-link clustering, distance metric learning"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206591895,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d8b595c9e969e5605f62da51b6c16dad8aad3e0e",
            "isKey": false,
            "numCitedBy": 790,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "An end-to-end real-time scene text localization and recognition method is presented. The real-time performance is achieved by posing the character detection problem as an efficient sequential selection from the set of Extremal Regions (ERs). The ER detector is robust to blur, illumination, color and texture variation and handles low-contrast text. In the first classification stage, the probability of each ER being a character is estimated using novel features calculated with O(1) complexity per region tested. Only ERs with locally maximal probability are selected for the second stage, where the classification is improved using more computationally expensive features. A highly efficient exhaustive search with feedback loops is then applied to group ERs into words and to select the most probable character segmentation. Finally, text is recognized in an OCR stage trained using synthetic fonts. The method was evaluated on two public datasets. On the ICDAR 2011 dataset, the method achieves state-of-the-art text localization results amongst published methods and it is the first one to report results for end-to-end text recognition. On the more challenging Street View Text dataset, the method achieves state-of-the-art recall. The robustness of the proposed method against noise and low contrast of characters is demonstrated by \u201cfalse positives\u201d caused by detected watermark text in the dataset."
            },
            "slug": "Real-time-scene-text-localization-and-recognition-Neumann-Matas",
            "title": {
                "fragments": [],
                "text": "Real-time scene text localization and recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "The proposed end-to-end real-time scene text localization and recognition method achieves state-of-the-art text localization results amongst published methods and it is the first one to report results for end- to-end text recognition."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40263913"
                        ],
                        "name": "Chucai Yi",
                        "slug": "Chucai-Yi",
                        "structuredName": {
                            "firstName": "Chucai",
                            "lastName": "Yi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chucai Yi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35484757"
                        ],
                        "name": "Yingli Tian",
                        "slug": "Yingli-Tian",
                        "structuredName": {
                            "firstName": "Yingli",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yingli Tian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206724376,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9cb107a5b3b6539a9b9a758d91871f8b2519c79d",
            "isKey": false,
            "numCitedBy": 380,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Text information in natural scene images serves as important clues for many image-based applications such as scene understanding, content-based image retrieval, assistive navigation, and automatic geocoding. However, locating text from a complex background with multiple colors is a challenging task. In this paper, we explore a new framework to detect text strings with arbitrary orientations in complex natural scene images. Our proposed framework of text string detection consists of two steps: 1) image partition to find text character candidates based on local gradient features and color uniformity of character components and 2) character candidate grouping to detect text strings based on joint structural features of text characters in each text string such as character size differences, distances between neighboring characters, and character alignment. By assuming that a text string has at least three characters, we propose two algorithms of text string detection: 1) adjacent character grouping method and 2) text line grouping method. The adjacent character grouping method calculates the sibling groups of each character candidate as string segments and then merges the intersecting sibling groups into text string. The text line grouping method performs Hough transform to fit text line among the centroids of text candidates. Each fitted text line describes the orientation of a potential text string. The detected text string is presented by a rectangle region covering all characters whose centroids are cascaded in its text line. To improve efficiency and accuracy, our algorithms are carried out in multi-scales. The proposed methods outperform the state-of-the-art results on the public Robust Reading Dataset, which contains text only in horizontal orientation. Furthermore, the effectiveness of our methods to detect text strings with arbitrary orientations is evaluated on the Oriented Scene Text Dataset collected by ourselves containing text strings in nonhorizontal orientations."
            },
            "slug": "Text-String-Detection-From-Natural-Scenes-by-and-Yi-Tian",
            "title": {
                "fragments": [],
                "text": "Text String Detection From Natural Scenes by Structure-Based Partition and Grouping"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A new framework to detect text strings with arbitrary orientations in complex natural scene images with outperform the state-of-the-art results on the public Robust Reading Dataset, which contains text only in horizontal orientation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1959339"
                        ],
                        "name": "Cunzhao Shi",
                        "slug": "Cunzhao-Shi",
                        "structuredName": {
                            "firstName": "Cunzhao",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cunzhao Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683416"
                        ],
                        "name": "Chunheng Wang",
                        "slug": "Chunheng-Wang",
                        "structuredName": {
                            "firstName": "Chunheng",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunheng Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2658590"
                        ],
                        "name": "Baihua Xiao",
                        "slug": "Baihua-Xiao",
                        "structuredName": {
                            "firstName": "Baihua",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Baihua Xiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145955846"
                        ],
                        "name": "Yang Zhang",
                        "slug": "Yang-Zhang",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yang Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145284815"
                        ],
                        "name": "Song Gao",
                        "slug": "Song-Gao",
                        "structuredName": {
                            "firstName": "Song",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song Gao"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[23] and some of the top scoring methods (Kim\u2019s method, Yi\u2019s method, TH-TextLoc system and Neumann\u2019s method) from ICDAR 2011 Competition."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 231
                            }
                        ],
                        "text": "More recently, Maximally Stable Extremal Regions (MSERs) based methods, which can be categorized as connected component based methods but using MSERs [16] as character candidates, have become the focus of several recent works [17]\u2013[23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6801531,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "22064d411a128de1a91ad87f86055e254c9c5321",
            "isKey": false,
            "numCitedBy": 191,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Scene-text-detection-using-graph-model-built-upon-Shi-Wang",
            "title": {
                "fragments": [],
                "text": "Scene text detection using graph model built upon maximally stable extremal regions"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40263913"
                        ],
                        "name": "Chucai Yi",
                        "slug": "Chucai-Yi",
                        "structuredName": {
                            "firstName": "Chucai",
                            "lastName": "Yi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chucai Yi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35484757"
                        ],
                        "name": "Yingli Tian",
                        "slug": "Yingli-Tian",
                        "structuredName": {
                            "firstName": "Yingli",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yingli Tian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 164
                            }
                        ],
                        "text": "Existing methods for scene text detection can roughly be categorized into three groups: sliding window based methods [6]\u2013[8], connected component based methods [9]\u2013[13], and hybrid methods [14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8284341,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b05b01d031beb0c840279c96e4052b33bbb0742",
            "isKey": false,
            "numCitedBy": 93,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Text-extraction-from-scene-images-by-character-and-Yi-Tian",
            "title": {
                "fragments": [],
                "text": "Text extraction from scene images by character appearance and structure modeling"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2463454"
                        ],
                        "name": "H. Koo",
                        "slug": "H.-Koo",
                        "structuredName": {
                            "firstName": "Hyung",
                            "lastName": "Koo",
                            "middleNames": [
                                "Il"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Koo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111426782"
                        ],
                        "name": "Duck Hoon Kim",
                        "slug": "Duck-Hoon-Kim",
                        "structuredName": {
                            "firstName": "Duck",
                            "lastName": "Kim",
                            "middleNames": [
                                "Hoon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Duck Hoon Kim"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7486022,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7b45fdcdfb642262cf5b2b2b9e574ba47f471d8a",
            "isKey": false,
            "numCitedBy": 239,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a new scene text detection algorithm based on two machine learning classifiers: one allows us to generate candidate word regions and the other filters out nontext ones. To be precise, we extract connected components (CCs) in images by using the maximally stable extremal region algorithm. These extracted CCs are partitioned into clusters so that we can generate candidate regions. Unlike conventional methods relying on heuristic rules in clustering, we train an AdaBoost classifier that determines the adjacency relationship and cluster CCs by using their pairwise relations. Then we normalize candidate word regions and determine whether each region contains text or not. Since the scale, skew, and color of each candidate can be estimated from CCs, we develop a text/nontext classifier for normalized images. This classifier is based on multilayer perceptrons and we can control recall and precision rates with a single free parameter. Finally, we extend our approach to exploit multichannel information. Experimental results on ICDAR 2005 and 2011 robust reading competition datasets show that our method yields the state-of-the-art performance both in speed and accuracy."
            },
            "slug": "Scene-Text-Detection-via-Connected-Component-and-Koo-Kim",
            "title": {
                "fragments": [],
                "text": "Scene Text Detection via Connected Component Clustering and Nontext Filtering"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A new scene text detection algorithm based on two machine learning classifiers that allows us to generate candidate word regions and the other filters out nontext ones, and extends the approach to exploit multichannel information."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145532509"
                        ],
                        "name": "Luk\u00e1s Neumann",
                        "slug": "Luk\u00e1s-Neumann",
                        "structuredName": {
                            "firstName": "Luk\u00e1s",
                            "lastName": "Neumann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luk\u00e1s Neumann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145564537"
                        ],
                        "name": "Jiri Matas",
                        "slug": "Jiri-Matas",
                        "structuredName": {
                            "firstName": "Jiri",
                            "lastName": "Matas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiri Matas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Scene text detection, maximally stable extremal regions, single-link clustering, distance metric learning"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 87
                            }
                        ],
                        "text": "An AdaBoost classifier is trained to decide whether an text candidate corresponding to the true text or not [28]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7249393,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68cc1d94c229c91e71efe6f3e2dcdc4ee2101196",
            "isKey": false,
            "numCitedBy": 166,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "An efficient method for text localization and recognition in real-world images is proposed. Thanks to effective pruning, it is able to exhaustively search the space of all character sequences in real time (200ms on a 640x480 image). The method exploits higher-order properties of text such as word text lines. We demonstrate that the grouping stage plays a key role in the text localization performance and that a robust and precise grouping stage is able to compensate errors of the character detector. The method includes a novel selector of Maximally Stable Extremal Regions (MSER) which exploits region topology. Experimental validation shows that 95.7% characters in the ICDAR dataset are detected using the novel selector of MSERs with a low sensitivity threshold. The proposed method was evaluated on the standard ICDAR 2003 dataset where it achieved state-of-the-art results in both text localization and recognition."
            },
            "slug": "Text-Localization-in-Real-World-Images-Using-Pruned-Neumann-Matas",
            "title": {
                "fragments": [],
                "text": "Text Localization in Real-World Images Using Efficiently Pruned Exhaustive Search"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "It is demonstrated that the grouping stage plays a key role in the text localization performance and that a robust and precise grouping stage is able to compensate errors of the character detector."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Document Analysis and Recognition"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49177577"
                        ],
                        "name": "Huizhong Chen",
                        "slug": "Huizhong-Chen",
                        "structuredName": {
                            "firstName": "Huizhong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huizhong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778174"
                        ],
                        "name": "Sam S. Tsai",
                        "slug": "Sam-S.-Tsai",
                        "structuredName": {
                            "firstName": "Sam",
                            "lastName": "Tsai",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sam S. Tsai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701294"
                        ],
                        "name": "Georg Schroth",
                        "slug": "Georg-Schroth",
                        "structuredName": {
                            "firstName": "Georg",
                            "lastName": "Schroth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Georg Schroth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12600623"
                        ],
                        "name": "David M. Chen",
                        "slug": "David-M.-Chen",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Chen",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David M. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2026212"
                        ],
                        "name": "R. Grzeszczuk",
                        "slug": "R.-Grzeszczuk",
                        "structuredName": {
                            "firstName": "Radek",
                            "lastName": "Grzeszczuk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Grzeszczuk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739786"
                        ],
                        "name": "B. Girod",
                        "slug": "B.-Girod",
                        "structuredName": {
                            "firstName": "Bernd",
                            "lastName": "Girod",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Girod"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "lly, CC-based methods and hybrid methods, is how to group character candidates into text candidates. The existing methods for text candidates construction fall into two general approaches: rule-based [16]\u2013 [18] and clustering-based methods [12]. Neumann and Matas [18] grouped character candidates using the text line constrains, whose basic assumption is that characters in a word can be \ufb01tted by one or"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "\ufb01ltered edges by running a set of tests (edge angle, relative position and size difference of adjacent character candidates) and used the remaining connected subgraphs as text candidates. Chen et al. [16] pairwised character candidates as clusters by putting constrains on stroke width and height difference; they then exploited a straight line to \ufb01t to the centroids of clusters and declared a line as t"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11311196,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6cb3153e5773053916a27bf3ab4530705a6bcf80",
            "isKey": false,
            "numCitedBy": 444,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Detecting text in natural images is an important prerequisite. In this paper, we propose a novel text detection algorithm, which employs edge-enhanced Maximally Stable Extremal Regions as basic letter candidates. These candidates are then filtered using geometric and stroke width information to exclude non-text objects. Letters are paired to identify text lines, which are subsequently separated into words. We evaluate our system using the ICDAR competition dataset and our mobile document database. The experimental results demonstrate the excellent performance of the proposed method."
            },
            "slug": "Robust-text-detection-in-natural-images-with-Stable-Chen-Tsai",
            "title": {
                "fragments": [],
                "text": "Robust text detection in natural images with edge-enhanced Maximally Stable Extremal Regions"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A novel text detection algorithm is proposed, which employs edge-enhanced Maximally Stable Extremal Regions as basic letter candidates and Letters are paired to identify text lines, which are subsequently separated into words."
            },
            "venue": {
                "fragments": [],
                "text": "2011 18th IEEE International Conference on Image Processing"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715066"
                        ],
                        "name": "T. Phan",
                        "slug": "T.-Phan",
                        "structuredName": {
                            "firstName": "Trung",
                            "lastName": "Phan",
                            "middleNames": [
                                "Quy"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Phan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744575"
                        ],
                        "name": "P. Shivakumara",
                        "slug": "P.-Shivakumara",
                        "structuredName": {
                            "firstName": "Palaiahnakote",
                            "lastName": "Shivakumara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Shivakumara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679749"
                        ],
                        "name": "C. Tan",
                        "slug": "C.-Tan",
                        "structuredName": {
                            "firstName": "Chew",
                            "lastName": "Tan",
                            "middleNames": [
                                "Lim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "\u2019s method [36] which also involves empirically selecting many parameters."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7268498,
            "fieldsOfStudy": [
                "Computer Science",
                "Art"
            ],
            "id": "275b27f86971cb4380b1caac79ad5825c1730759",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of text detection in natural scene images is challenging because of the unconstrained sizes, colors, backgrounds and alignments of the characters. This paper proposes novel symmetry features for this task. Within a text line, the intra-character symmetry captures the correspondence between the inner contour and the outer contour of a character while the inter-character symmetry helps to extract information from the gap region between two consecutive characters. A formulation based on Gradient Vector Flow is used to detect both types of symmetry points. These points are then grouped into text lines using the consistency in sizes, colors, and stroke and gap thickness. Therefore, unlike most existing methods which use only character features, our method exploits both the text features and the gap features to improve the detection result. Experimentally, our method compares well to the state-of-the-art on public datasets for natural scenes and street-level images, an emerging category of image data. The proposed technique can be used in a wide range of multimedia applications such as content-based image/video retrieval, mobile visual search and sign translation."
            },
            "slug": "Detecting-text-in-the-real-world-Phan-Shivakumara",
            "title": {
                "fragments": [],
                "text": "Detecting text in the real world"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The proposed novel symmetry features can be used in a wide range of multimedia applications such as content-based image/video retrieval, mobile visual search and sign translation."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Multimedia"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145532509"
                        ],
                        "name": "Luk\u00e1s Neumann",
                        "slug": "Luk\u00e1s-Neumann",
                        "structuredName": {
                            "firstName": "Luk\u00e1s",
                            "lastName": "Neumann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luk\u00e1s Neumann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145564537"
                        ],
                        "name": "Jiri Matas",
                        "slug": "Jiri-Matas",
                        "structuredName": {
                            "firstName": "Jiri",
                            "lastName": "Matas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiri Matas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 450338,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "da8154af82fd62944399fc7fad65e44d82ee9ee2",
            "isKey": false,
            "numCitedBy": 511,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "A general method for text localization and recognition in real-world images is presented. The proposed method is novel, as it (i) departs from a strict feed-forward pipeline and replaces it by a hypothesesverification framework simultaneously processing multiple text line hypotheses, (ii) uses synthetic fonts to train the algorithm eliminating the need for time-consuming acquisition and labeling of real-world training data and (iii) exploits Maximally Stable Extremal Regions (MSERs) which provides robustness to geometric and illumination conditions. \n \nThe performance of the method is evaluated on two standard datasets. On the Char74k dataset, a recognition rate of 72% is achieved, 18% higher than the state-of-the-art. The paper is first to report both text detection and recognition results on the standard and rather challenging ICDAR 2003 dataset. The text localization works for number of alphabets and the method is easily adapted to recognition of other scripts, e.g. cyrillics."
            },
            "slug": "A-Method-for-Text-Localization-and-Recognition-in-Neumann-Matas",
            "title": {
                "fragments": [],
                "text": "A Method for Text Localization and Recognition in Real-World Images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The paper is first to report both text detection and recognition results on the standard and rather challenging ICDAR 2003 dataset, and the text localization works for number of alphabets and the method is easily adapted to recognition of other scripts, e.g. cyrillics."
            },
            "venue": {
                "fragments": [],
                "text": "ACCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403970934"
                        ],
                        "name": "C. Mancas-Thillou",
                        "slug": "C.-Mancas-Thillou",
                        "structuredName": {
                            "firstName": "C\u00e9line",
                            "lastName": "Mancas-Thillou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Mancas-Thillou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50276543"
                        ],
                        "name": "B. Gosselin",
                        "slug": "B.-Gosselin",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Gosselin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Gosselin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "fore being recognized or retrieved. Existing methods for scene text detection can roughly be categorized into three groups: sliding window based methods [6]\u2013[8], connected component based methods [9]\u2013[11], and hybrid methods [12]. Sliding window based methods, also called as region based methods, engage a sliding window to search for possible texts in the image and then use machine learning techniques"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 42539643,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0aaca7527d703a6945ba73ce15e7e7353258fc8a",
            "isKey": false,
            "numCitedBy": 92,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Color-text-extraction-with-selective-metric-based-Mancas-Thillou-Gosselin",
            "title": {
                "fragments": [],
                "text": "Color text extraction with selective metric-based clustering"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40263913"
                        ],
                        "name": "Chucai Yi",
                        "slug": "Chucai-Yi",
                        "structuredName": {
                            "firstName": "Chucai",
                            "lastName": "Yi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chucai Yi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35484757"
                        ],
                        "name": "Yingli Tian",
                        "slug": "Yingli-Tian",
                        "structuredName": {
                            "firstName": "Yingli",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yingli Tian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12572576,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ba5deb71a5fb1a0a6be79e068ac6754e0c990e3",
            "isKey": false,
            "numCitedBy": 138,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a novel framework to extract text regions from scene images with complex backgrounds and multiple text appearances. This framework consists of three main steps: boundary clustering (BC), stroke segmentation, and string fragment classification. In BC, we propose a new bigram-color-uniformity-based method to model both text and attachment surface, and cluster edge pixels based on color pairs and spatial positions into boundary layers. Then, stroke segmentation is performed at each boundary layer by color assignment to extract character candidates. We propose two algorithms to combine the structural analysis of text stroke with color assignment and filter out background interferences. Further, we design a robust string fragment classification based on Gabor-based text features. The features are obtained from feature maps of gradient, stroke distribution, and stroke width. The proposed framework of text localization is evaluated on scene images, born-digital images, broadcast video images, and images of handheld objects captured by blind persons. Experimental results on respective datasets demonstrate that the framework outperforms state-of-the-art localization algorithms."
            },
            "slug": "Localizing-Text-in-Scene-Images-by-Boundary-Stroke-Yi-Tian",
            "title": {
                "fragments": [],
                "text": "Localizing Text in Scene Images by Boundary Clustering, Stroke Segmentation, and String Fragment Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The proposed framework of text localization is evaluated on scene images, born-digital images, broadcast video images, and images of handheld objects captured by blind persons and demonstrates that the framework outperforms state-of-the-art localization algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146721"
                        ],
                        "name": "C. Yao",
                        "slug": "C.-Yao",
                        "structuredName": {
                            "firstName": "Cong",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145905113"
                        ],
                        "name": "X. Bai",
                        "slug": "X.-Bai",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Bai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Bai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743698"
                        ],
                        "name": "Wenyu Liu",
                        "slug": "Wenyu-Liu",
                        "structuredName": {
                            "firstName": "Wenyu",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenyu Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146275501"
                        ],
                        "name": "Yi Ma",
                        "slug": "Yi-Ma",
                        "structuredName": {
                            "firstName": "Yi",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi Ma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144035504"
                        ],
                        "name": "Z. Tu",
                        "slug": "Z.-Tu",
                        "structuredName": {
                            "firstName": "Zhuowen",
                            "lastName": "Tu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Tu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "This SWT algorithm has been used in several recent research works [9], [38] and implemented in a popular computer vision library the libccv7."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14015069,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "955028d46ab7237a30cfaab3a351c34f38ee0be5",
            "isKey": false,
            "numCitedBy": 635,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "With the increasing popularity of practical vision systems and smart phones, text detection in natural scenes becomes a critical yet challenging task. Most existing methods have focused on detecting horizontal or near-horizontal texts. In this paper, we propose a system which detects texts of arbitrary orientations in natural images. Our algorithm is equipped with a two-level classification scheme and two sets of features specially designed for capturing both the intrinsic characteristics of texts. To better evaluate our algorithm and compare it with other competing algorithms, we generate a new dataset, which includes various texts in diverse real-world scenarios; we also propose a protocol for performance evaluation. Experiments on benchmark datasets and the proposed dataset demonstrate that our algorithm compares favorably with the state-of-the-art algorithms when handling horizontal texts and achieves significantly enhanced performance on texts of arbitrary orientations in complex natural scenes."
            },
            "slug": "Detecting-texts-of-arbitrary-orientations-in-images-Yao-Bai",
            "title": {
                "fragments": [],
                "text": "Detecting texts of arbitrary orientations in natural images"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A system which detects texts of arbitrary orientations in natural images using a two-level classification scheme and two sets of features specially designed for capturing both the intrinsic characteristics of texts to better evaluate its algorithm and compare it with other competing algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3200914"
                        ],
                        "name": "Asif Shahab",
                        "slug": "Asif-Shahab",
                        "structuredName": {
                            "firstName": "Asif",
                            "lastName": "Shahab",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Asif Shahab"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688013"
                        ],
                        "name": "F. Shafait",
                        "slug": "F.-Shafait",
                        "structuredName": {
                            "firstName": "Faisal",
                            "lastName": "Shafait",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Shafait"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145279674"
                        ],
                        "name": "A. Dengel",
                        "slug": "A.-Dengel",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Dengel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dengel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 95
                            }
                        ],
                        "text": "The ICDAR 2011 Robust Reading Competition (Challenge 2: Reading Text in Scene Images) database [17] is a widely used database2 for benchmarking scene text detection algorithms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 121
                            }
                        ],
                        "text": "MSER-based methods have reported promising performance on the widely used ICDAR 2011 Robust Reading Competition database [17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 226
                            }
                        ],
                        "text": "More recently, Maximally Stable Extremal Regions (MSERs) based methods, which can be categorized as connected component based methods but using MSERs [16] as character candidates, have become the focus of several recent works [17]\u2013[23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "Please note that without specification, we use the definitions in ICDAR 2011 competition [17] for text detection precision, recall and f measure calculation; and the speed (s) for each scheme"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1468345,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f4edbb12d346e873ca1faeff959aa7d4809495f",
            "isKey": true,
            "numCitedBy": 431,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Recognition of text in natural scene images is becoming a prominent research area due to the widespread availablity of imaging devices in low-cost consumer products like mobile phones. To evaluate the performance of recent algorithms in detecting and recognizing text from complex images, the ICDAR 2011 Robust Reading Competition was organized. Challenge 2 of the competition dealt specifically with detecting/recognizing text in natural scene images. This paper presents an overview of the approaches that the participants used, the evaluation measure, and the dataset used in the Challenge 2 of the contest. We also report the performance of all participating methods for text localization and word recognition tasks and compare their results using standard methods of area precision/recall and edit distance."
            },
            "slug": "ICDAR-2011-Robust-Reading-Competition-Challenge-2:-Shahab-Shafait",
            "title": {
                "fragments": [],
                "text": "ICDAR 2011 Robust Reading Competition Challenge 2: Reading Text in Scene Images"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An overview of the approaches that the participants used, the evaluation measure, and the dataset used in the ICDAR 2011 Robust Reading Competition for detecting/recognizing text in natural scene images is presented."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Document Analysis and Recognition"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144691984"
                        ],
                        "name": "Jung-Jin Lee",
                        "slug": "Jung-Jin-Lee",
                        "structuredName": {
                            "firstName": "Jung-Jin",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jung-Jin Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33427704"
                        ],
                        "name": "Pyoung-Hean Lee",
                        "slug": "Pyoung-Hean-Lee",
                        "structuredName": {
                            "firstName": "Pyoung-Hean",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pyoung-Hean Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50112753"
                        ],
                        "name": "Seong-Whan Lee",
                        "slug": "Seong-Whan-Lee",
                        "structuredName": {
                            "firstName": "Seong-Whan",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seong-Whan Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082451791"
                        ],
                        "name": "Christof Koch",
                        "slug": "Christof-Koch",
                        "structuredName": {
                            "firstName": "Christof",
                            "lastName": "Koch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christof Koch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 195727,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d5e40b5de129bcaf5bc5a56a23a95d71538c980",
            "isKey": false,
            "numCitedBy": 178,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Detecting text regions in natural scenes is an important part of computer vision. We propose a novel text detection algorithm that extracts six different classes features of text, and uses Modest AdaBoost with multi-scale sequential search. Experiments show that our algorithm can detect text regions with a f= 0.70, from the ICDAR 2003 datasets which include images with text of various fonts, sizes, colors, alphabets and scripts."
            },
            "slug": "AdaBoost-for-Text-Detection-in-Natural-Scene-Lee-Lee",
            "title": {
                "fragments": [],
                "text": "AdaBoost for Text Detection in Natural Scene"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "A novel text detection algorithm is proposed that extracts six different classes features of text, and uses Modest AdaBoost with multi-scale sequential search and can detect text regions with a f= 0.70 from the ICDAR 2003 datasets."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Document Analysis and Recognition"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121267347"
                        ],
                        "name": "K. Jung",
                        "slug": "K.-Jung",
                        "structuredName": {
                            "firstName": "Keechul",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144602022"
                        ],
                        "name": "K. Kim",
                        "slug": "K.-Kim",
                        "structuredName": {
                            "firstName": "Kwang",
                            "lastName": "Kim",
                            "middleNames": [
                                "In"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Scene text detection, maximally stable extremal regions, single-link clustering, distance metric learning"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5999466,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cedf72be1fe814ef2ee9d65633dc3226f80f0785",
            "isKey": false,
            "numCitedBy": 936,
            "numCiting": 100,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Text-information-extraction-in-images-and-video:-a-Jung-Kim",
            "title": {
                "fragments": [],
                "text": "Text information extraction in images and video: a survey"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2862313"
                        ],
                        "name": "J. Weinman",
                        "slug": "J.-Weinman",
                        "structuredName": {
                            "firstName": "Jerod",
                            "lastName": "Weinman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weinman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389846455"
                        ],
                        "name": "E. Learned-Miller",
                        "slug": "E.-Learned-Miller",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Learned-Miller",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Learned-Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733922"
                        ],
                        "name": "A. Hanson",
                        "slug": "A.-Hanson",
                        "structuredName": {
                            "firstName": "Allen",
                            "lastName": "Hanson",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hanson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5416971,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4b2a523d48cee04c09c327e14fb8928c5feff03c",
            "isKey": false,
            "numCitedBy": 193,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Scene text recognition (STR) is the recognition of text anywhere in the environment, such as signs and storefronts. Relative to document recognition, it is challenging because of font variability, minimal language context, and uncontrolled conditions. Much information available to solve this problem is frequently ignored or used sequentially. Similarity between character images is often overlooked as useful information. Because of language priors, a recognizer may assign different labels to identical characters. Directly comparing characters to each other, rather than only a model, helps ensure that similar instances receive the same label. Lexicons improve recognition accuracy but are used post hoc. We introduce a probabilistic model for STR that integrates similarity, language properties, and lexical decision. Inference is accelerated with sparse belief propagation, a bottom-up method for shortening messages by reducing the dependency between weakly supported hypotheses. By fusing information sources in one model, we eliminate unrecoverable errors that result from sequential processing, improving accuracy. In experimental results recognizing text from images of signs in outdoor scenes, incorporating similarity reduces character recognition error by 19 percent, the lexicon reduces word recognition error by 35 percent, and sparse belief propagation reduces the lexicon words considered by 99.9 percent with a 12X speedup and no loss in accuracy."
            },
            "slug": "Scene-Text-Recognition-Using-Similarity-and-a-with-Weinman-Learned-Miller",
            "title": {
                "fragments": [],
                "text": "Scene Text Recognition Using Similarity and a Lexicon with Sparse Belief Propagation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A probabilistic model for scene text recognition is introduced that integrates similarity, language properties, and lexical decision and is fusing information sources in one model to eliminate unrecoverable errors that result from sequential processing, improving accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145820427"
                        ],
                        "name": "Fei Yin",
                        "slug": "Fei-Yin",
                        "structuredName": {
                            "firstName": "Fei",
                            "lastName": "Yin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fei Yin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689269"
                        ],
                        "name": "Cheng-Lin Liu",
                        "slug": "Cheng-Lin-Liu",
                        "structuredName": {
                            "firstName": "Cheng-Lin",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cheng-Lin Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Scene text detection, maximally stable extremal regions, single-link clustering, distance metric learning"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4651364,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "559d7f70d6bb0a5f81790b449e415d919acc2f28",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Handwritten-Chinese-text-line-segmentation-by-with-Yin-Liu",
            "title": {
                "fragments": [],
                "text": "Handwritten Chinese text line segmentation by clustering with distance metric learning"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148896777"
                        ],
                        "name": "Kai Wang",
                        "slug": "Kai-Wang",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2490700"
                        ],
                        "name": "Boris Babenko",
                        "slug": "Boris-Babenko",
                        "structuredName": {
                            "firstName": "Boris",
                            "lastName": "Babenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Boris Babenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[37] (the whole database) to train our character classifier and profile the system on Epshtein et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14136313,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "32b8f58a038df83138435b12a499c8bf0de13811",
            "isKey": false,
            "numCitedBy": 909,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper focuses on the problem of word detection and recognition in natural images. The problem is significantly more challenging than reading text in scanned documents, and has only recently gained attention from the computer vision community. Sub-components of the problem, such as text detection and cropped image word recognition, have been studied in isolation [7, 4, 20]. However, what is unclear is how these recent approaches contribute to solving the end-to-end problem of word recognition. We fill this gap by constructing and evaluating two systems. The first, representing the de facto state-of-the-art, is a two stage pipeline consisting of text detection followed by a leading OCR engine. The second is a system rooted in generic object recognition, an extension of our previous work in [20]. We show that the latter approach achieves superior performance. While scene text recognition has generally been treated with highly domain-specific methods, our results demonstrate the suitability of applying generic computer vision methods. Adopting this approach opens the door for real world scene text recognition to benefit from the rapid advances that have been taking place in object recognition."
            },
            "slug": "End-to-end-scene-text-recognition-Wang-Babenko",
            "title": {
                "fragments": [],
                "text": "End-to-end scene text recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "While scene text recognition has generally been treated with highly domain-specific methods, the results demonstrate the suitability of applying generic computer vision methods."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744575"
                        ],
                        "name": "P. Shivakumara",
                        "slug": "P.-Shivakumara",
                        "structuredName": {
                            "firstName": "Palaiahnakote",
                            "lastName": "Shivakumara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Shivakumara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715066"
                        ],
                        "name": "T. Phan",
                        "slug": "T.-Phan",
                        "structuredName": {
                            "firstName": "Trung",
                            "lastName": "Phan",
                            "middleNames": [
                                "Quy"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Phan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679749"
                        ],
                        "name": "C. Tan",
                        "slug": "C.-Tan",
                        "structuredName": {
                            "firstName": "Chew",
                            "lastName": "Tan",
                            "middleNames": [
                                "Lim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 237,
                                "start": 234
                            }
                        ],
                        "text": "TEXT in images contains valuable information and is exploited in many content-based image and video applications, such as content-based web image search, video information retrieval, and mobile based text analysis and recognition [1]\u2013[5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 196066575,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1755d99d89dee5915df1df4a7991b87138e93c78",
            "isKey": false,
            "numCitedBy": 296,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a method based on the Laplacian in the frequency domain for video text detection. Unlike many other approaches which assume that text is horizontally-oriented, our method is able to handle text of arbitrary orientation. The input image is first filtered with Fourier-Laplacian. K-means clustering is then used to identify candidate text regions based on the maximum difference. The skeleton of each connected component helps to separate the different text strings from each other. Finally, text string straightness and edge density are used for false positive elimination. Experimental results show that the proposed method is able to handle graphics text and scene text of both horizontal and nonhorizontal orientation."
            },
            "slug": "A-Laplacian-Approach-to-Multi-Oriented-Text-in-Shivakumara-Phan",
            "title": {
                "fragments": [],
                "text": "A Laplacian Approach to Multi-Oriented Text Detection in Video"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Experimental results show that the proposed method is able to handle graphics text and scene text of both horizontal and nonhorizontal orientation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144602022"
                        ],
                        "name": "K. Kim",
                        "slug": "K.-Kim",
                        "structuredName": {
                            "firstName": "Kwang",
                            "lastName": "Kim",
                            "middleNames": [
                                "In"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121267347"
                        ],
                        "name": "K. Jung",
                        "slug": "K.-Jung",
                        "structuredName": {
                            "firstName": "Keechul",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152675651"
                        ],
                        "name": "J. Kim",
                        "slug": "J.-Kim",
                        "structuredName": {
                            "firstName": "Jin",
                            "lastName": "Kim",
                            "middleNames": [
                                "Hyung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kim"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17901853,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14bbcdc1744cc5982ffb64ea4755a72921d98d08",
            "isKey": false,
            "numCitedBy": 503,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "The current paper presents a novel texture-based method for detecting texts in images. A support vector machine (SVM) is used to analyze the textural properties of texts. No external texture feature extraction module is used, but rather the intensities of the raw pixels that make up the textural pattern are fed directly to the SVM, which works well even in high-dimensional spaces. Next, text regions are identified by applying a continuously adaptive mean shift algorithm (CAMSHIFT) to the results of the texture analysis. The combination of CAMSHIFT and SVMs produces both robust and efficient text detection, as time-consuming texture analyses for less relevant pixels are restricted, leaving only a small part of the input image to be texture-analyzed."
            },
            "slug": "Texture-Based-Approach-for-Text-Detection-in-Images-Kim-Jung",
            "title": {
                "fragments": [],
                "text": "Texture-Based Approach for Text Detection in Images Using Support Vector Machines and Continuously Adaptive Mean Shift Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The combination of CAMSHIFT and SVMs produces both robust and efficient text detection, as time-consuming texture analyses for less relevant pixels are restricted, leaving only a small part of the input image to be texture-analyzed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3126798"
                        ],
                        "name": "B. Epshtein",
                        "slug": "B.-Epshtein",
                        "structuredName": {
                            "firstName": "Boris",
                            "lastName": "Epshtein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Epshtein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20592981"
                        ],
                        "name": "E. Ofek",
                        "slug": "E.-Ofek",
                        "structuredName": {
                            "firstName": "Eyal",
                            "lastName": "Ofek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Ofek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743988"
                        ],
                        "name": "Y. Wexler",
                        "slug": "Y.-Wexler",
                        "structuredName": {
                            "firstName": "Yonatan",
                            "lastName": "Wexler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Wexler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 82
                            }
                        ],
                        "text": "We first compare the performance of our proposed algorithm with the SWT algorithm [9]9 on the ICDAR 2011 training set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 241
                            }
                        ],
                        "text": "In this section we present the performance comparison of three character extraction algorithms: our MSER-based pruning algorithm, the latest pruning method proposed by Neumann and Matas (the ER-based algorithm in [20]) and the SWT algorithm [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[9] with Stroke Width Transform (SWT) is language neutral and does not involve a classifier for character and text identification."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[9], which is even harder than the ICDAR database."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 160
                            }
                        ],
                        "text": "Existing methods for scene text detection can roughly be categorized into three groups: sliding window based methods [6]\u2013[8], connected component based methods [9]\u2013[13], and hybrid methods [14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 66
                            }
                        ],
                        "text": "This SWT algorithm has been used in several recent research works [9], [38] and implemented in a popular computer vision library the libccv7."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8890220,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "39c4ae83b5c92e0fa55de1ec7e5cf12589c408db",
            "isKey": true,
            "numCitedBy": 1470,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel image operator that seeks to find the value of stroke width for each image pixel, and demonstrate its use on the task of text detection in natural images. The suggested operator is local and data dependent, which makes it fast and robust enough to eliminate the need for multi-scale computation or scanning windows. Extensive testing shows that the suggested scheme outperforms the latest published algorithms. Its simplicity allows the algorithm to detect texts in many fonts and languages."
            },
            "slug": "Detecting-text-in-natural-scenes-with-stroke-width-Epshtein-Ofek",
            "title": {
                "fragments": [],
                "text": "Detecting text in natural scenes with stroke width transform"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A novel image operator is presented that seeks to find the value of stroke width for each image pixel, and its use on the task of text detection in natural images is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115196978"
                        ],
                        "name": "Huiping Li",
                        "slug": "Huiping-Li",
                        "structuredName": {
                            "firstName": "Huiping",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huiping Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3272081"
                        ],
                        "name": "O. Kia",
                        "slug": "O.-Kia",
                        "structuredName": {
                            "firstName": "Omid",
                            "lastName": "Kia",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Kia"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15485643,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f8f5c282dc11937d29183b955dc3e4fbb677571b",
            "isKey": false,
            "numCitedBy": 652,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "Text that appears in a scene or is graphically added to video can provide an important supplemental source of index information as well as clues for decoding the video's structure and for classification. In this work, we present algorithms for detecting and tracking text in digital video. Our system implements a scale-space feature extractor that feeds an artificial neural processor to detect text blocks. Our text tracking scheme consists of two modules: a sum of squared difference (SSD)-based module to find the initial position and a contour-based module to refine the position. Experiments conducted with a variety of video sources show that our scheme can detect and track text robustly."
            },
            "slug": "Automatic-text-detection-and-tracking-in-digital-Li-Doermann",
            "title": {
                "fragments": [],
                "text": "Automatic text detection and tracking in digital video"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work presents algorithms for detecting and tracking text in digital video that implements a scale-space feature extractor that feeds an artificial neural processor to detect text blocks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682664"
                        ],
                        "name": "Xu-Cheng Yin",
                        "slug": "Xu-Cheng-Yin",
                        "structuredName": {
                            "firstName": "Xu-Cheng",
                            "lastName": "Yin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xu-Cheng Yin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143680014"
                        ],
                        "name": "Hongwei Hao",
                        "slug": "Hongwei-Hao",
                        "structuredName": {
                            "firstName": "Hongwei",
                            "lastName": "Hao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hongwei Hao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144291081"
                        ],
                        "name": "Jun Sun",
                        "slug": "Jun-Sun",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jun Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753831"
                        ],
                        "name": "S. Naoi",
                        "slug": "S.-Naoi",
                        "structuredName": {
                            "firstName": "Satoshi",
                            "lastName": "Naoi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Naoi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11008288,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0bfd927854b1cfd649b02fd2a5cb6266a39ec106",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Document images captured by a mobile phone camera often have perspective distortions. In this paper, fast and robust vanishing point detection methods for such perspective documents are presented. Most of previous methods are either slow or unstable. Based on robust detection of text baselines and character tilt orientations, our proposed technology is fast and robust with the following features: (1) quick detection of vanishing point candidates by clustering and voting on the Gaussian sphere space, and (2) precise and efficient detection of the final vanishing points using a hybrid approach, which combines the results from clustering and projection analysis. The rectified image acceptance rate for Mobile Cam-based documents, signboards and posters is more than 98% with an average speed of about 100ms."
            },
            "slug": "Robust-Vanishing-Point-Detection-for-Documents-Yin-Hao",
            "title": {
                "fragments": [],
                "text": "Robust Vanishing Point Detection for MobileCam-Based Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The proposed technology is fast and robust with the following features: quick detection of vanishing point candidates by clustering and voting on the Gaussian sphere space, and precise and efficient detection of the final vanishing points using a hybrid approach, which combines the results from clusters and projection analysis."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Document Analysis and Recognition"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145815031"
                        ],
                        "name": "S. Lucas",
                        "slug": "S.-Lucas",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Lucas",
                            "middleNames": [
                                "M.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lucas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87531536"
                        ],
                        "name": "A. Panaretos",
                        "slug": "A.-Panaretos",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Panaretos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Panaretos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073684197"
                        ],
                        "name": "Luis Sosa",
                        "slug": "Luis-Sosa",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Sosa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luis Sosa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052189571"
                        ],
                        "name": "Anthony Tang",
                        "slug": "Anthony-Tang",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anthony Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108862960"
                        ],
                        "name": "Shirley Wong",
                        "slug": "Shirley-Wong",
                        "structuredName": {
                            "firstName": "Shirley",
                            "lastName": "Wong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shirley Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114080648"
                        ],
                        "name": "Robert Young",
                        "slug": "Robert-Young",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Young",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Young"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065499715"
                        ],
                        "name": "Kazuki Ashida",
                        "slug": "Kazuki-Ashida",
                        "structuredName": {
                            "firstName": "Kazuki",
                            "lastName": "Ashida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kazuki Ashida"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055905787"
                        ],
                        "name": "Hiroki Nagai",
                        "slug": "Hiroki-Nagai",
                        "structuredName": {
                            "firstName": "Hiroki",
                            "lastName": "Nagai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hiroki Nagai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47471571"
                        ],
                        "name": "Masayuki Okamoto",
                        "slug": "Masayuki-Okamoto",
                        "structuredName": {
                            "firstName": "Masayuki",
                            "lastName": "Okamoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Masayuki Okamoto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152933693"
                        ],
                        "name": "Hiroaki Yamamoto",
                        "slug": "Hiroaki-Yamamoto",
                        "structuredName": {
                            "firstName": "Hiroaki",
                            "lastName": "Yamamoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hiroaki Yamamoto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34593830"
                        ],
                        "name": "H. Miyao",
                        "slug": "H.-Miyao",
                        "structuredName": {
                            "firstName": "Hidetoshi",
                            "lastName": "Miyao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Miyao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146280843"
                        ],
                        "name": "JunMin Zhu",
                        "slug": "JunMin-Zhu",
                        "structuredName": {
                            "firstName": "JunMin",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "JunMin Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2873679"
                        ],
                        "name": "WuWen Ou",
                        "slug": "WuWen-Ou",
                        "structuredName": {
                            "firstName": "WuWen",
                            "lastName": "Ou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "WuWen Ou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144899680"
                        ],
                        "name": "Christian Wolf",
                        "slug": "Christian-Wolf",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Wolf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Wolf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680935"
                        ],
                        "name": "J. Jolion",
                        "slug": "J.-Jolion",
                        "structuredName": {
                            "firstName": "Jean-Michel",
                            "lastName": "Jolion",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Jolion"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1844464"
                        ],
                        "name": "L. Todoran",
                        "slug": "L.-Todoran",
                        "structuredName": {
                            "firstName": "Leon",
                            "lastName": "Todoran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Todoran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717056"
                        ],
                        "name": "M. Worring",
                        "slug": "M.-Worring",
                        "structuredName": {
                            "firstName": "Marcel",
                            "lastName": "Worring",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Worring"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46520089"
                        ],
                        "name": "Xiaofan Lin",
                        "slug": "Xiaofan-Lin",
                        "structuredName": {
                            "firstName": "Xiaofan",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaofan Lin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 103
                            }
                        ],
                        "text": "The definition of character-level recall is similar to the recall definition in ICDAR 2003 competition [39], except that the bounding rectangle is character level rather than text level."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 2250003,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a01deac56a81646e8d84cb7bf2d905714ff00808",
            "isKey": false,
            "numCitedBy": 235,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract.This paper describes the robust reading competitions for ICDAR 2003. With the rapid growth in research over the last few years on recognizing text in natural scenes, there is an urgent need to establish some common benchmark datasets and gain a clear understanding of the current state of the art. We use the term \u2018robust reading\u2019 to refer to text images that are beyond the capabilities of current commercial OCR packages. We chose to break down the robust reading problem into three subproblems and run competitions for each stage, and also a competition for the best overall system. The subproblems we chose were text locating, character recognition and word recognition. By breaking down the problem in this way, we hoped to gain a better understanding of the state of the art in each of the subproblems. Furthermore, our methodology involved storing detailed results of applying each algorithm to each image in the datasets, allowing researchers to study in depth the strengths and weaknesses of each algorithm. The text-locating contest was the only one to have any entries. We give a brief description of each entry and present the results of this contest, showing cases where the leading entries succeed and fail. We also describe an algorithm for combining the outputs of the individual text locators and show how the combination scheme improves on any of the individual systems."
            },
            "slug": "ICDAR-2003-robust-reading-competitions:-entries,-Lucas-Panaretos",
            "title": {
                "fragments": [],
                "text": "ICDAR 2003 robust reading competitions: entries, results, and future directions"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper broke down the robust reading problem into three subproblems and run competitions for each stage, and also a competition for the best overall system, and described an algorithm for combining the outputs of the individual text locators and showed how the combination scheme improves on any of theindividual systems."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Document Analysis and Recognition (IJDAR)"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403823956"
                        ],
                        "name": "Carlos Merino-Gracia",
                        "slug": "Carlos-Merino-Gracia",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Merino-Gracia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlos Merino-Gracia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3257286"
                        ],
                        "name": "Karel Lenc",
                        "slug": "Karel-Lenc",
                        "structuredName": {
                            "firstName": "Karel",
                            "lastName": "Lenc",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karel Lenc"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728108"
                        ],
                        "name": "M. Mirmehdi",
                        "slug": "M.-Mirmehdi",
                        "structuredName": {
                            "firstName": "Majid",
                            "lastName": "Mirmehdi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mirmehdi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Scene text detection, maximally stable extremal regions, single-link clustering, distance metric learning"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16811390,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b6037f073b1284e6830e0d0b5a9e33f2354591a2",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a mobile head-mounted device for detecting and tracking text that is encased in an ordinary flat-cap hat. The main parts of the device are an integrated camera and audio webcam together with a simple remote control system, all connected via a USB hub to a laptop. A near to real-time text detection algorithm (around 14 fps for 640\u00d7480 images) which uses Maximal Stable Extremal Regions (MSERs) for image segmentation is proposed. Comparative text detection results against the ICDAR 2003 text locating competition database along with performance figures are presented."
            },
            "slug": "A-Head-Mounted-Device-for-Recognizing-Text-in-Merino-Gracia-Lenc",
            "title": {
                "fragments": [],
                "text": "A Head-Mounted Device for Recognizing Text in Natural Scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A near to real-time text detection algorithm which uses Maximal Stable Extremal Regions (MSERs) for image segmentation is proposed and Comparative text detection results against the ICDAR 2003 text locating competition database along with performance figures are presented."
            },
            "venue": {
                "fragments": [],
                "text": "CBDAR"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144899680"
                        ],
                        "name": "Christian Wolf",
                        "slug": "Christian-Wolf",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Wolf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Wolf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680935"
                        ],
                        "name": "J. Jolion",
                        "slug": "J.-Jolion",
                        "structuredName": {
                            "firstName": "Jean-Michel",
                            "lastName": "Jolion",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Jolion"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "er:yfpan. It is worth noting that the evaluation scheme of ICDAR 2011 competition is not the same as of ICDAR 2003 and ICDAR 2005. The new scheme, the object count/area scheme proposed by Wolf et al. [39], is more complicated but offers several enhancements over the old scheme. Basically, these two scheme use the notation of precision, recall and fmeasure that is de\ufb01ned as recall= P jGj i=1 match G(G "
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "ea(D j) + area(G i) : (18) The above matching functions only consider one-to-one matches between groundtruth and detected rectangles, leaving room for ambiguity between detection quantity and quality [39]. In the new evaluation scheme, the matching functions are redesigned considering detection quality and different matching situations (oneto-one matchings, one-to-many matchings and manyto-one matchin"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4106614,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1deaf2a58ac783d1f89ff3b4711f6383c7550a80",
            "isKey": false,
            "numCitedBy": 326,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Evaluation of object detection algorithms is a non-trivial task: a detection result is usually evaluated by comparing the bounding box of the detected object with the bounding box of the ground truth object. The commonly used precision and recall measures are computed from the overlap area of these two rectangles. However, these measures have several drawbacks: they don't give intuitive information about the proportion of the correctly detected objects and the number of false alarms, and they cannot be accumulated across multiple images without creating ambiguity in their interpretation. Furthermore, quantitative and qualitative evaluation is often mixed resulting in ambiguous measures.In this paper we propose a new approach which tackles these problems. The performance of a detection algorithm is illustrated intuitively by performance graphs which present object level precision and recall depending on constraints on detection quality. In order to compare different detection algorithms, a representative single performance value is computed from the graphs. The influence of the test database on the detection performance is illustrated by performance/generality graphs. The evaluation method can be applied to different types of object detection algorithms. It has been tested on different text detection algorithms, among which are the participants of the ICDAR 2003 text detection competition."
            },
            "slug": "Object-count/area-graphs-for-the-evaluation-of-and-Wolf-Jolion",
            "title": {
                "fragments": [],
                "text": "Object count/area graphs for the evaluation of object detection and segmentation algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The performance of a detection algorithm is illustrated intuitively by performance graphs which present object level precision and recall depending on constraints on detection quality, and a representative single performance value is computed from the graphs."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Document Analysis and Recognition (IJDAR)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144752865"
                        ],
                        "name": "Jian Liang",
                        "slug": "Jian-Liang",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Liang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115196978"
                        ],
                        "name": "Huiping Li",
                        "slug": "Huiping-Li",
                        "structuredName": {
                            "firstName": "Huiping",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huiping Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5053740,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82b6f95e805a92887f8efccf5a0dc8d5783676f5",
            "isKey": false,
            "numCitedBy": 457,
            "numCiting": 131,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract.The increasing availability of high-performance, low-priced, portable digital imaging devices has created a tremendous opportunity for supplementing traditional scanning for document image acquisition. Digital cameras attached to cellular phones, PDAs, or wearable computers, and standalone image or video devices are highly mobile and easy to use; they can capture images of thick books, historical manuscripts too fragile to touch, and text in scenes, making them much more versatile than desktop scanners. Should robust solutions to the analysis of documents captured with such devices become available, there will clearly be a demand in many domains. Traditional scanner-based document analysis techniques provide us with a good reference and starting point, but they cannot be used directly on camera-captured images. Camera-captured images can suffer from low resolution, blur, and perspective distortion, as well as complex layout and interaction of the content and background. In this paper we present a survey of application domains, technical challenges, and solutions for the analysis of documents captured by digital cameras. We begin by describing typical imaging devices and the imaging process. We discuss document analysis from a single camera-captured image as well as multiple frames and highlight some sample applications under development and feasible ideas for future development."
            },
            "slug": "Camera-based-analysis-of-text-and-documents:-a-Liang-Doermann",
            "title": {
                "fragments": [],
                "text": "Camera-based analysis of text and documents: a survey"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A survey of application domains, technical challenges, and solutions for the analysis of documents captured by digital cameras, and some sample applications under development and feasible ideas for future development is presented."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Document Analysis and Recognition (IJDAR)"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39319377"
                        ],
                        "name": "Yu Zhong",
                        "slug": "Yu-Zhong",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Zhong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu Zhong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108698841"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 233,
                                "start": 230
                            }
                        ],
                        "text": "TEXT in images contains valuable information and is exploited in many content-based image and video applications, such as content-based web image search, video information retrieval, and mobile based text analysis and recognition [1]\u2013[5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6781817,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f7721138e41d82fedabca59c9a66e67d9b7053f3",
            "isKey": false,
            "numCitedBy": 330,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method to automatically locate captions in MPEG video. Caption text regions are segmented from the background using their distinguishing texture characteristics. This method first locates candidate text regions directly in the DCT compressed domain, and then reconstructs the candidate regions for further refinement in the spatial domain. Therefore, only a small amount of decoding is required. The proposed algorithm achieves about 4.0% false reject rate and less than 5.7% false positive rate on a variety of MPEG compressed video containing more than 42,000 frames."
            },
            "slug": "Automatic-caption-localization-in-compressed-video-Zhong-Zhang",
            "title": {
                "fragments": [],
                "text": "Automatic caption localization in compressed video"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This method first locates candidate text regions directly in the DCT compressed domain, and then reconstructs the candidate regions for further refinement in the spatial domain, so that only a small amount of decoding is required."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2155699044"
                        ],
                        "name": "Jing Zhang",
                        "slug": "Jing-Zhang",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110392"
                        ],
                        "name": "R. Kasturi",
                        "slug": "R.-Kasturi",
                        "structuredName": {
                            "firstName": "Rangachar",
                            "lastName": "Kasturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kasturi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Scene text detection, maximally stable extremal regions, single-link clustering, distance metric learning"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 28161769,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0567609da19ae90f1742800f1ff873b9f1bd411",
            "isKey": false,
            "numCitedBy": 195,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "Text extraction in video documents, as an important research field of content-based information indexing and retrieval, has been developing rapidly since 1990s. This has led to much progress in text extraction, performance evaluation, and related applications. By reviewing the approaches proposed during the past five years, this paper introduces the progress made in this area and discusses promising directions for future research."
            },
            "slug": "Extraction-of-Text-Objects-in-Video-Documents:-Zhang-Kasturi",
            "title": {
                "fragments": [],
                "text": "Extraction of Text Objects in Video Documents: Recent Progress"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "The progress made in text extraction in video documents is introduced and promising directions for future research are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "2008 The Eighth IAPR International Workshop on Document Analysis Systems"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145564537"
                        ],
                        "name": "Jiri Matas",
                        "slug": "Jiri-Matas",
                        "structuredName": {
                            "firstName": "Jiri",
                            "lastName": "Matas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiri Matas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700928"
                        ],
                        "name": "O. Chum",
                        "slug": "O.-Chum",
                        "structuredName": {
                            "firstName": "Ond\u0159ej",
                            "lastName": "Chum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067522034"
                        ],
                        "name": "Martin Urban",
                        "slug": "Martin-Urban",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Urban",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin Urban"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758039"
                        ],
                        "name": "T. Pajdla",
                        "slug": "T.-Pajdla",
                        "structuredName": {
                            "firstName": "Tom\u00e1s",
                            "lastName": "Pajdla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Pajdla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2104851,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9d5ea177c7fcaf88ec6f56cbeb3e9b74c08e98a3",
            "isKey": false,
            "numCitedBy": 3922,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "The wide-baseline stereo problem, i.e. the problem of establishing correspondences between a pair of images taken from different viewpoints is studied. A new set of image elements that are put into correspondence, the so called extremal regions, is introduced. Extremal regions possess highly desirable properties: the set is closed under 1. continuous (and thus projective) transformation of image coordinates and 2. monotonic transformation of image intensities. An efficient (near linear complexity) and practically fast detection algorithm (near frame rate) is presented for an affinely-invariant stable subset of extremal regions, the maximally stable extremal regions (MSER). A new robust similarity measure for establishing tentative correspondences is proposed. The robustness ensures that invariants from multiple measurement regions (regions obtained by invariant constructions from extremal regions), some that are significantly larger (and hence discriminative) than the MSERs, may be used to establish tentative correspondences. The high utility of MSERs, multiple measurement regions and the robust metric is demonstrated in wide-baseline experiments on image pairs from both indoor and outdoor scenes. Significant change of scale (3.5\u00d7), illumination conditions, out-of-plane rotation, occlusion , locally anisotropic scale change and 3D translation of the viewpoint are all present in the test problems. Good estimates of epipolar geometry (average distance from corresponding points to the epipolar line below 0.09 of the inter-pixel distance) are obtained."
            },
            "slug": "Robust-Wide-Baseline-Stereo-from-Maximally-Stable-Matas-Chum",
            "title": {
                "fragments": [],
                "text": "Robust Wide Baseline Stereo from Maximally Stable Extremal Regions"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "The wide-baseline stereo problem, i.e. the problem of establishing correspondences between a pair of images taken from different viewpoints, is studied and an efficient and practically fast detection algorithm is presented for an affinely-invariant stable subset of extremal regions, the maximally stable extremal region (MSER)."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694974"
                        ],
                        "name": "Dimosthenis Karatzas",
                        "slug": "Dimosthenis-Karatzas",
                        "structuredName": {
                            "firstName": "Dimosthenis",
                            "lastName": "Karatzas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dimosthenis Karatzas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688013"
                        ],
                        "name": "F. Shafait",
                        "slug": "F.-Shafait",
                        "structuredName": {
                            "firstName": "Faisal",
                            "lastName": "Shafait",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Shafait"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809705"
                        ],
                        "name": "S. Uchida",
                        "slug": "S.-Uchida",
                        "structuredName": {
                            "firstName": "Seiichi",
                            "lastName": "Uchida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Uchida"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35613969"
                        ],
                        "name": "M. Iwamura",
                        "slug": "M.-Iwamura",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Iwamura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Iwamura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2730090"
                        ],
                        "name": "L. G. I. Bigorda",
                        "slug": "L.-G.-I.-Bigorda",
                        "structuredName": {
                            "firstName": "Llu\u00eds",
                            "lastName": "Bigorda",
                            "middleNames": [
                                "G\u00f3mez",
                                "i"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. G. I. Bigorda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2729896"
                        ],
                        "name": "Sergi Robles Mestre",
                        "slug": "Sergi-Robles-Mestre",
                        "structuredName": {
                            "firstName": "Sergi",
                            "lastName": "Mestre",
                            "middleNames": [
                                "Robles"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sergi Robles Mestre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40016884"
                        ],
                        "name": "J. M. Romeu",
                        "slug": "J.-M.-Romeu",
                        "structuredName": {
                            "firstName": "Joan",
                            "lastName": "Romeu",
                            "middleNames": [
                                "Mas"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. M. Romeu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50313341"
                        ],
                        "name": "D. F. Mota",
                        "slug": "D.-F.-Mota",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mota",
                            "middleNames": [
                                "Fern\u00e1ndez"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. F. Mota"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467588"
                        ],
                        "name": "Jon Almaz\u00e1n",
                        "slug": "Jon-Almaz\u00e1n",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Almaz\u00e1n",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jon Almaz\u00e1n"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2578506"
                        ],
                        "name": "Llu\u00eds-Pere de las Heras",
                        "slug": "Llu\u00eds-Pere-de-las-Heras",
                        "structuredName": {
                            "firstName": "Llu\u00eds-Pere",
                            "lastName": "Heras",
                            "middleNames": [
                                "de",
                                "las"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Llu\u00eds-Pere de las Heras"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206777226,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fcd7b547bf0a6646a282f521db880e74974aa838",
            "isKey": false,
            "numCitedBy": 884,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "This report presents the final results of the ICDAR 2013 Robust Reading Competition. The competition is structured in three Challenges addressing text extraction in different application domains, namely born-digital images, real scene images and real-scene videos. The Challenges are organised around specific tasks covering text localisation, text segmentation and word recognition. The competition took place in the first quarter of 2013, and received a total of 42 submissions over the different tasks offered. This report describes the datasets and ground truth specification, details the performance evaluation protocols used and presents the final results along with a brief summary of the participating methods."
            },
            "slug": "ICDAR-2013-Robust-Reading-Competition-Karatzas-Shafait",
            "title": {
                "fragments": [],
                "text": "ICDAR 2013 Robust Reading Competition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The datasets and ground truth specification are described, the performance evaluation protocols used are details, and the final results are presented along with a brief summary of the participating methods."
            },
            "venue": {
                "fragments": [],
                "text": "2013 12th International Conference on Document Analysis and Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694974"
                        ],
                        "name": "Dimosthenis Karatzas",
                        "slug": "Dimosthenis-Karatzas",
                        "structuredName": {
                            "firstName": "Dimosthenis",
                            "lastName": "Karatzas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dimosthenis Karatzas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2729896"
                        ],
                        "name": "Sergi Robles Mestre",
                        "slug": "Sergi-Robles-Mestre",
                        "structuredName": {
                            "firstName": "Sergi",
                            "lastName": "Mestre",
                            "middleNames": [
                                "Robles"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sergi Robles Mestre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40016884"
                        ],
                        "name": "J. M. Romeu",
                        "slug": "J.-M.-Romeu",
                        "structuredName": {
                            "firstName": "Joan",
                            "lastName": "Romeu",
                            "middleNames": [
                                "Mas"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. M. Romeu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38417650"
                        ],
                        "name": "F. Nourbakhsh",
                        "slug": "F.-Nourbakhsh",
                        "structuredName": {
                            "firstName": "Farshad",
                            "lastName": "Nourbakhsh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Nourbakhsh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40813600"
                        ],
                        "name": "P. Roy",
                        "slug": "P.-Roy",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Roy",
                            "middleNames": [
                                "Pratim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Roy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 131
                            }
                        ],
                        "text": "Apart from the natural scene images track (Challenge 2), we also profile our system on the born-digital images track (Challenge 1) [35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4377688,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8c507148d502245c459df2ec883dc02fabc0ecad",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents the results of the first Challenge of ICDAR 2011 Robust Reading Competition. Challenge 1 is focused on the extraction of text from born-digital images, specifically from images found in Web pages and emails. The challenge was organized in terms of three tasks that look at different stages of the process: text localization, text segmentation and word recognition. In this paper we present the results of the challenge for all three tasks, and make an open call for continuous participation outside the context of ICDAR 2011."
            },
            "slug": "ICDAR-2011-Robust-Reading-Competition-Challenge-1:-Karatzas-Mestre",
            "title": {
                "fragments": [],
                "text": "ICDAR 2011 Robust Reading Competition - Challenge 1: Reading Text in Born-Digital Images (Web and Email)"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "This paper presents the results of the first Challenge of ICDAR 2011 Robust Reading Competition, focused on the extraction of text from born-digital images, specifically from images found in Web pages and emails."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Document Analysis and Recognition"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145277401"
                        ],
                        "name": "M. N. Murty",
                        "slug": "M.-N.-Murty",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Murty",
                            "middleNames": [
                                "Narasimha"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. N. Murty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704876"
                        ],
                        "name": "P. Flynn",
                        "slug": "P.-Flynn",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Flynn",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Flynn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 73
                            }
                        ],
                        "text": "Intuitively, single-link clustering produces clusters that are elongated [29] and thus is particularly suitable for the text candidates construction task."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12744045,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8ff7b7cf3849640b7cfb8f08e2946fd151fed34c",
            "isKey": false,
            "numCitedBy": 13832,
            "numCiting": 314,
            "paperAbstract": {
                "fragments": [],
                "text": "Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a difficult problem combinatorially, and differences in assumptions and contexts in different communities has made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overviewof pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques, and identify cross-cutting themes and recent advances. We also describe some important applications of clustering algorithms such as image segmentation, object recognition, and information retrieval."
            },
            "slug": "Data-clustering:-a-review-Jain-Murty",
            "title": {
                "fragments": [],
                "text": "Data clustering: a review"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An overview of pattern clustering methods from a statistical pattern recognition perspective is presented, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners."
            },
            "venue": {
                "fragments": [],
                "text": "CSUR"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47695762"
                        ],
                        "name": "M. Bilenko",
                        "slug": "M.-Bilenko",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Bilenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bilenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40632403"
                        ],
                        "name": "Sugato Basu",
                        "slug": "Sugato-Basu",
                        "structuredName": {
                            "firstName": "Sugato",
                            "lastName": "Basu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sugato Basu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 164
                            }
                        ],
                        "text": "One task of semi-supervised clustering is to learn a distance metric that satisfies the labels or constrains in the supervised data, given the clustering algorithm [30]\u2013[32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16157513,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "197e3a315c57c9278876d95b7e522700aa112886",
            "isKey": false,
            "numCitedBy": 908,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Semi-supervised clustering employs a small amount of labeled data to aid unsupervised learning. Previous work in the area has utilized supervised data in one of two approaches: 1) constraint-based methods that guide the clustering algorithm towards a better grouping of the data, and 2) distance-function learning methods that adapt the underlying similarity metric used by the clustering algorithm. This paper provides new methods for the two approaches as well as presents a new semi-supervised clustering algorithm that integrates both of these techniques in a uniform, principled framework. Experimental results demonstrate that the unified approach produces better clusters than both individual approaches as well as previously proposed semi-supervised clustering algorithms."
            },
            "slug": "Integrating-constraints-and-metric-learning-in-Bilenko-Basu",
            "title": {
                "fragments": [],
                "text": "Integrating constraints and metric learning in semi-supervised clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Experimental results demonstrate that the unified approach produces better clusters than both individual approaches as well as previously proposed semi-supervised clustering algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113414328"
                        ],
                        "name": "Fernando Pereira",
                        "slug": "Fernando-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 208
                            }
                        ],
                        "text": "[14] exploits a region detector to detect text candidates and extracts connected components as character candidates by local binarization; non-characters are eliminated with a Conditional Random Fields model [15], and characters can finally be grouped into text."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 219683473,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4ba954b0412773d047dc41231c733de0c1f4926",
            "isKey": false,
            "numCitedBy": 13406,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data."
            },
            "slug": "Conditional-Random-Fields:-Probabilistic-Models-for-Lafferty-McCallum",
            "title": {
                "fragments": [],
                "text": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work presents iterative parameter estimation algorithms for conditional random fields and compares the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143977260"
                        ],
                        "name": "E. Xing",
                        "slug": "E.-Xing",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Xing",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Xing"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145107462"
                        ],
                        "name": "Stuart J. Russell",
                        "slug": "Stuart-J.-Russell",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Russell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stuart J. Russell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2643381,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d1a2d203733208deda7427c8e20318334193d9d7",
            "isKey": false,
            "numCitedBy": 3025,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Many algorithms rely critically on being given a good metric over their inputs. For instance, data can often be clustered in many \"plausible\" ways, and if a clustering algorithm such as K-means initially fails to find one that is meaningful to a user, the only recourse may be for the user to manually tweak the metric until sufficiently good clusters are found. For these and other applications requiring good metrics, it is desirable that we provide a more systematic way for users to indicate what they consider \"similar.\" For instance, we may ask them to provide examples. In this paper, we present an algorithm that, given examples of similar (and, if desired, dissimilar) pairs of points in \u211dn, learns a distance metric over \u211dn that respects these relationships. Our method is based on posing metric learning as a convex optimization problem, which allows us to give efficient, local-optima-free algorithms. We also demonstrate empirically that the learned metrics can be used to significantly improve clustering performance."
            },
            "slug": "Distance-Metric-Learning-with-Application-to-with-Xing-Ng",
            "title": {
                "fragments": [],
                "text": "Distance Metric Learning with Application to Clustering with Side-Information"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper presents an algorithm that, given examples of similar (and, if desired, dissimilar) pairs of points in \ufffd\u201dn, learns a distance metric over \u211dn that respects these relationships."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687524"
                        ],
                        "name": "A. Vedaldi",
                        "slug": "A.-Vedaldi",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Vedaldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vedaldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2487006"
                        ],
                        "name": "B. Fulkerson",
                        "slug": "B.-Fulkerson",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Fulkerson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Fulkerson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1458265,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d720a95e1501922ea17ee31f299f43b2db5e15ef",
            "isKey": false,
            "numCitedBy": 3386,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "VLFeat is an open and portable library of computer vision algorithms. It aims at facilitating fast prototyping and reproducible research for computer vision scientists and students. It includes rigorous implementations of common building blocks such as feature detectors, feature extractors, (hierarchical) k-means clustering, randomized kd-tree matching, and super-pixelization. The source code and interfaces are fully documented. The library integrates directly with MATLAB, a popular language for computer vision research."
            },
            "slug": "Vlfeat:-an-open-and-portable-library-of-computer-Vedaldi-Fulkerson",
            "title": {
                "fragments": [],
                "text": "Vlfeat: an open and portable library of computer vision algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "VLFeat is an open and portable library of computer vision algorithms that includes rigorous implementations of common building blocks such as feature detectors, feature extractors, (hierarchical) k-means clustering, randomized kd-tree matching, and super-pixelization."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Multimedia"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784682"
                        ],
                        "name": "T. Hastie",
                        "slug": "T.-Hastie",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Hastie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hastie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761784"
                        ],
                        "name": "R. Tibshirani",
                        "slug": "R.-Tibshirani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tibshirani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tibshirani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 143
                            }
                        ],
                        "text": "Minimization of this objective function is a typical nonlinear optimization problem and can be solved by classic gradient optimization methods [33]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 40654213,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bee570503aaa0ed5bc5dd4cf6aa742df0b5cef87",
            "isKey": false,
            "numCitedBy": 13814,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book.\n\nThis major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression and path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for ``wide'' data (p bigger than n), including multiple testing and false discovery rates.\n\nTrevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS and invented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting."
            },
            "slug": "The-Elements-of-Statistical-Learning:-Data-Mining,-Hastie-Tibshirani",
            "title": {
                "fragments": [],
                "text": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction, 2nd Edition"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression and path algorithms for the lasso, non-negative matrix factorization, and spectral clustering."
            },
            "venue": {
                "fragments": [],
                "text": "Springer Series in Statistics"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144108246"
                        ],
                        "name": "D. Ruppert",
                        "slug": "D.-Ruppert",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Ruppert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ruppert"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118901444,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5176a2f31dace77db9135dde7020d2c37f78cca0",
            "isKey": false,
            "numCitedBy": 11808,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "In the words of the authors, the goal of this book was to \u201cbring together many of the important new ideas in learning, and explain them in a statistical framework.\u201d The authors have been quite successful in achieving this objective, and their work is a welcome addition to the statistics and learning literatures. Statistics has always been interdisciplinary, borrowing ideas from diverse \u008e elds and repaying the debt with contributions, both theoretical and practical, to the other intellectual disciplines. For statistical learning, this cross-fertilization is especially noticeable. This book is a valuable resource, both for the statistician needing an introduction to machine learning and related \u008e elds and for the computer scientist wishing to learn more about statistics. Statisticians will especially appreciate that it is written in their own language. The level of the book is roughly that of a second-year doctoral student in statistics, and it will be useful as a textbook for such students. In a stimulating article, Breiman (2001) argued that statistics has been focused too much on a \u201cdata modeling culture,\u201d where the model is paramount. Breiman argued instead for an \u201calgorithmic modeling culture,\u201d with emphasis on black-box types of prediction. Breiman\u2019s article is controversial, and in his discussion, Efron objects that \u201cprediction is certainly an interesting subject, but Leo\u2019s paper overstates both its role and our profession\u2019s lack of interest in it.\u201d Although I mostly agree with Efron, I worry that the courses offered by most statistics departments include little, if any, treatment of statistical learning and prediction. (Stanford, where Efron and the authors of this book teach, is an exception.) Graduate students in statistics certainly need to know more than they do now about prediction, machine learning, statistical learning, and data mining (not disjoint subjects). I hope that graduate courses covering the topics of this book will become more common in statistics curricula. Most of the book is focused on supervised learning, where one has inputs and outputs from some system and wishes to predict unknown outputs corresponding to known inputs. The methods discussed for supervised learning include linear and logistic regression; basis expansion, such as splines and wavelets; kernel techniques, such as local regression, local likelihood, and radial basis functions; neural networks; additive models; decision trees based on recursive partitioning, such as CART; and support vector machines. There is a \u008e nal chapter on unsupervised learning, including association rules, cluster analysis, self-organizing maps, principal components and curves, and independent component analysis. Many statisticians will be unfamiliar with at least some of these algorithms. Association rules are popular for mining commercial data in what is called \u201cmarket basket analysis.\u201d The aim is to discover types of products often purchased together. Such knowledge can be used to develop marketing strategies, such as store or catalog layouts. Self-organizing maps (SOMs) involve essentially constrained k-means clustering, where prototypes are mapped to a two-dimensional curved coordinate system. Independent components analysis is similar to principal components analysis and factor analysis, but it uses higher-order moments to achieve independence, not merely zero correlation between components. A strength of the book is the attempt to organize a plethora of methods into a coherent whole. The relationships among the methods are emphasized. I know of no other book that covers so much ground. Of course, with such broad coverage, it is not possible to cover any single topic in great depth, so this book will encourage further reading. Fortunately, each chapter includes bibliographic notes surveying the recent literature. These notes and the extensive references provide a good introduction to the learning literature, including much outside of statistics. The book might be more suitable as a textbook if less material were covered in greater depth; however, such a change would compromise the book\u2019s usefulness as a reference, and so I am happier with the book as it was written."
            },
            "slug": "The-Elements-of-Statistical-Learning:-Data-Mining,-Ruppert",
            "title": {
                "fragments": [],
                "text": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This book is a valuable resource, both for the statistician needing an introduction to machine learning and related \u008e elds and for the computer scientist wishing to learn more about statistics, and statisticians will especially appreciate that it is written in their own language."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144359603"
                        ],
                        "name": "Yiming Ying",
                        "slug": "Yiming-Ying",
                        "structuredName": {
                            "firstName": "Yiming",
                            "lastName": "Ying",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiming Ying"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5380819"
                        ],
                        "name": "Kaizhu Huang",
                        "slug": "Kaizhu-Huang",
                        "structuredName": {
                            "firstName": "Kaizhu",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaizhu Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145990261"
                        ],
                        "name": "C. Campbell",
                        "slug": "C.-Campbell",
                        "structuredName": {
                            "firstName": "Colin",
                            "lastName": "Campbell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Campbell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "r difference p (c1 u c1 v)2 + (c2 c2 )2 + (c3 c3 )2:  Stroke width difference abs(s u s v)=max(s u;s v): 6 3.2.3 Distance Metric Learning There are a variety of distance metric learning methods [31]\u2013[33]. More speci\ufb01cally, many clustering algorithms rely on a good distance metric over the input space. One task of semi-supervised clustering is to learn a distance metric that satis\ufb01es the labels or con"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10047484,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0798e1e7645d8416ff65bd72438703b0a4d1e16a",
            "isKey": false,
            "numCitedBy": 114,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we study the problem of learning a low-rank (sparse) distance matrix. We propose a novel metric learning model which can simultaneously conduct dimension reduction and learn a distance matrix. The sparse representation involves a mixed-norm regularization which is non-convex. We then show that it can be equivalently formulated as a convex saddle (min-max) problem. From this saddle representation, we develop an efficient smooth optimization approach [17] for sparse metric learning, although the learning model is based on a non-differentiable loss function. Finally, we run experiments to validate the effectiveness and efficiency of our sparse metric learning model on various datasets."
            },
            "slug": "Sparse-Metric-Learning-via-Smooth-Optimization-Ying-Huang",
            "title": {
                "fragments": [],
                "text": "Sparse Metric Learning via Smooth Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "This paper proposes a novel metric learning model which can simultaneously conduct dimension reduction and learn a distance matrix and develops an efficient smooth optimization approach for sparse metric learning, although the learning model is based on a non-differentiable loss function."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784955"
                        ],
                        "name": "J. Nocedal",
                        "slug": "J.-Nocedal",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Nocedal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nocedal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 207
                            }
                        ],
                        "text": "In Stage 1, C and M of different images are computed separately but are merged to feed to the optimization precess in the next stage; in Stage 2, the objective function are optimized using the L-BFGS method [34] and the parameters are updated."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9033333,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d2aaf56fb183ad66d099ac6c9110c5c365ab27f3",
            "isKey": false,
            "numCitedBy": 2413,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We study how to use the BFGS quasi-Newton matrices to precondition minimization methods for problems where the storage is critical. We give an update formula which generates matrices using information from the last m iterations, where m is any number supplied by the user. The quasi-Newton matrix is updated at every iteration by dropping the oldest information and replacing it by the newest informa- tion. It is shown that the matrices generated have some desirable properties. The resulting algorithms are tested numerically and compared with several well- known methods. 1. Introduction. For the problem of minimizing an unconstrained function / of n variables, quasi-Newton methods are widely employed (4). They construct a se- quence of matrices which in some way approximate the hessian of /(or its inverse). These matrices are symmetric; therefore, it is necessary to have n(n + l)/2 storage locations for each one. For large dimensional problems it will not be possible to re- tain the matrices in the high speed storage of a computer, and one has to resort to other kinds of algorithms. For example, one could use the methods (Toint (15), Shanno (12)) which preserve the sparsity structure of the hessian, or conjugate gradient methods (CG) which only have to store 3 or 4 vectors. Recently, some CG algorithms have been developed which use a variable amount of storage and which do not require knowledge about the sparsity structure of the problem (2), (7), (8). A disadvantage of these methods is that after a certain number of iterations the quasi-Newton matrix is discarded, and the algorithm is restarted using an initial matrix (usually a diagonal matrix). We describe an algorithm which uses a limited amount of storage and where the quasi-Newton matrix is updated continuously. At every step the oldest information contained in the matrix is discarded and replaced by new one. In this way we hope to have a more up to date model of our function. We will concentrate on the BFGS method since it is considered to be the most efficient. We believe that similar algo- rithms cannot be developed for the other members of the Broyden 0-class (1). Let / be the function to be nnnimized, g its gradient and h its hessian. We define"
            },
            "slug": "Updating-Quasi-Newton-Matrices-With-Limited-Storage-Nocedal",
            "title": {
                "fragments": [],
                "text": "Updating Quasi-Newton Matrices With Limited Storage"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An update formula which generates matrices using information from the last m iterations, where m is any number supplied by the user, and the BFGS method is considered to be the most efficient."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38666915"
                        ],
                        "name": "D. Klein",
                        "slug": "D.-Klein",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Klein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2833700"
                        ],
                        "name": "S. Kamvar",
                        "slug": "S.-Kamvar",
                        "structuredName": {
                            "firstName": "Sepandar",
                            "lastName": "Kamvar",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kamvar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 169
                            }
                        ],
                        "text": "One task of semi-supervised clustering is to learn a distance metric that satisfies the labels or constrains in the supervised data, given the clustering algorithm [30]\u2013[32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8968350,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "51556ef5eb3e1e18540177ce0201a047537d673d",
            "isKey": false,
            "numCitedBy": 605,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an improved method for clustering in the presence of very limited supervisory information, given as pairwise instance constraints. By allowing instance-level constraints to have space-level inductive implications, we are able to successfully incorporate constraints for a wide range of data set types. Our method greatly improves on the previously studied constrained k-means algorithm, generally requiring less than half as many constraints to achieve a given accuracy on a range of real-world data, while also being more robust when over-constrained. We additionally discuss an active learning algorithm which increases the value of constraints even further."
            },
            "slug": "From-Instance-level-Constraints-to-Space-Level-the-Klein-Kamvar",
            "title": {
                "fragments": [],
                "text": "From Instance-level Constraints to Space-Level Constraints: Making the Most of Prior Knowledge in Data Clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "By allowing instance-level constraints to have space-level inductive implications, this work is able to successfully incorporate constraints for a wide range of data set types, and greatly improves on the previously studied constrained k-means algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3087848"
                        ],
                        "name": "R. Dubes",
                        "slug": "R.-Dubes",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Dubes",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dubes"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "e. 3.2 Text Candidates Construction 3.2.1 Text Candidates Construction Algorithm Overview Text candidates are constructed by clustering character candidates using the single-link clustering algorithm [29]. Intuitively, single-link clustering produce clusters that are elongated [30] and thus is particularly suitable for the text candidates construction task. Singlelink clustering belongs to the family "
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 29535089,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aa4bddbd10eafd8e1b54338517eedfee408f03ae",
            "isKey": true,
            "numCitedBy": 10558,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Algorithms-for-Clustering-Data-Jain-Dubes",
            "title": {
                "fragments": [],
                "text": "Algorithms for Clustering Data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "These results are publicly available at http://www.cvc.uab.es/ic dar2011competition/?com=results, where \" TexStar\" is our method. 4"
            },
            "venue": {
                "fragments": [],
                "text": "These results are publicly available at http://www.cvc.uab.es/ic dar2011competition/?com=results, where \" TexStar\" is our method. 4"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 121
                            }
                        ],
                        "text": "Existing methods for scene text detection can roughly be categorized into three groups: sliding window based methods [6]\u2013[8], connected component based methods [9]\u2013[13], and hybrid methods [14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Texture-base approach for text detection in images using support vector machines and continuously adaptive mean shift algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell"
            },
            "year": 2003
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 19,
            "methodology": 14,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 48,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/Robust-Text-Detection-in-Natural-Scene-Images-Yin-Yin/9ac1c5f094be1e16512a9a7fd817e0b414632027?sort=total-citations"
}