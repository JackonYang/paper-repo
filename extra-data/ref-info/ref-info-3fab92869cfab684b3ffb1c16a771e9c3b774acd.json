{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145528516"
                        ],
                        "name": "M. Mu\u00f1oz",
                        "slug": "M.-Mu\u00f1oz",
                        "structuredName": {
                            "firstName": "Marcia",
                            "lastName": "Mu\u00f1oz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mu\u00f1oz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474158"
                        ],
                        "name": "Vasin Punyakanok",
                        "slug": "Vasin-Punyakanok",
                        "structuredName": {
                            "firstName": "Vasin",
                            "lastName": "Punyakanok",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vasin Punyakanok"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144590225"
                        ],
                        "name": "D. Roth",
                        "slug": "D.-Roth",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2680974"
                        ],
                        "name": "Dav Zimak",
                        "slug": "Dav-Zimak",
                        "structuredName": {
                            "firstName": "Dav",
                            "lastName": "Zimak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dav Zimak"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 186
                            }
                        ],
                        "text": "Our modeling of the problem is a modification of our earlier work on this topic that has bee n found to be quite successful compared to other learning methods attempted on this proble m [21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 127
                            }
                        ],
                        "text": "Our work here focus es on OC modeling which has been shown to be more robust than the IO, especially with f airly long phrases [21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 86
                            }
                        ],
                        "text": "Our formali sms directly applies to natural language problems such as shallow parsing [7, 23, 5, 3, 21], c omputational biology problems such as identifying splice sites [8, 4, 15], and problem s in information extraction [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1599627,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a31d6f597f78599c6df66dedf64cd5bc05c5327a",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "A SNoW based learning approach to shallow parsing tasks is presented and studied experimentally. The approach learns to identify syntactic patterns by combining simple predictors to produce a coherent inference. Two instantiations of this approach are studied and experimental results for Noun-Phrases (NP) and Subject-Verb (SV) phrases that compare favorably with the best published results are presented. In doing that, we compare two ways of modeling the problem of learning to recognize patterns and suggest that shallow parsing patterns are better learned using open/close predictors than using inside/outside predictors.} thus contribute to the understanding of how to model shallow parsing tasks as learning problems."
            },
            "slug": "A-Learning-Approach-to-Shallow-Parsing-Mu\u00f1oz-Punyakanok",
            "title": {
                "fragments": [],
                "text": "A Learning Approach to Shallow Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work compares two ways of modeling the problem of learning to recognize patterns and suggests that shallow parsing patterns are better learned using open/close predictors than using inside/outside predictors and thus contribute to the understanding of how to model shallow parsing tasks as learning problems."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144590225"
                        ],
                        "name": "D. Roth",
                        "slug": "D.-Roth",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Roth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1785,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3ed17a1114e2dc48597ab17cc8d5234006f525c9",
            "isKey": false,
            "numCitedBy": 222,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "We analyze a few of the commonly used statistics based and machine learning algorithms for natural language disambiguation tasks and observe that they can be recast as learning linear separators in the feature space. Each of the methods makes a priori assumptions which it employs, given the data, when searching for its hypothesis. Nevertheless, as we show, it searches a space that is as rich as the space of all linear separators. We use this to build an argument for a data driven approach which merely searches for a good linear separator in the feature space, without further assumptions on the domain or a specific problem.We present such an approach - a sparse network of linear separators, utilizing the Winnow learning algorithm - and show how to use it in a variety of ambiguity resolution problems. The learning approach presented is attribute-efficient and, therefore, appropriate for domains having very large number of attributes.In particular, we present an extensive experimental comparison of our approach with other methods on several well studied lexical disambiguation tasks such as context-sensitive spelling correction, prepositional phrase attachment and part of speech tagging. In all cases we show that our approach either outperforms other methods tried for these tasks or performs comparably to the best."
            },
            "slug": "Learning-to-Resolve-Natural-Language-Ambiguities:-A-Roth",
            "title": {
                "fragments": [],
                "text": "Learning to Resolve Natural Language Ambiguities: A Unified Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An extensive experimental comparison of the approach with other methods on several well studied lexical disambiguation tasks such as context-sensitive spelling correction, prepositional phrase attachment and part of speech tagging shows that it outperforms other methods tried for these tasks or performs comparably to the best."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741124"
                        ],
                        "name": "L. Valiant",
                        "slug": "L.-Valiant",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Valiant",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Valiant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 61
                            }
                        ],
                        "text": "To learn these classifiers we follow the projection approach [26] and separate P( sis', 0) to many functions Ps' (slo) according to the previous states s'."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14204125,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14a2e8174947556d4743cc11dae49751ea9867ab",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "A method of combining learning algorithms is described that preserves attribute-efficiency. It yields learning algorithms that require a number of examples that is polynomial in the number of relevant variables and logarithmic in the number of irrelevant ones. The algorithms are simple to implement and realizable on networks with a number of nodes linear in the total number of variables. They include generalizations of Littlestone's Winnow algorithm, and are, therefore, good candidates for experimentation on domains having very large numbers of attributes but where nonlinear hypotheses are sought."
            },
            "slug": "Projection-Learning-Valiant",
            "title": {
                "fragments": [],
                "text": "Projection learning"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A method of combining learning algorithms is described that preserves attribute-efficiency and yields learning algorithms that require a number of examples that is polynomial in the number of relevant variables and logarithmic in the total number of variables."
            },
            "venue": {
                "fragments": [],
                "text": "COLT' 98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758106"
                        ],
                        "name": "Dayne Freitag",
                        "slug": "Dayne-Freitag",
                        "structuredName": {
                            "firstName": "Dayne",
                            "lastName": "Freitag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dayne Freitag"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": "A re cent work [19] is similar to our PMM but is using maximum entropy classifiers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 775373,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bece46ed303f8eaef2affae2cba4e0aef51fe636",
            "isKey": false,
            "numCitedBy": 1551,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Hidden Markov models (HMMs) are a powerful probabilistic tool for modeling sequential data, and have been applied with success to many text-related tasks, such as part-of-speech tagging, text segmentation and information extraction. In these cases, the observations are usually modeled as multinomial distributions over a discrete vocabulary, and the HMM parameters are set to maximize the likelihood of the observations. This paper presents a new Markovian sequence model, closely related to HMMs, that allows observations to be represented as arbitrary overlapping features (such as word, capitalization, formatting, part-of-speech), and defines the conditional probability of state sequences given observation sequences. It does this by using the maximum entropy framework to fit a set of exponential models that represent the probability of a state given an observation and the previous state. We present positive experimental results on the segmentation of FAQ\u2019s."
            },
            "slug": "Maximum-Entropy-Markov-Models-for-Information-and-McCallum-Freitag",
            "title": {
                "fragments": [],
                "text": "Maximum Entropy Markov Models for Information Extraction and Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A new Markovian sequence model is presented that allows observations to be represented as arbitrary overlapping features (such as word, capitalization, formatting, part-of-speech), and defines the conditional probability of state sequences given observation sequences."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 273,
                                "start": 260
                            }
                        ],
                        "text": "The observation that shallow syntactic information can be extr acted using local information \u2013 by examining the pattern itself, its nearby context and the l ocal part-of-speech information \u2013 has motivated the use of learning methods to recognize thes patterns [7, 23, 3, 5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 86
                            }
                        ],
                        "text": "Our formali sms directly applies to natural language problems such as shallow parsing [7, 23, 5, 3, 21], c omputational biology problems such as identifying splice sites [8, 4, 15], and problem s in information extraction [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3166885,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10",
            "isKey": false,
            "numCitedBy": 1058,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A program that tags each word in an input sentence with the most likely part of speech has been written. The program uses a linear-time dynamic programming algorithm to find an assignment of parts of speech to words that optimizes the product of (a) lexical probabilities (probability of observing part of speech i given word i) and (b) contextual probabilities (probability of observing part of speech i given n following parts of speech). Program performance is encouraging; a 400-word sample is presented and is judged to be 99.5% correct.<<ETX>>"
            },
            "slug": "A-Stochastic-Parts-Program-and-Noun-Phrase-Parser-Church",
            "title": {
                "fragments": [],
                "text": "A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A program that tags each word in an input sentence with the most likely part of speech has been written and performance is encouraging; a 400-word sample is presented and is judged to be 99.5% correct."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144628595"
                        ],
                        "name": "S. Argamon",
                        "slug": "S.-Argamon",
                        "structuredName": {
                            "firstName": "Shlomo",
                            "lastName": "Argamon",
                            "middleNames": [
                                "Engelson"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Argamon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7465342"
                        ],
                        "name": "Ido Dagan",
                        "slug": "Ido-Dagan",
                        "structuredName": {
                            "firstName": "Ido",
                            "lastName": "Dagan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ido Dagan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2349412"
                        ],
                        "name": "Yuval Krymolowski",
                        "slug": "Yuval-Krymolowski",
                        "structuredName": {
                            "firstName": "Yuval",
                            "lastName": "Krymolowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuval Krymolowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We develop efficient combination algorithms underboth models and study them experimentally in the context of shallow parsing."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 63
                            }
                        ],
                        "text": "Formally, we now make the following independence assumption: P (st|st\u22121, st\u22122, . . . , s1, ot, ot\u22121, . . . , o1) = P (st|st\u22121, ot)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8074746,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "7fad831935254c9c9ec39ffb03752a3f736c3f76",
            "isKey": false,
            "numCitedBy": 118,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": "Recognizing shallow linguistic patterns, such as basic syntactic relationships between words, is a common task in applied natural language and text processing. The common practice for approaching this task is by tedious manual definition of possible pattern structures, often in the form of regular expressions or finite automata. This paper presents a novel memory-based learning method that recognizes shallow patterns in new text based on a bracketed training corpus. The training data are stored as-is, in efficient suffix-tree data structures. Generalization is performed on-line at recognition time by comparing subsequences of the new text to positive and negative evidence in the corpus. This way, no information in the training is lost, as can happen in other learning systems that construct a single generalized model at the time of training. The paper presents experimental results for recognizing noun phrase, subject-verb and verb-object patterns in English. Since the learning approach enables easy porting to new domains, we plan to apply it to syntactic patterns in other languages and to sub-language patterns for information extraction."
            },
            "slug": "A-Memory-Based-Approach-to-Learning-Shallow-Natural-Argamon-Dagan",
            "title": {
                "fragments": [],
                "text": "A Memory-Based Approach to Learning Shallow Natural Language Patterns"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel memory-based learning method that recognizes shallow patterns in new text based on a bracketed training corpus that enables easy porting to new domains and to sub-language patterns for information extraction."
            },
            "venue": {
                "fragments": [],
                "text": "COLING-ACL"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697386"
                        ],
                        "name": "D. Appelt",
                        "slug": "D.-Appelt",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Appelt",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Appelt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3187096"
                        ],
                        "name": "Jerry R. Hobbs",
                        "slug": "Jerry-R.-Hobbs",
                        "structuredName": {
                            "firstName": "Jerry",
                            "lastName": "Hobbs",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jerry R. Hobbs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143626545"
                        ],
                        "name": "J. Bear",
                        "slug": "J.-Bear",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Bear",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bear"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "26701145"
                        ],
                        "name": "David J. Israel",
                        "slug": "David-J.-Israel",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Israel",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Israel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145340591"
                        ],
                        "name": "M. Tyson",
                        "slug": "M.-Tyson",
                        "structuredName": {
                            "firstName": "Mabry",
                            "lastName": "Tyson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Tyson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 287
                            }
                        ],
                        "text": "The approaches are studied experimentally in the context of shallow parsing \u2013 the task of identifying syntactic sequences in sentences [14, 1, 11] \u2013 which has been found useful in many large-scale language processing applications i ncluding information extraction and text summarization [12, 2]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11268011,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "75288ecdeb29f093190c1a0130be2d24619238ed",
            "isKey": false,
            "numCitedBy": 351,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Approaches to text processing that rely on parsing the text with a context-free grammar tend to be slow and error-prone because of the massive ambiguity of long sentences. In contrast, FASTUS employs a nondeterministic finite-state language model that produces a phrasal decomposition of a sentence into noun groups, verb groups and particles. Another finite-state machine recognizes domain-specific phrases based on combinations of the heads of the constituents found in the first pass. FASTUS has been evaluated on several blind tests that demonstrate that state-of-the-art performance on information-extraction tasks is obtainable with surprisingly little computational effort."
            },
            "slug": "FASTUS:-A-Finite-state-Processor-for-Information-Appelt-Hobbs",
            "title": {
                "fragments": [],
                "text": "FASTUS: A Finite-state Processor for Information Extraction from Real-world Text"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "FASTUS has been evaluated on several blind tests that demonstrate that state-of-the-art performance on information-extraction tasks is obtainable with surprisingly little computational effort."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748501"
                        ],
                        "name": "Claire Cardie",
                        "slug": "Claire-Cardie",
                        "structuredName": {
                            "firstName": "Claire",
                            "lastName": "Cardie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Claire Cardie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34277946"
                        ],
                        "name": "D. Pierce",
                        "slug": "D.-Pierce",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Pierce",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pierce"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 273,
                                "start": 260
                            }
                        ],
                        "text": "The observation that shallow syntactic information can be extr acted using local information \u2013 by examining the pattern itself, its nearby context and the l ocal part-of-speech information \u2013 has motivated the use of learning methods to recognize thes patterns [7, 23, 3, 5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 86
                            }
                        ],
                        "text": "Our formali sms directly applies to natural language problems such as shallow parsing [7, 23, 5, 3, 21], c omputational biology problems such as identifying splice sites [8, 4, 15], and problem s in information extraction [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1623,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f5bb34e38e3403054d4396fc48882f02eae1ffcc",
            "isKey": false,
            "numCitedBy": 87,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Finding simple, non-recursive, base noun phrases is an important subtask for many natural language processing applications. While previous empirical methods for base NP identification have been rather complex, this paper instead proposes a very simple algorithm that is tailored to the relative simplicity of the task. In particular, we present a corpus-based approach for finding base NPs by matching part-of-speech tag sequences. The training phase of the algorithm is based on two successful techniques: first the base NP grammar is read from a \"treebank\" corpus; then the grammar is improved by selecting rules with high \"benefit\" scores. Using this simple algorithm with a naive heuristic for matching rules, we achieve surprising accuracy in an evaluation on the Penn Treebank Wall Street Journal."
            },
            "slug": "Error-Driven-Pruning-of-Treebank-Grammars-for-Base-Cardie-Pierce",
            "title": {
                "fragments": [],
                "text": "Error-Driven Pruning of Treebank Grammars for Base Noun Phrase Identification"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper presents a corpus-based approach for finding base NPs by matching part-of-speech tag sequences, and achieves surprising accuracy in an evaluation on the Penn Treebank Wall Street Journal."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746048"
                        ],
                        "name": "R. Khardon",
                        "slug": "R.-Khardon",
                        "structuredName": {
                            "firstName": "Roni",
                            "lastName": "Khardon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Khardon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144590225"
                        ],
                        "name": "D. Roth",
                        "slug": "D.-Roth",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Roth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 494561,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d3096681c103d02d7c59fc936da35a4b03262359",
            "isKey": false,
            "numCitedBy": 138,
            "numCiting": 87,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new framework for the study of reasoning. The Learning (in order) to Reason approach developed here views learning as an integral part of the inference process, and suggests that learning and reasoning should be studied together.\nThe Learning to Reason framework combines the interfaces to the world used by known learning models with the reasoning task and a performance criterion suitable for it. In this framework, the intelligent agent is given access to its favorite learning interface, and is also given a grace period in with it can interact with this interface and construct a representation KB of the world W. The reasoning performance is measured only after this period, when the agent is presented with queries \u03b1 from some query language, relevant to the world, and has to answer whether W implies \u03b1.\nThe approach is meant to overcome the main computational difficulties in the traditional treatment of reasoning which stem from its separation from the \u201cworld\u201d. Since the agent interacts with the world when construction its knowledge representation it can choose a representation that is useful for the task at hand. Moreover, we can now make explicit the dependence of the reasoning performance on the environment the agent interacts with.\nWe show how previous results from learning theory and reasoning fit into this framwork and illustrate the usefulness of the Learning to Reason approach by exhibiting new results that are not possible in the traditional setting. First, we give Learning to Reason algorithms for classes of propositional languages for which there are no efficient reasoning algorithms, when represented as a traditional (formula-based) knowledge base. Second, we exhibit a Learning to Reason algorithm for a class of propositional languages that is not know to be learnable in the traditional sense."
            },
            "slug": "Learning-to-reason-Khardon-Roth",
            "title": {
                "fragments": [],
                "text": "Learning to reason"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "This work introduces a new framework for the study of reasoning, and gives Learning to Reason algorithms for classes of propositional languages for which there are no efficient reasoning algorithms, when represented as a traditional (formula-based) knowledge base."
            },
            "venue": {
                "fragments": [],
                "text": "JACM"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788050"
                        ],
                        "name": "R. Grishman",
                        "slug": "R.-Grishman",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Grishman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Grishman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 286
                            }
                        ],
                        "text": "The approaches are studied experimentally in the context of shallow parsing - the task of identifying syntactic sequences in sentences [14, 1, 11] - which has been found useful in many large-scale language processing applications including information extraction and text summarization [12, 2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10820476,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "d5bfbc7a36b3a39a0fa70ad09a83fbec052f7cef",
            "isKey": false,
            "numCitedBy": 167,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Over the past five MUCs, New York University has clung faithfully to the idea that information extraction should begin with a phase of full syntactic analysis, followed by a semantic analysis of the syntactic structure. Because we have a good, broad-coverage English grammar and a moderately effective method for recovering from parse failures, this approach held us in fairly good stead."
            },
            "slug": "The-NYU-system-for-MUC-6-or-where's-the-syntax-Grishman",
            "title": {
                "fragments": [],
                "text": "The NYU system for MUC-6 or where's the syntax?"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Over the past five MUCs, New York University has clung faithfully to the idea that information extraction should begin with a phase of full syntactic analysis, followed by a semantic analysis of the syntactic structure."
            },
            "venue": {
                "fragments": [],
                "text": "MUC"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35400286"
                        ],
                        "name": "Z. Harris",
                        "slug": "Z.-Harris",
                        "structuredName": {
                            "firstName": "Zellig",
                            "lastName": "Harris",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Harris"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We develop efficient combination algorithms underboth models and study them experimentally in the context of shallow parsing."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 144751010,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "0725438da27f7f83334cae357bd4415cf93d2344",
            "isKey": false,
            "numCitedBy": 276,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper defines a formal relation among sentences, by virtue of which one sentence structure may be called a transform of another sentence structure (e. g. the active and the passive, or in a different way question and answer). The relation is based on comparing the individual co-occurrences of morphemes. By investigating the individual co-occurrences (\u00a7 1.2; \u00a7 2) we can characterize the distribution of certain classes which may not be definable in ordinary linguistic terms (e. g. pronouns, \u00a7 2.6). More important, we can then proceed to define transformation (\u00a7 1.3), based on two structures having the same set of individual co-occurrences. This relation yields unique analyses of certain structures and distinctions which could not be analyzed in ordinary linguistic terms (\u00a7 3). It replaces a large part of the complexities of constituent analysis and sentence structure, at the cost of adding a level to grammatical analysis. It also has various analytic and practical applications (\u00a7 5.7), and can enter into a more algebraic analysis of language structure (\u00a7 5.2, 4, 6) than is natural for the usual classificatory linguistics. A list of English transformations is given in \u00a7 4. The main argument can be followed in \u00a7 1.11 (Co-Occurrence Defined), \u00a7 1.2 (Constructional Status), \u00a7 1.3 (Transformation Defined), \u00a7 2.9 (Summary of Constructions), \u00a7 3.9 (Summary of Sentence Sequences), \u00a75 (The Place of Transformations in Linguistic Structure).1"
            },
            "slug": "Co-Occurrence-and-Transformation-in-Linguistic-Harris",
            "title": {
                "fragments": [],
                "text": "Co-Occurrence and Transformation in Linguistic Structure"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1957
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746017"
                        ],
                        "name": "G. Grefenstette",
                        "slug": "G.-Grefenstette",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Grefenstette",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Grefenstette"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 135
                            }
                        ],
                        "text": "The approaches are studied experimentally in the context of shallow parsing \u2013 the task of identifying syntactic sequences in sentences [14, 1, 11] \u2013 which has been found useful in many large-scale language processing applications i ncluding information extraction and text summarization [12, 2]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14558325,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fed3002240ebfcfbad4ff472748f46191e17e4e0",
            "isKey": false,
            "numCitedBy": 104,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "As large on-line corpora become more prevalent, a number of attempts have been made to automatically extract thesaurus-like relations directly from text using knowledge poor methods. In the absence of any specific application, comparing the results of these attempts is difficult. Here we propose an evaluation method using gold standards, i.e., pre-existing hand-compiled resources, as a means of comparing extraction techniques. Using this evaluation method, we compare two semantic extraction techniques which produce similar word lists, one using syntactic context of words , and the other using windows of heuristically tagged words. The two techniques are very similar except that in one case selective natural language processing, a partial syntactic analysis, is performed. On a 4 megabyte corpus, syntactic contexts produce significantly better results against the gold standards for the most characteristk: words in the corpus, while windows produce better results for rare words."
            },
            "slug": "Evaluation-Techniques-for-Automatic-Semantic-and-Grefenstette",
            "title": {
                "fragments": [],
                "text": "Evaluation Techniques for Automatic Semantic Extraction: Comparing Syntactic and Window Based Approaches"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "An evaluation method using gold standards, i.e., pre-existing hand-compiled resources, is proposed as a means of comparing extraction techniques, which compare two semantic extraction techniques which produce similar word lists."
            },
            "venue": {
                "fragments": [],
                "text": "Workshop On The Acquisition Of Lexical Knowledge From Text"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72537573"
                        ],
                        "name": "Gregory Grefenstetti",
                        "slug": "Gregory-Grefenstetti",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Grefenstetti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gregory Grefenstetti"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61937302,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ded8f7265b7208d4fbca21c1fdaf2fc01ac3c512",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "As large on-line corpora become more prevalent, a number of attempts have been made to automatically extract thesaurus-like relations directly from text using knowledge poor methods. In the absence of any specific application, comparing the results of these attempts is difficult. Here we propose an evaluation method using gold standards, i.e., pre-existing hand-compiled resources, as a means of comparing extraction techniques. Using this evaluation method, we compare two semantic extraction techniques which produce similar word lists, one using syntactic context of words , and the other using windows of heuristically tagged words. The two techniques are very similar except that in one case selective natural language processing, a partial syntactic analysis, is performed. On a 4 megabyte corpus, syntactic contexts produce significantly better results against the gold standards for the most characteristk: words in the corpus, while windows produce better results for rare words."
            },
            "slug": "Evaluation-techniques-for-automatic-semantic-and-Grefenstetti",
            "title": {
                "fragments": [],
                "text": "Evaluation techniques for automatic semantic extraction: comparing syntactic and window based approaches"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "An evaluation method using gold standards, i.e., pre-existing hand-compiled resources, is proposed as a means of comparing extraction techniques, which compare two semantic extraction techniques which produce similar word lists."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2424234"
                        ],
                        "name": "Beatrice Santorini",
                        "slug": "Beatrice-Santorini",
                        "structuredName": {
                            "firstName": "Beatrice",
                            "lastName": "Santorini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beatrice Santorini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2063206"
                        ],
                        "name": "Mary Ann Marcinkiewicz",
                        "slug": "Mary-Ann-Marcinkiewicz",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Marcinkiewicz",
                            "middleNames": [
                                "Ann"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mary Ann Marcinkiewicz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 123
                            }
                        ],
                        "text": "The data sets used are the standard data sets for this problem [23, 3, 1] taken from the Wall Street Journal corpus in the Penn Treebank [18]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 138
                            }
                        ],
                        "text": "The data sets used are the standard data sets for this problem [23 , 3, 1] taken from the Wall Street Journal corpus in the Penn Treebank [18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 252796,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b44fcbeea9415d400c5f5789d6b892b6f98daff",
            "isKey": false,
            "numCitedBy": 8177,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : As a result of this grant, the researchers have now published oil CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, with over 3 million words of that material assigned skeletal grammatical structure. This material now includes a fully hand-parsed version of the classic Brown corpus. About one half of the papers at the ACL Workshop on Using Large Text Corpora this past summer were based on the materials generated by this grant."
            },
            "slug": "Building-a-Large-Annotated-Corpus-of-English:-The-Marcus-Santorini",
            "title": {
                "fragments": [],
                "text": "Building a Large Annotated Corpus of English: The Penn Treebank"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "As a result of this grant, the researchers have now published on CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, which includes a fully hand-parsed version of the classic Brown corpus."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144590225"
                        ],
                        "name": "D. Roth",
                        "slug": "D.-Roth",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715634"
                        ],
                        "name": "Ming-Hsuan Yang",
                        "slug": "Ming-Hsuan-Yang",
                        "structuredName": {
                            "firstName": "Ming-Hsuan",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Hsuan Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145237406"
                        ],
                        "name": "N. Ahuja",
                        "slug": "N.-Ahuja",
                        "structuredName": {
                            "firstName": "Narendra",
                            "lastName": "Ahuja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ahuja"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 105
                            }
                        ],
                        "text": "SNoW has already been used successfully for a variety of tasks in natural language and visual processing [10, 25]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 763306,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9b1fea77753b6e1a6126665f87a7ff3837bb87ae",
            "isKey": false,
            "numCitedBy": 168,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "A learning account for the problem of object recognition is developed within the PAC (Probably Approximately Correct) model of learnability. The proposed approach makes no assumptions on the distribution of the observed objects, but quantifies success relative to its past experience. Most importantly, the success of learning an object representation is naturally tied to the ability to represent at as a function of some intermediate representations extracted from the image. We evaluate this approach an a large scale experimental study in which the SNoW learning architecture is used to learn representations for the 100 objects in the Columbia Object Image Database (COIL-100). The SNoW-based method is shown to outperform other methods in terms of recognition rates; its performance degrades gracefully when the training data contains fewer views and in the presence of occlusion noise."
            },
            "slug": "Learning-to-recognize-objects-Roth-Yang",
            "title": {
                "fragments": [],
                "text": "Learning to recognize objects"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The SNoW-based method is shown to outperform other methods in terms of recognition rates; its performance degrades gracefully when the training data contains fewer views and in the presence of occlusion noise."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No.PR00662)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35551590"
                        ],
                        "name": "Steven P. Abney",
                        "slug": "Steven-P.-Abney",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Abney",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven P. Abney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We develop efficient combination algorithms underboth models and study them experimentally in the context of shallow parsing."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9716882,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "56d7826f3afaa374077f87ca3529709b1ca7e044",
            "isKey": false,
            "numCitedBy": 992,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "I begin with an intuition: when I read a sentence, I read it a chunk at a time. For example, the previous sentence breaks up something like this: \n \n(1) \n \n[I begin] [with an intuition]: [when I read] [a sentence], [I read it] [a chunk] [at a time] \n \n \n \n \n \n \nThese chunks correspond in some way to prosodic patterns. It appears, for instance, that the strongest stresses in the sentence fall one to a chunk, and pauses are most likely to fall between chunks. Chunks also represent a grammatical watershed of sorts. The typical chunk consists of a single content word surrounded by a constellation of function words, matching a fixed template. A simple context-free grammar is quite adequate to describe the structure of chunks. By contrast, the relationships between chunks are mediated more by lexical selection than by rigid templates. Co-occurrence of chunks is determined not just by their syntactic categories, but is sensitive to the precise words that head them; and the order in which chunks occur is much more flexible than the order of words within chunks."
            },
            "slug": "Parsing-By-Chunks-Abney",
            "title": {
                "fragments": [],
                "text": "Parsing By Chunks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The typical chunk consists of a single content word surrounded by a constellation of function words, matching a fixed template, and the relationships between chunks are mediated more by lexical selection than by rigid templates."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744313"
                        ],
                        "name": "L. Ramshaw",
                        "slug": "L.-Ramshaw",
                        "structuredName": {
                            "firstName": "Lance",
                            "lastName": "Ramshaw",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ramshaw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 273,
                                "start": 260
                            }
                        ],
                        "text": "The observation that shallow syntactic information can be extr acted using local information \u2013 by examining the pattern itself, its nearby context and the l ocal part-of-speech information \u2013 has motivated the use of learning methods to recognize thes patterns [7, 23, 3, 5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 63
                            }
                        ],
                        "text": "The data sets used are the standard data sets for this problem [23 , 3, 1] taken from the Wall Street Journal corpus in the Penn Treebank [18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 86
                            }
                        ],
                        "text": "Our formali sms directly applies to natural language problems such as shallow parsing [7, 23, 5, 3, 21], c omputational biology problems such as identifying splice sites [8, 4, 15], and problem s in information extraction [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 725590,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d9c71db75046473f0e3d3229950d7c84c09afd5e",
            "isKey": false,
            "numCitedBy": 1530,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Transformation-based learning, a technique introduced by Eric Brill (1993b), has been shown to do part-of-speech tagging with fairly high accuracy. This same method can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive \u201cbaseNP\u201d chunks. For this purpose, it is convenient to view chunking as a tagging problem by encoding the chunk structure in new tags attached to each word. In automatic tests using Treebank-derived data, this technique achieved recall and precision rates of roughly 93% for baseNP chunks (trained on 950K words) and 88% for somewhat more complex chunks that partition the sentence (trained on 200K words). Working in this new application and with larger template and training sets has also required some interesting adaptations to the transformation-based learning approach."
            },
            "slug": "Text-Chunking-using-Transformation-Based-Learning-Ramshaw-Marcus",
            "title": {
                "fragments": [],
                "text": "Text Chunking using Transformation-Based Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work has shown that the transformation-based learning approach can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive \u201cbaseNP\u201d chunks."
            },
            "venue": {
                "fragments": [],
                "text": "VLC@ACL"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751239"
                        ],
                        "name": "R. Dechter",
                        "slug": "R.-Dechter",
                        "structuredName": {
                            "firstName": "Rina",
                            "lastName": "Dechter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dechter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 2
                            }
                        ],
                        "text": "2 we assume to have local signals that indicate the state."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1277010,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ab3724bb2078b90c3307745bc32a9bd402ec20aa",
            "isKey": false,
            "numCitedBy": 284,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "A constraint satisfaction problem (csp) de ned over a constraint network consists of a nite set of variables, each associated with a domain of values, and a set of constraints. A solution is an assignment of a value to each variable from its domain such that all the constraints are satis ed. Typical constraint satisfaction problems are to determine whether a solution exists, to nd one or all solutions and to nd an optimal solution relative to a given cost function. An example of a constraint satisfaction problem is the well known k-colorability. The problem is to color, if possible, a given graph with k colors only, such that any two adjacent nodes have di erent colors. A constraint satisfaction formulation of this problem associates the nodes of the graph with variables, the possible colors are their domains and the inequality constraints between adjacent nodes are the constraints of the problem. Each constraint of a csp may be expressed as a relation, de ned on some subset of variables, denoting their legal combinations of values. As well, constraints"
            },
            "slug": "Constraint-Satisfaction-Dechter",
            "title": {
                "fragments": [],
                "text": "Constraint Satisfaction"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 30991537,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e342aa20d7be3560b8ff6cbf85f98e8631f63328",
            "isKey": false,
            "numCitedBy": 87,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "This group works towards automatic transcription of continuous speech with a vocabulary and syntax as unrestricted as possible. It is a long-term effort; however, an experimental system is operational. The acoustic processor contains a spectrum analyzer based on the Fast Fourier Transform and a phone segmenter/recognizer which makes use of transitional and steady-state information in its classification. The linguistic processor accepts an imperfect string of phones and produces an estimated transcription of the speech input."
            },
            "slug": "Continuous-speech-recognition-Jelinek",
            "title": {
                "fragments": [],
                "text": "Continuous speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "This group works towards automatic transcription of continuous speech with a vocabulary and syntax as unrestricted as possible and an experimental system is operational."
            },
            "venue": {
                "fragments": [],
                "text": "SGAR"
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761615"
                        ],
                        "name": "J. Fickett",
                        "slug": "J.-Fickett",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Fickett",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Fickett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2843620,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "f6059f9cc31ae027b700048b129792cbaa029443",
            "isKey": false,
            "numCitedBy": 155,
            "numCiting": 353,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Gene-Identification-Problem:-An-Overview-for-Fickett",
            "title": {
                "fragments": [],
                "text": "The Gene Identification Problem: An Overview for Developers"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Chem."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712517"
                        ],
                        "name": "L. Rabiner",
                        "slug": "L.-Rabiner",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Rabiner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rabiner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 65
                            }
                        ],
                        "text": "Working within a concrete task llows us to compare the approaches experimentally for phrase types such as base Noun Phrases (NPs) and SubjectVerb phrases (SVs) that differ significantly in their statistical properties, including length and internal dependencies."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13618539,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "8fe2ea0a67954f1380b3387e3262f1cdb9f9b3e5",
            "isKey": false,
            "numCitedBy": 24804,
            "numCiting": 98,
            "paperAbstract": {
                "fragments": [],
                "text": "The fabric comprises a novel type of netting which will have particular utility in screening out mosquitoes and like insects and pests. The fabric is defined of voids having depth as well as width and length. The fabric is usable as a material from which to form clothing for wear, or bed coverings, or sleeping bags, etc., besides use simply as a netting."
            },
            "slug": "A-Tutorial-on-Hidden-Markov-Models-and-Selected-Rabiner",
            "title": {
                "fragments": [],
                "text": "A Tutorial on Hidden Markov Models and Selected Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The fabric comprises a novel type of netting which will have particular utility in screening out mosquitoes and like insects and pests."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3151044"
                        ],
                        "name": "C. Burge",
                        "slug": "C.-Burge",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burge",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burge"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145737605"
                        ],
                        "name": "S. Karlin",
                        "slug": "S.-Karlin",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Karlin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Karlin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We develop efficient combination algorithms underboth models and study them experimentally in the context of shallow parsing."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15275320,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "22568d6cd1feb27394f3137437986607ebe90b62",
            "isKey": false,
            "numCitedBy": 573,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Finding-the-genes-in-genomic-DNA.-Burge-Karlin",
            "title": {
                "fragments": [],
                "text": "Finding the genes in genomic DNA."
            },
            "venue": {
                "fragments": [],
                "text": "Current opinion in structural biology"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144628595"
                        ],
                        "name": "S. Argamon",
                        "slug": "S.-Argamon",
                        "structuredName": {
                            "firstName": "Shlomo",
                            "lastName": "Argamon",
                            "middleNames": [
                                "Engelson"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Argamon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7465342"
                        ],
                        "name": "Ido Dagan",
                        "slug": "Ido-Dagan",
                        "structuredName": {
                            "firstName": "Ido",
                            "lastName": "Dagan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ido Dagan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2349412"
                        ],
                        "name": "Yuval Krymolowski",
                        "slug": "Yuval-Krymolowski",
                        "structuredName": {
                            "firstName": "Yuval",
                            "lastName": "Krymolowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuval Krymolowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The result of interest is Recall Precision Precision Recall (here ). The data sets used are the standard data sets for this problem [23,  3 , 21] taken from the Wall Street Journal corpus in the Penn Treebank [18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The observation that shallow syntactic information can be extracted using local information \u2010 by examining the pattern itself, its nearby context and the local part-of-speech information \u2010 has motivated the use of learning methods to recognize these patterns [7, 23,  3 , 5]. In this work we study the identification of two types of phrases, base Noun Phrases (NP) and Subject Verb (SV) patterns."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Our formalisms directly applies to natural language problems such as shallow parsing [7, 23, 5,  3 , 21], computational biology problems such as identifying splice sites [8, 4, 15], and problems in information extraction [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62625908,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4c6423808324ac451d3ed3564524496df115c951",
            "isKey": true,
            "numCitedBy": 32,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Memory-Based-Approach-to-Learning-Shallow-Natural-Argamon-Dagan",
            "title": {
                "fragments": [],
                "text": "A Memory-Based Approach to Learning Shallow Natural Language Patterns"
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 78
                            }
                        ],
                        "text": "The classifier we use to learn the states as a function of the obs rvation is SNoW [24, 6], a multi-class classifier that is specifically tailored for large scale learning tasks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 4
                            }
                        ],
                        "text": "The SNoW learning architecture learns a sparse network of linear functio s, in which the targets (states, in this case) are represented as linear functions over a common features space."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 11
                            }
                        ],
                        "text": "Typically, SNoW is used as a classifier,and predicts using a winnertake-all mechanism over the activation value of the target classes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 120
                            }
                        ],
                        "text": "In particular, the fact that the significant improvements both probabilistic methods achieve whent ir input is given by SNoW confirms the claim that the output of SNoW can be used reliablys a probabilistic classifier."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 105
                            }
                        ],
                        "text": "For the easier NP task, the HMM model is competitive with the others when the classifiers used are NB or SNoW."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "The NB (naive Bayes) and SNoW classifier use the same feature set, conjunctions of size3 of POS tags (POS and words, resp.) in a window of size 6."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 49
                            }
                        ],
                        "text": "Our experiments make use of the SNoW classifie r [6, 24] and we provide a way to combine its scores in a probabilistic framework; we al so exhibit the improvements of the standard hidden Markov model (HMM) when allowing stat es to depend on a richer structure of the observation via the use of classifiers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "SNoW has already been used successfully for a variety of tasks in natural language and visual processing [10, 25]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "Our experiments make use of the SNoW classifier [6, 24] and we provide a way to combine its scores in a probabilistic framework; we also exhibit the improvements of the standard hidden Markov model (HMM) when allowing states to depend on a richer structure of the observation via the use of classifiers."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 84
                            }
                        ],
                        "text": "The classifier we use to learn the states as a function of the ob s rvation is SNoW [24, 6], a multi-class classifier that is specifically tailored for lar ge scale learning tasks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The SNoW learn  ing architecture. Technical Report UIUCDCS-R-99-2101, UIUC Computer Science Departme  n"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 78
                            }
                        ],
                        "text": "The classifier we use to learn the states as a function of the obs rvation is SNoW [24, 6], a multi-class classifier that is specifically tailored for large scale learning tasks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 4
                            }
                        ],
                        "text": "The SNoW learning architecture learns a sparse network of linear functio s, in which the targets (states, in this case) are represented as linear functions over a common features space."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 11
                            }
                        ],
                        "text": "Typically, SNoW is used as a classifier,and predicts using a winnertake-all mechanism over the activation value of the target classes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 120
                            }
                        ],
                        "text": "In particular, the fact that the significant improvements both probabilistic methods achieve whent ir input is given by SNoW confirms the claim that the output of SNoW can be used reliablys a probabilistic classifier."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 105
                            }
                        ],
                        "text": "For the easier NP task, the HMM model is competitive with the others when the classifiers used are NB or SNoW."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "The NB (naive Bayes) and SNoW classifier use the same feature set, conjunctions of size3 of POS tags (POS and words, resp.) in a window of size 6."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 49
                            }
                        ],
                        "text": "Our experiments make use of the SNoW classifie r [6, 24] and we provide a way to combine its scores in a probabilistic framework; we al so exhibit the improvements of the standard hidden Markov model (HMM) when allowing stat es to depend on a richer structure of the observation via the use of classifiers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "SNoW has already been used successfully for a variety of tasks in natural language and visual processing [10, 25]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "Our experiments make use of the SNoW classifier [6, 24] and we provide a way to combine its scores in a probabilistic framework; we also exhibit the improvements of the standard hidden Markov model (HMM) when allowing states to depend on a richer structure of the observation via the use of classifiers."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 84
                            }
                        ],
                        "text": "The classifier we use to learn the states as a function of the ob s rvation is SNoW [24, 6], a multi-class classifier that is specifically tailored for lar ge scale learning tasks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The SNoW learn ing architecture"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report UIUCDCS-R-99-2101,"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712517"
                        ],
                        "name": "L. Rabiner",
                        "slug": "L.-Rabiner",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Rabiner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rabiner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145778742"
                        ],
                        "name": "B. Juang",
                        "slug": "B.-Juang",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Juang",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Juang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "This state produces the next observation, and the process goes on until it reaches a desig nated final state [22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60838227,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "81d734347d5d6732be09493180387bd640d3490f",
            "isKey": false,
            "numCitedBy": 625,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-tutorial-on-Hidden-Markov-Models-Rabiner-Juang",
            "title": {
                "fragments": [],
                "text": "A tutorial on Hidden Markov Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 186
                            }
                        ],
                        "text": "Our modeling of the problem is a modification of our earlier work on this topic that has bee n found to be quite successful compared to other learning methods attempted on this proble m [21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 127
                            }
                        ],
                        "text": "Our work here focus es on OC modeling which has been shown to be more robust than the IO, especially with f airly long phrases [21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 86
                            }
                        ],
                        "text": "Our formali sms directly applies to natural language problems such as shallow parsing [7, 23, 5, 3, 21], c omputational biology problems such as identifying splice sites [8, 4, 15], and problem s in information extraction [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A learning approac  h to shallow parsing"
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP-VLC\u201999"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 286
                            }
                        ],
                        "text": "The approaches are studied experimentally in the context of shallow parsing \u2013 the task of identifying syntactic sequences in sentences [14, 1, 11] \u2013 which has been found useful in many large-scale language processing applications including information extraction and text summarization [12, 2]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The NYU system for MUC-6 or where\u2019s syntax? In B"
            },
            "venue": {
                "fragments": [],
                "text": "Sundheim, editor, Proceedings of the Sixth Message Understanding Conference  . Morgan Kaufmann Publishers"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 138
                            }
                        ],
                        "text": "Several attempts to combine classifiers, mostly neural netw orks, into HMMs have been made in speech recognition works in the last decade [20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Continuous speech recogniti  o"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Signal Processing Magazine, 12(3):24\u201342"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 222
                            }
                        ],
                        "text": "Our formali sms directly applies to natural language problems such as shallow parsing [7, 23, 5, 3, 21], c omputational biology problems such as identifying splice sites [8, 4, 15], and problem s in information extraction [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information extraction usin  g HMMs and shrinkage"
            },
            "venue": {
                "fragments": [],
                "text": "InPapers from the AAAI-99 Workshop on Machine Learning for Informati  on Extraction, 31\u201336"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We develop efficient combination algorithms underboth models and study them experimentally in the context of shallow parsing."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Computational genefinding"
            },
            "venue": {
                "fragments": [],
                "text": "Trends in Biochemical Sciences, Supplementary Guide to Bioinformatics"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 162
                            }
                        ],
                        "text": "A satisfying assignment for the resulting2-CNF formula can therefore be computed in polynomial time, b ut the corresponding optimization problem is still NP-hard [13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 153
                            }
                        ],
                        "text": "A satisfying assignment for the resulting2-CNF formula can therefore be computed in polynomial time, but the corresponding optimization problem is still NP-hard [13]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A bounded approximation for the m  ini um cost 2-SAT problems"
            },
            "venue": {
                "fragments": [],
                "text": "Algorithmica, 8:103\u2013117"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 135
                            }
                        ],
                        "text": "The approaches are studied experimentally in the context of shallow parsing \u2013 the task of identifying syntactic sequences in sentences [14, 1, 11] \u2013 which has been found useful in many large-scale language processing applications i ncluding information extraction and text summarization [12, 2]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Co-occurrence and transformation in ling  uistic structure.Language"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1957
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We develop efficient combination algorithms underboth models and study them experimentally in the context of shallow parsing."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The SNoW learning architecture"
            },
            "venue": {
                "fragments": [],
                "text": "The SNoW learning architecture"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We develop efficient combination algorithms underboth models and study them experimentally in the context of shallow parsing."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The NYU system for MUC-6 or where's syntax? In B. Sundheim"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Sixth Message Understanding Conference"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 168
                            }
                        ],
                        "text": "Our formalisms directly applies to natural language problems such as shallow parsing [7, 23, 5, 3, 21], computational biology problems such as identifying splice sites [8,4, 15], and problems in information extraction [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Computational genefinding. Trends in Biochemical Sciences, Supplementary Guide to Bioinformatics, pages"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 170
                            }
                        ],
                        "text": "Our formali sms directly applies to natural language problems such as shallow parsing [7, 23, 5, 3, 21], c omputational biology problems such as identifying splice sites [8, 4, 15], and problem s in information extraction [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The gene identification problem: An overvi  w for developers.Computers and Chemistry, 20:103\u2013118"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We develop efficient combination algorithms underboth models and study them experimentally in the context of shallow parsing."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information extraction using HMMs and shrinkage"
            },
            "venue": {
                "fragments": [],
                "text": "Papers from the AAAJ-99 Workshop on Machine Learning for Information Extraction"
            },
            "year": 1999
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 12,
            "methodology": 22
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 38,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/The-Use-of-Classifiers-in-Sequential-Inference-Punyakanok-Roth/3fab92869cfab684b3ffb1c16a771e9c3b774acd?sort=total-citations"
}