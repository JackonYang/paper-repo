{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686671"
                        ],
                        "name": "L. Csat\u00f3",
                        "slug": "L.-Csat\u00f3",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Csat\u00f3",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Csat\u00f3"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691022"
                        ],
                        "name": "M. Opper",
                        "slug": "M.-Opper",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Opper",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Opper"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Much remains to be done to enable us the apply these methods to the very large, complicated models used in weather forecasting (see for example Dance, 2004, for a discussion of current issues). Future work will explore further the links between our methods and the cutting edge data assimilation methods developped by Eyink et al. (2004); Apte et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "However, because we can always explicitly compute all prior marginals (at observations and test points) for a Gaussian process (Csat\u00f3 and Opper, 2002), essentially nearly all current work in this direction boils down to the approximation of a multivariate (but finite dimensional) posterior density."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11375333,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6df761a4db0be30776366024bf7ecaa60dd4d05b",
            "isKey": false,
            "numCitedBy": 714,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop an approach for sparse representations of gaussian process (GP) models (which are Bayesian types of kernel machines) in order to overcome their limitations for large data sets. The method is based on a combination of a Bayesian on-line algorithm, together with a sequential construction of a relevant subsample of the data that fully specifies the prediction of the GP model. By using an appealing parameterization and projection techniques in a reproducing kernel Hilbert space, recursions for the effective parameters and a sparse gaussian approximation of the posterior process are obtained. This allows for both a propagation of predictions and Bayesian error measures. The significance and robustness of our approach are demonstrated on a variety of experiments."
            },
            "slug": "Sparse-On-Line-Gaussian-Processes-Csat\u00f3-Opper",
            "title": {
                "fragments": [],
                "text": "Sparse On-Line Gaussian Processes"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "An approach for sparse representations of gaussian process (GP) models (which are Bayesian types of kernel machines) in order to overcome their limitations for large data sets is developed based on a combination of a Bayesian on-line algorithm and a sequential construction of a relevant subsample of data that fully specifies the prediction of the GP model."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "17585310"
                        ],
                        "name": "M. I. Jordan",
                        "slug": "M.-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144215175"
                        ],
                        "name": "R. Jacobs",
                        "slug": "R.-Jacobs",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Jacobs",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jacobs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 261,
                                "start": 240
                            }
                        ],
                        "text": "\u2026an effective approach to solving many learning problems (Dempster, Laird, & Rubin, 1977; MacLachlan & Bashford, 1988), and has been employed with particular success in the setting of mixture models and more general latent variable models (Jordan & Jacobs, 1994; Jelinek, 1997; Rubin & Thayer, 1983)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 67000854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6d8a7fc2e2d53923832f9404376512068ca2a57",
            "isKey": false,
            "numCitedBy": 2136,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a tree-structured architecture for supervised learning. The statistical model underlying the architecture is a hierarchical mixture model in which both the mixture coefficients and the mixture components are generalized linear models (GLIM's). Learning is treated as a maximum likelihood problem; in particular, we present an Expectation-Maximization (EM) algorithm for adjusting the parameters of the architecture. We also develop an on-line learning algorithm in which the parameters are updated incrementally. Comparative simulation results are presented in the robot dynamics domain."
            },
            "slug": "Hierarchical-Mixtures-of-Experts-and-the-EM-Jordan-Jacobs",
            "title": {
                "fragments": [],
                "text": "Hierarchical mixtures of experts and the EM algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "An Expectation-Maximization (EM) algorithm for adjusting the parameters of the tree-structured architecture for supervised learning and an on-line learning algorithm in which the parameters are updated incrementally."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50785579"
                        ],
                        "name": "N. Friedman",
                        "slug": "N.-Friedman",
                        "structuredName": {
                            "firstName": "Nir",
                            "lastName": "Friedman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Friedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770007"
                        ],
                        "name": "M. Goldszmidt",
                        "slug": "M.-Goldszmidt",
                        "structuredName": {
                            "firstName": "Mois\u00e9s",
                            "lastName": "Goldszmidt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Goldszmidt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2158320653"
                        ],
                        "name": "Thomas J. Lee",
                        "slug": "Thomas-J.-Lee",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Lee",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas J. Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 180
                            }
                        ],
                        "text": "In the classification setting, the MT model builds on the seminal work on tree-based classifiers by Chow and Liu (1968), and on recent extensions due to Friedman et al. (1997) and Friedman, Goldszmidt, and Lee (1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 105
                            }
                        ],
                        "text": "Such models, which\u2014as we discuss in Section 1.1\u2014have been studied previously by Friedman et al. (1997) and Friedman et al. (1998), will be referred to generically as mixtures with observed choice variable."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2591581,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c9b863c1737c733d51f988ea88f698949d466438",
            "isKey": true,
            "numCitedBy": 85,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "In a recent paper, Friedman, Geiger, and Goldszmidt [8] introduced a classifier based on Bayesian networks, called Tree Augmented Naive Bayes (TAN), that outperforms naive Bayes and performs competitively with C4.5 and other state-of-the-art methods. This classifier has several advantages including robustness and polynomial computational complexity. One limitation of the TAN classifier is that it applies only to discrete attributes, and thus, continuous attributes must be prediscretized. In this paper, we extend TAN to deal with continuous attributes directly via parametric (e.g., Gaussians) and semiparametric (e.g., mixture of Gaussians) conditional probabilities. The result is a classifier that can represent and combine both discrete and continuous attributes. In addition, we propose a new method that takes advantage of the modeling language of Bayesian networks in order to represent attributes both in discrete and continuous form simultaneously, and use both versions in the classification. This automates the process of deciding which form of the attribute is most relevant to the classification task. It also avoids the commitment to either a discretized or a (semi)parametric form, since different attributes may correlate better with one version or the other. Our empirical results show that this latter method usually achieves classification performance that is as good as or better than either the purely discrete or the purely continuous TAN models."
            },
            "slug": "Bayesian-Network-Classification-with-Continuous-the-Friedman-Goldszmidt",
            "title": {
                "fragments": [],
                "text": "Bayesian Network Classification with Continuous Attributes: Getting the Best of Both Discretization and Parametric Fitting"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper extends TAN to deal with continuous attributes directly via parametric and semiparametric conditional probabilities and proposes a new method that takes advantage of the modeling language of Bayesian networks in order to represent attributes both in discrete and continuous form simultaneously, and use both versions in the classification."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50785579"
                        ],
                        "name": "N. Friedman",
                        "slug": "N.-Friedman",
                        "structuredName": {
                            "firstName": "Nir",
                            "lastName": "Friedman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Friedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 447055,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "41e0bb90262160c26d8c9ec216716d57122c8672",
            "isKey": false,
            "numCitedBy": 688,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years there has been a flurry of works on learning Bayesian networks from data. One of the hard problems in this area is how to effectively learn the structure of a belief network from incomplete data--that is, in the presence of missing values or hidden variables. In a recent paper, I introduced an algorithm called Structural EM that combines the standard Expectation Maximization (EM) algorithm, which optimizes parameters, with structure search for model selection. That algorithm learns networks based on penalized likelihood scores, which include the BIC/MDL score and various approximations to the Bayesian score. In this paper, I extend Structural EM to deal directly with Bayesian model selection. I prove the convergence of the resulting algorithm and show how to apply it for learning a large class of probabilistic models, including Bayesian networks and some variants thereof."
            },
            "slug": "The-Bayesian-Structural-EM-Algorithm-Friedman",
            "title": {
                "fragments": [],
                "text": "The Bayesian Structural EM Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper extends Structural EM to deal directly with Bayesian model selection and proves the convergence of the resulting algorithm and shows how to apply it for learning a large class of probabilistic models, including Bayesian networks and some variants thereof."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746034"
                        ],
                        "name": "L. Getoor",
                        "slug": "L.-Getoor",
                        "structuredName": {
                            "firstName": "Lise",
                            "lastName": "Getoor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Getoor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 743435,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "611dac316bf03112c778cf7365d08e4a9d171876",
            "isKey": false,
            "numCitedBy": 1170,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "A large portion of real-world data is stored in commercial relational database systems. In contrast, most statistical learning methods work only with \"flat\" data representations. Thus, to apply these methods, we are forced to convert our data into a flat form, thereby losing much of the relational structure present in our database. This paper builds on the recent work on probabilistic relational models (PRMs), and describes how to learn them from databases. PRMs allow the properties of an object to depend probabilistically both on other properties of that object and on properties of related objects. Although PRMs are significantly more expressive than standard models, such as Bayesian networks, we show how to extend well-known statistical methods for learning Bayesian networks to learn these models. We describe both parameter estimation and structure learning -- the automatic induction of the dependency structure in a model. Moreover, we show how the learning procedure can exploit standard database retrieval techniques for efficient learning from large datasets. We present experimental results on both real and synthetic relational databases."
            },
            "slug": "Learning-Probabilistic-Relational-Models-Getoor",
            "title": {
                "fragments": [],
                "text": "Learning Probabilistic Relational Models"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper describes both parameter estimation and structure learning -- the automatic induction of the dependency structure in a model and shows how the learning procedure can exploit standard database retrieval techniques for efficient learning from large datasets."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50785579"
                        ],
                        "name": "N. Friedman",
                        "slug": "N.-Friedman",
                        "structuredName": {
                            "firstName": "Nir",
                            "lastName": "Friedman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Friedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770007"
                        ],
                        "name": "M. Goldszmidt",
                        "slug": "M.-Goldszmidt",
                        "structuredName": {
                            "firstName": "Mois\u00e9s",
                            "lastName": "Goldszmidt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Goldszmidt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "TANB is the Tree Augmented Naive Bayes classifier of [ 24 ], NB is the Naive Bayes classifier, Tree represents a mixture of trees with m = 1, MT is a mixture of trees with m = 3. KBNN is the Knowledge based neural net, NN is a neural net."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We also learned naive Bayes (NB) and Tree Augmented Naive Bayes (TANB) models [ 24 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8544653,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cbea68b4fca4e095e2dc93031877c1855c683de9",
            "isKey": false,
            "numCitedBy": 236,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work in supervised learning has shown that a surprisingly simple Bayesian classifier with strong assumptions of independence among features, called naive Bayes, is competitive with state of the art classifiers such as C4.5. This fact raises the question of whether a classifier with less restrictive assumptions can perform even better. In this paper we examine and evaluate approaches for inducing classifiers from data, based on recent results in the theory of learning Bayesian networks. Bayesian networks are factored representations of probability distributions that generalize the naive Bayes classifier and explicitly represent statements about independence. Among these approaches we single out a method we call Tree Augmented Naive Bayes (TAN), which outperforms naive Bayes, yet at the same time maintains the computational simplicity (no search involved) and robustness which are characteristic of naive Bayes. We experimentally tested these approaches using benchmark problems from the U. C. Irvine repository, and compared them against C4.5, naive Bayes, and wrapper-based feature selection methods."
            },
            "slug": "Building-Classifiers-Using-Bayesian-Networks-Friedman-Goldszmidt",
            "title": {
                "fragments": [],
                "text": "Building Classifiers Using Bayesian Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Tree Augmented Naive Bayes (TAN) is single out, which outperforms naive Bayes, yet at the same time maintains the computational simplicity and robustness which are characteristic of naive Baye."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI, Vol. 2"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70219052"
                        ],
                        "name": "Wray L. Buntine",
                        "slug": "Wray-L.-Buntine",
                        "structuredName": {
                            "firstName": "Wray",
                            "lastName": "Buntine",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wray L. Buntine"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 260,
                                "start": 247
                            }
                        ],
                        "text": "\u2026it is possible to view a wide variety of classical machine learning architectures as instances of graphical models, and the graphical model framework provides a natural design procedure for exploring architectural variations on classical themes (Buntine, 1996; Smyth, Heckerman, & Jordan, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7434212,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "09c8db333ffbd1acb2fa78df5ed6efd6a06b415b",
            "isKey": false,
            "numCitedBy": 558,
            "numCiting": 254,
            "paperAbstract": {
                "fragments": [],
                "text": "The literature review presented discusses different methods under the general rubric of learning Bayesian networks from data, and includes some overlapping work on more general probabilistic networks. Connections are drawn between the statistical, neural network, and uncertainty communities, and between the different methodological communities, such as Bayesian, description length, and classical statistics. Basic concepts for learning and Bayesian networks are introduced and methods are then reviewed. Methods are discussed for learning parameters of a probabilistic network, for learning the structure, and for learning hidden variables. The article avoids formal definitions and theorems, as these are plentiful in the literature, and instead illustrates key concepts with simplified examples."
            },
            "slug": "A-Guide-to-the-Literature-on-Learning-Probabilistic-Buntine",
            "title": {
                "fragments": [],
                "text": "A Guide to the Literature on Learning Probabilistic Networks from Data"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The literature review presented discusses different methods under the general rubric of learning Bayesian networks from data, and includes some overlapping work on more general probabilistic networks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Knowl. Data Eng."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057429665"
                        ],
                        "name": "Stefano Monti",
                        "slug": "Stefano-Monti",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Monti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefano Monti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726406"
                        ],
                        "name": "G. Cooper",
                        "slug": "G.-Cooper",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Cooper",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Cooper"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 273,
                                "start": 253
                            }
                        ],
                        "text": "\u2026Meila\u0306 and Michael I. Jordan.\nfor classification, prediction and density estimation (Bishop, 1999; Friedman, Geiger, & Goldszmidt, 1997; Heckerman, Geiger, & Chickering, 1995; Hinton, Dayan, Frey, & Neal, 1995; Friedman, Getoor, Koller, & Pfeffer, 1996; Monti & Cooper, 1998; Saul & Jordan, 1999)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 135
                            }
                        ],
                        "text": "Kontkanen, Myllymaki, and Tirri (1996) study a MF in which a hidden variable is used for classification; this approach was extended by Monti and Cooper (1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6684855,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1fd253f0c30206b14b620970e96aa90ad1381239",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a new Bayesian network model for classification that combines the naive Bayes (NB) classifier and the finite mixture (FM) classifier. The resulting classifier aims at relaxing the strong assumptions on which the two component models are based, in an attempt to improve on their classification performance, both in terms of accuracy and in terms of calibration of the estimated probabilities. The proposed classifier is obtained by superimposing a finite mixture model on the set of feature variables of a naive Bayes model.. We present experimental results that compare the predictive performance on real datasets of the new classifier with the predictive performance of the NB classifier and the FM classifier."
            },
            "slug": "A-Bayesian-Network-Classifier-that-Combines-a-Model-Monti-Cooper",
            "title": {
                "fragments": [],
                "text": "A Bayesian Network Classifier that Combines a Finite Mixture Model and a NaIve Bayes Model"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A new Bayesian network model for classification that combines the naive Bayes (NB) classifier and the finite mixture (FM) classifiers aims at relaxing the strong assumptions on which the two component models are based, in an attempt to improve on their classification performance."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102636025"
                        ],
                        "name": "C. Chow",
                        "slug": "C.-Chow",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Chow",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Chow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2119394814"
                        ],
                        "name": "C. N. Liu",
                        "slug": "C.-N.-Liu",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Liu",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. N. Liu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 12
                            }
                        ],
                        "text": "As shown by Chow and Liu (1968), the tree distribution that maximizes the likelihood of a set of observations on M nodes\u2014as well as the parameters of the tree\u2014can be found in time quadratic in the number of variables in the domain."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 100
                            }
                        ],
                        "text": "In the classification setting, the MT model builds on the seminal work on tree-based classifiers by Chow and Liu (1968), and on recent extensions due to Friedman et al. (1997) and Friedman, Goldszmidt, and Lee (1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 61
                            }
                        ],
                        "text": "The solution to the learning problem is an algorithm, due to Chow and Liu (1968), that has quadratic complexity in n (see Figure 3)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 120
                            }
                        ],
                        "text": "This is equivalent to training a mixture of trees with observed choice variable, the choice variable being the class c (Chow & Liu, 1968; Friedman et al., 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 27127853,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "683fe3bbf2b2e628cf40d90e35fb39effc63b7e9",
            "isKey": true,
            "numCitedBy": 2729,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "A method is presented to approximate optimally an n -dimensional discrete probability distribution by a product of second-order distributions, or the distribution of the first-order tree dependence. The problem is to find an optimum set of n - 1 first order dependence relationship among the n variables. It is shown that the procedure derived in this paper yields an approximation of a minimum difference in information. It is further shown that when this procedure is applied to empirical observations from an unknown distribution of tree dependence, the procedure is the maximum-likelihood estimate of the distribution."
            },
            "slug": "Approximating-discrete-probability-distributions-Chow-Liu",
            "title": {
                "fragments": [],
                "text": "Approximating discrete probability distributions with dependence trees"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It is shown that the procedure derived in this paper yields an approximation of a minimum difference in information when applied to empirical observations from an unknown distribution of tree dependence, and the procedure is the maximum-likelihood estimate of the distribution."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145646162"
                        ],
                        "name": "Craig Boutilier",
                        "slug": "Craig-Boutilier",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Boutilier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Craig Boutilier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50785579"
                        ],
                        "name": "N. Friedman",
                        "slug": "N.-Friedman",
                        "structuredName": {
                            "firstName": "Nir",
                            "lastName": "Friedman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Friedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770007"
                        ],
                        "name": "M. Goldszmidt",
                        "slug": "M.-Goldszmidt",
                        "structuredName": {
                            "firstName": "Mois\u00e9s",
                            "lastName": "Goldszmidt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Goldszmidt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8303823,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5bdd9a3317b43966f97b5c70c55c46fd19335049",
            "isKey": false,
            "numCitedBy": 774,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Bayesian networks provide a language for qualitatively representing the conditional independence properties of a distribution, This allows a natural and compact representation of the distribution, eases knowledge acquisition, and supports effective inference algorithms. It is well-known, however, that there are certain independencies that we cannot capture qualitatively within the Bayesian network structure: independencies that hold only in certain contexts, i.e., given a specific assignment of values to certain variables, In this paper, we propose a formal notion of context-specific independence (CSI), based on regularities in the conditional probability tables (CPTs) at a node. We present a technique, analogous to (and based on) d-separation, for determining when such independence holds in a given network. We then focus on a particular qualitative representation scheme--tree-structured CPTs-- for capturing CSI. We suggest ways in which this representation can be used to support effective inference algorithms, in particular, we present a structural decomposition of the resulting network which can improve the performance of clustering algorithms, and an alternative algorithm based on outset conditioning."
            },
            "slug": "Context-Specific-Independence-in-Bayesian-Networks-Boutilier-Friedman",
            "title": {
                "fragments": [],
                "text": "Context-Specific Independence in Bayesian Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper proposes a formal notion of context-specific independence (CSI), based on regularities in the conditional probability tables (CPTs) at a node, and proposes a technique, analogous to (and based on) d-separation, for determining when such independence holds in a given network."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113861474"
                        ],
                        "name": "C. Bishop",
                        "slug": "C.-Bishop",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Bishop",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bishop"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "for classification, prediction and density estimation (Bishop, 1999; Friedman, Geiger, & Goldszmidt, 1997; Heckerman, Geiger, & Chickering, 1995; Hinton, Dayan, Frey, & Neal, 1995; Friedman, Getoor, Koller, & Pfeffer, 1996; Monti & Cooper, 1998; Saul & Jordan, 1999)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 119661135,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "423fb8794df50ff79bd4d0e1921d718ee4e3ac57",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "A powerful approach to probabilistic modelling involves supplementing a set of observed variables with additional latent, or hidden, variables. By defining a joint distribution over visible and latent variables, the corresponding distribution of the observed variables is then obtained by marginalization. This allows relatively complex distributions to be expressed in terms of more tractable joint distributions over the expanded variable space. One well-known example of a hidden variable model is the mixture distribution in which the hidden variable is the discrete component label. In the case of continuous latent variables we obtain models such as factor analysis. The structure of such probabilistic models can be made particularly transparent by giving them a graphical representation, usually in terms of a directed acyclic graph, or Bayesian network. In this chapter we provide an overview of latent variable models for representing continuous variables. We show how a particular form of linear latent variable model can be used to provide a probabilistic formulation of the well-known technique of principal components analysis (PCA). By extending this technique to mixtures, and hierarchical mixtures, of probabilistic PCA models we are led to a powerful interactive algorithm for data visualization. We also show how the probabilistic PCA approach can be generalized to non-linear latent variable models leading to the Generative Topographic Mapping algorithm (GTM). Finally, we show how GTM can itself be extended to model temporal data."
            },
            "slug": "Latent-Variable-Models-Bishop",
            "title": {
                "fragments": [],
                "text": "Latent Variable Models"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This chapter provides an overview of latent variable models for representing continuous variables and shows how a particular form of linear latent variable model can be used to provide a probabilistic formulation of the well-known technique of principal components analysis (PCA)."
            },
            "venue": {
                "fragments": [],
                "text": "Learning in Graphical Models"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50860274"
                        ],
                        "name": "Padhraic Smyth",
                        "slug": "Padhraic-Smyth",
                        "structuredName": {
                            "firstName": "Padhraic",
                            "lastName": "Smyth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Padhraic Smyth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48099028"
                        ],
                        "name": "D. Heckerman",
                        "slug": "D.-Heckerman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Heckerman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Heckerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10043879,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0999dc17b35c0d893974f03d98293f71f27698b",
            "isKey": false,
            "numCitedBy": 327,
            "numCiting": 87,
            "paperAbstract": {
                "fragments": [],
                "text": "Graphical techniques for modeling the dependencies of random variables have been explored in a variety of different areas, including statistics, statistical physics, artificial intelligence, speech recognition, image processing, and genetics. Formalisms for manipulating these models have been developed relatively independently in these research communities. In this paper we explore hidden Markov models (HMMs) and related structures within the general framework of probabilistic independence networks (PINs). The paper presents a self-contained review of the basic principles of PINs. It is shown that the well-known forward-backward (F-B) and Viterbi algorithms for HMMs are special cases of more general inference algorithms for arbitrary PINs. Furthermore, the existence of inference and estimation algorithms for more general graphical models provides a set of analysis tools for HMM practitioners who wish to explore a richer class of HMM structures. Examples of relatively complex models to handle sensor fusion and coarticulation in speech recognition are introduced and treated within the graphical model framework to illustrate the advantages of the general approach."
            },
            "slug": "Probabilistic-Independence-Networks-for-Hidden-Smyth-Heckerman",
            "title": {
                "fragments": [],
                "text": "Probabilistic Independence Networks for Hidden Markov Probability Models"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that the well-known forward-backward and Viterbi algorithms for HMMs are special cases of more general inference algorithms for arbitrary PINs and the existence of inference and estimation algorithms for more general graphical models provides a set of analysis tools for HMM practitioners who wish to explore a richer class of HMM structures."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112427179"
                        ],
                        "name": "Jie Cheng",
                        "slug": "Jie-Cheng",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Cheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Cheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145931269"
                        ],
                        "name": "D. Bell",
                        "slug": "D.-Bell",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Bell",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1500379841"
                        ],
                        "name": "Weiru Liu",
                        "slug": "Weiru-Liu",
                        "structuredName": {
                            "firstName": "Weiru",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weiru Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17756251,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bca86c8d46fc36b3fbef513c961f2f47ed7a6a76",
            "isKey": false,
            "numCitedBy": 294,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper provides algorithms that use an information-theoretic analysis to learn Bayesian network structures from data. Based on our three-phase learning framework, we develop efficient algorithms that can effectively learn Bayesian networks, requiring only polynomial numbers of conditional independence (CI) tests in typical cases. We provide precise conditions that specify when these algorithms are guaranteed to be correct as well as empirical evidence (from real world applications and simulation tests) that demonstrates that these systems work efficiently and reliably in practice. \uf6d9 2002 Elsevier Science B.V. All rights reserved."
            },
            "slug": "Learning-belief-networks-from-data:-an-information-Cheng-Bell",
            "title": {
                "fragments": [],
                "text": "Learning belief networks from data: an information theory based approach"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "Algorithms that use an information-theoretic analysis to learn Bayesian network structures from data, requiring only polynomial numbers of conditional independence tests in typical cases are provided."
            },
            "venue": {
                "fragments": [],
                "text": "CIKM '97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2316129"
                        ],
                        "name": "M. Meil\u0103",
                        "slug": "M.-Meil\u0103",
                        "structuredName": {
                            "firstName": "Marina",
                            "lastName": "Meil\u0103",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Meil\u0103"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11067688,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "12557ae89dfc1026e2bc8ee8530172566541135f",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a probability model, the mixture of trees that can account for sparse, dynamically changing dependence relationships. We present a family of efficient algorithms that use EM and the Minimum Spanning Tree algorithm to find the ML and MAP mixture of trees for a variety of priors, including the Dirichlet and the MDL priors."
            },
            "slug": "Estimating-Dependency-Structure-as-a-Hidden-Meil\u0103-Jordan",
            "title": {
                "fragments": [],
                "text": "Estimating Dependency Structure as a Hidden Variable"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A family of efficient algorithms that use EM and the Minimum Spanning Tree algorithm to find the ML and MAP mixture of trees for a variety of priors, including the Dirichlet and the MDL priors are presented."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735963"
                        ],
                        "name": "Raffaella Settimi",
                        "slug": "Raffaella-Settimi",
                        "structuredName": {
                            "firstName": "Raffaella",
                            "lastName": "Settimi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raffaella Settimi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109683373"
                        ],
                        "name": "Jim Q. Smith",
                        "slug": "Jim-Q.-Smith",
                        "structuredName": {
                            "firstName": "Jim",
                            "lastName": "Smith",
                            "middleNames": [
                                "Q."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jim Q. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14667385,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "3768d88fd61e8de1e7376588d8b6d7c17636f43d",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we investigate the geometry of the likelihood of the unknown parameters in a simple class of Bayesian directed graphs with hidden variables. This enables us, before any numerical algorithms are employed, to obtain certain insights in the nature of the unidentifiability inherent in such models, the way posterior densities will be sensitive to prior densities and the typical geometrical form these posterior densities might take. Many of these insights carry over into more complicated Bayesian networks with systematic missing data."
            },
            "slug": "On-the-Geometry-of-Bayesian-Graphical-Models-with-Settimi-Smith",
            "title": {
                "fragments": [],
                "text": "On the Geometry of Bayesian Graphical Models with Hidden Variables"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "The geometry of the likelihood of the unknown parameters in a simple class of Bayesian directed graphs with hidden variables is investigated to obtain certain insights in the nature of the unidentifiability inherent in such models."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31579914"
                        ],
                        "name": "R. Hofmann",
                        "slug": "R.-Hofmann",
                        "structuredName": {
                            "firstName": "Reimar",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hofmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700754"
                        ],
                        "name": "Volker Tresp",
                        "slug": "Volker-Tresp",
                        "structuredName": {
                            "firstName": "Volker",
                            "lastName": "Tresp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Volker Tresp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6338543,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f8b6999fa84023803f9666faf0940ffd0219d4ce",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of learning structure in nonlinear Markov networks with continuous variables. This can be viewed as non-Gaussian multidimensional density estimation exploiting certain conditional independencies in the variables. Markov networks are a graphical way of describing conditional independencies well suited to model relationships which do not exhibit a natural causal ordering. We use neural network structures to model the quantitative relationships between variables. The main focus in this paper will be on learning the structure for the purpose of gaining insight into the underlying process. Using two data sets we show that interesting structures can be found using our approach. Inference will be briefly addressed."
            },
            "slug": "Nonlinear-Markov-Networks-for-Continuous-Variables-Hofmann-Tresp",
            "title": {
                "fragments": [],
                "text": "Nonlinear Markov Networks for Continuous Variables"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The main focus in this paper will be on learning the structure for the purpose of gaining insight into the underlying process using neural network structures to model the quantitative relationships between variables."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40484982"
                        ],
                        "name": "P. Cheeseman",
                        "slug": "P.-Cheeseman",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Cheeseman",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Cheeseman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87277825"
                        ],
                        "name": "J. Stutz",
                        "slug": "J.-Stutz",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Stutz",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Stutz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 267
                            }
                        ],
                        "text": "\u2026methods for approximating the posterior include choosing a single maximum a posteriori (MAP) estimate, replacing the continuous space of models by a finite set Q of high posterior probability (Heckerman et al., 1995), and expanding the posterior around its mode(s) (Cheeseman & Stutz, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 22
                            }
                        ],
                        "text": "The Auto-Class model (Cheeseman & Stutz, 1995) is a mixture of factorial distributions (MF), and its excellent cost/performance ratio motivates the MT model in much the same way as the Naive Bayes model motivates the TANB model in the classification setting."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6176762,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "42f75b297aed474599c8e598dd211a1999804138",
            "isKey": false,
            "numCitedBy": 1298,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe AutoClass an approach to unsupervised classi cation based upon the classical mixture model supplemented by a Bayesian method for determining the optimal classes We include a moderately detailed exposition of the mathematics behind the AutoClass system We emphasize that no current unsupervised classi cation system can produce maximally useful results when operated alone It is the interaction between domain experts and the machine searching over the model space that generates new knowledge Both bring unique information and abilities to the database analysis task and each enhances the others e ectiveness We illustrate this point with several applications of AutoClass to complex real world databases and describe the resulting successes and failures"
            },
            "slug": "Bayesian-Classification-(AutoClass):-Theory-and-Cheeseman-Stutz",
            "title": {
                "fragments": [],
                "text": "Bayesian Classification (AutoClass): Theory and Results"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It is emphasized that no current unsupervised classi cation system can produce maximally useful results when operated alone and that it is the interaction between domain experts and the machine searching over the model space that generates new knowledge."
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Knowledge Discovery and Data Mining"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144527211"
                        ],
                        "name": "D. Geiger",
                        "slug": "D.-Geiger",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Geiger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geiger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1604461,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b0daa933f70082494de9e17c47bbe2ddfc2987e9",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We provide a classification of graphical models according to their representation as subfamilies of exponential families. Undirected graphical models with no hidden variables are linear exponential families (LEFs), directed acyclic graphical models and chain graphs with no hidden variables, including Bayesian networks with several families of local distributions, are curved exponential families (CEFs) and graphical models with hidden variables are stratified exponential families (SEFs). An SEF is a finite union of CEFs satisfying a frontier condition. In addition, we illustrate how one can automatically generate independence and non-independence constraints on the distributions over the observable variables implied by a Bayesian network with hidden variables. The relevance of these results for model selection is examined."
            },
            "slug": "Graphical-Models-and-Exponential-Families-Geiger",
            "title": {
                "fragments": [],
                "text": "Graphical Models and Exponential Families"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is illustrated how one can automatically generate independence and non-independence constraints on the distributions over the observable variables implied by a Bayesian network with hidden variables."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784682"
                        ],
                        "name": "T. Hastie",
                        "slug": "T.-Hastie",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Hastie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hastie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761784"
                        ],
                        "name": "R. Tibshirani",
                        "slug": "R.-Tibshirani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tibshirani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tibshirani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 72
                            }
                        ],
                        "text": "This is similar in spirit to the \u201cmixture discriminant analysis\u201d model of Hastie and Tibshirani (1996), where a mixture of Gaussians is used for each class in a multiway classification problem."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 118694839,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9b819c146dba12efdc7996d975fb3d4fac2faa9d",
            "isKey": false,
            "numCitedBy": 793,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Fisher-Rao linear discriminant analysis (LDA) is a valuable tool for multigroup classification. LDA is equivalent to maximum likelihood classification assuming Gaussian distributions for each class. In this paper, we fit Gaussian mixtures to each class to facilitate effective classification in non-normal settings, especially when the classes are clustered. Low dimensional views are an important by-product of LDA-our new techniques inherit this feature. We can control the within-class spread of the subclass centres relative to the between-class spread. Our technique for fitting these models permits a natural blend with nonparametric versions of LDA."
            },
            "slug": "Discriminant-Analysis-by-Gaussian-Mixtures-Hastie-Tibshirani",
            "title": {
                "fragments": [],
                "text": "Discriminant Analysis by Gaussian Mixtures"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper fits Gaussian mixtures to each class to facilitate effective classification in non-normal settings, especially when the classes are clustered."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 81
                            }
                        ],
                        "text": "The EM algorithm can be adapted to maximize the log posterior for every fixed m (Neal & Hinton, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17947141,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "9f87a11a523e4680e61966e36ea2eac516096f23",
            "isKey": false,
            "numCitedBy": 2597,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved. We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables. From this perspective, it is easy to justify an incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step. This variant is shown empirically to give faster convergence in a mixture estimation problem. A variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible."
            },
            "slug": "A-View-of-the-Em-Algorithm-that-Justifies-Sparse,-Neal-Hinton",
            "title": {
                "fragments": [],
                "text": "A View of the Em Algorithm that Justifies Incremental, Sparse, and other Variants"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step is shown empirically to give faster convergence in a mixture estimation problem."
            },
            "venue": {
                "fragments": [],
                "text": "Learning in Graphical Models"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 275
                            }
                        ],
                        "text": "\u2026Meila\u0306 and Michael I. Jordan.\nfor classification, prediction and density estimation (Bishop, 1999; Friedman, Geiger, & Goldszmidt, 1997; Heckerman, Geiger, & Chickering, 1995; Hinton, Dayan, Frey, & Neal, 1995; Friedman, Getoor, Koller, & Pfeffer, 1996; Monti & Cooper, 1998; Saul & Jordan, 1999)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 17467575,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2d663c3792a6a5ea8974bd8073e4e714e2ffccbc",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a learning algorithm for unsupervised neural networks based on ideas from statistical mechanics. The algorithm is derived from a mean field approximation for large,layered sigmoid belief networks. We show how to (approximately) infer the statistics of these networks without resort to sampling. This is done by solving the mean field equations, which relate the statistics of each unit to those of its Markov blanket. Using these statistics as target values, the weights in the network are adapted by a local delta rule. We evaluate the strengths and weaknesses of these networks for problems in statistical pattern recognition."
            },
            "slug": "A-Mean-Field-Learning-Algorithm-for-Unsupervised-Saul-Jordan",
            "title": {
                "fragments": [],
                "text": "A Mean Field Learning Algorithm for Unsupervised Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "A learning algorithm for unsupervised neural networks based on ideas from statistical mechanics, derived from a mean field approximation for large,layered sigmoid belief networks, is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "Learning in Graphical Models"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726406"
                        ],
                        "name": "G. Cooper",
                        "slug": "G.-Cooper",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Cooper",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Cooper"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 73
                            }
                        ],
                        "text": "For general multiply connected Bayes nets, inference is provably NP-hard [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 43363498,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ed5324bb3a19f0dcc2e90e482c06373b934fc28c",
            "isKey": false,
            "numCitedBy": 2047,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Computational-Complexity-of-Probabilistic-Using-Cooper",
            "title": {
                "fragments": [],
                "text": "The Computational Complexity of Probabilistic Inference Using Bayesian Belief Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70219052"
                        ],
                        "name": "Wray L. Buntine",
                        "slug": "Wray-L.-Buntine",
                        "structuredName": {
                            "firstName": "Wray",
                            "lastName": "Buntine",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wray L. Buntine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2221879"
                        ],
                        "name": "P. Friedland",
                        "slug": "P.-Friedland",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Friedland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Friedland"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 260,
                                "start": 247
                            }
                        ],
                        "text": "\u2026it is possible to view a wide variety of classical machine learning architectures as instances of graphical models, and the graphical model framework provides a natural design procedure for exploring architectural variations on classical themes (Buntine, 1996; Smyth, Heckerman, & Jordan, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 57693363,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cbb8d28f4f30a6053522cc9f859e00aae7246634",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This literature review discusses different methods under the general rubric of learning Bayesian networks from data, and more generally, learning probabilistic graphical models. Because many problems in artificial intelligence, statistics and neural networks can be represented as a probabilistic graphical model, this area provides a unifying perspective on learning. This paper organizes the research in this area along methodological lines of increasing complexity."
            },
            "slug": "A-Guide-to-the-Literature-on-Learning-Graphical-Buntine-Friedland",
            "title": {
                "fragments": [],
                "text": "A Guide to the Literature on Learning Graphical Models"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This literature review discusses different methods under the general rubric of learning Bayesian networks from data, and more generally, learning probabilistic graphical models."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34631309"
                        ],
                        "name": "R. Cowell",
                        "slug": "R.-Cowell",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Cowell",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cowell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144845491"
                        ],
                        "name": "A. Dawid",
                        "slug": "A.-Dawid",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Dawid",
                            "middleNames": [
                                "Philip"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dawid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050895"
                        ],
                        "name": "S. Lauritzen",
                        "slug": "S.-Lauritzen",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Lauritzen",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lauritzen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48616434"
                        ],
                        "name": "D. Spiegelhalter",
                        "slug": "D.-Spiegelhalter",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Spiegelhalter",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Spiegelhalter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 186
                            }
                        ],
                        "text": "Later research extended this early work by first considering general singly-connected graphs (Pearl, 1988), and then considering graphs with arbitrary (acylic) patterns of connectivity (Cowell et al., 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 32379969,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3f680d9c248d396bb3920fcf98ce9a7ba0a9c88",
            "isKey": false,
            "numCitedBy": 1475,
            "numCiting": 314,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nProbabilistic expert systems are graphical networks that support the modelling of uncertainty and decisions in large complex domains, while retaining ease of calculation. Building on original research by the authors over a number of years, this book gives a thorough and rigorous mathematical treatment of the underlying ideas, structures, and algorithms, emphasizing those cases in which exact answers are obtainable. The book will be of interest to researchers and graduate students in artificial intelligence who desire an understanding of the mathematical and statistical basis of probabilistic expert systems, and to students and research workers in statistics wanting an introduction to this fascinating and rapidly developing field. The careful attention to detail will also make this work an important reference source for all those involved in the theory and applications of probabilistic expert systems."
            },
            "slug": "Probabilistic-Networks-and-Expert-Systems-Cowell-Dawid",
            "title": {
                "fragments": [],
                "text": "Probabilistic Networks and Expert Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This book gives a thorough and rigorous mathematical treatment of the underlying ideas, structures, and algorithms of probabilistic expert systems, emphasizing those cases in which exact answers are obtainable."
            },
            "venue": {
                "fragments": [],
                "text": "Information Science and Statistics"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14290328,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a120c05ad7cd4ce2eb8fb9697e16c7c4877208a5",
            "isKey": false,
            "numCitedBy": 601,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Connectionist-Learning-of-Belief-Networks-Neal",
            "title": {
                "fragments": [],
                "text": "Connectionist Learning of Belief Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118141153,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ba1905e1a34bfd7588a173cbe5ef8c0dea2062d",
            "isKey": false,
            "numCitedBy": 120,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Graphical models enhance the representational power of probability models through qualitative characterization of their properties. This also leads to greater efficiency in terms of the computational algorithms that empower such representations. The increasing complexity of these models, however, quickly renders exact probabilistic calculations infeasible. We propose a principled framework for approximating graphical models based on variational methods. \nWe develop variational techniques from the perspective that unifies and expands their applicability to graphical models. These methods allow the (recursive) computation of upper and lower bounds on the quantities of interest. Such bounds yield considerably more information than mere approximations and provide an inherent error metric for tailoring the approximations individually to the cases considered. These desirable properties, concomitant to the variational methods, are unlikely to arise as a result of other deterministic or stochastic approximations. \nThe thesis consists of the development of this variational methodology for probabilistic inference, Bayesian estimation, and towards efficient diagnostic reasoning in the domain of internal medicine. (Copies available exclusively from MIT Libraries, Rm. 14-0551, Cambridge, MA 02139-4307. Ph. 617-253-5668; Fax 617-253-1690.)"
            },
            "slug": "Variational-methods-for-inference-and-estimation-in-Jaakkola-Jordan",
            "title": {
                "fragments": [],
                "text": "Variational methods for inference and estimation in graphical models"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This thesis proposes a principled framework for approximating graphical models based on variational methods and develops variational techniques from the perspective that unifies and expands their applicability to graphical models."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144771934"
                        ],
                        "name": "J. Jeffers",
                        "slug": "J.-Jeffers",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Jeffers",
                            "middleNames": [
                                "N.",
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Jeffers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33931257"
                        ],
                        "name": "J. Whittaker",
                        "slug": "J.-Whittaker",
                        "structuredName": {
                            "firstName": "Joe",
                            "lastName": "Whittaker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Whittaker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 65025812,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b9284cbb40f368b8d13e5aac055893db3d2811a3",
            "isKey": false,
            "numCitedBy": 922,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "The Wiley Paperback Series makes valuable content more accessible to a new generation of statisticians, mathematicians and scientists.Graphical models--a subset of log-linear models--reveal the interrelationships between multiple variables and features of the underlying conditional independence. This introduction to the use of graphical models in the description and modeling of multivariate systems covers conditional independence, several types of independence graphs, Gaussian models, issues in model selection, regression and decomposition. Many numerical examples and exercises with solutions are included.This book is aimed at students who require a course on applied multivariate statistics unified by the concept of conditional independence and researchers concerned with applying graphical modelling techniques."
            },
            "slug": "Graphical-Models-in-Applied-Multivariate-Jeffers-Whittaker",
            "title": {
                "fragments": [],
                "text": "Graphical Models in Applied Multivariate Statistics."
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This introduction to the use of graphical models in the description and modeling of multivariate systems covers conditional independence, several types of independence graphs, Gaussian models, issues in model selection, regression and decomposition."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710305"
                        ],
                        "name": "D. Madigan",
                        "slug": "D.-Madigan",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Madigan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Madigan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804386"
                        ],
                        "name": "A. Raftery",
                        "slug": "A.-Raftery",
                        "structuredName": {
                            "firstName": "Adrian",
                            "lastName": "Raftery",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Raftery"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18709953,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "cbb7afa526e1f947520ffeec8f734794802ee3c7",
            "isKey": false,
            "numCitedBy": 1301,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract We consider the problem of model selection and accounting for model uncertainty in high-dimensional contingency tables, motivated by expert system applications. The approach most used currently is a stepwise strategy guided by tests based on approximate asymptotic P values leading to the selection of a single model; inference is then conditional on the selected model. The sampling properties of such a strategy are complex, and the failure to take account of model uncertainty leads to underestimation of uncertainty about quantities of interest. In principle, a panacea is provided by the standard Bayesian formalism that averages the posterior distributions of the quantity of interest under each of the models, weighted by their posterior model probabilities. Furthermore, this approach is optimal in the sense of maximizing predictive ability. But this has not been used in practice, because computing the posterior model probabilities is hard and the number of models is very large (often greater than 1..."
            },
            "slug": "Model-Selection-and-Accounting-for-Model-in-Models-Madigan-Raftery",
            "title": {
                "fragments": [],
                "text": "Model Selection and Accounting for Model Uncertainty in Graphical Models Using Occam's Window"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144527211"
                        ],
                        "name": "D. Geiger",
                        "slug": "D.-Geiger",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Geiger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geiger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48099028"
                        ],
                        "name": "D. Heckerman",
                        "slug": "D.-Heckerman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Heckerman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Heckerman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 143
                            }
                        ],
                        "text": "One can also consider probabilistic mixtures of more general graphical models; indeed, the general case is the Bayesian multinet introduced by Geiger and Heckerman (1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 618316,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e45ea5b03d3ab4508eda2ef94e80ddb245758c8",
            "isKey": false,
            "numCitedBy": 263,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Knowledge-Representation-and-Inference-in-Networks-Geiger-Heckerman",
            "title": {
                "fragments": [],
                "text": "Knowledge Representation and Inference in Similarity Networks and Bayesian Multinets"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2037240"
                        ],
                        "name": "U. Essen",
                        "slug": "U.-Essen",
                        "structuredName": {
                            "firstName": "Ute",
                            "lastName": "Essen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Essen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795942"
                        ],
                        "name": "Reinhard Kneser",
                        "slug": "Reinhard-Kneser",
                        "structuredName": {
                            "firstName": "Reinhard",
                            "lastName": "Kneser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Reinhard Kneser"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 206560877,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b3e53b9c0e3a7a60e7a5295e9b08af74d6fb3dbf",
            "isKey": false,
            "numCitedBy": 599,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract In this paper, we study the problem of stochastic language modelling from the viewpoint of introducing suitable structures into the conditional probability distributions. The task of these distributions is to predict the probability of a new word by looking at M or even all predecessor words. The conventional approach is to limit M to 1 or 2 and to interpolate the resulting bigram and trigram models with a unigram model in a linear fashion. However, there are many other structures that can be used to model the probabilistic dependences between the predecessor word and the word to be predicted. The structures considered in this paper are: nonlinear interpolation as an alternative to linear interpolation; equivalence classes for word histories and single words; cache memory and word associations. For the optimal estimation of nonlinear and linear interpolation parameters, the leaving-one-out method is systematically used. For the determination of word equivalence classes in a bigram model, an automatic clustering procedure has been adapted. To capture long-distance dependences, we consider various models for word-by-word dependences; the cache model may be viewed as a special type of self-association. Experimental results are presented for two text databases, a Germany database and an English database."
            },
            "slug": "On-structuring-probabilistic-dependences-in-Ney-Essen",
            "title": {
                "fragments": [],
                "text": "On structuring probabilistic dependences in stochastic language modelling"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "The problem of stochastic language modelling is studied from the viewpoint of introducing suitable structures into the conditional probability distributions, and nonlinear interpolation as an alternative to linear interpolation; equivalence classes for word histories and single words; cache memory and word associations are considered."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Speech Lang."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144527211"
                        ],
                        "name": "D. Geiger",
                        "slug": "D.-Geiger",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Geiger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geiger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 137
                            }
                        ],
                        "text": "Introducing additional edges between the input variables yields the Tree Augmented Naive Bayes (TANB) classifier (Friedman et al., 1997; Geiger, 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1568101,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "70fe236bdc5071ffd3b13bf518946702c9ba6895",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-Entropy-based-Learning-Algorithm-of-Bayesian-Geiger",
            "title": {
                "fragments": [],
                "text": "An Entropy-based Learning Algorithm of Bayesian Conditional Trees"
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145500409"
                        ],
                        "name": "G. Shafer",
                        "slug": "G.-Shafer",
                        "structuredName": {
                            "firstName": "Glenn",
                            "lastName": "Shafer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Shafer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707788"
                        ],
                        "name": "P. P. Shenoy",
                        "slug": "P.-P.-Shenoy",
                        "structuredName": {
                            "firstName": "Prakash",
                            "lastName": "Shenoy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. P. Shenoy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 271,
                                "start": 250
                            }
                        ],
                        "text": "Even more importantly, the graph-theoretic framework has allowed for the development of general inference algorithms, which in many cases provide orders of magnitude speedups over brute-force methods (Cowell, Dawid, Lauritzen, & Spiegelhalter, 1999; Shafer & Shenoy, 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 535323,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ac789d8e373cbb3260b2881d66fe00789adf291f",
            "isKey": false,
            "numCitedBy": 264,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we give a simple account of local computation of marginal probabilities when the joint probability distribution is given in factored form and the sets of variables involved in the factors form a hypertree. Previous expositions of such local computation have emphasized conditional probability. We believe this emphasis is misplaced. What is essential to local computation is a factorization. It is not essential that this factorization be interpreted in terms of conditional probabilities. The account given here avoids the divisions required by conditional probabilities and generalizes readily to alternative measures of subjective probability, such as Dempster-Shafer or Spohnian belief functions."
            },
            "slug": "Probability-propagation-Shafer-Shenoy",
            "title": {
                "fragments": [],
                "text": "Probability propagation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The account given here avoids the divisions required by conditional probabilities and generalizes readily to alternative measures of subjective probability, such as Dempster-Shafer or Spohnian belief functions."
            },
            "venue": {
                "fragments": [],
                "text": "Annals of Mathematics and Artificial Intelligence"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40484982"
                        ],
                        "name": "P. Cheeseman",
                        "slug": "P.-Cheeseman",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Cheeseman",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Cheeseman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144308823"
                        ],
                        "name": "Matthew Self",
                        "slug": "Matthew-Self",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Self",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew Self"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113380562"
                        ],
                        "name": "James Kelly",
                        "slug": "James-Kelly",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Kelly",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Kelly"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114766082"
                        ],
                        "name": "Will Taylor",
                        "slug": "Will-Taylor",
                        "structuredName": {
                            "firstName": "Will",
                            "lastName": "Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Will Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059655854"
                        ],
                        "name": "Don Freeman",
                        "slug": "Don-Freeman",
                        "structuredName": {
                            "firstName": "Don",
                            "lastName": "Freeman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Don Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87277825"
                        ],
                        "name": "J. Stutz",
                        "slug": "J.-Stutz",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Stutz",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Stutz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2951993,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "d88ffb6c9a6a6a834da4704224b5663a3a5cc430",
            "isKey": false,
            "numCitedBy": 200,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a Bayesian technique for unsupervised classification of data and its computer implementation, AutoClass. Given real valued or discrete data, AutoClass determines the most probable number of classes present in the data, the most probable descriptions of those classes, and each object's probability of membership in each class. The program performs as well as or better than other automatic classification systems when run on the same data and contains no ad hoc similarity measures or stopping criteria. AutoClass has been applied to several databases in which it has discovered classes representing previously unsuspected phenomena."
            },
            "slug": "Bayesian-Classification-Cheeseman-Self",
            "title": {
                "fragments": [],
                "text": "Bayesian Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A Bayesian technique for unsupervised classification of data and its computer implementation, AutoClass, which performs as well as or better than other automatic classification systems when run on the same data and contains no ad hoc similarity measures or stopping criteria."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144845491"
                        ],
                        "name": "A. Dawid",
                        "slug": "A.-Dawid",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Dawid",
                            "middleNames": [
                                "Philip"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dawid"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61247712,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9abff70b0acf9c6bb74d85f3141d76bef2039a5",
            "isKey": false,
            "numCitedBy": 288,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "A probabilistic expert system provides a graphical representation of a joint probability distribution which can be used to simplify and localize calculations. Jensenet al. (1990) introduced a \u2018flow-propagation\u2019 algorithm for calculating marginal and conditional distributions in such a system. This paper analyses that algorithm in detail, and shows how it can be modified to perform other tasks, including maximization of the joint density and simultaneous \u2018fast retraction\u2019 of evidence entered on several variables."
            },
            "slug": "Applications-of-a-general-propagation-algorithm-for-Dawid",
            "title": {
                "fragments": [],
                "text": "Applications of a general propagation algorithm for probabilistic expert systems"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper analyses a \u2018flow-propagation\u2019 algorithm for calculating marginal and conditional distributions in a probabilistic expert system in detail, and shows how it can be modified to perform other tasks, including maximization of the joint density and simultaneous 'fast retraction' of evidence entered on several variables."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760402"
                        ],
                        "name": "A. Moore",
                        "slug": "A.-Moore",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Moore",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Moore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115790999"
                        ],
                        "name": "Mary S. Lee",
                        "slug": "Mary-S.-Lee",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Lee",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mary S. Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6552210,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c8a05fae8d715685c22a8360c61b5f641e7a7025",
            "isKey": false,
            "numCitedBy": 267,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces new algorithms and data structures for quick counting for machine learning datasets. We focus on the counting task of constructing contingency tables, but our approach is also applicable to counting the number of records in a dataset that match conjunctive queries. Subject to certain assumptions, the costs of these operations can be shown to be independent of the number of records in the dataset and loglinear in the number of non-zero entries in the contingency table. \n \nWe provide a very sparse data structure, the ADtree, to minimize memory use. We provide analytical worst-case bounds for this structure for several models of data distribution. We empirically demonstrate that tractably-sized data structures can be produced for large real-world datasets by (a) using a sparse tree structure that never allocates memory for counts of zero, (b) never allocating memory for counts that can be deduced from other counts, and (c) not bothering to expand the tree fully near its leaves. \n \nWe show how the ADtree can be used to accelerate Bayes net structure finding algorithms, rule learning algorithms, and feature selection algorithms, and we provide a number of empirical results comparing ADtree methods against traditional direct counting approaches. We also discuss the possible uses of ADtrees in other machine learning methods, and discuss the merits of ADtrees in comparison with alternative representations such as kd-trees, R-trees and Frequent Sets."
            },
            "slug": "Cached-Sufficient-Statistics-for-Efficient-Machine-Moore-Lee",
            "title": {
                "fragments": [],
                "text": "Cached Sufficient Statistics for Efficient Machine Learning with Large Datasets"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A very sparse data structure, the ADtree, is provided to minimize memory use and it is empirically demonstrated that tractably-sized data structures can be produced for large real-world datasets by using a sparse tree structure that never allocates memory for counts of zero."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050895"
                        ],
                        "name": "S. Lauritzen",
                        "slug": "S.-Lauritzen",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Lauritzen",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lauritzen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 136
                            }
                        ],
                        "text": "Much progress has been made on the former problem, much of it cast within the framework of the expectation-maximization (EM) algorithm (Lauritzen, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122985977,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "2e67c0312b81b698834315ea33f8a23be6eed6eb",
            "isKey": false,
            "numCitedBy": 765,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-EM-algorithm-for-graphical-association-models-Lauritzen",
            "title": {
                "fragments": [],
                "text": "The EM algorithm for graphical association models with missing data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358192"
                        ],
                        "name": "E. Adelson",
                        "slug": "E.-Adelson",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Adelson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Adelson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 90
                            }
                        ],
                        "text": "Why Pearl's algorithm performs well in these cases is a topic of intense current research [69]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13951920,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46bc5c5f6a846a741a0464241fda05bec9fb0ed1",
            "isKey": false,
            "numCitedBy": 125,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Local belief propagation rules of the sort proposed by P earl (1988) are guaranteed to converge to the optimal beliefs for singly connected networks. Recently, a n umber of researchers have empirically demonstrated good performance of these same algorithms on networks with loops, but a theoretical understanding of this performance has yet to be achieved. Here we l a y a foundation for an understanding of belief propagation in networks with loops. For networks with a single loop, we derive an analytical relationship between the steady state beliefs in the loopy network and the true posterior probability. Using this relationship we show a category of networks for which the MAP estimate obtained by belief update and by belief revision can be proven to be optimal (although the beliefs will be incorrect). We s h o w h o w nodes can use local information in the messages they receive in order to correct the steady state beliefs. Furthermore we p r o ve that for all networks with a single loop, the MAP estimate obtained by belief revision at convergence is guaranteed to give the globally optimal sequence of states. The result is independent of the length of the cycle and the size of the state space. For networks with multiple loops, we i n troduce the concept of a \\balanced network\" and show simulation results comparing belief revision and update in such networks. We show t h a t t h e T urbo code structure is balanced and present simulations on a toy T urbo code problem indicating the decoding obtained by belief revision at convergence is signiicantly more likely to be correct. A a b Figure 1: a. An example of the types of problems typically solved using belief propagation. Observed nodes are denoted by lled circles. A link between any t wo nodes implies a probabilistic compatability constraint. b. A simple network with a loop. Although belief propagation rules can be generalized to this network, a theoretical understanding of the algorithms behavior in such a n e t work has yet to be achieved."
            },
            "slug": "Belief-Propagation-and-Revision-in-Networks-with-Adelson",
            "title": {
                "fragments": [],
                "text": "Belief Propagation and Revision in Networks with Loops"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that for all networks with a single loop, the MAP estimate obtained by belief revision at convergence is guaranteed to give the globally optimal sequence of states and the result is independent of the length of the cycle and the size of the state space."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 32
                            }
                        ],
                        "text": "Chain graphs were introduced by Lauritzen (1996); they represent a superclass of both Bayesian networks and Markov random fields."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6286159,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e16a25faf7428e1fc5ed0a10b8196c0499c7fd0d",
            "isKey": false,
            "numCitedBy": 3412,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical applications in fields such as bioinformatics, information retrieval, speech processing, image processing and communications often involve large-scale models in which thousands or millions of random variables are linked in complex ways. Graphical models provide a general methodology for approaching these problems, and indeed many of the models developed by researchers in these applied fields are instances of the general graphical model formalism. We review some of the basic ideas underlying graphical models, including the algorithmic ideas that allow graphical models to be deployed in large-scale data analysis problems. We also present examples of graphical models in bioinformatics, error-control coding and language processing."
            },
            "slug": "Graphical-Models-Jordan",
            "title": {
                "fragments": [],
                "text": "Graphical Models"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Some of the basic ideas underlying graphical models are reviewed, including the algorithmic ideas that allow graphical models to be deployed in large-scale data analysis problems and examples of graphical models in bioinformatics, error-control coding and language processing are presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1880237"
                        ],
                        "name": "S. Dasgupta",
                        "slug": "S.-Dasgupta",
                        "structuredName": {
                            "firstName": "Sanjoy",
                            "lastName": "Dasgupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dasgupta"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 66
                            }
                        ],
                        "text": "Fitting the best polytree to an arbitrary distribution is NP-hard [33, 13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12088261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d8bcbf76a7a7fcda926d79ff430e0b337f95dbc",
            "isKey": false,
            "numCitedBy": 123,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the task of learning the maximum-likelihood polytree from data. Our first result is a performance guarantee establishing that the optimal branching (or Chow-Liu tree), which can be computed very easily, constitutes a good approximation to the best polytree. We then show that it is not possible to do very much better, since the learning problem is NP-hard even to approximately solve within some constant factor."
            },
            "slug": "Learning-Polytrees-Dasgupta",
            "title": {
                "fragments": [],
                "text": "Learning Polytrees"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A performance guarantee is established that the optimal branching (or Chow-Liu tree), which can be computed very easily, constitutes a good approximation to the best polytree."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690163"
                        ],
                        "name": "G. McLachlan",
                        "slug": "G.-McLachlan",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "McLachlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. McLachlan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2410937"
                        ],
                        "name": "K. Basford",
                        "slug": "K.-Basford",
                        "structuredName": {
                            "firstName": "Kaye",
                            "lastName": "Basford",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Basford"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 155
                            }
                        ],
                        "text": "Inspired by the success of mixture models in providing simple, effective generalizations of classical methods in many simpler density estimation settings (MacLachlan & Bashford, 1988), we consider a generalization of tree distributions known as the mixtures-of-trees (MT) model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 142
                            }
                        ],
                        "text": "The expectation-maximization (EM) algorithm provides an effective approach to solving many learning problems (Dempster, Laird, & Rubin, 1977; MacLachlan & Bashford, 1988), and has been employed with particular success in the setting of mixture models and more general latent variable models (Jordan\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 119405289,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "034a70c9fbe8e0075f5a5b3a7b06bdf7d3cab4a1",
            "isKey": false,
            "numCitedBy": 2073,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "General Introduction Introduction History of Mixture Models Background to the General Classification Problem Mixture Likelihood Approach to Clustering Identifiability Likelihood Estimation for Mixture Models via EM Algorithm Start Values for EMm Algorithm Properties of Likelihood Estimators for Mixture Models Information Matrix for Mixture Models Tests for the Number of Components in a Mixture Partial Classification of the Data Classification Likelihood Approach to Clustering Mixture Models with Normal Components Likelihood Estimation for a Mixture of Normal Distribution Normal Homoscedastic Components Asymptotic Relative Efficiency of the Mixture Likelihood Approach Expected and Observed Information Matrices Assessment of Normality for Component Distributions: Partially Classified Data Assessment of Typicality: Partially Classified Data Assessment of Normality and Typicality: Unclassified Data Robust Estimation for Mixture Models Applications of Mixture Models to Two-Way Data Sets Introduction Clustering of Hemophilia Data Outliers in Darwin's Data Clustering of Rare Events Latent Classes of Teaching Styles Estimation of Mixing Proportions Introduction Likelihood Estimation Discriminant Analysis Estimator Asymptotic Relative Efficiency of Discriminant Analysis Estimator Moment Estimators Minimum Distance Estimators Case Study Homogeneity of Mixing Proportions Assessing the Performance of the Mixture Likelihood Approach to Clustering Introduction Estimators of the Allocation Rates Bias Correction of the Estimated Allocation Rates Estimated Allocation Rates of Hemophilia Data Estimated Allocation Rates for Simulated Data Other Methods of Bias Corrections Bias Correction for Estimated Posterior Probabilities Partitioning of Treatment Means in ANOVA Introduction Clustering of Treatment Means by the Mixture Likelihood Approach Fitting of a Normal Mixture Model to a RCBD with Random Block Effects Some Other Methods of Partitioning Treatment Means Example 1 Example 2 Example 3 Example 4 Mixture Likelihood Approach to the Clustering of Three-Way Data Introduction Fitting a Normal Mixture Model to Three-Way Data Clustering of Soybean Data Multidimensional Scaling Approach to the Analysis of Soybean Data References Appendix"
            },
            "slug": "Mixture-models-:-inference-and-applications-to-McLachlan-Basford",
            "title": {
                "fragments": [],
                "text": "Mixture models : inference and applications to clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The Mixture Likelihood Approach to Clustering and the Case Study Homogeneity of Mixing Proportions Assessing the Performance of the Mixture likelihood approach toClustering."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050895"
                        ],
                        "name": "S. Lauritzen",
                        "slug": "S.-Lauritzen",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Lauritzen",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lauritzen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144845491"
                        ],
                        "name": "A. Dawid",
                        "slug": "A.-Dawid",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Dawid",
                            "middleNames": [
                                "Philip"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dawid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144238498"
                        ],
                        "name": "B. N. Larsen",
                        "slug": "B.-N.-Larsen",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Larsen",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. N. Larsen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "74208271"
                        ],
                        "name": "H.-G. Leimer",
                        "slug": "H.-G.-Leimer",
                        "structuredName": {
                            "firstName": "H.-G.",
                            "lastName": "Leimer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H.-G. Leimer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The set of conditional independencies associated with a tree distribution are readily characterized ( Lauritzen, Dawid, Larsen, & Leimer, 1990 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 20450895,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f78884604e3fb89b1fb24d2a9403191dc9e63bd3",
            "isKey": false,
            "numCitedBy": 572,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate directed Markov fields over finite graphs without positivity assumptions on the densities involved. A criterion for conditional independence of two groups of variables given a third is given and named as the directed, global Markov property. We give a simple proof of the fact that the directed, local Markov property and directed, global Markov property are equivalent and \u2013 in the case of absolute continuity w. r. t. a product measure \u2013 equivalent to the recursive factorization of densities. It is argued that our criterion is easy to use, it is sharper than that given by Kiiveri, Speed, and Carlin and equivalent to that of Pearl. It follows that our criterion cannot be sharpened."
            },
            "slug": "Independence-properties-of-directed-markov-fields-Lauritzen-Dawid",
            "title": {
                "fragments": [],
                "text": "Independence properties of directed markov fields"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "A criterion for conditional independence of two groups of variables given a third is given and named as the directed, global Markov property and it is argued that this criterion is easy to use, it is sharper than that given by Kiiveri, Speed, and Carlin and equivalent to that of Pearl."
            },
            "venue": {
                "fragments": [],
                "text": "Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143672554"
                        ],
                        "name": "Denise Draper",
                        "slug": "Denise-Draper",
                        "structuredName": {
                            "firstName": "Denise",
                            "lastName": "Draper",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Denise Draper"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38413017"
                        ],
                        "name": "S. Hanks",
                        "slug": "S.-Hanks",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Hanks",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hanks"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1912808,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "25e2cc30c561f79a0c397638c53e23e0b56907a3",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Localized-Partial-Evaluation-of-Belief-Networks-Draper-Hanks",
            "title": {
                "fragments": [],
                "text": "Localized Partial Evaluation of Belief Networks"
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194361"
                        ],
                        "name": "S. Geman",
                        "slug": "S.-Geman",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707642"
                        ],
                        "name": "D. Geman",
                        "slug": "D.-Geman",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5837272,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "459b30a9a960080f3b313e41886b1aa0e51e882c",
            "isKey": false,
            "numCitedBy": 18711,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (``annealing''), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel ``relaxation'' algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios."
            },
            "slug": "Stochastic-Relaxation,-Gibbs-Distributions,-and-the-Geman-Geman",
            "title": {
                "fragments": [],
                "text": "Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "The analogy between images and statistical mechanics systems is made and the analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations, creating a highly parallel ``relaxation'' algorithm for MAP estimation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087276313"
                        ],
                        "name": "Uue Kjjrull",
                        "slug": "Uue-Kjjrull",
                        "structuredName": {
                            "firstName": "Uue",
                            "lastName": "Kjjrull",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Uue Kjjrull"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15292951,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4edd0b973238fd26969f8ce7901fb20646619185",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Due to the general inherent complexity of inference in Bayesian networks, the need to compromise the exactitude of inference arises frequently. A scheme for reduction of complexity by enforcing additional conditional independences is investigated. The enforcement of independences is achieved through edge removals in a triangulated graph. The removal of a single edge may imply an enormous reduction of complexity, since other edges may become superruous by its removal. The approximation scheme presented has several appealing features. Most notably among these, a bound on the overall approximation error can be computed locally, the bound on the error by a series of approximations equals the sum of the bounds of the errors of the individual approximations, and the innuence of an approximation attenuates with increasing`distance' from edge removed. The scheme compares in some cases very favorably with the approximation method suggested by Jensen & Andersen (1990)."
            },
            "slug": "Approximation-of-Bayesian-Networks-through-Edge-D-Kjjrull",
            "title": {
                "fragments": [],
                "text": "Approximation of Bayesian Networks through Edge Removals D Approximation of Bayesian Networks through Edge Removals"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A scheme for reduction of complexity by enforcing additional conditional independences is investigated, which compares in some cases very favorably with the approximation method suggested by Jensen & Andersen (1990)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143648560"
                        ],
                        "name": "P. Spirtes",
                        "slug": "P.-Spirtes",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Spirtes",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Spirtes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[61] develop a method for scoring equivalence classes of DAGs with hidden variables in the special case when the dependence relationships are jointly Gaussian, augmented by a heuristic search algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13639275,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "81a639fcb49984aa1a8a7622306ab5d12c6b5dc0",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Following the terminology of Lauritzen et. al. (1990) say that a probability measure over a set of variables V satisfies the local directed Markov property for a directed acyclic graph (DAG) G with vertices V if and only if for every Win V, Wis independent of the set of all its non-descendants conditional on the set of its parents. One natural question that arises with respect to DAGs is when two DAGs are \"statistically equivalent\". One interesting sense of \"statistical equivalence\" is \"conditional independence equivalence\" which holds when two DAGs entail the same set of conditional independence relations. In the case of DAGs, conditional independence equivalence also corresponds to a variety of other natural senses of statistical equivalence (such as representing the same set of distributions). Theorems characterizing conditional independence equivalence for directed acyclic graphs and that can be used as the basis for polynomial time algorithms for checking conditional independence equivalence were provided by Verma and Pearl (1990), and Frydenberg (1990). The question we will examine is how to extend these results to cases where a DAG may have latent (unmeasured) variables or selection bias (i.e. some of the variables in the DAG have been conditioned on.) Conditional independence equivalence is of interest in part because there are algorithms for constructing DAGs with latent variables and selection bias that are based on observed conditional independence relations. For this class of algorithms, it is impossible to determine which of two conditional independence equivalent causal structures generated a given probability distribution, given only the set of conditional independence and dependence relations true of the observed distribution. We will describe a polynomial (in the number of vertices) time algorithm for determining when two DAGs which may have latent variables or selection bias are conditional independence equivalent. A DAG G entails a conditional independence relation R if and only if R is true in every probability measure satisfying the local directed Markov property for G. (We place definitions and sets of variables in boldface.) Pearl, Geiger, and Verma (Pearl 1988) have shown that there is a graphical relation, ct-separation, that holds among three disjoint sets of variable A, and B, and C in DAG G if and only if G entails that A is independent of B given C. A vertex Y is a collider on an undirected path U if U contains a subpath X \u2794 Y ~ Z. Say that a vertex V on an undirected path U between X and Y is active on U given Z (Z not containing X and Y) if and only if either V is not a collider on U and not in Z, or V is a collider on U and is an ancestor of Z. For three disjoint sets of variables A, B, and C, A is d-connected to B given C in graph G, if and only if there is an undirected path from some member of A to a member of B such that every vertex on U is active given C; for three disjoint sets of variables A, B, and C, A is dseparated from B given C in graph G, if and only A is not cl-connected to B given C. Two DAGs are conditional independence equivalent if and only if they have the same vertices and entail the same set of conditional independence relations . If two DAGs G 1 and G2 are conditional independence equivalent, the set of distributions that satisfy the local directed Markov property for G1 equals the set of distribution that satisfy the local directed Markov property for G2\u2022 Theorems that provide the basis for polynomial time algorithms for testing conditional independence equivalence for DAGs were given in Verma and Pearl (1990), for cyclic directed graphs in Richardson (1994), and for directed acyclic graphs with latent variables in Spirtes and Verma (1992)."
            },
            "slug": "A-Polynomial-Time-Algorithm-For-Determining-DAG-in-Spirtes",
            "title": {
                "fragments": [],
                "text": "A Polynomial Time Algorithm For Determining DAG Equivalence in the Presence of Latent Variables and Selection Bias"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69495445"
                        ],
                        "name": "Dorothy T. Thayer",
                        "slug": "Dorothy-T.-Thayer",
                        "structuredName": {
                            "firstName": "Dorothy",
                            "lastName": "Thayer",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dorothy T. Thayer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 298,
                                "start": 278
                            }
                        ],
                        "text": "\u2026an effective approach to solving many learning problems (Dempster, Laird, & Rubin, 1977; MacLachlan & Bashford, 1988), and has been employed with particular success in the setting of mixture models and more general latent variable models (Jordan & Jacobs, 1994; Jelinek, 1997; Rubin & Thayer, 1983)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 123437256,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5e9222ee44916c976c80f11303002e850de0c63e",
            "isKey": false,
            "numCitedBy": 579,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "The details of EM algorithms for maximum likelihood factor analysis are presented for both the exploratory and confirmatory models. The algorithm is essentially the same for both cases and involves only simple least squares regression operations; the largest matrix inversion required is for aq \u00d7q symmetric matrix whereq is the matrix of factors. The example that is used demonstrates that the likelihood for the factor analysis model may have multiple modes that are not simply rotations of each other; such behavior should concern users of maximum likelihood factor analysis and certainly should cast doubt on the general utility of second derivatives of the log likelihood as measures of precision of estimation."
            },
            "slug": "EM-algorithms-for-ML-factor-analysis-Rubin-Thayer",
            "title": {
                "fragments": [],
                "text": "EM algorithms for ML factor analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767011"
                        ],
                        "name": "M. Noordewier",
                        "slug": "M.-Noordewier",
                        "structuredName": {
                            "firstName": "Michiel",
                            "lastName": "Noordewier",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Noordewier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804836"
                        ],
                        "name": "G. Towell",
                        "slug": "G.-Towell",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Towell",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Towell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734317"
                        ],
                        "name": "J. Shavlik",
                        "slug": "J.-Shavlik",
                        "structuredName": {
                            "firstName": "Jude",
                            "lastName": "Shavlik",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shavlik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1242670,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a080a28ff7fb3b58fa8cd7123a473c5e75bf46e1",
            "isKey": false,
            "numCitedBy": 195,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the application of a hybrid symbolic/connectionist machine learning algorithm to the task of recognizing important genetic sequences. The symbolic portion of the KBANN system utilizes inference rules that provide a roughly-correct method for recognizing a class of DNA sequences known as eukaryotic splice-junctions. We then map this \"domain theory\" into a neural network and provide training examples. Using the samples, the neural network's learning algorithm adjusts the domain theory so that it properly classifies these DNA sequences. Our procedure constitutes a general method for incorporating preexisting knowledge into artificial neural networks. We present an experiment in molecular genetics that demonstrates the value of doing so."
            },
            "slug": "Training-Knowledge-Based-Neural-Networks-to-Genes-Noordewier-Towell",
            "title": {
                "fragments": [],
                "text": "Training Knowledge-Based Neural Networks to Recognize Genes"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This work describes the application of a hybrid symbolic/connectionist machine learning algorithm to the task of recognizing important genetic sequences and presents an experiment in molecular genetics that demonstrates the value of doing so."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804104"
                        ],
                        "name": "R. Zemel",
                        "slug": "R.-Zemel",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zemel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zemel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 130
                            }
                        ],
                        "text": "The \u201cbars\u201d problem is a benchmark structure learning problem for unsupervised learning algorithms in the neural network literature (Dayan & Zemel, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 22
                            }
                        ],
                        "text": "By way of comparison, Dayan and Zemel (1995) examined two training methods and the structure was recovered in 27 and respectively 69 cases out of 100."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 243
                            }
                        ],
                        "text": "We trained models with m = 2, 3, . . . , and evaluated the models on a validation set of size 100 to choose the final values of m and of the smoothing parameter \u03b1. Typical values for l in the literature are l = 4, 5 ; we choose l = 5 following Dayan and Zemel (1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8079644,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68ad90dad6eaff3570af48c16c34d9de12d810d9",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "If different causes can interact on any occasion to generate a set of patterns, then systems modeling the generation have to model the interaction too. We discuss a way of combining multiple causes that is based on the Integrated Segmentation and Recognition architecture of Keeler et al. (1991). It is more cooperative than the scheme embodied in the mixture of experts architecture, which insists that just one cause generate each output, and more competitive than the noisy-or combination function, which was recently suggested by Saund (1994a,b). Simulations confirm its efficacy."
            },
            "slug": "Competition-and-Multiple-Cause-Models-Dayan-Zemel",
            "title": {
                "fragments": [],
                "text": "Competition and Multiple Cause Models"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work discusses a way of combining multiple causes that is more cooperative than the scheme embodied in the mixture of experts architecture, and more competitive than the noisy-or combination function, which was recently suggested by Saund (1994a,b)."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 77
                            }
                        ],
                        "text": "The assumptions allowing us to define this prior are explicated by Meila\u0306 and Jaakkola (2000) and parallel the reasoning of Heckerman et al. (1995) for general Bayes nets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 102
                            }
                        ],
                        "text": "Given that the number of all undirected tree structures over n variables is nn\u22122, this result (Meila\u0306 & Jaakkola, 2000) is quite surprising."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "As in many areas of Machine Learning, we are using the variational method of approximating an intractable probability distribution by a tractable one (Jaakkola, 2001; Beal, 2003; Winn, 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 118538409,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a9273ea2f53a74530527eab71f9b1c8acca06f0c",
            "isKey": false,
            "numCitedBy": 336,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Tutorial topics \u2022 A bit of history \u2022 Examples of variational methods \u2022 A brief intro to graphical models \u2022 Variational mean field theory \u2013 Accuracy of variational mean field \u2013 Structured mean field theory \u2022 Variational methods in Bayesian estimation \u2022 Convex duality and variational factorization methods \u2013 Example: variational inference and the QMR-DT Variational methods \u2022 Classical setting: \" finding the extremum of an integral involving a function and its derivatives \" Example: finding the trajectory of a particle under external field \u2022 The key idea here is that the problem of interest is formulated as an optimization problem Variational methods cont'd \u2022 Variational methods have a long history in physics, statistics, control theory as well as economics. \u2013 calculus of variations (physics) \u2013 linear/non-linear moments problems (statistics) \u2013 dynamic programming (control theory) \u2022 Variational formulations appear naturally also in machine learning contexts: \u2013 regularization theory \u2013 maximum entropy estimation \u2022 Recently variational methods been used and further developed in the context of approximate inference and estimation Examples of variational methods \u2022 In classical examples the formulation itself is given but for us this is one of the key problems \u2022 We provide here a few examples that highlight 1. how to cast problems as optimization problems 2. how to find an approximate solution when the exact solution is not feasible \u2022 The examples we use involve a) finite element methods for solving differential equations b) large deviation methods (Chernoff bound)"
            },
            "slug": "Tutorial-on-variational-approximation-methods-Jaakkola",
            "title": {
                "fragments": [],
                "text": "Tutorial on variational approximation methods"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This tutorial provides a few examples that highlight how to cast problems as optimization problems and how to find an approximate solution when the exact solution is not feasible."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33734211"
                        ],
                        "name": "R. Sibson",
                        "slug": "R.-Sibson",
                        "structuredName": {
                            "firstName": "Robin",
                            "lastName": "Sibson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sibson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 45115551,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "facd2ebb43cf75ec7d4bef3b9eaaaaca97600805",
            "isKey": false,
            "numCitedBy": 1139,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Main point Sibson gives an O(n 2) algorithm for single-linkage clustering, and proves that this algorithm achieves the theoretically optimal lower time bound for obtaining a single-linkage dendrogram. This improves upon the naive O(n 3) implementation of single linkage clustering. A single linkage dendrogram is a tree, where each level of the tree corresponds to a different threshold dissimilarity measure h. The nodes of a dataset are grouped into \" equivalence classes \" c(h) at each level of the dendrogram, where two classes C i and C j are merged if there is a pair of \" OTU's \" (vertices) v i \u2208 C i and v j \u2208 C j such that the dissimilarity measure between v i and v j is less than h, or D(v i , v j) < h. For example, consider a set of 10 vertices v 1 ,. .. , v 10 for which the dissimilarity matrix D is given below, with D ij equal to the dissimilarity between v i and v j. Suppose we take four cutoff dissimilarity measures h 1 , h 2 , h 3 , h 4 and produce the dendrogram according to these thresholds. An example illustrating how the 10 vertices are grouped into equivalence classes at each level is shown in Figure 1. Since no dissimilarity is at or below 1, each vertex or \" OTU \" is its own equivalence class at the level corresponding to h 1 = 1. At the next level, however, we see that some classes have been merged together because several dissimilarity measures are below h 2 = 2. We can see that c(h 2) consists of 6 equivalence classes, c(h 3) has 3 equivalence classes, and c(h 4 = 4) aggregates all the vertices into one equivalence class. In single linkage clustering, the number of levels in the tree is determined by the nearest-neighbor criterion \u2013 at each level, at least one new merge is made between two clusters, and the merge is made for clusters C i and C j if the minimal distance between vertices v i \u2208 C i and v j \u2208 C j is the smallest such distance across all the clusters. In other words, the nearest neighbors between clusters C j and C i are found, and if these neighbors are closer than all the other nearest-neighbor pairs, then C i and C \u2026"
            },
            "slug": "SLINK:-An-Optimally-Efficient-Algorithm-for-the-Sibson",
            "title": {
                "fragments": [],
                "text": "SLINK: An Optimally Efficient Algorithm for the Single-Link Cluster Method"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "Sibson gives an O(n 2) algorithm for single-linkage clustering, and proves that this algorithm achieves the theoretically optimal lower time bound for obtaining a single- linkage dendrogram."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. J."
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35191477"
                        ],
                        "name": "F. V. Jensen",
                        "slug": "F.-V.-Jensen",
                        "structuredName": {
                            "firstName": "Finn",
                            "lastName": "Jensen",
                            "middleNames": [
                                "Verner"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. V. Jensen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 61412478,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3febde16cb99b8107fecff79905ca61a5e8cd170",
            "isKey": false,
            "numCitedBy": 1493,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Computational modelling of probability has become a major part of automated decision support systems. In this book, the principal ideas of probabilistic reasoning - known as Bayesian networks - are outlined and their practical implications illustrated. The book is intended for MSc students in knowledge-based systems, artificial intelligence and statistics, and for professionals in decision support systems applications and research."
            },
            "slug": "An-introduction-to-Bayesian-networks-Jensen",
            "title": {
                "fragments": [],
                "text": "An introduction to Bayesian networks"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "The principal ideas of probabilistic reasoning - known as Bayesian networks - are outlined and their practical implications illustrated and are intended for MSc students in knowledge-based systems, artificial intelligence and statistics, and for professionals in decision support systems applications and research."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143774015"
                        ],
                        "name": "A. Becker",
                        "slug": "A.-Becker",
                        "structuredName": {
                            "firstName": "Ann",
                            "lastName": "Becker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Becker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144527211"
                        ],
                        "name": "D. Geiger",
                        "slug": "D.-Geiger",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Geiger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geiger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5055742,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "78097f6d232d06349e1de4457c48128ca1fa2a1a",
            "isKey": false,
            "numCitedBy": 88,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Au algorithm is developed for findiug a close to optimal junction tree of a given graph G. The algorithm has a worst case complexity O(ckna) where a and c are constants, n is the nmnber of vertices, and k is the size of the largest clique in a juuction tree of G in which this size is minimized. The algorithm guarantees that the logarithm of the size of the state space of the heaviest clique in the junction tree produced is less than a constant factor off the optional value. When k = O(log n) our algorithm yields a polynomial inference algorithm for Bayesian networks."
            },
            "slug": "A-sufficiently-fast-algorithm-for-finding-close-to-Becker-Geiger",
            "title": {
                "fragments": [],
                "text": "A sufficiently fast algorithm for finding close to optimal junction trees"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "The Au algorithm is developed for findiug a close to optimal junction tree of a given graph G in which this size is minimized and yields a polynomial inference algorithm for Bayesian networks."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728509"
                        ],
                        "name": "L. M. D. Campos",
                        "slug": "L.-M.-D.-Campos",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Campos",
                            "middleNames": [
                                "M.",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. M. D. Campos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690193"
                        ],
                        "name": "J. Huete",
                        "slug": "J.-Huete",
                        "structuredName": {
                            "firstName": "Juan",
                            "lastName": "Huete",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Huete"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3141957,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "45b5388736f5fab45a354cb3db032e93c58da495",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Decomposable dependency models and their graphical counterparts, i.e., chordal graphs, possess a number of interesting and useful properties. On the basis of two characterizations of decomposable models in terms of independence relationships, we develop an exact algorithm for recovering the chordal graphical representation of any given decomposable model. We also propose an algorithm for learning chordal approximations of dependency models isomorphic to general undirected graphs."
            },
            "slug": "Algorithms-for-Learning-Decomposable-Models-and-Campos-Huete",
            "title": {
                "fragments": [],
                "text": "Algorithms for Learning Decomposable Models and Chordal Graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "An exact algorithm for recovering the chordal graphical representation of any given decomposable model is developed and an algorithm for learning chordal approximations of dependency models isomorphic to general undirected graphs is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2268269"
                        ],
                        "name": "G. Eyink",
                        "slug": "G.-Eyink",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Eyink",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Eyink"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143819746"
                        ],
                        "name": "J. Restrepo",
                        "slug": "J.-Restrepo",
                        "structuredName": {
                            "firstName": "Juan",
                            "lastName": "Restrepo",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Restrepo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "By contrast, the DW system is a standard data assimilation benchmark (see Miller et al., 1994; Eyinck and Restrepo, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16960937,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "9c76bda9da9a2c03103f7bdd823be576f40d1f32",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "The effective action provides an appropriate cost function to determine most probable (or optimal) histories for nonlinear dynamics with strong noise. In such strong-coupling problems, a nonperturbative technique is required to calculate the effective action. We have proposed a Rayleigh\u2013Ritz variational approximation, which employs simple moment-closures or intuitive guesses of the statistics to calculate the effective action. We consider here an application to climate dynamics, within a simple \u201cbimodal\u201d Langevin model similar to that proposed by C. Nicolis and G. Nicolis [Tellus33:225 (1981)]. Capturing climate state transitions even in this simple model is known to present a serious problem for standard methods of data assimilation. In contrast, it is shown that the effective action for the climate history is already well-approximated by a one-moment closure and that the optimal, minimizing history robustly tracks climate change, even with large observation errors. Furthermore, the Hessian of the effective action provides the ensemble variance as a realistic measure of confidence level in the predicted optimal history."
            },
            "slug": "Most-Probable-Histories-for-Nonlinear-Dynamics:-Eyink-Restrepo",
            "title": {
                "fragments": [],
                "text": "Most Probable Histories for Nonlinear Dynamics: Tracking Climate Transitions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145430701"
                        ],
                        "name": "J. Pearl",
                        "slug": "J.-Pearl",
                        "structuredName": {
                            "firstName": "Judea",
                            "lastName": "Pearl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pearl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 94
                            }
                        ],
                        "text": "Later research extended this early work by first considering general singly-connected graphs (Pearl, 1988), and then considering graphs with arbitrary (acylic) patterns of connectivity (Cowell et al., 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 193
                            }
                        ],
                        "text": "Probabilistic inference has become a core technology in AI, largely due to developments in graph-theoretic methods for the representation and manipulation of complex probability distributions (Pearl, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 180
                            }
                        ],
                        "text": "Trees also have the virtue that probabilistic inference is guaranteed to be efficient, and indeed historically the earliest research in AI on efficient inference focused on trees (Pearl, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 32583695,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "70ef29e6f0ce082bb8a47fd85b9bfb7cc0f20c93",
            "isKey": false,
            "numCitedBy": 18219,
            "numCiting": 230,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nProbabilistic Reasoning in Intelligent Systems is a complete andaccessible account of the theoretical foundations and computational methods that underlie plausible reasoning under uncertainty. The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic. The author distinguishes syntactic and semantic approaches to uncertainty\u0097and offers techniques, based on belief networks, that provide a mechanism for making semantics-based systems operational. Specifically, network-propagation techniques serve as a mechanism for combining the theoretical coherence of probability theory with modern demands of reasoning-systems technology: modular declarative inputs, conceptually meaningful inferences, and parallel distributed computation. Application areas include diagnosis, forecasting, image interpretation, multi-sensor fusion, decision support systems, plan recognition, planning, speech recognition\u0097in short, almost every task requiring that conclusions be drawn from uncertain clues and incomplete information. \nProbabilistic Reasoning in Intelligent Systems will be of special interest to scholars and researchers in AI, decision theory, statistics, logic, philosophy, cognitive psychology, and the management sciences. Professionals in the areas of knowledge-based systems, operations research, engineering, and statistics will find theoretical and computational tools of immediate practical use. The book can also be used as an excellent text for graduate-level courses in AI, operations research, or applied probability."
            },
            "slug": "Probabilistic-reasoning-in-intelligent-systems-of-Pearl",
            "title": {
                "fragments": [],
                "text": "Probabilistic reasoning in intelligent systems - networks of plausible inference"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic."
            },
            "venue": {
                "fragments": [],
                "text": "Morgan Kaufmann series in representation and reasoning"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804836"
                        ],
                        "name": "G. Towell",
                        "slug": "G.-Towell",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Towell",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Towell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734317"
                        ],
                        "name": "J. Shavlik",
                        "slug": "J.-Shavlik",
                        "structuredName": {
                            "firstName": "Jude",
                            "lastName": "Shavlik",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shavlik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For the first series, we compared our model\u2019s performance against the reported results of [ 65 ] and [52] who used multilayer neural networks and knowledge-based neural networks for the same task."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 2357730,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b29884885401d12299a01b0eae099f425dd32e1",
            "isKey": false,
            "numCitedBy": 140,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose and empirically evaluate a method for the extraction of expert-comprehensible rules from trained neural networks. Our method operates in the context of a three-step process for learning that uses rule-based domain knowledge in combination with neural networks. Empirical tests using real-worlds problems from molecular biology show that the rules our method extracts from trained neural networks: closely reproduce the accuracy of the network from which they came, are superior to the rules derived by a learning system that directly refines symbolic rules, and are expert-comprehensible."
            },
            "slug": "Interpretation-of-Artificial-Neural-Networks:-into-Towell-Shavlik",
            "title": {
                "fragments": [],
                "text": "Interpretation of Artificial Neural Networks: Mapping Knowledge-Based Neural Networks into Rules"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Empirical tests show that the rules the method extracts from trained neural networks: closely reproduce the accuracy of the network from which they came, are superior to the rules derived by a learning system that directly refines symbolic rules, and are expert-comprehensible."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2078327"
                        ],
                        "name": "G. Evensen",
                        "slug": "G.-Evensen",
                        "structuredName": {
                            "firstName": "Geir",
                            "lastName": "Evensen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Evensen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8521616"
                        ],
                        "name": "P. Leeuwen",
                        "slug": "P.-Leeuwen",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Leeuwen",
                            "middleNames": [
                                "Jan",
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Leeuwen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7355435,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "4bb38fa3cfd67ce9047bbf85fec3f5dc3fa18165",
            "isKey": false,
            "numCitedBy": 496,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "It is formally proved that the general smoother for nonlinear dynamics can be formulated as a sequential method, that is, observations can be assimilated sequentially during a forward integration. The general filter can be derived from the smoother and it is shown that the general smoother and filter solutions at the final time become identical, as is expected from linear theory. Then, a new smoother algorithm based on ensemble statistics is presented and examined in an example with the Lorenz equations. The new smoother can be computed as a sequential algorithm using only forward-in-time model integrations. It bears a strong resemblance with the ensemble Kalman filter. The difference is that every time a new dataset is available during the forward integration, an analysis is computed for all previous times up to this time. Thus, the first guess for the smoother is the ensemble Kalman filter solution, and the smoother estimate provides an improvement of this, as one would expect a smoother to do. The method is demonstrated in this paper in an intercomparison with the ensemble Kalman filter and the ensemble smoother introduced by van Leeuwen and Evensen, and it is shown to be superior in an application with the Lorenz equations. Finally, a discussion is given regarding the properties of the analysis schemes when strongly non-Gaussian distributions are used. It is shown that in these cases more sophisticated analysis schemes based on Bayesian statistics must be used."
            },
            "slug": "An-ensemble-Kalman-smoother-for-nonlinear-dynamics-Evensen-Leeuwen",
            "title": {
                "fragments": [],
                "text": "An ensemble Kalman smoother for nonlinear dynamics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 276,
                                "start": 263
                            }
                        ],
                        "text": "\u2026an effective approach to solving many learning problems (Dempster, Laird, & Rubin, 1977; MacLachlan & Bashford, 1988), and has been employed with particular success in the setting of mixture models and more general latent variable models (Jordan & Jacobs, 1994; Jelinek, 1997; Rubin & Thayer, 1983)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12495425,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "231f6de83cfa4d641da1681e97a11b689a48e3aa",
            "isKey": false,
            "numCitedBy": 2251,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "The speech recognition problem hidden Markov models the acoustic model basic language modelling the Viterbi search hypothesis search on a tree and the fast match elements of information theory the complexity of tasks - the quality of language models the expectation - maximization algorithm and its consequences decision trees and tree language models phonetics from orthography - spelling-to-base from mappings triphones and allophones maximum entropy probability estimation and language models three applications of maximum entropy estimation to language modelling estimation of probabilities from counts and the Back-Off method."
            },
            "slug": "Statistical-methods-for-speech-recognition-Jelinek",
            "title": {
                "fragments": [],
                "text": "Statistical methods for speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The speech recognition problem hidden Markov models the acoustic model basic language modelling the Viterbi search hypothesis search on a tree and the fast match elements of information theory."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110627313"
                        ],
                        "name": "Robert N. Miller",
                        "slug": "Robert-N.-Miller",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Miller",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert N. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1869954"
                        ],
                        "name": "M. Ghil",
                        "slug": "M.-Ghil",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Ghil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ghil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103082746"
                        ],
                        "name": "F. Gauthiez",
                        "slug": "F.-Gauthiez",
                        "structuredName": {
                            "firstName": "F.",
                            "lastName": "Gauthiez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Gauthiez"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121273908,
            "fieldsOfStudy": [
                "Physics",
                "Environmental Science"
            ],
            "id": "e0067faa87b063049db367290bd25d2f44ed2705",
            "isKey": false,
            "numCitedBy": 437,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Advanced data assimilation methods are applied to simple but highly nonlinear problems. The dynamical systems studied here are the stochastically forced double well and the Lorenz model. In both systems, linear approximation of the dynamics about the critical points near which regime transitions occur is not always sufficient to track their occurrence or nonoccurrence. Straightforward application of the extended Kalman filter yields mixed results. The ability of the extended Kalman filter to track transitions of the double-well system from one stable critical point to the other depends on the frequency and accuracy of the observations relative to the mean-square amplitude of the stochastic forcing. The ability of the filter to track the chaotic trajectories of the Lorenz model is limited to short times, as is the ability of strong-constraint variational methods. Examples are given to illustrate the difficulties involved, and qualitative explanations for these difficulties are provided. Three gene..."
            },
            "slug": "Advanced-data-assimilation-in-strongly-nonlinear-Miller-Ghil",
            "title": {
                "fragments": [],
                "text": "Advanced data assimilation in strongly nonlinear dynamical systems"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The ability of the extended Kalman filter to track transitions of the double-well system from one stable critical point to the other depends on the frequency and accuracy of the observations relative to the mean-square amplitude of the stochastic forcing."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749650"
                        ],
                        "name": "B. Frey",
                        "slug": "B.-Frey",
                        "structuredName": {
                            "firstName": "Brendan",
                            "lastName": "Frey",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Frey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 79
                            }
                        ],
                        "text": "In Figure 13 we compare our results (for m = 32) with the results published by Frey et al. (1996)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 147
                            }
                        ],
                        "text": "\u2026(2) mixture of factorial distributions (MF), (3) the UNIX \u201cgzip\u201d compression program, (4) the Helmholtz Machine, trained by the wake-sleep algorithm (Frey et al., 1996) (HWS), (5) the same Helmholtz Machine where a mean field approximation was used for training (HMF), (5) a fully observed and\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 86
                            }
                        ],
                        "text": "These datasets, as well as the training conditions that we employed, are described by Frey, Hinton, and Dayan (1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2527241,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cbb0362cbfef094dbed0907329e6057dc5d09714",
            "isKey": true,
            "numCitedBy": 58,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The wake-sleep algorithm (Hinton, Dayan, Frey and Neal 1995) is a relatively efficient method of fitting a multilayer stochastic generative model to high-dimensional data. In addition to the top-down connections in the generative model, it makes use of bottom-up connections for approximating the probability distribution over the hidden units given the data, and it trains these bottom-up connections using a simple delta rule. We use a variety of synthetic and real data sets to compare the performance of the wake-sleep algorithm with Monte Carlo and mean field methods for fitting the same generative model and also compare it with other models that are less powerful but easier to fit."
            },
            "slug": "Does-the-Wake-sleep-Algorithm-Produce-Good-Density-Frey-Hinton",
            "title": {
                "fragments": [],
                "text": "Does the Wake-sleep Algorithm Produce Good Density Estimators?"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work uses a variety of synthetic and real data sets to compare the performance of the wake-sleep algorithm with Monte Carlo and mean field methods for fitting the same generative model and also compares it with other models that are less powerful but easier to fit."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145872308"
                        ],
                        "name": "J. Whitaker",
                        "slug": "J.-Whitaker",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Whitaker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Whitaker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3370716"
                        ],
                        "name": "T. Hamill",
                        "slug": "T.-Hamill",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hamill",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hamill"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The most popular sequential method is called the ensemble Kalman filter, a simplified Monte Carlo approach (Evensen and van Leeuwen, 2000; Whitaker and Hamill, 2002), whereas more advanced techniques include Bayesian sequential MCMC methods (Golightly and Wilkinson, 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14050790,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "87d76a485c00b7145236ddc0f2533d2107dea0e2",
            "isKey": false,
            "numCitedBy": 1384,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "The ensemble Kalman filter (EnKF) is a data assimilation scheme based on the traditional Kalman filter update equation. An ensemble of forecasts are used to estimate the background-error covariances needed to compute the Kalman gain. It is known that if the same observations and the same gain are used to update each member of the ensemble, the ensemble will systematically underestimate analysis-error covariances. This will cause a degradation of subsequent analyses and may lead to filter divergence. For large ensembles, it is known that this problem can be alleviated by treating the observations as random variables, adding random perturbations to them with the correct statistics. Two important consequences of sampling error in the estimate of analysis-error covariances in the EnKF are discussed here. The first results from the analysis-error covariance being a nonlinear function of the backgrounderror covariance in the Kalman filter. Due to this nonlinearity, analysis-error covariance estimates may be negatively biased, even if the ensemble background-error covariance estimates are unbiased. This problem must be dealt with in any Kalman filter\u2010based ensemble data assimilation scheme. A second consequence of sampling error is particular to schemes like the EnKF that use perturbed observations. While this procedure gives asymptotically correct analysis-error covariance estimates for large ensembles, the addition of perturbed observations adds an additional source of sampling error related to the estimation of the observation-error covariances. In addition to reducing the accuracy of the analysis-error covariance estimate, this extra source of sampling error increases the probability that the analysis-error covariance will be underestimated. Because of this, ensemble data assimilation methods that use perturbed observations are expected to be less accurate than those which do not. Several ensemble filter formulations have recently been proposed that do not require perturbed observations. This study examines a particularly simple implementation called the ensemble square root filter, or EnSRF. The EnSRF uses the traditional Kalman gain for updating the ensemble mean but uses a \u2018\u2018reduced\u2019\u2019 Kalman gain to update deviations from the ensemble mean. There is no additional computational cost incurred by the EnSRF relative to the EnKF when the observations have independent errors and are processed one at a time. Using a hierarchy of perfect model assimilation experiments, it is demonstrated that the elimination of the sampling error associated with the perturbed observations makes the EnSRF more accurate than the EnKF for the same ensemble size."
            },
            "slug": "Ensemble-Data-Assimilation-without-Perturbed-Whitaker-Hamill",
            "title": {
                "fragments": [],
                "text": "Ensemble Data Assimilation without Perturbed Observations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3472959"
                        ],
                        "name": "C. Rasmussen",
                        "slug": "C.-Rasmussen",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Rasmussen",
                            "middleNames": [
                                "Edward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rasmussen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32536207"
                        ],
                        "name": "D. Camp",
                        "slug": "D.-Camp",
                        "structuredName": {
                            "firstName": "Drew",
                            "lastName": "Camp",
                            "middleNames": [
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Camp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2713302"
                        ],
                        "name": "M. Revow",
                        "slug": "M.-Revow",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Revow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Revow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2601226"
                        ],
                        "name": "R. Kustra",
                        "slug": "R.-Kustra",
                        "structuredName": {
                            "firstName": "Rafal",
                            "lastName": "Kustra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kustra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761784"
                        ],
                        "name": "R. Tibshirani",
                        "slug": "R.-Tibshirani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tibshirani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tibshirani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 123772066,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4c678012d28757921853ef98dbbbe36a1a804e69",
            "isKey": false,
            "numCitedBy": 76,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "This manual describes the preliminary release of the DELVE environment. Some features described here have not yet implemented, as noted. Support for regression tasks is presently somewhat more developed than that for classiication tasks. We recommend that you exercise caution when using this version of DELVE for real work, as it is possible that bugs remain in the software. We hope that you will send us reports of any problems you encounter, as well as any other comments you may have on the software or manual, at the e-mail address below. Please mention the version number of the manual and/or the software with any comments you send. All Rights Reserved Permission to use, copy, modify, and distribute this software and its documentation for non-commercial purposes only is hereby granted without fee, provided that the above copyright notice appears in all copies and that both the copyright notice and this permission notice appear in supporting documentation, and that the name of The University of Toronto not be used in advertising or publicity pertaining to distribution of the software without speciic, written prior permission. The University of Toronto makes no representations about the suitability of this software for any purpose. It is provided \\as is\" without express or implied warranty. The University of Toronto disclaims all warranties with regard to this software, including all implied warranties of merchantability and tness. In no event shall the University of Toronto be liable for any special, indirect or consequential damages or any damages whatsoever resulting from loss of use, data or proots, whether in an action of contract, negligence or other tortious action, arising out of or in connection with the use or performance of this software. If you publish results obtained using DELVE, please cite this manual, and mention the version number of the software that you used."
            },
            "slug": "The-delve-manual-Rasmussen-Neal",
            "title": {
                "fragments": [],
                "text": "The delve manual"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "This manual describes the preliminary release of the DELVE environment, and recommends that you exercise caution when using this version of DELVE for real work, as it is possible that bugs remain in the software."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51222978"
                        ],
                        "name": "P. Mazur",
                        "slug": "P.-Mazur",
                        "structuredName": {
                            "firstName": "P\u00e9ter",
                            "lastName": "Mazur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Mazur"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123530867,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "927e9d6a05476c8c5f874ec48729d7257bbba79c",
            "isKey": false,
            "numCitedBy": 1084,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-theory-of-brownian-motion-Mazur",
            "title": {
                "fragments": [],
                "text": "On the theory of brownian motion"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1959
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35191477"
                        ],
                        "name": "F. V. Jensen",
                        "slug": "F.-V.-Jensen",
                        "structuredName": {
                            "firstName": "Finn",
                            "lastName": "Jensen",
                            "middleNames": [
                                "Verner"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. V. Jensen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120177012"
                        ],
                        "name": "Frank Jensen",
                        "slug": "Frank-Jensen",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Jensen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frank Jensen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5505903,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "523db2d9c2b68246a0e102ba4144a0b3162ba810",
            "isKey": false,
            "numCitedBy": 206,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Optimal-junction-Trees-Jensen-Jensen",
            "title": {
                "fragments": [],
                "text": "Optimal junction Trees"
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1988445"
                        ],
                        "name": "Thomas Verma",
                        "slug": "Thomas-Verma",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Verma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Verma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145430701"
                        ],
                        "name": "J. Pearl",
                        "slug": "J.-Pearl",
                        "structuredName": {
                            "firstName": "Judea",
                            "lastName": "Pearl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pearl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2134915,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "afb8f307e3cdbe4e2915cd5a365d1ea0fcad542a",
            "isKey": false,
            "numCitedBy": 273,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-Algorithm-for-Deciding-if-a-Set-of-Observed-Has-Verma-Pearl",
            "title": {
                "fragments": [],
                "text": "An Algorithm for Deciding if a Set of Observed Independencies Has a Causal Explanation"
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2986916"
                        ],
                        "name": "E. W. Steeg",
                        "slug": "E.-W.-Steeg",
                        "structuredName": {
                            "firstName": "Evan",
                            "lastName": "Steeg",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. W. Steeg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[63] and [17] directly find dependencies in discrete databases by counting coincidences in subsamples."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 124942233,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8b994ced581a7c42e316cec2716a6876836bc755",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 170,
            "paperAbstract": {
                "fragments": [],
                "text": "The protein structure prediction problem (PSP) is one of the central problems in molecular and structural biology. A computational method that could produce a correct detailed three-dimensional structural model for a protein, given its linear sequence of amino acids, would greatly accelerate progress in the biomedical sciences and industries. This thesis presents PSP as a combinatorial optimization problem, the most straightforward formulations of which require search of an exponentially-large conformation space and are known to be NP-Hard. This otherwise intractable search can in practice be reduced or eliminated through the discovery and use of motifs. Motifs are abstractions of observed patterns that encode structurally important relationships among constituent parts of a complex object like a protein tertiary structure. Motif discovery is accomplished by particular combinatorial search and statistical estimation methods. \nThis thesis explores in detail two particular motif discovery subproblems, and discusses how their solutions can be applied to the overall structure prediction problem: (1) For a complex multi-stage prediction task, what makes a good intermediate representation language? We address this question by presenting and analyzing methods for the discovery of protein secondary structure classes that are more predictable from amino acid sequence than the standard classes of $\\alpha$-helix, $\\beta$-sheet, and \"random coil\". (2) Given a database of M objects, each characterized by values $a\\sb{ij}\\in {\\cal A}\\sb{j}$ for each of N discrete variables $\\{c\\sb{j}\\}\\sbsp{j=1}{N},$ return the list of \"most interesting\" higher-order features $\\gamma\\sb{l},$ i.e., sets of $k\\sb{l}$ variables with highest estimated correlation, for any $2 \\le k\\sb{l} \\le N$. In the PSP context, the problem is the detection of correlations between amino acid residues in an aligned set of evolutionarily-related protein sequences. We present and analyze a fast procedure, based on multinomial sampling and a novel coding scheme, that avoids the exhaustive search, prior limits on the order k, and exponentially large parameter space of other methods. \nThe focus of this thesis is PSP, but the techniques and analysis are also aimed at wider application to other hard, multi-stage prediction problems."
            },
            "slug": "Automated-motif-discovery-in-protein-structure-Hinton-Steeg",
            "title": {
                "fragments": [],
                "text": "Automated motif discovery in protein structure prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This thesis presents PSP as a combinatorial optimization problem, and presents and analyze a fast procedure, based on multinomial sampling and a novel coding scheme, that avoids the exhaustive search, prior limits on the order k, and exponentially large parameter space of other methods."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736088"
                        ],
                        "name": "H. Gabow",
                        "slug": "H.-Gabow",
                        "structuredName": {
                            "firstName": "Harold",
                            "lastName": "Gabow",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Gabow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694216"
                        ],
                        "name": "Z. Galil",
                        "slug": "Z.-Galil",
                        "structuredName": {
                            "firstName": "Zvi",
                            "lastName": "Galil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Galil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145022"
                        ],
                        "name": "T. Spencer",
                        "slug": "T.-Spencer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Spencer",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Spencer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721050"
                        ],
                        "name": "R. Tarjan",
                        "slug": "R.-Tarjan",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tarjan",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tarjan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 35618095,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93fd9a6ad790d4b1adb35efc41ef39c2030bba6b",
            "isKey": false,
            "numCitedBy": 518,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, Fredman and Tarjan invented a new, especially efficient form of heap (priority queue). Their data structure, theFibonacci heap (or F-heap) supports arbitrary deletion inO(logn) amortized time and other heap operations inO(1) amortized time. In this paper we use F-heaps to obtain fast algorithms for finding minimum spanning trees in undirected and directed graphs. For an undirected graph containingn vertices andm edges, our minimum spanning tree algorithm runs inO(m log\u03b2 (m, n)) time, improved fromO(m\u03b2(m, n)) time, where\u03b2(m, n)=min {i|log(i)n \u2266m/n}. Our minimum spanning tree algorithm for directed graphs runs inO(n logn + m) time, improved fromO(n log n +m log log log(m/n+2)n). Both algorithms can be extended to allow a degree constraint at one vertex."
            },
            "slug": "Efficient-algorithms-for-finding-minimum-spanning-Gabow-Galil",
            "title": {
                "fragments": [],
                "text": "Efficient algorithms for finding minimum spanning trees in undirected and directed graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 35,
                "text": "This paper uses F-heaps to obtain fast algorithms for finding minimum spanning trees in undirected and directed graphs and can be extended to allow a degree constraint at one vertex."
            },
            "venue": {
                "fragments": [],
                "text": "Comb."
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115282352"
                        ],
                        "name": "Joy A. Thomas",
                        "slug": "Joy-A.-Thomas",
                        "structuredName": {
                            "firstName": "Joy",
                            "lastName": "Thomas",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joy A. Thomas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 190432,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7dbdb4209626fd92d2436a058663206216036e68",
            "isKey": false,
            "numCitedBy": 42795,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface to the Second Edition. Preface to the First Edition. Acknowledgments for the Second Edition. Acknowledgments for the First Edition. 1. Introduction and Preview. 1.1 Preview of the Book. 2. Entropy, Relative Entropy, and Mutual Information. 2.1 Entropy. 2.2 Joint Entropy and Conditional Entropy. 2.3 Relative Entropy and Mutual Information. 2.4 Relationship Between Entropy and Mutual Information. 2.5 Chain Rules for Entropy, Relative Entropy, and Mutual Information. 2.6 Jensen's Inequality and Its Consequences. 2.7 Log Sum Inequality and Its Applications. 2.8 Data-Processing Inequality. 2.9 Sufficient Statistics. 2.10 Fano's Inequality. Summary. Problems. Historical Notes. 3. Asymptotic Equipartition Property. 3.1 Asymptotic Equipartition Property Theorem. 3.2 Consequences of the AEP: Data Compression. 3.3 High-Probability Sets and the Typical Set. Summary. Problems. Historical Notes. 4. Entropy Rates of a Stochastic Process. 4.1 Markov Chains. 4.2 Entropy Rate. 4.3 Example: Entropy Rate of a Random Walk on a Weighted Graph. 4.4 Second Law of Thermodynamics. 4.5 Functions of Markov Chains. Summary. Problems. Historical Notes. 5. Data Compression. 5.1 Examples of Codes. 5.2 Kraft Inequality. 5.3 Optimal Codes. 5.4 Bounds on the Optimal Code Length. 5.5 Kraft Inequality for Uniquely Decodable Codes. 5.6 Huffman Codes. 5.7 Some Comments on Huffman Codes. 5.8 Optimality of Huffman Codes. 5.9 Shannon-Fano-Elias Coding. 5.10 Competitive Optimality of the Shannon Code. 5.11 Generation of Discrete Distributions from Fair Coins. Summary. Problems. Historical Notes. 6. Gambling and Data Compression. 6.1 The Horse Race. 6.2 Gambling and Side Information. 6.3 Dependent Horse Races and Entropy Rate. 6.4 The Entropy of English. 6.5 Data Compression and Gambling. 6.6 Gambling Estimate of the Entropy of English. Summary. Problems. Historical Notes. 7. Channel Capacity. 7.1 Examples of Channel Capacity. 7.2 Symmetric Channels. 7.3 Properties of Channel Capacity. 7.4 Preview of the Channel Coding Theorem. 7.5 Definitions. 7.6 Jointly Typical Sequences. 7.7 Channel Coding Theorem. 7.8 Zero-Error Codes. 7.9 Fano's Inequality and the Converse to the Coding Theorem. 7.10 Equality in the Converse to the Channel Coding Theorem. 7.11 Hamming Codes. 7.12 Feedback Capacity. 7.13 Source-Channel Separation Theorem. Summary. Problems. Historical Notes. 8. Differential Entropy. 8.1 Definitions. 8.2 AEP for Continuous Random Variables. 8.3 Relation of Differential Entropy to Discrete Entropy. 8.4 Joint and Conditional Differential Entropy. 8.5 Relative Entropy and Mutual Information. 8.6 Properties of Differential Entropy, Relative Entropy, and Mutual Information. Summary. Problems. Historical Notes. 9. Gaussian Channel. 9.1 Gaussian Channel: Definitions. 9.2 Converse to the Coding Theorem for Gaussian Channels. 9.3 Bandlimited Channels. 9.4 Parallel Gaussian Channels. 9.5 Channels with Colored Gaussian Noise. 9.6 Gaussian Channels with Feedback. Summary. Problems. Historical Notes. 10. Rate Distortion Theory. 10.1 Quantization. 10.2 Definitions. 10.3 Calculation of the Rate Distortion Function. 10.4 Converse to the Rate Distortion Theorem. 10.5 Achievability of the Rate Distortion Function. 10.6 Strongly Typical Sequences and Rate Distortion. 10.7 Characterization of the Rate Distortion Function. 10.8 Computation of Channel Capacity and the Rate Distortion Function. Summary. Problems. Historical Notes. 11. Information Theory and Statistics. 11.1 Method of Types. 11.2 Law of Large Numbers. 11.3 Universal Source Coding. 11.4 Large Deviation Theory. 11.5 Examples of Sanov's Theorem. 11.6 Conditional Limit Theorem. 11.7 Hypothesis Testing. 11.8 Chernoff-Stein Lemma. 11.9 Chernoff Information. 11.10 Fisher Information and the Cram-er-Rao Inequality. Summary. Problems. Historical Notes. 12. Maximum Entropy. 12.1 Maximum Entropy Distributions. 12.2 Examples. 12.3 Anomalous Maximum Entropy Problem. 12.4 Spectrum Estimation. 12.5 Entropy Rates of a Gaussian Process. 12.6 Burg's Maximum Entropy Theorem. Summary. Problems. Historical Notes. 13. Universal Source Coding. 13.1 Universal Codes and Channel Capacity. 13.2 Universal Coding for Binary Sequences. 13.3 Arithmetic Coding. 13.4 Lempel-Ziv Coding. 13.5 Optimality of Lempel-Ziv Algorithms. Compression. Summary. Problems. Historical Notes. 14. Kolmogorov Complexity. 14.1 Models of Computation. 14.2 Kolmogorov Complexity: Definitions and Examples. 14.3 Kolmogorov Complexity and Entropy. 14.4 Kolmogorov Complexity of Integers. 14.5 Algorithmically Random and Incompressible Sequences. 14.6 Universal Probability. 14.7 Kolmogorov complexity. 14.9 Universal Gambling. 14.10 Occam's Razor. 14.11 Kolmogorov Complexity and Universal Probability. 14.12 Kolmogorov Sufficient Statistic. 14.13 Minimum Description Length Principle. Summary. Problems. Historical Notes. 15. Network Information Theory. 15.1 Gaussian Multiple-User Channels. 15.2 Jointly Typical Sequences. 15.3 Multiple-Access Channel. 15.4 Encoding of Correlated Sources. 15.5 Duality Between Slepian-Wolf Encoding and Multiple-Access Channels. 15.6 Broadcast Channel. 15.7 Relay Channel. 15.8 Source Coding with Side Information. 15.9 Rate Distortion with Side Information. 15.10 General Multiterminal Networks. Summary. Problems. Historical Notes. 16. Information Theory and Portfolio Theory. 16.1 The Stock Market: Some Definitions. 16.2 Kuhn-Tucker Characterization of the Log-Optimal Portfolio. 16.3 Asymptotic Optimality of the Log-Optimal Portfolio. 16.4 Side Information and the Growth Rate. 16.5 Investment in Stationary Markets. 16.6 Competitive Optimality of the Log-Optimal Portfolio. 16.7 Universal Portfolios. 16.8 Shannon-McMillan-Breiman Theorem (General AEP). Summary. Problems. Historical Notes. 17. Inequalities in Information Theory. 17.1 Basic Inequalities of Information Theory. 17.2 Differential Entropy. 17.3 Bounds on Entropy and Relative Entropy. 17.4 Inequalities for Types. 17.5 Combinatorial Bounds on Entropy. 17.6 Entropy Rates of Subsets. 17.7 Entropy and Fisher Information. 17.8 Entropy Power Inequality and Brunn-Minkowski Inequality. 17.9 Inequalities for Determinants. 17.10 Inequalities for Ratios of Determinants. Summary. Problems. Historical Notes. Bibliography. List of Symbols. Index."
            },
            "slug": "Elements-of-Information-Theory-Cover-Thomas",
            "title": {
                "fragments": [],
                "text": "Elements of Information Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author examines the role of entropy, inequality, and randomness in the design of codes and the construction of codes in the rapidly changing environment."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2469500"
                        ],
                        "name": "P. Courtier",
                        "slug": "P.-Courtier",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Courtier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Courtier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144272151"
                        ],
                        "name": "J. Thepaut",
                        "slug": "J.-Thepaut",
                        "structuredName": {
                            "firstName": "Jean-Noel",
                            "lastName": "Thepaut",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Thepaut"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145302057"
                        ],
                        "name": "A. Hollingsworth",
                        "slug": "A.-Hollingsworth",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Hollingsworth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hollingsworth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A widely used alternative to sequential treatments of the data assimilation problem is the so called 4DVARmethod which seeks to find the most probable model trajectory over a given time window, typically using the model equations as a strong constraint, using a simple variational approach (Courtier et al., 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122632362,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "9dc613f23b569423f909ad857fe97a74296a53d6",
            "isKey": false,
            "numCitedBy": 1482,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY An order of magnitude reduction in the cost of fourdimensional variational assimilation (4D-Var) is required before operational implementation is possible. Reconditioning is considered and, although it offers a signi6cant reduction in cost, it seems that it is unlikely to provide a reduction as large as an order of magnitude. An approximation to 4D-Var, namely the incremental approach, is then considered and is shown to produce the same result at the end of the assimilation window as an extended Kalman filter in which no approximations are made in the assimilating model but in which instead a simplitied evolution of the forecast error is introduced. This approach provides the flexibility for a cost-benefit trade-off of 4D-Var to be made. The development of variational four-dimensional assimilation (4D-Var) from the stage of being a theoretical possibility to being a practical reality is progressing at a rapid pace. The first results of four-dimensional variational assimilation using real observations were provided by Thbpaut et al. (1993b) using an adiabatic primitive-equation model at truncations \"21 and T42. More recently Andersson ef al. (1994) used 4D-Var with a T63 model to assimilate remotely-sensed data such as infrared and microwave TOVS radiance measurements, while Thdpaut et d. (1993a) used 4D-Var with the same model to assimilate normalized radar backscatter cross-section measurements from the ERS-1 scatterometer."
            },
            "slug": "A-strategy-for-operational-implementation-of-using-Courtier-Thepaut",
            "title": {
                "fragments": [],
                "text": "A strategy for operational implementation of 4D\u2010Var, using an incremental approach"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72275082"
                        ],
                        "name": "H. Risken",
                        "slug": "H.-Risken",
                        "structuredName": {
                            "firstName": "Hannes",
                            "lastName": "Risken",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Risken"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120848013,
            "fieldsOfStudy": [
                "Physics",
                "Mathematics"
            ],
            "id": "709f915e2cd858652e5a3d939789a0cab7163500",
            "isKey": false,
            "numCitedBy": 2847,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Introduction.- 1.1 Brownian Motion.- 1.1.1 Deterministic Differential Equation.- 1.1.2 Stochastic Differential Equation.- 1.1.3 Equation of Motion for the Distribution Function.- 1.2 Fokker-Planck Equation.- 1.2.1 Fokker-Planck Equation for One Variable.- 1.2.2 Fokker-Planck Equation for N Variables.- 1.2.3 How Does a Fokker-Planck Equation Arise?.- 1.2.4 Purpose of the Fokker-Planck Equation.- 1.2.5 Solutions of the Fokker-Planck Equation.- 1.2.6 Kramers and Smoluchowski Equations.- 1.2.7 Generalizations of the Fokker-Planck Equation.- 1.3 Boltzmann Equation.- 1.4 Master Equation.- 2. Probability Theory.- 2.1 Random Variable and Probability Density.- 2.2 Characteristic Function and Cumulants.- 2.3 Generalization to Several Random Variables.- 2.3.1 Conditional Probability Density.- 2.3.2 Cross Correlation.- 2.3.3 Gaussian Distribution.- 2.4 Time-Dependent Random Variables.- 2.4.1 Classification of Stochastic Processes.- 2.4.2 Chapman-Kolmogorov Equation.- 2.4.3 Wiener-Khintchine Theorem.- 2.5 Several Time-Dependent Random Variables.- 3. Langevin Equations.- 3.1 Langevin Equation for Brownian Motion.- 3.1.1 Mean-Squared Displacement.- 3.1.2 Three-Dimensional Case.- 3.1.3 Calculation of the Stationary Velocity Distribution Function.- 3.2 Ornstein-Uhlenbeck Process.- 3.2.1 Calculation of Moments.- 3.2.2 Correlation Function.- 3.2.3 Solution by Fourier Transformation.- 3.3 Nonlinear Langevin Equation, One Variable.- 3.3.1 Example.- 3.3.2 Kramers-Moyal Expansion Coefficients.- 3.3.3 Ito's and Stratonovich's Definitions.- 3.4 Nonlinear Langevin Equations, Several Variables.- 3.4.1 Determination of the Langevin Equation from Drift and Diffusion Coefficients.- 3.4.2 Transformation of Variables.- 3.4.3 How to Obtain Drift and Diffusion Coefficients for Systems.- 3.5 Markov Property.- 3.6 Solutions of the Langevin Equation by Computer Simulation.- 4. Fokker-Planck Equation.- 4.1 Kramers-Moyal Forward Expansion.- 4.1.1 Formal Solution.- 4.2 Kramers-Moyal Backward Expansion.- 4.2.1 Formal Solution.- 4.2.2 Equivalence of the Solutions of the Forward and Backward Equations.- 4.3 Pawula Theorem.- 4.4 Fokker-Planck Equation for One Variable.- 4.4.1 Transition Probability Density for Small Times.- 4.4.2 Path Integral Solutions.- 4.5 Generation and Recombination Processes.- 4.6 Application of Truncated Kramers-Moyal Expansions.- 4.7 Fokker-Planck Equation for N Variables.- 4.7.1 Probability Current.- 4.7.2 Joint Probability Distribution.- 4.7.3 Transition Probability Density for Small Times.- 4.8 Examples for Fokker-Planck Equations with Several Variables.- 4.8.1 Three-Dimensional Brownian Motion without Position Variable.- 4.8.2 One-Dimensional Brownian Motion in a Potential.- 4.8.3 Three-Dimensional Brownian Motion in an External Force.- 4.8.4 Brownian Motion of Two Interacting Particles in an External Potential.- 4.9 Transformation of Variables.- 4.10 Covariant Form of the Fokker-Planck Equation.- 5. Fokker-Planck Equation for One Variable Methods of Solution.- 5.1 Normalization.- 5.2 Stationary Solution.- 5.3 Ornstein-Uhlenbeck Process.- 5.4 Eigenfunction Expansion.- 5.5 Examples.- 5.5.1 Parabolic Potential.- 5.5.2 Inverted Parabolic Potential.- 5.5.3 Infinite Square Well for the Schrudinger Potential.- 5.5.4 V-Shaped Potential for the Fokker-Planck Equation.- 5.6 Jump Conditions.- 5.7 A Bistable Model Potential.- 5.8 Eigenfunctions and Eigenvalues of Inverted Potentials.- 5.9 Approximate and Numerical Methods for Determining Eigenvalues and Eigenfunctions.- 5.9.1 Variational Method.- 5.9.2 Numerical Integration.- 5.9.3 Expansion into a Complete Set.- 5.10 Diffusion Over a Barrier.- 5.10.1 Kramers' Escape Rate.- 5.10.2 Bistable and Metastable Potential.- 6. Fokker-Planck Equation for Several Variables Methods of Solution.- 6.1 Approach of the Solutions to a Limit Solution.- 6.2 Expansion into a Biorthogonal Set.- 6.3 Transformation of the Fokker-Planck Operator, Eigenfunction Expansions.- 6.4 Detailed Balance.- 6.5 Ornstein-Uhlenbeck Process.- 6.6 Further Methods for Solving the Fokker-Planck Equation.- 6.6.1 Transformation of Variables.- 6.6.2 Variational Method.- 6.6.3 Reduction to an Hermitian Problem.- 6.6.4 Numerical Integration.- 6.6.5 Expansion into Complete Sets.- 6.6.6 Matrix Continued-Fraction Method.- 6.6.7 WKB Method.- 7. Linear Response and Correlation Functions.- 7.1 Linear Response Function.- 7.2 Correlation Functions.- 7.3 Susceptibility.- 8. Reduction of the Number of Variables.- 8.1 First-Passage Time Problems.- 8.2 Drift and Diffusion Coefficients Independent of Some Variables.- 8.2.1 Time Integrals of Markovian Variables.- 8.3 Adiabatic Elimination of Fast Variables.- 8.3.1 Linear Process with Respect to the Fast Variable.- 8.3.2 Connection to the Nakajima-Zwanzig Projector Formalism.- 9. Solutions of Tridiagonal Recurrence Relations, Application to Ordinary and Partial Differential Equations.- 9.1 Applications and Forms of Tridiagonal Recurrence Relations.- 9.1.1 Scalar Recurrence Relation.- 9.1.2 Vector Recurrence Relation.- 9.2 Solutions of Scalar Recurrence Relations.- 9.2.1 Stationary Solution.- 9.2.2 Initial Value Problem.- 9.2.3 Eigenvalue Problem.- 9.3 Solutions of Vector Recurrence Relations.- 9.3.1 Initial Value Problem.- 9.3.2 Eigenvalue Problem.- 9.4 Ordinary and Partial Differential Equations with Multiplicative Harmonic Time-Dependent Parameters.- 9.4.1 Ordinary Differential Equations.- 9.4.2 Partial Differential Equations.- 9.5 Methods for Calculating Continued Fractions.- 9.5.1 Ordinary Continued Fractions.- 9.5.2 Matrix Continued Fractions.- 10. Solutions of the Kramers Equation.- 10.1 Forms of the Kramers Equation.- 10.1.1 Normalization of Variables.- 10.1.2 Reversible and Irreversible Operators.- 10.1.3 Transformation of the Operators.- 10.1.4 Expansion into Hermite Functions.- 10.2 Solutions for a Linear Force.- 10.2.1 Transition Probability.- 10.2.2 Eigenvalues and Eigenfunctions.- 10.3 Matrix Continued-Fraction Solutions of the Kramers Equation.- 10.3.1 Initial Value Problem.- 10.3.2 Eigenvalue Problem.- 10.4 Inverse Friction Expansion.- 10.4.1 Inverse Friction Expansion for K0(t), G0,0(t) and L0(t).- 10.4.2 Determination of Eigenvalues and Eigenvectors.- 10.4.3 Expansion for the Green's Function Gn,m(t).- 10.4.4 Position-Dependent Friction.- 11. Brownian Motion in Periodic Potentials.- 11.1 Applications.- 11.1.1 Pendulum.- 11.1.2 Superionic Conductor.- 11.1.3 Josephson Tunneling Junction.- 11.1.4 Rotation of Dipoles in a Constant Field.- 11.1.5 Phase-Locked Loop.- 11.1.6 Connection to the Sine-Gordon Equation.- 11.2 Normalization of the Langevin and Fokker-Planck Equations.- 11.3 High-Friction Limit.- 11.3.1 Stationary Solution.- 11.3.2 Time-Dependent Solution.- 11.4 Low-Friction Limit.- 11.4.1 Transformation to E and x Variables.- 11.4.2 'Ansatz' for the Stationary Distribution Functions.- 11.4.3 x-Independent Functions.- 11.4.4 x-Dependent Functions.- 11.4.5 Corrected x-Independent Functions and Mobility.- 11.5 Stationary Solutions for Arbitrary Friction.- 11.5.1 Periodicity of the Stationary Distribution Function.- 11.5.2 Matrix Continued-Fraction Method.- 11.5.3 Calculation of the Stationary Distribution Function.- 11.5.4 Alternative Matrix Continued Fraction for the Cosine Potential.- 11.6 Bistability between Running and Locked Solution.- 11.6.1 Solutions Without Noise.- 11.6.2 Solutions With Noise.- 11.6.3 Low-Friction Mobility With Noise.- 11.7 Instationary Solutions.- 11.7.1 Diffusion Constant.- 11.7.2 Transition Probability for Large Times.- 11.8 Susceptibilities.- 11.8.1 Zero-Friction Limit.- 11.9 Eigenvalues and Eigenfunctions.- 11.9.1 Eigenvalues and Eigenfunctions in the Low-Friction Limit.- 12. Statistical Properties of Laser Light.- 12.1 Semiclassical Laser Equations.- 12.1.1 Equations Without Noise.- 12.1.2 Langevin Equation.- 12.1.3 Laser Fokker-Planck Equation.- 12.2 Stationary Solution and Its Expectation Values.- 12.3 Expansion in Eigenmodes.- 12.4 Expansion into a Complete Set Solution by Matrix Continued Fractions.- 12.4.1 Determination of Eigenvalues.- 12.5 Transient Solution.- 12.5.1 Eigenfunction Method.- 12.5.2 Expansion into a Complete Set.- 12.5.3 Solution for Large Pump Parameters.- 12.6 Photoelectron Counting Distribution.- 12.6.1 Counting Distribution for Short Intervals.- 12.6.2 Expectation Values for Arbitrary Intervals.- Appendices.- A1 Stochastic Differential Equations with Colored Gaussian Noise.- A2 Boltzmann Equation with BGK and SW Collision Operators.- A3 Evaluation of a Matrix Continued Fraction for the Harmonic Oscillator.- A4 Damped Quantum-Mechanical Harmonic Oscillator.- A5 Alternative Derivation of the Fokker-Planck Equation.- A6 Fluctuating Control Parameter.- S. Supplement to the Second Edition.- S.1 Solutions of the Fokker-Planck Equation by Computer Simulation (Sect. 3.6).- S.2 Kramers-Moyal Expansion (Sect. 4.6).- S.3 Example for the Covariant Form of the Fokker-Planck Equation (Sect. 4.10).- S.4 Connection to Supersymmetry and Exact Solutions of the One Variable Fokker-Planck Equation (Chap. 5).- S.5 Nondifferentiability of the Potential for the Weak Noise Expansion (Sects. 6.6 and 6.7).- S.6 Further Applications of Matrix Continued-Fractions (Chap. 9).- S.7 Brownian Motion in a Double-Well Potential (Chaps. 10 and 11).- S.8 Boundary Layer Theory (Sect. 11.4).- S.9 Calculation of Correlation Times (Sect. 7.12).- S.10 Colored Noise (Appendix A1).- S.11 Fokker-Planck Equation with a Non-Positive-Definite Diffusion Matrix and Fokker-Planck Equation with Additional Third-Order-Derivative Terms.- References."
            },
            "slug": "The-Fokker-Planck-equation-:-methods-of-solution-Risken",
            "title": {
                "fragments": [],
                "text": "The Fokker-Planck equation : methods of solution and applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46831169"
                        ],
                        "name": "G. Hinton",
                        "slug": "G.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749650"
                        ],
                        "name": "B. Frey",
                        "slug": "B.-Frey",
                        "structuredName": {
                            "firstName": "Brendan",
                            "lastName": "Frey",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Frey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145572884"
                        ],
                        "name": "R. Neal",
                        "slug": "R.-Neal",
                        "structuredName": {
                            "firstName": "R",
                            "lastName": "Neal",
                            "middleNames": [
                                "M"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 871473,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6dd01cd9c17d1491ead8c9f97597fbc61dead8ea",
            "isKey": false,
            "numCitedBy": 1001,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "An unsupervised learning algorithm for a multilayer network of stochastic neurons is described. Bottom-up \"recognition\" connections convert the input into representations in successive hidden layers, and top-down \"generative\" connections reconstruct the representation in one layer from the representation in the layer above. In the \"wake\" phase, neurons are driven by recognition connections, and generative connections are adapted to increase the probability that they would reconstruct the correct activity vector in the layer below. In the \"sleep\" phase, neurons are driven by generative connections, and recognition connections are adapted to increase the probability that they would produce the correct activity vector in the layer above."
            },
            "slug": "The-\"wake-sleep\"-algorithm-for-unsupervised-neural-Hinton-Dayan",
            "title": {
                "fragments": [],
                "text": "The \"wake-sleep\" algorithm for unsupervised neural networks."
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "An unsupervised learning algorithm for a multilayer network of stochastic neurons is described, where bottom-up \"recognition\" connections convert the input into representations in successive hidden layers, and top-down \"generative\" connections reconstruct the representation in one layer from the representations in the layer above."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145298005"
                        ],
                        "name": "Catherine Blake",
                        "slug": "Catherine-Blake",
                        "structuredName": {
                            "firstName": "Catherine",
                            "lastName": "Blake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Catherine Blake"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62622768,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e068be31ded63600aea068eacd12931efd2a1029",
            "isKey": false,
            "numCitedBy": 13446,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "UCI-Repository-of-machine-learning-databases-Blake",
            "title": {
                "fragments": [],
                "text": "UCI Repository of machine learning databases"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770859"
                        ],
                        "name": "R. Gallager",
                        "slug": "R.-Gallager",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Gallager",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gallager"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12709402,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "206f827fad201506c315d40c1469b41a45141893",
            "isKey": false,
            "numCitedBy": 10569,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "A low-density parity-check code is a code specified by a parity-check matrix with the following properties: each column contains a small fixed number j \\geq 3 of l's and each row contains a small fixed number k > j of l's. The typical minimum distance of these codes increases linearly with block length for a fixed rate and fixed j . When used with maximum likelihood decoding on a sufficiently quiet binary-input symmetric channel, the typical probability of decoding error decreases exponentially with block length for a fixed rate and fixed j . A simple but nonoptimum decoding scheme operating directly from the channel a posteriori probabilities is described. Both the equipment complexity and the data-handling capacity in bits per second of this decoder increase approximately linearly with block length. For j > 3 and a sufficiently low rate, the probability of error using this decoder on a binary symmetric channel is shown to decrease at least exponentially with a root of the block length. Some experimental results show that the actual probability of decoding error is much smaller than this theoretical bound."
            },
            "slug": "Low-density-parity-check-codes-Gallager",
            "title": {
                "fragments": [],
                "text": "Low-density parity-check codes"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A simple but nonoptimum decoding scheme operating directly from the channel a posteriori probabilities is described and the probability of error using this decoder on a binary symmetric channel is shown to decrease at least exponentially with a root of the block length."
            },
            "venue": {
                "fragments": [],
                "text": "IRE Trans. Inf. Theory"
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145691986"
                        ],
                        "name": "P. Phillips",
                        "slug": "P.-Phillips",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Phillips",
                            "middleNames": [
                                "Jonathon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Phillips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40638847"
                        ],
                        "name": "Hyeonjoon Moon",
                        "slug": "Hyeonjoon-Moon",
                        "structuredName": {
                            "firstName": "Hyeonjoon",
                            "lastName": "Moon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hyeonjoon Moon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2958806"
                        ],
                        "name": "S. A. Rizvi",
                        "slug": "S.-A.-Rizvi",
                        "structuredName": {
                            "firstName": "Syed",
                            "lastName": "Rizvi",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. A. Rizvi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3313513"
                        ],
                        "name": "Patrick J. Rauss",
                        "slug": "Patrick-J.-Rauss",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Rauss",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Patrick J. Rauss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 497801,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "791e530f6a4098bb39696d1476032821a7a1c569",
            "isKey": false,
            "numCitedBy": 2335,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Two of the most critical requirements in support of producing reliable face-recognition systems are a large database of facial images and a testing procedure to evaluate systems. The Face Recognition Technology (FERET) program has addressed both issues through the FERET database of facial images and the establishment of the FERET tests. To date, 14,126 images from 1199 individuals are included in the FERET database, which is divided into development and sequestered portions. In September 1996, the FERET program administered the third in a series of FERET face-recognition tests. The primary objectives of the third test were to (1) assess the state of the art, (2) identify future areas of research, and (3) measure algorithm performance on large databases."
            },
            "slug": "The-FERET-evaluation-methodology-for-algorithms-Phillips-Moon",
            "title": {
                "fragments": [],
                "text": "The FERET evaluation methodology for face-recognition algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Two of the most critical requirements in support of producing reliable face-recognition systems are a large database of facial images and a testing procedure to evaluate systems."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2450839"
                        ],
                        "name": "M. Fredman",
                        "slug": "M.-Fredman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Fredman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Fredman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721050"
                        ],
                        "name": "R. Tarjan",
                        "slug": "R.-Tarjan",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tarjan",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tarjan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 193986,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "93e12a891485806085ad95d4144a21a55d7fc996",
            "isKey": false,
            "numCitedBy": 2130,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we develop a new data structure for implementing heaps (priority queues). Our structure, <italic>Fibonacci heaps</italic> (abbreviated <italic>F-heaps</italic>), extends the binomial queues proposed by Vuillemin and studied further by Brown. F-heaps support arbitrary deletion from an <italic>n</italic>-item heap in <italic>O</italic>(log <italic>n</italic>) amortized time and all other standard heap operations in <italic>O</italic>(1) amortized time. Using F-heaps we are able to obtain improved running times for several network optimization algorithms. In particular, we obtain the following worst-case bounds, where <italic>n</italic> is the number of vertices and <italic>m</italic> the number of edges in the problem graph:<list><item><italic>O</italic>(<italic>n</italic> log <italic>n</italic> + <italic>m</italic>) for the single-source shortest path problem with nonnegative edge lengths, improved from <italic>O</italic>(<italic>m</italic>log<subscrpt>(<italic>m/n</italic>+2)</subscrpt><italic>n</italic>);\n</item><item><italic>O</italic>(<italic>n</italic><supscrpt>2</supscrpt>log <italic>n</italic> + <italic>nm</italic>) for the all-pairs shortest path problem, improved from <italic>O</italic>(<italic>nm</italic> log<subscrpt>(<italic>m/n</italic>+2)</subscrpt><italic>n</italic>);\n</item><item><italic>O</italic>(<italic>n</italic><supscrpt>2</supscrpt>log <italic>n</italic> + <italic>nm</italic>) for the assignment problem (weighted bipartite matching), improved from <italic>O</italic>(<italic>nm</italic>log<subscrpt>(<italic>m/n</italic>+2)</subscrpt><italic>n</italic>);\n</item><item><italic>O</italic>(<italic>m\u03b2</italic>(<italic>m, n</italic>)) for the minimum spanning tree problem, improved from <italic>O</italic>(<italic>m</italic>log log<subscrpt>(<italic>m/n</italic>+2)</subscrpt><italic>n</italic>); where <italic>\u03b2</italic>(<italic>m, n</italic>) = min {<italic>i</italic> \u21bf log<supscrpt>(<italic>i</italic>)</supscrpt><italic>n</italic> \u2264 <italic>m/n</italic>}. Note that <italic>\u03b2</italic>(<italic>m, n</italic>) \u2264 log<supscrpt>*</supscrpt><italic>n</italic> if <italic>m</italic> \u2265 <italic>n</italic>.\n</item></list>Of these results, the improved bound for minimum spanning trees is the most striking, although all the results give asymptotic improvements for graphs of appropriate densities."
            },
            "slug": "Fibonacci-heaps-and-their-uses-in-improved-network-Fredman-Tarjan",
            "title": {
                "fragments": [],
                "text": "Fibonacci Heaps and Their Uses in Improved Network Optimization Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Using F-heaps, a new data structure for implementing heaps that extends the binomial queues proposed by Vuillemin and studied further by Brown, the improved bound for minimum spanning trees is the most striking."
            },
            "venue": {
                "fragments": [],
                "text": "FOCS"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145878706"
                        ],
                        "name": "D. Michie",
                        "slug": "D.-Michie",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Michie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Michie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48616434"
                        ],
                        "name": "D. Spiegelhalter",
                        "slug": "D.-Spiegelhalter",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Spiegelhalter",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Spiegelhalter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107314775"
                        ],
                        "name": "C. C. Taylor",
                        "slug": "C.-C.-Taylor",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Taylor",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. C. Taylor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15773445,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fad1bd501aa769f7701c1016f8a4d1473ca77601",
            "isKey": false,
            "numCitedBy": 2683,
            "numCiting": 122,
            "paperAbstract": {
                "fragments": [],
                "text": "Survey of previous comparisons and theoretical work descriptions of methods dataset descriptions criteria for comparison and methodology (including validation) empirical results machine learning on machine learning."
            },
            "slug": "Machine-Learning,-Neural-and-Statistical-Michie-Spiegelhalter",
            "title": {
                "fragments": [],
                "text": "Machine Learning, Neural and Statistical Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A survey of previous comparisons and theoretical work descriptions of methods dataset descriptions criteria for comparison and methodology (including validation) empirical results machine learning on machine learning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2725018"
                        ],
                        "name": "E. Kalnay",
                        "slug": "E.-Kalnay",
                        "structuredName": {
                            "firstName": "Eugenia",
                            "lastName": "Kalnay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Kalnay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A particular issue we address in this paper is the use of observations, together with dynamics defined by a stochastic differential equation to infer the posterior distribution of the state of the system, a process often referred to in meteorology as data assimilation (Kalnay, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15830046,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "73452deffbdc7636502756e3961ab657ddb0ccc2",
            "isKey": false,
            "numCitedBy": 1132,
            "numCiting": 693,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Historical overview 2. The continuous equations 3. Discretization of the equations 4. Introduction to the parameterizations of subgrid-scale physical processes 5. Data assimilation 6. Atmospheric predictability and ensemble forecasting References Appendix A. The early history of numerical weather prediction Appendix B. List of acronyms Appendix C. Coding and checking the linear tangent and adjoint models Appendix D. Post processing of numerical model output to obtain station weather forecasts."
            },
            "slug": "Atmospheric-Modeling,-Data-Assimilation-and-Kalnay",
            "title": {
                "fragments": [],
                "text": "Atmospheric Modeling, Data Assimilation and Predictability"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This work focuses on the post processing of numerical model output to obtain station weather forecasts and the parameterizations of subgrid-scale physical processes."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1833925"
                        ],
                        "name": "C. Berrou",
                        "slug": "C.-Berrou",
                        "structuredName": {
                            "firstName": "Claude",
                            "lastName": "Berrou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Berrou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1870588"
                        ],
                        "name": "A. Glavieux",
                        "slug": "A.-Glavieux",
                        "structuredName": {
                            "firstName": "Alain",
                            "lastName": "Glavieux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Glavieux"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12044273,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "388fa95178e9b142b0ddec1dbc5dacbbab53ad8f",
            "isKey": false,
            "numCitedBy": 2930,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new family of convolutional codes, nicknamed turbo-codes, built from a particular concatenation of two recursive systematic codes, linked together by nonuniform interleaving. Decoding calls on iterative processing in which each component decoder takes advantage of the work of the other at the previous step, with the aid of the original concept of extrinsic information. For sufficiently large interleaving sizes, the correcting performance of turbo-codes, investigated by simulation, appears to be close to the theoretical limit predicted by Shannon."
            },
            "slug": "Near-optimum-error-correcting-coding-and-decoding:-Berrou-Glavieux",
            "title": {
                "fragments": [],
                "text": "Near optimum error correcting coding and decoding: turbo-codes"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A new family of convolutional codes, nicknamed turbo-codes, built from a particular concatenation of two recursive systematic codes, linked together by nonuniform interleaving appears to be close to the theoretical limit predicted by Shannon."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Commun."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1910641"
                        ],
                        "name": "P. Kloeden",
                        "slug": "P.-Kloeden",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Kloeden",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Kloeden"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2306575"
                        ],
                        "name": "E. Platen",
                        "slug": "E.-Platen",
                        "structuredName": {
                            "firstName": "Eckhard",
                            "lastName": "Platen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Platen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In the experiments we consider the same sample as the one considered by Miller et al. (1994); Eyink et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 116903781,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ffec6b958f89ee996372c36b0aaf8f90bdc09fe2",
            "isKey": false,
            "numCitedBy": 4737,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract A method is proposed for the numerical solution of It\u00f4 stochastic differential equations by means of a second-order Runge\u2013Kutta iterative scheme rather than the less efficient Euler iterative scheme. It requires the Runge\u2013Kutta iterative scheme to be applied to a different stochastic differential equation obtained by subtraction of a correction term from the given one."
            },
            "slug": "The-numerical-solution-of-stochastic-differential-Kloeden-Platen",
            "title": {
                "fragments": [],
                "text": "The numerical solution of stochastic differential equations"
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of the Australian Mathematical Society. Series B. Applied Mathematics"
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103288583"
                        ],
                        "name": "G. J. Haltiner",
                        "slug": "G.-J.-Haltiner",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Haltiner",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. J. Haltiner"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "(Haltiner and Williams, 1980)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 122458299,
            "fieldsOfStudy": [
                "Environmental Science",
                "Physics"
            ],
            "id": "051e32f5802f8703336c6112f31f28405a501dcb",
            "isKey": false,
            "numCitedBy": 654,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "An advanced, updated, and self-contained treatment. Includes the fundamental system of equations governing large-scale atmospheric motions, coordinate systems, atmospheric wave motions, energetics, hyperbolic and elliptic equations, moisture modeling, solar and terrestrial radiation modeling, seasonal and climate prediction. Presupposes a knowledge of mathematics through calculus, some vector analysis, and introductory meteorology."
            },
            "slug": "Numerical-Prediction-and-Dynamic-Meteorology-Haltiner",
            "title": {
                "fragments": [],
                "text": "Numerical Prediction and Dynamic Meteorology"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35043531"
                        ],
                        "name": "A. Dempster",
                        "slug": "A.-Dempster",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Dempster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7890796"
                        ],
                        "name": "N. Laird",
                        "slug": "N.-Laird",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Laird",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Laird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 106
                            }
                        ],
                        "text": "These two steps iterate and are proved to converge to a local maximum of the (incomplete) log-likelihood (Dempster et al., 1977)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4193919,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "d36efb9ad91e00faa334b549ce989bfae7e2907a",
            "isKey": false,
            "numCitedBy": 48406,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Vibratory power unit for vibrating conveyers and screens comprising an asynchronous polyphase motor, at least one pair of associated unbalanced masses disposed on the shaft of said motor, with the first mass of a pair of said unbalanced masses being rigidly fastened to said shaft and with said second mass of said pair being movably arranged relative to said first mass, means for controlling and regulating the conveying rate during conveyer operation by varying the rotational speed of said motor between predetermined minimum and maximum values, said second mass being movably outwardly by centrifugal force against the pressure of spring means, said spring means being prestressed in such a manner that said second mass is, at rotational motor speeds lower than said minimum speed, held in its initial position, and at motor speeds between said lower and upper values in positions which are radially offset with respect to the axis of said motor to an extent depending on the value of said rotational motor speed."
            },
            "slug": "Maximum-likelihood-from-incomplete-data-via-the-EM-Dempster-Laird",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144572614"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122801915,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "33f275df4188cf8d51f3a85bd95ed2afa64196e4",
            "isKey": false,
            "numCitedBy": 2785,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors report the empirical performance of Gallager's low density parity check codes on Gaussian channels. They show that performance substantially better than that of standard convolutional and concatenated codes can be achieved; indeed the performance is almost as close to the Shannon limit as that of turbo codes."
            },
            "slug": "Near-Shannon-limit-performance-of-low-density-check-Mackay-Neal",
            "title": {
                "fragments": [],
                "text": "Near Shannon limit performance of low density parity check codes"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "The authors report the empirical performance of Gallager's low density parity check codes on Gaussian channels, showing that performance substantially better than that of standard convolutional and concatenated codes can be achieved."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "17034236"
                        ],
                        "name": "Grace Jordison",
                        "slug": "Grace-Jordison",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Jordison",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Grace Jordison"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 52870834,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "f9d838defd257e45da6143dfa05951e8f00fd6f0",
            "isKey": false,
            "numCitedBy": 871,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "No wonder you activities are, reading will be always needed. It is not only to fulfil the duties that you need to finish in deadline time. Reading will encourage your mind and thoughts. Of course, reading will greatly develop your experiences about everything. Reading molecular biology of the gene is also a way as one of the collective books that gives many advantages. The advantages are not only for you, but for the other peoples with those meaningful benefits."
            },
            "slug": "Molecular-Biology-of-the-Gene-Jordison",
            "title": {
                "fragments": [],
                "text": "Molecular Biology of the Gene"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Reading molecular biology of the gene is also a way as one of the collective books that gives many advantages, not only for you, but for the other peoples with those meaningful benefits."
            },
            "venue": {
                "fragments": [],
                "text": "The Yale Journal of Biology and Medicine"
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721050"
                        ],
                        "name": "R. Tarjan",
                        "slug": "R.-Tarjan",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tarjan",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tarjan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 116926536,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e60cbae0a44a5b5d94888946ef2a15c7036ce853",
            "isKey": false,
            "numCitedBy": 2154,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Foundations Disjoint Sets Heaps Search Trees Linking and Cutting Trees Minimum Spanning Trees Shortest Paths Network Flows Matchings."
            },
            "slug": "Data-structures-and-network-algorithms-Tarjan",
            "title": {
                "fragments": [],
                "text": "Data structures and network algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This paper presents a meta-trees tree model that automates the very labor-intensive and therefore time-heavy and therefore expensive process of manually selecting trees to grow in a graph."
            },
            "venue": {
                "fragments": [],
                "text": "CBMS-NSF regional conference series in applied mathematics"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 649,
                                "start": 61
                            }
                        ],
                        "text": "In the following we replicated the experimental procedure of Kontkanen et al. (1996) and Michie et al. (1994) as closely as possible. The test and training set sizes were 70 and 620 respectively. For each value of m we ran our algorithm for a fixed number of epochs on the training set and then recorded the performance on the test set. This was repeated 20 times for each m, each time with a random start and with a random split between the test and the training set. Because of the small data set size we used edge pruning with \u03b2 \u221d 1/m. The best performance of the mixtures of trees is compared to the published results of Kontkanen et al. (1996) and Michie et al."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 61
                            }
                        ],
                        "text": "In the following we replicated the experimental procedure of Kontkanen et al. (1996) and Michie et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 73
                            }
                        ],
                        "text": "The results for mixtures of factorial distribution are those reported by Kontkanen et al. (1996)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 0
                            }
                        ],
                        "text": "Kontkanen, Myllymaki, and Tirri (1996) study a MF in which a hidden variable is used for classification; this approach was extended by Monti and Cooper (1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 674,
                                "start": 61
                            }
                        ],
                        "text": "In the following we replicated the experimental procedure of Kontkanen et al. (1996) and Michie et al. (1994) as closely as possible. The test and training set sizes were 70 and 620 respectively. For each value of m we ran our algorithm for a fixed number of epochs on the training set and then recorded the performance on the test set. This was repeated 20 times for each m, each time with a random start and with a random split between the test and the training set. Because of the small data set size we used edge pruning with \u03b2 \u221d 1/m. The best performance of the mixtures of trees is compared to the published results of Kontkanen et al. (1996) and Michie et al. (1994) for the same dataset in Table 6."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 61
                            }
                        ],
                        "text": "In the following we replicated the experimental procedure of Kontkanen et al. (1996) and Michie et al. (1994) as closely as possible."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 233
                            }
                        ],
                        "text": "In the testing phase, a new instance x \u2208 \u2126(V ) is classified by picking the most likely value of the class variable given the settings of the other variables:\nc(x) = argmax xc Q(xc, x)\nSimilarly, for the MF classifier (termed \u201cD-SIDE\u201d by Kontkanen et al., 1996), Q above is an MF trained on {c}\u22c3 V ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Constructing Bayesian finite mixture models by the EM algorithm (Tech"
            },
            "venue": {
                "fragments": [],
                "text": "Rep. No. C-1996-9)"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 73
                            }
                        ],
                        "text": "Identifiability of hidden variable models is an area of current research [29, 58, 62]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 8
                            }
                        ],
                        "text": "Theorem [62] An H-model over a domain Vh consisting of binary variables only is identifiable if either (a) or (b) hold: (a) p > 3"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": "However, in the case of models with binary variables only, [62] gives the following sufficient condition that is highly relevant to the present problem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On identification of graphical log-linear models with one unobserved variable"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "The assumptions allowing us to define this prior are explicated in [31] and parallel the reasoning of [22] for general Bayes nets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 67
                            }
                        ],
                        "text": "The assumptions allowing us to define this prior are explicated by Meila\u0306 and Jaakkola (2000) and parallel the reasoning of Heckerman et al. (1995) for general Bayes nets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 94
                            }
                        ],
                        "text": "Given that the number of all undirected tree structures over n variables is nn\u22122, this result (Meila\u0306 & Jaakkola, 2000) is quite surprising."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 94
                            }
                        ],
                        "text": "Given that the number of all undirected tree structures over n variables is nn\u22122, this result [31, 32] is quite surprising."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian learning of tree distributions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 40
                            }
                        ],
                        "text": "bases, and an additional class variable (Rasmussen et al., 1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 212
                            }
                        ],
                        "text": "Based on the strong showing of the single tree model on the SPLICE task, we pursued a second series of experiments in which we compare the tree model with a larger collection of methods from the DELVE repository (Rasmussen et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 79
                            }
                        ],
                        "text": "We eliminated 15 examples from the original data set that had ambiguous inputs (Noordewier, Towell, & Shavlik, 1991; Rasmussen et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The DELVE Manual. http://www.cs.utoronto.ca/\u223cdelve"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3878580"
                        ],
                        "name": "J. Loehlin",
                        "slug": "J.-Loehlin",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Loehlin",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Loehlin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 169292655,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "fba236b4f67ab9c6068a89331fea8060eeb1e51f",
            "isKey": false,
            "numCitedBy": 455,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Latent-variable-models-Loehlin",
            "title": {
                "fragments": [],
                "text": "Latent variable models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145392702"
                        ],
                        "name": "P. Green",
                        "slug": "P.-Green",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Green",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Green"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 131
                            }
                        ],
                        "text": "Similar approaches of approximate inference by Monte Carlo techniques have been devised and used for Bayes nets of small size also [31, 60]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 125093681,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3f1bb45d5d20c107daa9dbc489019cf22a3a6e6b",
            "isKey": false,
            "numCitedBy": 4620,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Markov-chain-Monte-Carlo-in-Practice-Green",
            "title": {
                "fragments": [],
                "text": "Markov chain Monte Carlo in Practice"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2748598"
                        ],
                        "name": "S. Kullback",
                        "slug": "S.-Kullback",
                        "structuredName": {
                            "firstName": "Solomon",
                            "lastName": "Kullback",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kullback"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102909471"
                        ],
                        "name": "R. A. Leibler",
                        "slug": "R.-A.-Leibler",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Leibler",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. A. Leibler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The Kullback-Leibler (KL) divergence (Kullback and Leibler, 1951) between the approximating posterior process and the exact one is one between processes (i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120349231,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c054360ec3ccadae977fdd0d77694c9655478a41",
            "isKey": false,
            "numCitedBy": 10534,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-Information-and-Sufficiency-Kullback-Leibler",
            "title": {
                "fragments": [],
                "text": "On Information and Sufficiency"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1951
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34631309"
                        ],
                        "name": "R. Cowell",
                        "slug": "R.-Cowell",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Cowell",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cowell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 108494397,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2a76158a32f842f40e1dade2d933aec6fbaecb59",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Sampling-without-replacement-in-junction-trees-Cowell",
            "title": {
                "fragments": [],
                "text": "Sampling without replacement in junction trees"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050895"
                        ],
                        "name": "S. Lauritzen",
                        "slug": "S.-Lauritzen",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Lauritzen",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lauritzen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "74007305"
                        ],
                        "name": "Dawid Ap",
                        "slug": "Dawid-Ap",
                        "structuredName": {
                            "firstName": "Dawid",
                            "lastName": "Ap",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dawid Ap"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144845491"
                        ],
                        "name": "A. Dawid",
                        "slug": "A.-Dawid",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Dawid",
                            "middleNames": [
                                "Philip"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dawid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144238498"
                        ],
                        "name": "B. N. Larsen",
                        "slug": "B.-N.-Larsen",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Larsen",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. N. Larsen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "74208271"
                        ],
                        "name": "H.-G. Leimer",
                        "slug": "H.-G.-Leimer",
                        "structuredName": {
                            "firstName": "H.-G.",
                            "lastName": "Leimer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H.-G. Leimer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 68144742,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4f0d4ff1fce75ed6cf038e36f023d224adffaa96",
            "isKey": false,
            "numCitedBy": 134,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Independence-properties-of-directed-Markov-fields.-Lauritzen-Ap",
            "title": {
                "fragments": [],
                "text": "Independence properties of directed Markov fields. Networks, 20, 491-505"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35191477"
                        ],
                        "name": "F. V. Jensen",
                        "slug": "F.-V.-Jensen",
                        "structuredName": {
                            "firstName": "Finn",
                            "lastName": "Jensen",
                            "middleNames": [
                                "Verner"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. V. Jensen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050895"
                        ],
                        "name": "S. Lauritzen",
                        "slug": "S.-Lauritzen",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Lauritzen",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lauritzen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34760449"
                        ],
                        "name": "K. Olesen",
                        "slug": "K.-Olesen",
                        "structuredName": {
                            "firstName": "Kristian",
                            "lastName": "Olesen",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Olesen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 53880385,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb75e5a3b46ec37a72922c706acd87ebab35b666",
            "isKey": false,
            "numCitedBy": 763,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Bayesian-updating-in-causal-probabilistic-networks-Jensen-Lauritzen",
            "title": {
                "fragments": [],
                "text": "Bayesian updating in causal probabilistic networks by local computations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1409282926"
                        ],
                        "name": "Marina Meila-Predoviciu",
                        "slug": "Marina-Meila-Predoviciu",
                        "structuredName": {
                            "firstName": "Marina",
                            "lastName": "Meila-Predoviciu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marina Meila-Predoviciu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "For other versions of the acCL algorithm see [34]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "It can be shown [34] that the computation of the cooccurrence counts and the construction of the lists C(u), V 0(u), u \u2208 V takes an amount of time proportional to the number of cooccurrences NC , up to a logarithmic factor: O(s(2)N log(s(2)N/n))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 37065179,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79403433bb75f70a4779ad198032bd5959213beb",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-with-Mixtures-of-Trees-Meila-Predoviciu",
            "title": {
                "fragments": [],
                "text": "Learning with Mixtures of Trees"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 213
                            }
                        ],
                        "text": "If one chooses the edge penalties to be proportional to the increase in the number of parameters caused by the addition of edge uv to the tree,\n\u03b2uv = 1 2 (ru \u2212 1)(rv \u2212 1) log N\nthen a Minimum Description Length (MDL) (Rissanen, 1989) type of prior is implemented."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9365056,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "72247e4e34de0dd2d0428522ded24b49fb1632be",
            "isKey": false,
            "numCitedBy": 1772,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Stochastic-Complexity-in-Statistical-Inquiry-Rissanen",
            "title": {
                "fragments": [],
                "text": "Stochastic Complexity in Statistical Inquiry"
            },
            "venue": {
                "fragments": [],
                "text": "World Scientific Series in Computer Science"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50785579"
                        ],
                        "name": "N. Friedman",
                        "slug": "N.-Friedman",
                        "structuredName": {
                            "firstName": "Nir",
                            "lastName": "Friedman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Friedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746034"
                        ],
                        "name": "L. Getoor",
                        "slug": "L.-Getoor",
                        "structuredName": {
                            "firstName": "Lise",
                            "lastName": "Getoor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Getoor"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 31748241,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "35b4c056725f7242833aaed32f8af5f3eebeed30",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Efficient-learning-using-constrained-sufficient-Friedman-Getoor",
            "title": {
                "fragments": [],
                "text": "Efficient learning using constrained sufficient statistics"
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1774907"
                        ],
                        "name": "K. Fraughnaugh",
                        "slug": "K.-Fraughnaugh",
                        "structuredName": {
                            "firstName": "Kathryn",
                            "lastName": "Fraughnaugh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fraughnaugh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 44579003,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54006d6fb211a090c98fee9d8103479b022c0db2",
            "isKey": false,
            "numCitedBy": 4394,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Introduction-to-graph-theory-Fraughnaugh",
            "title": {
                "fragments": [],
                "text": "Introduction to graph theory"
            },
            "venue": {
                "fragments": [],
                "text": "Networks"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48616434"
                        ],
                        "name": "D. Spiegelhalter",
                        "slug": "D.-Spiegelhalter",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Spiegelhalter",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Spiegelhalter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70654112"
                        ],
                        "name": "Andrew Thomas",
                        "slug": "Andrew-Thomas",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Thomas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Thomas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34949056"
                        ],
                        "name": "N. G. Best",
                        "slug": "N.-G.-Best",
                        "structuredName": {
                            "firstName": "Nicola",
                            "lastName": "Best",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. G. Best"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2089883"
                        ],
                        "name": "W. Gilks",
                        "slug": "W.-Gilks",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Gilks",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gilks"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 131
                            }
                        ],
                        "text": "Similar approaches of approximate inference by Monte Carlo techniques have been devised and used for Bayes nets of small size also [31, 60]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 115455672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "478195f845581114c5dde5a1cf21f5786d7127a1",
            "isKey": false,
            "numCitedBy": 541,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "BUGS-Bayesian-inference-Using-Gibbs-Sampling-0.50-Spiegelhalter-Thomas",
            "title": {
                "fragments": [],
                "text": "BUGS - Bayesian inference Using Gibbs Sampling Version 0.50"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning mixtures of Bayes networks (Tech. Rep. Nos. MSR\u2013POR\u201397\u201330). Microsoft Research"
            },
            "venue": {
                "fragments": [],
                "text": "Learning mixtures of Bayes networks (Tech. Rep. Nos. MSR\u2013POR\u201397\u201330). Microsoft Research"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Tractable Bayesian learning of tree distributions"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 16th Conference on Uncertainty in AI"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning with mixtures of trees. Unpublished doctoral dissertation"
            },
            "venue": {
                "fragments": [],
                "text": "Learning with mixtures of trees. Unpublished doctoral dissertation"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 150
                            }
                        ],
                        "text": "\u2026explored as the underlying architectures in systems\nc\u00a92000 Marina Meila\u0306 and Michael I. Jordan.\nfor classification, prediction and density estimation (Bishop, 1999; Friedman, Geiger, & Goldszmidt, 1997; Heckerman, Geiger, & Chickering, 1995; Hinton, Dayan, Frey, & Neal, 1995; Friedman, Getoor,\u2026"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Latent variable models Learning in Graphical Models"
            },
            "venue": {
                "fragments": [],
                "text": "Latent variable models Learning in Graphical Models"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 67
                            }
                        ],
                        "text": "The assumptions allowing us to define this prior are explicated by Meila\u0306 and Jaakkola (2000) and parallel the reasoning of Heckerman et al. (1995) for general Bayes nets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 94
                            }
                        ],
                        "text": "Given that the number of all undirected tree structures over n variables is nn\u22122, this result (Meila\u0306 & Jaakkola, 2000) is quite surprising."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 94
                            }
                        ],
                        "text": "Given that the number of all undirected tree structures over n variables is nn\u22122, this result [31, 32] is quite surprising."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Tractable Bayesian learning of tree distributions"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings of the 16th Conference on Uncertainty in AI (San Francisco,"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 0
                            }
                        ],
                        "text": "Kontkanen, Myllymaki, and Tirri (1996) study a MF in which a hidden variable is used for classification; this approach was extended by Monti and Cooper (1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 233
                            }
                        ],
                        "text": "In the testing phase, a new instance x \u2208 \u2126(V ) is classified by picking the most likely value of the class variable given the settings of the other variables:\nc(x) = argmax xc Q(xc, x)\nSimilarly, for the MF classifier (termed \u201cD-SIDE\u201d by Kontkanen et al., 1996), Q above is an MF trained on {c}\u22c3 V ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Constructing Bayesian finite mixture models by the EM algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Constructing Bayesian finite mixture models by the EM algorithm"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Discovering dependencies by repeated sampling"
            },
            "venue": {
                "fragments": [],
                "text": "Discovering dependencies by repeated sampling"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 111
                            }
                        ],
                        "text": "Second, the H model is a mixture model and we also know that learning mixtures in certain instances is NP-hard [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the complexity of clustering algorithms"
            },
            "venue": {
                "fragments": [],
                "text": "Optimierung und Operations Research, Lecture Notes in Economics and Mathematical Systems,"
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 250,
                                "start": 242
                            }
                        ],
                        "text": "The existent approaches include: pruning the model and performing exact inference on the reduced model [40], cutting loops and bounding the incurred error [19], variational methods to bound the node probabilities in sigmoidal belief networks [35, 39]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning and inference in graphical models, chapter An introduction to variational methods for graphical models, pages 75-104"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Irvine Machine Learning Repository. ftp://ftp.ics.uci.edu/pub/machine-learning- databases"
            },
            "venue": {
                "fragments": [],
                "text": "Irvine Machine Learning Repository. ftp://ftp.ics.uci.edu/pub/machine-learning- databases"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "[63] and [17] directly find dependencies in discrete databases by counting coincidences in subsamples."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Discovering dependencies by repeated sampling"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Molecular Biology of the Gene I. The Benjamin"
            },
            "venue": {
                "fragments": [],
                "text": "Molecular Biology of the Gene I. The Benjamin"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mushroom database. U.C. Irvine Machine Learning Repository"
            },
            "venue": {
                "fragments": [],
                "text": "Mushroom database. U.C. Irvine Machine Learning Repository"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 72
                            }
                        ],
                        "text": "This is similar in spirit to the \u201cmixture discriminant analysis\u201d model of Hastie and Tibshirani (1996), where a mixture of Gaussians is used for each class in a multiway classification problem."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Discriminant analysis by mixture modeling"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Royal Statistical Society B"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "[64] and [17] directly find dependencies in discrete databases by counting coincidences in subsamples."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dependency detection in subquadratic time"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian learning of tree distributions"
            },
            "venue": {
                "fragments": [],
                "text": "Bayesian learning of tree distributions"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": ", \"Molecular Biology of the Gene\" [68]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Molecular Biology of the Gene, volume I"
            },
            "venue": {
                "fragments": [],
                "text": "The Benjamin/Cummings Publishing Company,"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 172
                            }
                        ],
                        "text": "The idea of learning tractable but simple belief networks and superimposing a mixture to account for the remaining dependencies was developed independently of this work by [65] into mixtures of Gaussian belief networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning mixtures of Bayes networks"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report MSR-POR-97-30, Microsoft Research,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 273,
                                "start": 253
                            }
                        ],
                        "text": "\u2026Meila\u0306 and Michael I. Jordan.\nfor classification, prediction and density estimation (Bishop, 1999; Friedman, Geiger, & Goldszmidt, 1997; Heckerman, Geiger, & Chickering, 1995; Hinton, Dayan, Frey, & Neal, 1995; Friedman, Getoor, Koller, & Pfeffer, 1996; Monti & Cooper, 1998; Saul & Jordan, 1999)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 135
                            }
                        ],
                        "text": "Kontkanen, Myllymaki, and Tirri (1996) study a MF in which a hidden variable is used for classification; this approach was extended by Monti and Cooper (1998). The idea of learning tractable but simple belief networks and superimposing a mixture to account for the remaining dependencies was developed independently of our work by Thiesson, Meek, Chickering, and Heckerman (1997), who studied mixtures of Gaussian belief networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 135
                            }
                        ],
                        "text": "Kontkanen, Myllymaki, and Tirri (1996) study a MF in which a hidden variable is used for classification; this approach was extended by Monti and Cooper (1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Bayesian network classfier that combines a finite mixture model and a naive Bayes model (Tech"
            },
            "venue": {
                "fragments": [],
                "text": "Rep. No. ISSP-98-01). University of Pittsburgh"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dependency detection in subquadratic time"
            },
            "venue": {
                "fragments": [],
                "text": "Dependency detection in subquadratic time"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning mixtures of Bayes networks (Tech"
            },
            "venue": {
                "fragments": [],
                "text": "Rep. Nos. MSR\u2013POR\u201397\u201330). Microsoft Research"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Irvine Machine Learning Repository. ftp://ftp.ics.uci.edu/pub/machine-learning-databases"
            },
            "venue": {
                "fragments": [],
                "text": "Irvine Machine Learning Repository. ftp://ftp.ics.uci.edu/pub/machine-learning-databases"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Molecular Biology of the Gene, volume I. The Benjamin"
            },
            "venue": {
                "fragments": [],
                "text": "Molecular Biology of the Gene, volume I. The Benjamin"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 34
                            }
                        ],
                        "text": ", \u201cMolecular Biology of the Gene\u201d (Watson et al., 1987)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 85
                            }
                        ],
                        "text": "The same figure displays the \u201ctrue\u201d encodings of the IE and EI junctions as given by Watson et al. (1987). The match between the two encodings is almost perfect."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Molecular Biology of the Gene (Vol. I, 4 ed.)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 34,
            "methodology": 27,
            "result": 5
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 123,
        "totalPages": 13
    },
    "page_url": "https://www.semanticscholar.org/paper/Learning-with-Mixtures-of-Trees-Meil\u0103-Jordan/445cac87f39f44f128db8b0a48032fea845fec16?sort=total-citations"
}