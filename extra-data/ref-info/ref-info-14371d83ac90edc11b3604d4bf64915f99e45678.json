{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49983785"
                        ],
                        "name": "J. L. Fisher",
                        "slug": "J.-L.-Fisher",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Fisher",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. L. Fisher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1409052480"
                        ],
                        "name": "S. C. Hinds",
                        "slug": "S.-C.-Hinds",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Hinds",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. C. Hinds"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1402307078"
                        ],
                        "name": "D. D'Amato",
                        "slug": "D.-D'Amato",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "D'Amato",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. D'Amato"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61327298,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23fdda45268bf3898a95bad889e7a169081c4a77",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "A rule-based system for automatically segmenting a document image into regions of text and nontext is presented. The initial stages of the system perform image enhancement functions such as adaptive thresholding, morphological processing, and skew detection and correction. The image segmentation process consists of smearing the original image via the run length smoothing algorithm, calculating the connected components locations and statistics, and filtering (segmenting) the image based on these statistics. The text regions can be converted (via an optical character reader) to a computer-searchable form, and the nontext regions can be extracted and preserved. The rule-based structure allows easy fine tuning of the algorithmic steps to produce robust rules, to incorporate additional tools (as they become available), and to handle special segmentation needs.<<ETX>>"
            },
            "slug": "A-rule-based-system-for-document-image-segmentation-Fisher-Hinds",
            "title": {
                "fragments": [],
                "text": "A rule-based system for document image segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A rule-based system for automatically segmenting a document image into regions of text and nontext is presented and allows easy fine tuning of the algorithmic steps to produce robust rules, to incorporate additional tools (as they become available), and to handle special segmentation needs."
            },
            "venue": {
                "fragments": [],
                "text": "[1990] Proceedings. 10th International Conference on Pattern Recognition"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145820949"
                        ],
                        "name": "T. Pavlidis",
                        "slug": "T.-Pavlidis",
                        "structuredName": {
                            "firstName": "Theodosios",
                            "lastName": "Pavlidis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Pavlidis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2461436"
                        ],
                        "name": "Jiangying Zhou",
                        "slug": "Jiangying-Zhou",
                        "structuredName": {
                            "firstName": "Jiangying",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiangying Zhou"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 23410608,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "999e76f9115af2741b0cd973d875163ae714d5da",
            "isKey": false,
            "numCitedBy": 281,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Page-segmentation-and-classification-Pavlidis-Zhou",
            "title": {
                "fragments": [],
                "text": "Page segmentation and classification"
            },
            "venue": {
                "fragments": [],
                "text": "CVGIP Graph. Model. Image Process."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145211604"
                        ],
                        "name": "K. Karu",
                        "slug": "K.-Karu",
                        "structuredName": {
                            "firstName": "Kalle",
                            "lastName": "Karu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Karu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 20133451,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bfbafbc47d9ad9722047bf7c15d88d9bf4762744",
            "isKey": false,
            "numCitedBy": 216,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Texture segmentation using multichannel filtering involves applying a set of masks to an input image, and then grouping the pixels based on the responses to these masks. We solve the problem of finding an optimal set of masks by designing a neural network which is trained to maximize a relevant function. Two algorithms, the centroid algorithm and the gradient descent algorithm, are used to train the network. Experimental results on segmenting two natural textures and extracting barcodes in an image are reported, and the error rates compared for both the algorithms with different network configurations. The centroid algorithm gives better results in small parameter spaces, whereas the gradient descent algorithm works better with more parameters. Our method of automatically generating texture discrimination masks not only results in a good segmentation performance, but also reduces the dimensionality of the feature space compared to previously published multichannel filtering methods.<<ETX>>"
            },
            "slug": "Learning-texture-discrimination-masks-Jain-Karu",
            "title": {
                "fragments": [],
                "text": "Learning Texture Discrimination Masks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The method of automatically generating texture discrimination masks not only results in a good segmentation performance, but also reduces the dimensionality of the feature space compared to previously published multichannel filtering methods."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111216375"
                        ],
                        "name": "Dacheng Wang",
                        "slug": "Dacheng-Wang",
                        "structuredName": {
                            "firstName": "Dacheng",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dacheng Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696384"
                        ],
                        "name": "S. Srihari",
                        "slug": "S.-Srihari",
                        "structuredName": {
                            "firstName": "Sargur",
                            "lastName": "Srihari",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Srihari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 26685555,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "873fa0ca7454bd1ca4de25128c522e088b635ddc",
            "isKey": false,
            "numCitedBy": 250,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Classification-of-newspaper-image-blocks-using-Wang-Srihari",
            "title": {
                "fragments": [],
                "text": "Classification of newspaper image blocks using texture analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Graph. Image Process."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148700"
                        ],
                        "name": "O. Akindele",
                        "slug": "O.-Akindele",
                        "structuredName": {
                            "firstName": "O.",
                            "lastName": "Akindele",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Akindele"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2128453"
                        ],
                        "name": "A. Bela\u00efd",
                        "slug": "A.-Bela\u00efd",
                        "structuredName": {
                            "firstName": "Abdel",
                            "lastName": "Bela\u00efd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bela\u00efd"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 41120654,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "34f76569e4e33d2572ece2ae268960423b89f29f",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "A page segmentation method that allows one to cut a document page image into polygonal blocks as well as into classical rectangular blocks is described. The intercolumn and interparagraph gaps are extracted as horizontal and vertical lines. The points of intersection between these lines are treated as vertices of polygonal blocks. With the aid of the 4-connected chain codes and an intersection table, simple isothetic polygonal blocks are constructed from these points of intersection.<<ETX>>"
            },
            "slug": "Page-segmentation-by-segment-tracing-Akindele-Bela\u00efd",
            "title": {
                "fragments": [],
                "text": "Page segmentation by segment tracing"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A page segmentation method that allows one to cut a document page image into polygonal blocks as well as into classical rectangular blocks is described."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 2nd International Conference on Document Analysis and Recognition (ICDAR '93)"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1896610"
                        ],
                        "name": "S. Randriamasy",
                        "slug": "S.-Randriamasy",
                        "structuredName": {
                            "firstName": "Sabine",
                            "lastName": "Randriamasy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Randriamasy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1820595"
                        ],
                        "name": "L. Vincent",
                        "slug": "L.-Vincent",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Vincent",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Vincent"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6941199,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9f05833210f2b2e2f495594aeb98ff66e30c8fa1",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A method for automatically evaluating the quality of document page segmentation algorithms is introduced. Many different zoning techniques are now available but there is no robust method available to benchmark and evaluate them reliably. Our proposed strategy is a region-based approach, in which segmentation results are compared with manually generated \"ground truth files\", describing all possible correct segmentations. A segmentation ground truthing scheme has been proposed. The evaluation of segmentation quality is achieved by testing the overlap between the two sets of regions. In fact, the regions are defined as the \"black\" pixels contained in the extracted polygons. An explicit specification of segmentation errors and a numerical evaluation are derived. The algorithm is simple and fast, and provides a multi-level output for each segmentation.<<ETX>>"
            },
            "slug": "Benchmarking-page-segmentation-algorithms-Randriamasy-Vincent",
            "title": {
                "fragments": [],
                "text": "Benchmarking page segmentation algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A method for automatically evaluating the quality of document page segmentation algorithms is introduced, in which segmentation results are compared with manually generated \"ground truth files\", describing all possible correct segmentations."
            },
            "venue": {
                "fragments": [],
                "text": "1994 Proceedings of IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3326060"
                        ],
                        "name": "F. Farrokhnia",
                        "slug": "F.-Farrokhnia",
                        "structuredName": {
                            "firstName": "Farshid",
                            "lastName": "Farrokhnia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Farrokhnia"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 21804891,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e468e08612b1448c7e814e3d32c4384a06cfbe3c",
            "isKey": false,
            "numCitedBy": 2456,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "A texture segmentation algorithm inspired by the multichannel filtering theory for visual information processing in the early stages of the human visual system is presented. The channels are characterized by a bank of Gabor filters that nearly uniformly covers the spatial-frequency domain. A systematic filter selection scheme based on reconstruction of the input image from the filtered images is proposed. Texture features are obtained by subjecting each (selected) filtered image to a nonlinear transformation and computing a measure of energy in a window around each pixel. An unsupervised square-error clustering algorithm is then used to integrate the feature images and produce a segmentation. A simple procedure to incorporate spatial adjacency information in the clustering process is proposed. Experiments on images with natural textures as well as artificial textures with identical second and third-order statistics are reported. The algorithm appears to perform as predicted by preattentive texture discrimination by a human.<<ETX>>"
            },
            "slug": "Unsupervised-texture-segmentation-using-Gabor-Jain-Farrokhnia",
            "title": {
                "fragments": [],
                "text": "Unsupervised texture segmentation using Gabor filters"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A texture segmentation algorithm inspired by the multichannel filtering theory for visual information processing in the early stages of the human visual system is presented and appears to perform as predicted by preattentive texture discrimination by a human."
            },
            "venue": {
                "fragments": [],
                "text": "1990 IEEE International Conference on Systems, Man, and Cybernetics Conference Proceedings"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145916951"
                        ],
                        "name": "G. Nagy",
                        "slug": "G.-Nagy",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Nagy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Nagy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145062511"
                        ],
                        "name": "S. Seth",
                        "slug": "S.-Seth",
                        "structuredName": {
                            "firstName": "Sharad",
                            "lastName": "Seth",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Seth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145266621"
                        ],
                        "name": "M. Viswanathan",
                        "slug": "M.-Viswanathan",
                        "structuredName": {
                            "firstName": "Mahesh",
                            "lastName": "Viswanathan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Viswanathan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 2530196,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "62a53caea5213ea177298d7b2aff292b1386c37a",
            "isKey": false,
            "numCitedBy": 434,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Gobbledoc, a system providing remote access to stored documents, which is based on syntactic document analysis and optical character recognition (OCR), is discussed. In Gobbledoc, image processing, document analysis, and OCR operations take place in batch mode when the documents are acquired. The document image acquisition process and the knowledge base that must be entered into the system to process a family of page images are described. The process by which the X-Y tree data structure converts a 2-D page-segmentation problem into a series of 1-D string-parsing problems that can be tackled using conventional compiler tools is also described. Syntactic analysis is used in Gobbledoc to divide each page into labeled rectangular blocks. Blocks labeled text are converted by OCR to obtain a secondary (ASCII) document representation. Since such symbolic files are better suited for computerized search than for human access to the document content and because too many visual layout clues are lost in the OCR process (including some special characters), Gobbledoc preserves the original block images for human browsing. Storage, networking, and display issues specific to document images are also discussed.<<ETX>>"
            },
            "slug": "A-prototype-document-image-analysis-system-for-Nagy-Seth",
            "title": {
                "fragments": [],
                "text": "A prototype document image analysis system for technical journals"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The document image acquisition process and the knowledge base that must be entered into the system to process a family of page images are described, and the process by which the X-Y tree data structure converts a 2-D page-segmentation problem into a series of 1-D string-parsing problems that can be tackled using conventional compiler tools."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398550688"
                        ],
                        "name": "L. O'Gorman",
                        "slug": "L.-O'Gorman",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "O'Gorman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. O'Gorman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 22995244,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d85097da36118fbccfeb7802abf89bf4b4c63a3e",
            "isKey": false,
            "numCitedBy": 728,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Page layout analysis is a document processing technique used to determine the format of a page. This paper describes the document spectrum (or docstrum), which is a method for structural page layout analysis based on bottom-up, nearest-neighbor clustering of page components. The method yields an accurate measure of skew, within-line, and between-line spacings and locates text lines and text blocks. It is advantageous over many other methods in three main ways: independence from skew angle, independence from different text spacings, and the ability to process local regions of different text orientations within the same image. Results of the method shown for several different page formats and for randomly oriented subpages on the same image illustrate the versatility of the method. We also discuss the differences, advantages, and disadvantages of the docstrum with respect to other lay-out methods. >"
            },
            "slug": "The-Document-Spectrum-for-Page-Layout-Analysis-O'Gorman",
            "title": {
                "fragments": [],
                "text": "The Document Spectrum for Page Layout Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "The document spectrum (or docstrum), which is a method for structural page layout analysis based on bottom-up, nearest-neighbor clustering of page components, yields an accurate measure of skew, within-line, and between-line spacings and locates text lines and text blocks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4723637"
                        ],
                        "name": "J. Mao",
                        "slug": "J.-Mao",
                        "structuredName": {
                            "firstName": "Jianchang",
                            "lastName": "Mao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Mao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 11686184,
            "fieldsOfStudy": [
                "Mathematics",
                "Environmental Science",
                "Computer Science"
            ],
            "id": "0b1c14ccf1aad87f215bfa5c6678d975d44ffb3a",
            "isKey": false,
            "numCitedBy": 795,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Texture-classification-and-segmentation-using-Mao-Jain",
            "title": {
                "fragments": [],
                "text": "Texture classification and segmentation using multiresolution simultaneous autoregressive models"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145470231"
                        ],
                        "name": "Kwan Y. Wong",
                        "slug": "Kwan-Y.-Wong",
                        "structuredName": {
                            "firstName": "Kwan",
                            "lastName": "Wong",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kwan Y. Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34896449"
                        ],
                        "name": "R. Casey",
                        "slug": "R.-Casey",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Casey",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Casey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1880661"
                        ],
                        "name": "F. Wahl",
                        "slug": "F.-Wahl",
                        "structuredName": {
                            "firstName": "Friedrich",
                            "lastName": "Wahl",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Wahl"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15921038,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7abca302c74d2f5adfd323a28e26d40b019df2b5",
            "isKey": false,
            "numCitedBy": 594,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper outlines the requirements and components for a proposed Document Analysis System, which assists a user in encoding printed documents for computer processing. Several critical functions have been investigated and the technical approaches are discussed. The first is the segmentation and classification of digitized printed documents into regions of text and images. A nonlinear, run-length smoothing algorithm has been used for this purpose. By using the regular features of text lines, a linear adaptive classification scheme discriminates text regions from others. The second technique studied is an adaptive approach to the recognition of the hundreds of font styles and sizes that can occur on printed documents. A preclassifier is constructed during the input process and used to speed up a well-known pattern-matching method for clustering characters from an arbitrary print source into a small sample of prototypes. Experimental results are included."
            },
            "slug": "Document-Analysis-System-Wong-Casey",
            "title": {
                "fragments": [],
                "text": "Document Analysis System"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "The requirements and components for a proposed Document Analysis System, which assists a user in encoding printed documents for computer processing, are outlined and several critical functions have been investigated and the technical approaches are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "IBM J. Res. Dev."
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3055160"
                        ],
                        "name": "David J. Ittner",
                        "slug": "David-J.-Ittner",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Ittner",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Ittner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723766"
                        ],
                        "name": "H. Baird",
                        "slug": "H.-Baird",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Baird",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Baird"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5360061,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4bc5bf7115142885992f99516169ea24bf18b529",
            "isKey": false,
            "numCitedBy": 86,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A system for isolating blocks, lines, words, and symbols within images of machine-printed textual documents that is, to a large existent, independent of language and writing system is described. This is achieved by exploiting a small number of nearly universal typesetting and layout conventions. The system does not require prior knowledge of page orientation (module 90/spl deg/), and copes well with nonzero skew and shear angles (within 10/spl deg/). Also it locates blocks of text without reliance on detailed a priori layout models, and in spite of unknown or mixed horizontal and vertical text-line orientations. Within blocks, it infers text-line orientation and isolates lines, without knowledge of the language, symbol set, text sizes, or the number of text lines. Segmentation into words and symbols, and determination of reading order, normally require some knowledge of the language: this is held to minimum by relying on shape-driven algorithms. The underlying algorithms are based on Fourier theory, digital signal processing, computational geometry, and statistical decision theory. Most of the computation occurs within algorithms that possess unambiguous semantics (that is, heuristics are kept to a minimum). The effectiveness of the method on English, Japanese, Hebrew, Thai, and Korean documents is discussed.<<ETX>>"
            },
            "slug": "Language-free-layout-analysis-Ittner-Baird",
            "title": {
                "fragments": [],
                "text": "Language-free layout analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "A system for isolating blocks, lines, words, and symbols within images of machine-printed textual documents that is, to a large existent, independent of language and writing system is described, achieved by exploiting a small number of nearly universal typesetting and layout conventions."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 2nd International Conference on Document Analysis and Recognition (ICDAR '93)"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1470838762"
                        ],
                        "name": "Mihran T\u00fcceryan",
                        "slug": "Mihran-T\u00fcceryan",
                        "structuredName": {
                            "firstName": "Mihran",
                            "lastName": "T\u00fcceryan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mihran T\u00fcceryan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295483"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5635365,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "11d1ad1b22610daba6ee797ec213eb29ff36b418",
            "isKey": false,
            "numCitedBy": 2296,
            "numCiting": 175,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter reviews and discusses various aspects of texture analysis. The concentration is o the various methods of extracting textural features from images. The geometric, random field, fractal, and signal processing models of texture are presented. The major classes of texture processing pro lems such as segmentation, classification, and shape from texture are discussed. The possible applic tion areas of texture such as automated inspection, document processing, and remote sensing a summarized. A bibliography is provided at the end for further reading."
            },
            "slug": "Texture-Analysis-T\u00fcceryan-Jain",
            "title": {
                "fragments": [],
                "text": "Texture Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The geometric, random field, fractal, and signal processing models of texture are presented and major classes of texture processing such as segmentation, classification, and shape from texture are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "Handbook of Pattern Recognition and Computer Vision"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710238"
                        ],
                        "name": "R. Haralick",
                        "slug": "R.-Haralick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Haralick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haralick"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 13615381,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d91e0d202fa23b7a2e81c5b3b04eb4cc5327b0f9",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "Document image understanding encompasses the technology required to make paper documents equivalent to other computer exchange media like floppies, tapes, and CDROMs. The physical reader of the paper document is the scanner just like the physical reader of the floppy is the floppy drive and the physical reader of the tape cartridge is the tape cartridge drive, and the physical reader of the CDROM is the CDROM drive. In the survey presented, we restrict ourselves to documents such as business letters, forms, and scientific and technical articles such as those found in archival journals and technical conferences. Understanding such documents involves estimating the rotation skew of each document page, determining the geometric page layout, labeling blocks as text or non-text, determining the read order for text blocks, recognizing the text of text blocks through an OCR system, determining the logical page layout, and formatting the data and information of the document in a suitable way for use by a word processing system or by an information retrieval system.<<ETX>>"
            },
            "slug": "Document-image-understanding:-geometric-and-logical-Haralick",
            "title": {
                "fragments": [],
                "text": "Document image understanding: geometric and logical layout"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "Document image understanding encompasses the technology required to make paper documents equivalent to other computer exchange media like floppies, tapes, and CDROMs and restricts ourselves to documents such as business letters, forms, and scientific and technical articles such as those found in archival journals and technical conferences."
            },
            "venue": {
                "fragments": [],
                "text": "1994 Proceedings of IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2258400"
                        ],
                        "name": "\u00d8. Trier",
                        "slug": "\u00d8.-Trier",
                        "structuredName": {
                            "firstName": "\u00d8ivind",
                            "lastName": "Trier",
                            "middleNames": [
                                "Due"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00d8. Trier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15780310,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3a74fe4e79add9de4803a825b6eae013215dfe7",
            "isKey": false,
            "numCitedBy": 731,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a methodology for evaluation of low-level image analysis methods, using binarization (two-level thresholding) as an example. Binarization of scanned gray scale images is the first step in most document image analysis systems. Selection of an appropriate binarization method for an input image domain is a difficult problem. Typically, a human expert evaluates the binarized images according to his/her visual criteria. However, to conduct an objective evaluation, one needs to investigate how well the subsequent image analysis steps will perform on the binarized image. We call this approach goal-directed evaluation, and it can be used to evaluate other low-level image processing methods as well. Our evaluation of binarization methods is in the context of digit recognition, so we define the performance of the character recognition module as the objective measure. Eleven different locally adaptive binarization methods were evaluated, and Niblack's method gave the best performance."
            },
            "slug": "Goal-Directed-Evaluation-of-Binarization-Methods-Trier-Jain",
            "title": {
                "fragments": [],
                "text": "Goal-Directed Evaluation of Binarization Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "This paper presents a methodology for evaluation of low-level image analysis methods, using binarization (two-level thresholding) as an example, and defines the performance of the character recognition module as the objective measure."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49750929"
                        ],
                        "name": "C. Chen",
                        "slug": "C.-Chen",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Chen",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144105639"
                        ],
                        "name": "L. Pau",
                        "slug": "L.-Pau",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Pau",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Pau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143653107"
                        ],
                        "name": "P. S. Wang",
                        "slug": "P.-S.-Wang",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Wang",
                            "middleNames": [
                                "S.",
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. S. Wang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58410480,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fc6955ad31ad5934535052c6ebf4316748a3ded6",
            "isKey": false,
            "numCitedBy": 995,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Both pattern recognition and computer vision have experienced rapid progress in the last twenty-five years. This book provides the latest advances on pattern recognition and computer vision along with their many applications. It features articles written by renowned leaders in the field while topics are presented in readable form to a wide range of readers. The book is divided into five parts: basic methods in pattern recognition, basic methods in computer vision and image processing, recognition applications, life science and human identification, and systems and technology."
            },
            "slug": "Handbook-of-Pattern-Recognition-and-Computer-Vision-Chen-Pau",
            "title": {
                "fragments": [],
                "text": "Handbook of Pattern Recognition and Computer Vision"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This book provides the latest advances on pattern recognition and computer vision along with their many applications and features articles written by renowned leaders in the field while topics are presented in readable form to a wide range of readers."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744200"
                        ],
                        "name": "I. T. Phillips",
                        "slug": "I.-T.-Phillips",
                        "structuredName": {
                            "firstName": "Ihsin",
                            "lastName": "Phillips",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. T. Phillips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111237291"
                        ],
                        "name": "Su S. Chen",
                        "slug": "Su-S.-Chen",
                        "structuredName": {
                            "firstName": "Su",
                            "lastName": "Chen",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Su S. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710238"
                        ],
                        "name": "R. Haralick",
                        "slug": "R.-Haralick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Haralick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haralick"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 3258255,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de1580d4cc0c47e4985c8ce52d106c6a7432ff70",
            "isKey": false,
            "numCitedBy": 127,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "The design of a comprehensive standard document database for machine-printed documents is presented. The effort to produce a series of carefully ground-truthed document databases to be issued on CD-ROMs is described in detail. The databases can be utilized by the OCR and document understanding community as a common platform to develop, test, and evaluate their algorithms.<<ETX>>"
            },
            "slug": "CD-ROM-document-database-standard-Phillips-Chen",
            "title": {
                "fragments": [],
                "text": "CD-ROM document database standard"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "The design of a comprehensive standard document database for machine-printed documents is presented and can be utilized by the OCR and document understanding community as a common platform to develop, test, and evaluate their algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 2nd International Conference on Document Analysis and Recognition (ICDAR '93)"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2499975"
                        ],
                        "name": "J. Kanai",
                        "slug": "J.-Kanai",
                        "structuredName": {
                            "firstName": "Junichi",
                            "lastName": "Kanai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kanai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688975"
                        ],
                        "name": "T. Nartker",
                        "slug": "T.-Nartker",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Nartker",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Nartker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688357"
                        ],
                        "name": "S. V. Rice",
                        "slug": "S.-V.-Rice",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Rice",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. V. Rice"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145916951"
                        ],
                        "name": "G. Nagy",
                        "slug": "G.-Nagy",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Nagy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Nagy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 28850566,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ef93cb20d9efbe6974eea654d780626b09777b5a",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Requirements for the objective evaluation of automated data-entry systems are presented. Because the cost of correcting errors dominates the document conversion process, the most important characteristic of an OCR device is accuracy. However, different measures of accuracy (error metrics) are appropriate for different applications, and at the character, word, text-line, text-block, and document levels. For wholly objective assessment, OCR devices must be tested under programmed, rather than interactive, control.<<ETX>>"
            },
            "slug": "Performance-metrics-for-document-understanding-Kanai-Nartker",
            "title": {
                "fragments": [],
                "text": "Performance metrics for document understanding systems"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "Requirements for the objective evaluation of automated data-entry systems are presented and different measures of accuracy (error metrics) are appropriate for different applications, and at the character, word, text-line,Text-block, and document levels."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 2nd International Conference on Document Analysis and Recognition (ICDAR '93)"
            },
            "year": 1993
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {},
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 18,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Page-segmentation-using-tecture-analysis-Jain-Zhong/14371d83ac90edc11b3604d4bf64915f99e45678?sort=total-citations"
}