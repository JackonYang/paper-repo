{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744109"
                        ],
                        "name": "S. Salzberg",
                        "slug": "S.-Salzberg",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Salzberg",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Salzberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800681"
                        ],
                        "name": "Alberto Maria Segre",
                        "slug": "Alberto-Maria-Segre",
                        "structuredName": {
                            "firstName": "Alberto",
                            "lastName": "Segre",
                            "middleNames": [
                                "Maria"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alberto Maria Segre"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 55
                            }
                        ],
                        "text": "The more popular decision tree algorithmslike CART and C4.5 work on attributes that are symbolic."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 45
                            }
                        ],
                        "text": "The SVM formulation (1.9) | (1.10) can be viewed as a transformationof (19.1) in which = 2 and and kwk2 is minimized."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 179
                            }
                        ],
                        "text": "SVDT performs top down induction of decision trees (TDIDT) like manyother decision tree algorithms including CHAID, CART, MSMT, C4.5, and OC1(Breiman et al., 1984; Bennett, 1992; Quinlan, 1993; Murthy et al., 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 22
                            }
                        ],
                        "text": "For comparison, C4.5 (Quinlan, 1993) produced a treeconsisting of 251 univariate decisions with 66% testing set accuracy."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "C4.5 took 110\n1998/08/25 16:31\n19.3 Nonlinear Separation via Decision Trees 315\nPSfrag replacements\nD0D1 D2L2 L3 L1 L4 1 attr82 margin48 attr28 margin 49 attr17 margin Class A1Resp Rate: 85.6%Targ Class: 59.3%Total Pop: 33.6%Class A2Resp Rate: 21.0%Targ Class: 21.1%Total Pop: 48.8%Class A2Resp Rate: 45.5%Targ Class: 9.2%Total Pop: 9.8% Class A1Resp Rate 73.1%Targ Class 10.4%Total Pop: 9.8%Figure 19.2 Decision tree for Business I test data, target class A1minutes to construct the decision tree on the same platform."
                    },
                    "intents": []
                }
            ],
            "corpusId": 60499165,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7feb0fc888cd55360949554db032d7d1cba9e947",
            "isKey": true,
            "numCitedBy": 7025,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Algorithms for constructing decision trees are among the most well known and widely used of all machine learning methods. Among decision tree algorithms, J. Ross Quinlan's ID3 and its successor, C4.5, are probably the most popular in the machine learning community. These algorithms and variations on them have been the subject of numerous research papers since Quinlan introduced ID3. Until recently, most researchers looking for an introduction to decision trees turned to Quinlan's seminal 1986 Machine Learning journal article [Quinlan, 1986]. In his new book, C4.5: Programs for Machine Learning, Quinlan has put together a definitive, much needed description of his complete system, including the latest developments. As such, this book will be a welcome addition to the library of many researchers and students."
            },
            "slug": "Programs-for-Machine-Learning-Salzberg-Segre",
            "title": {
                "fragments": [],
                "text": "Programs for Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "In his new book, C4.5: Programs for Machine Learning, Quinlan has put together a definitive, much needed description of his complete system, including the latest developments, which will be a welcome addition to the library of many researchers and students."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 36
                            }
                        ],
                        "text": "Each sample belongs to one often classes: the integers 0 through 9."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 9
                            }
                        ],
                        "text": "In -SVM (Vapnik, 1995; Cortes and Vapnik, 1995) a quadratic programis solved to construct a discriminant function to separate one class from the re-maining 1 classes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 227
                            }
                        ],
                        "text": "PSfrag replacements+\nPiecewise-Linear Separator 2nd Pair of Planes 1st Pair of PlanesFigure 19.1 Piecewise-linear discriminant constructed by MSMand Mangasarian, 1990; Mangasarian et al., 1990), the method performs poorlyon noisy data sets since it minimizes the largest error in each class."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 149
                            }
                        ],
                        "text": "\u2026unique, then the SVM solution with\n1998/08/25 16:31\n19.2 Two MPM Methods for Classi cation 311su ciently large C will be the optimal solution of RLP with the least 2-norm of w.If the 1-norm objective term, kwk1, is added, RLP can be generalized to constructa SVM variation with 1-norm capacity\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 102
                            }
                        ],
                        "text": "The original SVM method for multiclassproblems was to nd separate two-class discriminants (Cortes and Vapnik, 1995;Vapnik, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 46
                            }
                        ],
                        "text": "This set is refered to as the testing set in (Vapnik, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 46
                            }
                        ],
                        "text": "Figure 19.6 Left = solution found by RLP; Right = solution found by S3VM-MIPData Set Dim Points CV-size RLP S3VM -MIP p-valueBright 14 2462 50* 0.02 0.018 0.343Cancer 9 699 70 0.036 0.034 0.591Cancer(Prognostic) 30 569 57 0.035 0.033 0.678Dim 14 4192 50* 0.064 0.054 0.096Heart 13 297 30 0.173 0.160\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 172
                            }
                        ],
                        "text": "Vapnik brie y presented this problem at the NIPS1997 Support Vector Machine Workshop (see chapter 3) and it also can be found inchapter 10 of (Vapnik, 1979) and brie y in (Vapnik, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7138354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8213dbed4db44e113af3ed17d6dad57471a0c048",
            "isKey": true,
            "numCitedBy": 38755,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?."
            },
            "slug": "The-Nature-of-Statistical-Learning-Theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics for Engineering and Information Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747026"
                        ],
                        "name": "O. Mangasarian",
                        "slug": "O.-Mangasarian",
                        "structuredName": {
                            "firstName": "Olvi",
                            "lastName": "Mangasarian",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Mangasarian"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 272,
                                "start": 254
                            }
                        ],
                        "text": "1 IntroductionIn this chapter we investigate the relationship between Support Vector Machines(SVM) for classi cation and a family of mathematical programming methods pri-marily stemming from Mangasarian's Multisurface Method of Pattern Recognition(MSM) (Mangasiarian, 1965; Mangasarian, 1968)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 155
                            }
                        ],
                        "text": "1998/08/25 16:31\n19.2 Two MPM Methods for Classi cation 30919.2 Two MPM Methods for Classi cationMangasarian's Multisurface Method of Pattern Recognition (Mangasiarian, 1965;Mangasarian, 1968) is very similar in derivation to the Generalized Portrait Methodof Vapnik and Chervonenkis (1974)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 207
                            }
                        ],
                        "text": "1998/08/25 16:31\nAdvances in Kernel MethodsSupport Vector Learning edited byBernhard Sch olkopfChristopher J.C. BurgesAlexander J. Smola\nThe MIT PressCambridge, MassachusettsLondon, England\n1998/08/25 16:31\nContents Preface IX1 Introduction to Support Vector Learning 12 Roadmap 17I Theory 233 Three Remarks on the Support Vector Method of FunctionEstimation 25Vladimir Vapnik4 Generalization Performance of Support Vector Machines andOther Pattern Classi ers 43Peter Bartlett & John Shawe-Taylor5 Bayesian Voting Schemes and Large Margin Classi ers 55Nello Cristianini & John Shawe-Taylor6 Support Vector Machines, Reproducing Kernel Hilbert Spaces,and Randomized GACV 69Grace Wahba7 Geometry and Invariance in Kernel Based Methods 89Christopher J. C. Burges8 On the Annealed VC Entropy for Margin Classi ers:A Statistical Mechanics Study 117Manfred Opper9 Entropy Numbers, Operators and Support Vector Kernels 127Robert C. Williamson, Alex J. Smola & Bernhard Sch olkopf\n1998/08/25 16:31\nvi II Implementations 14510 Solving the Quadratic Programming Problem Arising in SupportVector Classi cation 147Linda Kaufman11 Making Large-Scale Support Vector Machine Learning Practical 169Thorsten Joachims12 Fast Training of Support Vector Machines Using SequentialMinimal Optimization 185John C. PlattIII Applications 20913 Support Vector Machines for Dynamic Reconstruction of aChaotic System 211Davide Mattera & Simon Haykin14 Using Support Vector Machines for Time Series Prediction 243Klaus-Robert M uller, Alex J. Smola, Gunnar R atsch,Bernhard Sch olkopf, Jens Kohlmorgen & Vladimir Vapnik15 Pairwise Classi cation and Support Vector Machines 255Ulrich Kre elIV Extensions of the Algorithm 26916 Reducing the Run-time Complexity in Support Vector Machines 271Edgar E. Osuna & Federico Girosi17 Support Vector Regression with ANOVA Decomposition Kernels 285Mark O. Stitson, Alex Gammerman, Vladimir Vapnik,Volodya Vovk, Chris Watkins & Jason Weston18 Support Vector Density Estimation 293Jason Weston, Alex Gammerman, Mark O. Stitson,Vladimir Vapnik, Volodya Vovk & Chris Watkins19 Combining Support Vector and Mathematical ProgrammingMethods for Classi cation 307Kristin P. Bennett20 Kernel Principal Component Analysis 327Bernhard Sch olkopf, Alex J. Smola & Klaus-Robert M uller\n1998/08/25 16:31\nviiReferences 353Index 373\n1998/08/25 16:31"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 135
                            }
                        ],
                        "text": "\u2026J.C. BurgesAlexander J. Smola\nThe MIT PressCambridge, MassachusettsLondon, England\n1998/08/25 16:31\nContents Preface IX1 Introduction to Support Vector Learning 12 Roadmap 17I Theory 233 Three Remarks on the Support Vector Method of FunctionEstimation 25Vladimir Vapnik4\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 365,
                                "start": 346
                            }
                        ],
                        "text": "Kristin P. BennettMathematical Sciences DepartmentRensselaer Polytechnic InstituteTroy, NY 12180, USAbennek@rpi.eduhttp://www.math.rpi.edu/ bennekWe examine the relationship between Support Vector Machines (SVM) for clas-si cation and a family of mathematical programming methods (MPM) primarilystemming from Mangasarian's Multisurface Method of Pattern Recognition."
                    },
                    "intents": []
                }
            ],
            "corpusId": 121255639,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "cde0ef5bbc2355f22d48015495fe8627b4093fa3",
            "isKey": true,
            "numCitedBy": 350,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "A pattern separation problem is basically a problem of obtaining a criterion for distinguishing between the elements of two disjoint sets of patterns. The patterns are usually represented by points in a Euclidean space. One way to achieve separation is to construct a plane or a nonlinear surface such that one set of patterns lies on one side of the plane or the surface, and the other set of patterns on the other side. Recently, it has been shown that linear and ellipsoidal separation may be achieved by nonlinear programming. In this work it is shown that both linear and nonlinear separation may be achieved by linear programming."
            },
            "slug": "Linear-and-Nonlinear-Separation-of-Patterns-by-Mangasarian",
            "title": {
                "fragments": [],
                "text": "Linear and Nonlinear Separation of Patterns by Linear Programming"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145728220"
                        ],
                        "name": "K. Bennett",
                        "slug": "K.-Bennett",
                        "structuredName": {
                            "firstName": "Kristin",
                            "lastName": "Bennett",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Bennett"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 278
                            }
                        ],
                        "text": "\u2026InstituteTroy, NY 12180, USAbennek@rpi.eduhttp://www.math.rpi.edu/ bennekWe examine the relationship between Support Vector Machines (SVM) for clas-si cation and a family of mathematical programming methods (MPM) primarilystemming from Mangasarian's Multisurface Method of Pattern Recognition."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 228
                            }
                        ],
                        "text": "1 IntroductionIn this chapter we investigate the relationship between Support Vector Machines(SVM) for classi cation and a family of mathematical programming methods pri-marily stemming from Mangasarian's Multisurface Method of Pattern Recognition(MSM) (Mangasiarian, 1965; Mangasarian, 1968)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026Vapnik, Volodya Vovk & Chris Watkins19 Combining Support Vector and Mathematical ProgrammingMethods for Classi cation 307Kristin P. Bennett20 Kernel Principal Component Analysis 327Bernhard Sch olkopf, Alex J. Smola & Klaus-Robert M uller\n1998/08/25 16:31\nviiReferences 353Index 373\n1998/08/25 16:31"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 164
                            }
                        ],
                        "text": "SVDT performs top down induction of decision trees (TDIDT) like manyother decision tree algorithms including CHAID, CART, MSMT, C4.5, and OC1(Breiman et al., 1984; Bennett, 1992; Quinlan, 1993; Murthy et al., 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 134
                            }
                        ],
                        "text": "1998/08/25 16:31\n19.2 Two MPM Methods for Classi cation 30919.2 Two MPM Methods for Classi cationMangasarian's Multisurface Method of Pattern Recognition (Mangasiarian, 1965;Mangasarian, 1968) is very similar in derivation to the Generalized Portrait Methodof Vapnik and Chervonenkis (1974)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 445,
                                "start": 441
                            }
                        ],
                        "text": "1998/08/25 16:31\nAdvances in Kernel MethodsSupport Vector Learning edited byBernhard Sch olkopfChristopher J.C. BurgesAlexander J. Smola\nThe MIT PressCambridge, MassachusettsLondon, England\n1998/08/25 16:31\nContents Preface IX1 Introduction to Support Vector Learning 12 Roadmap 17I Theory 233 Three Remarks on the Support Vector Method of FunctionEstimation 25Vladimir Vapnik4 Generalization Performance of Support Vector Machines andOther Pattern Classi ers 43Peter Bartlett & John Shawe-Taylor5 Bayesian Voting Schemes and Large Margin Classi ers 55Nello Cristianini & John Shawe-Taylor6 Support Vector Machines, Reproducing Kernel Hilbert Spaces,and Randomized GACV 69Grace Wahba7 Geometry and Invariance in Kernel Based Methods 89Christopher J. C. Burges8 On the Annealed VC Entropy for Margin Classi ers:A Statistical Mechanics Study 117Manfred Opper9 Entropy Numbers, Operators and Support Vector Kernels 127Robert C. Williamson, Alex J. Smola & Bernhard Sch olkopf\n1998/08/25 16:31\nvi II Implementations 14510 Solving the Quadratic Programming Problem Arising in SupportVector Classi cation 147Linda Kaufman11 Making Large-Scale Support Vector Machine Learning Practical 169Thorsten Joachims12 Fast Training of Support Vector Machines Using SequentialMinimal Optimization 185John C. PlattIII Applications 20913 Support Vector Machines for Dynamic Reconstruction of aChaotic System 211Davide Mattera & Simon Haykin14 Using Support Vector Machines for Time Series Prediction 243Klaus-Robert M uller, Alex J. Smola, Gunnar R atsch,Bernhard Sch olkopf, Jens Kohlmorgen & Vladimir Vapnik15 Pairwise Classi cation and Support Vector Machines 255Ulrich Kre elIV Extensions of the Algorithm 26916 Reducing the Run-time Complexity in Support Vector Machines 271Edgar E. Osuna & Federico Girosi17 Support Vector Regression with ANOVA Decomposition Kernels 285Mark O. Stitson, Alex Gammerman, Vladimir Vapnik,Volodya Vovk, Chris Watkins & Jason Weston18 Support Vector Density Estimation 293Jason Weston, Alex Gammerman, Mark O. Stitson,Vladimir Vapnik, Volodya Vovk & Chris Watkins19 Combining Support Vector and Mathematical ProgrammingMethods for Classi cation 307Kristin P. Bennett20 Kernel Principal Component Analysis 327Bernhard Sch olkopf, Alex J. Smola & Klaus-Robert M uller\n1998/08/25 16:31\nviiReferences 353Index 373\n1998/08/25 16:31"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 56
                            }
                        ],
                        "text": "Thus a great potential exists for interactionbetween the two approaches."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 71
                            }
                        ],
                        "text": "RLP (19.3) has also been successfullyused in decision tree algorithms (Bennett, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 350,
                                "start": 346
                            }
                        ],
                        "text": "Kristin P. BennettMathematical Sciences DepartmentRensselaer Polytechnic InstituteTroy, NY 12180, USAbennek@rpi.eduhttp://www.math.rpi.edu/ bennekWe examine the relationship between Support Vector Machines (SVM) for clas-si cation and a family of mathematical programming methods (MPM) primarilystemming from Mangasarian's Multisurface Method of Pattern Recognition."
                    },
                    "intents": []
                }
            ],
            "corpusId": 59718399,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9692ef7b5be91022750552563eb2224b04022c2e",
            "isKey": true,
            "numCitedBy": 127,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Decision-Tree-Construction-Via-Linear-Programming-Bennett",
            "title": {
                "fragments": [],
                "text": "Decision Tree Construction Via Linear Programming"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145728220"
                        ],
                        "name": "K. Bennett",
                        "slug": "K.-Bennett",
                        "structuredName": {
                            "firstName": "Kristin",
                            "lastName": "Bennett",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Bennett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116668597"
                        ],
                        "name": "Donghui Wu",
                        "slug": "Donghui-Wu",
                        "structuredName": {
                            "firstName": "Donghui",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donghui Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2894031"
                        ],
                        "name": "Leonardo Auslender",
                        "slug": "Leonardo-Auslender",
                        "structuredName": {
                            "firstName": "Leonardo",
                            "lastName": "Auslender",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Leonardo Auslender"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 38
                            }
                        ],
                        "text": "The interested reader should consult (Bennett et al.,1998) for full details of these experiments."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 160
                            }
                        ],
                        "text": "RLP with 1-norm capacity control has been investigated in several papers (Ben-nett and Bredensteiner, 1998; Bredensteiner, 1997; Bradley and Mangasarian,1998a; Bennett et al., 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 4
                            }
                        ],
                        "text": "In (Bennett et al.,1998) results are given on three business problems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 71
                            }
                        ],
                        "text": "Full detailsSupport VectorDecision Trees of this work can be found in (Bennett et al., 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 54
                            }
                        ],
                        "text": "These results are drawn primarily from existing work (Bennett et al.,1998; Bredensteiner and Bennett, 1998; Bennett and Demiriz, 1998)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15043488,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a64fec49a357f1b206eeabdb3f79ec041eaed61",
            "isKey": true,
            "numCitedBy": 42,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a support vector decision tree method for customer targeting in the framework of large databases (database marketing). The goal is to provide a tool to identify the best customers based on historical data. This tool is then used to forecast the best potential customers among a pool of prospects. We begin by regressively constructing a decision tree. Each decision consists of a linear combination of independent attributes. A linear program motivated by the support vector machine method from Vapnik's statistical learning theory is used to construct each decision. This linear program automatically selects the relevant subset of attributes for each decision. Each customer is scored based on the decision tree. A gain chart table is used to verify the goodness-of-fit of the targeting, to determine the likely prospects and the expected utility or profit. Successful results are given for three industrial problems."
            },
            "slug": "On-support-vector-decision-trees-for-database-Bennett-Wu",
            "title": {
                "fragments": [],
                "text": "On support vector decision trees for database marketing"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A support vector decision tree method for customer targeting in the framework of large databases (database marketing) is introduced to provide a tool to identify the best customers based on historical data."
            },
            "venue": {
                "fragments": [],
                "text": "IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339)"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 41
                            }
                        ],
                        "text": "The Multicategory Discrimination Method (Bennett and Mangasarian, 1993, 1994)constructs a piecewise-linear discriminant for the -class problem using a singlelinear program."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 186
                            }
                        ],
                        "text": "\u2026; ; i6=j (19.9)\n1998/08/25 16:31\n19.4 Multicategory Classi cation 319or equivalentlyAi(wi wj) ( i j)e e; i; j = 1; : : : ; ; i6=j (19.10)The M-RLP method1 proposed and investigated in (Bennett and Mangasarian,1993, 1994) can be used to nd (wi; i); i = 1; : : : ; satisfying the inequalities(19.10)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 110
                            }
                        ],
                        "text": "To make MSM more tolerant of noise, Bennett and Mangasarian proposed theRobust Linear Programming method (RLP) (Bennett and Mangasarian, 1992)using the following linear program:minw; ; lXi=1 i isubject to yi(w xi ) + i 1 i 0 i = 1; ::; ` (19.3)Robust LinearProgrammingMethod where i > 0 is the xed\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 141
                            }
                        ],
                        "text": "The LP approach hasbeen to construct directly classi cation functions such that for each point thecorresponding class function is maximized (Bennett and Mangasarian, 1993, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multicategory separation via linear programming"
            },
            "venue": {
                "fragments": [],
                "text": "Optimization Methods and Software"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2647543"
                        ],
                        "name": "Erin J. Bredensteiner",
                        "slug": "Erin-J.-Bredensteiner",
                        "structuredName": {
                            "firstName": "Erin",
                            "lastName": "Bredensteiner",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Erin J. Bredensteiner"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026Vapnik, Volodya Vovk & Chris Watkins19 Combining Support Vector and Mathematical ProgrammingMethods for Classi cation 307Kristin P. Bennett20 Kernel Principal Component Analysis 327Bernhard Sch olkopf, Alex J. Smola & Klaus-Robert M uller\n1998/08/25 16:31\nviiReferences 353Index 373\n1998/08/25 16:31"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 37
                            }
                        ],
                        "text": "See(Bredensteiner and Bennett, 1998; Bredensteiner, 1997) for full details on theproblem formulation and results."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 274
                            }
                        ],
                        "text": "\u2026constructingthe best linear discriminant using the minimum number of attributes; and misclas-si cation minimization: explicitly minimizing the number of points misclassi ed(Bradley and Mangasarian, 1998a; Bradley et al., 1995; Bredensteiner and Ben-nett, 1997; Bennett and Bredensteiner, 1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 108
                            }
                        ],
                        "text": "RLP with 1-norm capacity control has been investigated in several papers (Ben-nett and Bredensteiner, 1998; Bredensteiner, 1997; Bradley and Mangasarian,1998a; Bennett et al., 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 52
                            }
                        ],
                        "text": "The papers(Mangasarian, 1997; Bradley et al., 1998; Bredensteiner, 1997) all contain interest-ing reviews."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 97
                            }
                        ],
                        "text": "Full details on all the results of this section can be found in (Bredensteinerand Bennett, 1998; Bredensteiner, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 124175030,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "e136c5d4b0438f99fa215f58f1009b73fded0108",
            "isKey": true,
            "numCitedBy": 5,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Large quantities of data are generated in business, industry, and government. The purpose of this research is to develop tools to interpret this data using mathematical programming techniques for machine learning. The fundamental problem of machine learning considered is the discrimination between the elements of two or more sets in $R\\sp{n}.$ Each dimension of the space represents a feature or attribute of the elements of the set. In this research, we propose several methods of determining both linear and nonlinear discrimination functions using mathematical programming. \nFirst, we investigate the problem of constructing a linear discriminant function from a geometric perspective. Under this framework we show the close relationship between existing approaches: two linear programming methods: the robust linear programming problem (RLP) and the multisurface method (MSM); and a quadratic generalized optimal plane program (GOP). A novel variation of RLP is proposed (RLP-P) combining the benefits of all three approaches. Theoretical and computational studies of the methods are performed. \nTwo novel methods for constructing linear discriminants are proposed using parametric bilinear programming. In the first method, a parametric error function is introduced that combines minimizing the number of points misclassified and the magnitude of the misclassified points. The second method is a feature selection problem that explicitly minimizes the number of features in each multivariate decision. We propose two versions: feature minimization with bounded accuracy and limited feature minimization. A Frank-Wolfe method is used to solve the bilinear subproblems. Computational results compare favorably with linear programming and other popular classification methods. \nFinally, we propose a piecewise-nonlinear classification function for discriminating between more than two sets. This function is found by mapping the original attributes into a higher dimensional space and performing piecewise-linear discrimination in that space. This approach is advantageous since when the problem is mapped into a higher dimension feature space the number of variables and constraints in the dual problem does not grow. We propose a novel approach using the two class linear programming problem RLP-P on the multiclass data. Computationally, the RLP-P method performed superior to a prior linear programming method as well as quadratic programming problems."
            },
            "slug": "Optimization-methods-in-data-mining-and-machine-Bredensteiner",
            "title": {
                "fragments": [],
                "text": "Optimization methods in data mining and machine learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A novel approach using the two class linear programming problem RLP-P on the multiclass data is proposed, which performed superior to a prior linear programming method as well as quadratic programming problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143615900"
                        ],
                        "name": "F. Smith",
                        "slug": "F.-Smith",
                        "structuredName": {
                            "firstName": "Fred",
                            "lastName": "Smith",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 0
                            }
                        ],
                        "text": "Smith (1968) proposed Problem (19.3) with i = 1\u0300 ,but w = 0 may be the unique optimal solution of the Smith formulation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206617276,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "033d548254af32941dd979e69d1b8a2a2932a12e",
            "isKey": true,
            "numCitedBy": 164,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract\u2014A common nonparametric method for designing linear discriminant functions for pattern classification is the iterative, or \"adaptive,\" weight adjustment procedure, which designs the discriminant function to do well on a set of typical patterns. This paper presents a linear programming formulation of discriminant function design which minimizes the same objective function as the \"fixed-increment\" adaptive method. With this formulation, as with the adaptive methods, weights which tend to minimize the number of classification errors are computed for both separable and nonseparable pattern sets, and not just for separable pattern sets as has been the emphasis in previous linear programming formulations."
            },
            "slug": "Pattern-Classifier-Design-by-Linear-Programming-Smith",
            "title": {
                "fragments": [],
                "text": "Pattern Classifier Design by Linear Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A linear programming formulation of discriminant function design which minimizes the same objective function as the \"fixed-increment\" adaptive method is presented."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Computers"
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787993"
                        ],
                        "name": "R. Fourer",
                        "slug": "R.-Fourer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Fourer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fourer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1847322"
                        ],
                        "name": "B. Kernighan",
                        "slug": "B.-Kernighan",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Kernighan",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Kernighan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 4
                            }
                        ],
                        "text": "The AMPL code is available on request from the author athttp://www.math.rpi.edu/ bennek."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 132
                            }
                        ],
                        "text": "Our results are consistant with the statisticallearning theory results that incorporating working data improves generalizationwhen insu cient training information is available."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 59
                            }
                        ],
                        "text": "Using the mathematical programming modeling language AMPL (Fourer et al.,1993), we were able to express the problem in approximately thirty lines of codeplus a data le and solve it using CPLEX.3 If the 2-norm is used for marginmaximization, then the problem becomes a quadratic integer program."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60753226,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "912a2dc05a3e0f5fc778f4d0ed18286d005c1ab1",
            "isKey": true,
            "numCitedBy": 3537,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Practical large-scale mathematical programming involves more than just the application of an algorithm to minimize or maximize an objective function. Before any optimizing routine can be invoked, considerable effort must be expended to formulate the underlying model and to generate the requisite computational data structures. AMPL is a new language designed to make these steps easier and less error-prone. AMPL closely resembles the symbolic algebraic notation that many modelers use to describe mathematical programs, yet it is regular and formal enough to be processed by a computer system; it is particularly notable for the generality of its syntax and for the variety of its indexing operations. We have implemented an efficient translator that takes as input a linear AMPL model and associated data, and produces output suitable for standard linear programming optimizers. Both the language and the translator admit straightforward extensions to more general mathematical programs that incorporate nonlinear expressions or discrete variables."
            },
            "slug": "AMPL:-A-Modeling-Language-for-Mathematical-Fourer-Kernighan",
            "title": {
                "fragments": [],
                "text": "AMPL: A Modeling Language for Mathematical Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An efficient translator is implemented that takes as input a linear AMPL model and associated data, and produces output suitable for standard linear programming optimizers."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145728220"
                        ],
                        "name": "K. Bennett",
                        "slug": "K.-Bennett",
                        "structuredName": {
                            "firstName": "Kristin",
                            "lastName": "Bennett",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Bennett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747026"
                        ],
                        "name": "O. Mangasarian",
                        "slug": "O.-Mangasarian",
                        "structuredName": {
                            "firstName": "Olvi",
                            "lastName": "Mangasarian",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Mangasarian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 41
                            }
                        ],
                        "text": "The Multicategory Discrimination Method (Bennett and Mangasarian, 1993, 1994)constructs a piecewise-linear discriminant for the -class problem using a singlelinear program."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 186
                            }
                        ],
                        "text": "\u2026; ; i6=j (19.9)\n1998/08/25 16:31\n19.4 Multicategory Classi cation 319or equivalentlyAi(wi wj) ( i j)e e; i; j = 1; : : : ; ; i6=j (19.10)The M-RLP method1 proposed and investigated in (Bennett and Mangasarian,1993, 1994) can be used to nd (wi; i); i = 1; : : : ; satisfying the inequalities(19.10)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 141
                            }
                        ],
                        "text": "The LP approach hasbeen to construct directly classi cation functions such that for each point thecorresponding class function is maximized (Bennett and Mangasarian, 1993, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9646239,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "08dd170bedcf0834b9f4b0c4f8743d7052e62665",
            "isKey": true,
            "numCitedBy": 15,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "A parallel algorithm is proposed for a fundamental problem of machine learning, that of multicategory discrimination. The algorithm is based on minimizing an error function associated with a set of highly structured linear inequalities. These inequalities characterize piecewise-linear separation of k sets by the maximum of k affine functions. The error function has a Lipschitz continuous gradient that allows the use of fast serial and parallel unconstrained minimization algorithms. A serial quasi-Newton algorithm is considerably faster than previous linear programming (LP) formulations. A parallel gradient distribution algorithm is used to parallelize the error-minimization problem. Preliminary computational results are given for both a DECstation 5000/125 and a Thinking Machines Corporation CM-5 multiprocessor."
            },
            "slug": "Serial-and-Parallel-Multicategory-Discrimination-Bennett-Mangasarian",
            "title": {
                "fragments": [],
                "text": "Serial and Parallel Multicategory Discrimination"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A serial quasi-Newton algorithm is proposed for a fundamental problem of machine learning, that of multicategory discrimination, based on minimizing an error function associated with a set of highly structured linear inequalities."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Optim."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4562073"
                        ],
                        "name": "C. Watkins",
                        "slug": "C.-Watkins",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Watkins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Watkins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 4
                            }
                        ],
                        "text": "In (Weston and Watkins, 1998) a -class formulation very similar toProblem (19.13) is proposed except the margin maximization term that minimizeskwi wjk2 is omitted.1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "1998/08/25 16:31"
                    },
                    "intents": []
                }
            ],
            "corpusId": 7359186,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "89a37349688b49bbfc9fd643db5a41b9071f9ca2",
            "isKey": true,
            "numCitedBy": 1309,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multi-Class-Support-Vector-Machines-Weston-Watkins",
            "title": {
                "fragments": [],
                "text": "Multi-Class Support Vector Machines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145728220"
                        ],
                        "name": "K. Bennett",
                        "slug": "K.-Bennett",
                        "structuredName": {
                            "firstName": "Kristin",
                            "lastName": "Bennett",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Bennett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2647543"
                        ],
                        "name": "Erin J. Bredensteiner",
                        "slug": "Erin-J.-Bredensteiner",
                        "structuredName": {
                            "firstName": "Erin",
                            "lastName": "Bredensteiner",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Erin J. Bredensteiner"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 91
                            }
                        ],
                        "text": "Smith (1968) proposed Problem (19.3) with i = 1\u0300 ,but w = 0 may be the unique optimal solution of the Smith formulation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 140
                            }
                        ],
                        "text": "Thus alternative SVM-basedalgorithms that consider and optimize all the decisions in the tree simultaneouslyhave been proposed (Blue, 1998; Bennett and Blue, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "While MSM was successfully used in an initial automatedbreast cancer diagnosis system at the University of Wisconsin-Madison (Wolberg\n1998/08/25 16:31"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14273700,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0ebe4b852b57530be9e1502f81f1a441a55d6d54",
            "isKey": true,
            "numCitedBy": 72,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The classification problem of constructing a plane to separate the members of two sets can be formulated as a parametric bilinear program. This approach was originally created to minimize the number of points misclassified. However, a novel interpretation of the algorithm is that the subproblems represent alternative error functions of the misclassified points. Each subproblem identifies a specified number of outliers and minimizes the magnitude of the errors on the remaining points. A tuning set is used to select the best result among the subproblems. A parametric Frank-Wolfe method was used to solve the bilinear subproblems. Computational results on a number of datasets indicate that the results compare very favorably with linear programming and heuristic search approaches. The algorithm can be used as part of a decision tree algorithm to create nonlinear classifiers."
            },
            "slug": "A-Parametric-Optimization-Method-for-Machine-Bennett-Bredensteiner",
            "title": {
                "fragments": [],
                "text": "A Parametric Optimization Method for Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "The classification problem of constructing a plane to separate the members of two sets can be formulated as a parametric bilinear program, where the subproblems represent alternative error functions of the misclassified points."
            },
            "venue": {
                "fragments": [],
                "text": "INFORMS J. Comput."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145728220"
                        ],
                        "name": "K. Bennett",
                        "slug": "K.-Bennett",
                        "structuredName": {
                            "firstName": "Kristin",
                            "lastName": "Bennett",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Bennett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747026"
                        ],
                        "name": "O. Mangasarian",
                        "slug": "O.-Mangasarian",
                        "structuredName": {
                            "firstName": "Olvi",
                            "lastName": "Mangasarian",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Mangasarian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 112
                            }
                        ],
                        "text": "To make MSM more tolerant of noise, Bennett and Mangasarian proposed theRobust Linear Programming method (RLP) (Bennett and Mangasarian, 1992)using the following linear program:minw; ; lXi=1 i isubject to yi(w xi ) + i 1 i 0 i = 1; ::; ` (19.3)Robust LinearProgrammingMethod where i > 0 is the xed\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15917152,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4c5e562437ee94fb6e4d60ec559386dd0a433513",
            "isKey": true,
            "numCitedBy": 796,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "A single linear programming formulation is proposed which generates a plane that of minimizes an average sum of misclassified points belonging to two disjoint points sets in n-dimensional real space. When the convex hulls of the two sets are also disjoint, the plane completely separates the two sets. When the convex hulls intersect, our linear program, unlike all previously proposed linear programs, is guaranteed to generate some error-minimizing plane, without the imposition of extraneous normalization constraints that inevitably fail to handle certain cases. The effectiveness of the proposed linear program has been demonstrated by successfully testing it on a number of databases. In addition, it has been used in conjunction with the multisurface method of piecewise-linear separation to train a feed-forward neural network with a single hidden layer."
            },
            "slug": "Robust-linear-programming-discrimination-of-two-Bennett-Mangasarian",
            "title": {
                "fragments": [],
                "text": "Robust linear programming discrimination of two linearly inseparable sets"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A single linear programming formulation is proposed which generates a plane that of minimizes an average sum of misclassified points belonging to two disjoint points sets in n-dimensional real space, without the imposition of extraneous normalization constraints that inevitably fail to handle certain cases."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2104932993"
                        ],
                        "name": "MinimizationO. L. Mangasarian",
                        "slug": "MinimizationO.-L.-Mangasarian",
                        "structuredName": {
                            "firstName": "MinimizationO.",
                            "lastName": "Mangasarian",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "MinimizationO. L. Mangasarian"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 53
                            }
                        ],
                        "text": "The Multicategory Discrimination Method (Bennett and Mangasarian, 1993, 1994)constructs a piecewise-linear discriminant for the -class problem using a singlelinear program."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 198
                            }
                        ],
                        "text": "\u2026; ; i6=j (19.9)\n1998/08/25 16:31\n19.4 Multicategory Classi cation 319or equivalentlyAi(wi wj) ( i j)e e; i; j = 1; : : : ; ; i6=j (19.10)The M-RLP method1 proposed and investigated in (Bennett and Mangasarian,1993, 1994) can be used to nd (wi; i); i = 1; : : : ; satisfying the inequalities(19.10)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 153
                            }
                        ],
                        "text": "The LP approach hasbeen to construct directly classi cation functions such that for each point thecorresponding class function is maximized (Bennett and Mangasarian, 1993, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17385794,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1980c3b8ffe4e88528b2273529d377d738aa9b37",
            "isKey": true,
            "numCitedBy": 23,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of minimizing the number of misclassiied points by a plane, attempting to separate two point sets with intersecting convex hulls in n-dimensional real space, is formulated as a linear program with equilibrium constraints (LPEC). This general LPEC can be converted to an exact penalty problem with a quadratic objective and linear constraints. A Frank-Wolfe-type algorithm is proposed for the penalty problem that terminates at a stationary point or a global solution. Novel aspects of the approach include: (i) A linear complementarity formulation of the step function that \\counts\" misclassiications, (ii) Exact penalty formulation without boundedness, nondegeneracy or constraint qualiication assumptions, (iii) An exact solution extraction from the sequence of minimizers of the penalty function for a nite value of the penalty parameter for the general LPEC and an explicitly exact solution for the LPEC with uncoupled constraints, and (iv) A parametric quadratic programming formulation of the LPEC associated with the misclassiication minimization problem."
            },
            "slug": "Misclassiication-Minimization-Mangasarian",
            "title": {
                "fragments": [],
                "text": "Misclassiication Minimization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 260
                            }
                        ],
                        "text": "1998/08/25 16:31\n19.2 Two MPM Methods for Classi cation 30919.2 Two MPM Methods for Classi cationMangasarian's Multisurface Method of Pattern Recognition (Mangasiarian, 1965;Mangasarian, 1968) is very similar in derivation to the Generalized Portrait Methodof Vapnik and Chervonenkis (1974)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theory of Pattern Recognition in Russian]. Nauka, Moscow"
            },
            "venue": {
                "fragments": [],
                "text": "Theorie der Zeichenerkennung"
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026Vapnik, Volodya Vovk & Chris Watkins19 Combining Support Vector and Mathematical ProgrammingMethods for Classi cation 307Kristin P. Bennett20 Kernel Principal Component Analysis 327Bernhard Sch olkopf, Alex J. Smola & Klaus-Robert M uller\n1998/08/25 16:31\nviiReferences 353Index 373\n1998/08/25 16:31"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 114
                            }
                        ],
                        "text": "Prior reviews cover how MPM are used for classi cation, clustering, andfunction approximation (Mangasarian, 1997; Bradley et al., 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1200,
                                "start": 1191
                            }
                        ],
                        "text": "1998/08/25 16:31\nAdvances in Kernel MethodsSupport Vector Learning edited byBernhard Sch olkopfChristopher J.C. BurgesAlexander J. Smola\nThe MIT PressCambridge, MassachusettsLondon, England\n1998/08/25 16:31\nContents Preface IX1 Introduction to Support Vector Learning 12 Roadmap 17I Theory 233 Three Remarks on the Support Vector Method of FunctionEstimation 25Vladimir Vapnik4 Generalization Performance of Support Vector Machines andOther Pattern Classi ers 43Peter Bartlett & John Shawe-Taylor5 Bayesian Voting Schemes and Large Margin Classi ers 55Nello Cristianini & John Shawe-Taylor6 Support Vector Machines, Reproducing Kernel Hilbert Spaces,and Randomized GACV 69Grace Wahba7 Geometry and Invariance in Kernel Based Methods 89Christopher J. C. Burges8 On the Annealed VC Entropy for Margin Classi ers:A Statistical Mechanics Study 117Manfred Opper9 Entropy Numbers, Operators and Support Vector Kernels 127Robert C. Williamson, Alex J. Smola & Bernhard Sch olkopf\n1998/08/25 16:31\nvi II Implementations 14510 Solving the Quadratic Programming Problem Arising in SupportVector Classi cation 147Linda Kaufman11 Making Large-Scale Support Vector Machine Learning Practical 169Thorsten Joachims12 Fast Training of Support Vector Machines Using SequentialMinimal Optimization 185John C. PlattIII Applications 20913 Support Vector Machines for Dynamic Reconstruction of aChaotic System 211Davide Mattera & Simon Haykin14 Using Support Vector Machines for Time Series Prediction 243Klaus-Robert M uller, Alex J. Smola, Gunnar R atsch,Bernhard Sch olkopf, Jens Kohlmorgen & Vladimir Vapnik15 Pairwise Classi cation and Support Vector Machines 255Ulrich Kre elIV Extensions of the Algorithm 26916 Reducing the Run-time Complexity in Support Vector Machines 271Edgar E. Osuna & Federico Girosi17 Support Vector Regression with ANOVA Decomposition Kernels 285Mark O. Stitson, Alex Gammerman, Vladimir Vapnik,Volodya Vovk, Chris Watkins & Jason Weston18 Support Vector Density Estimation 293Jason Weston, Alex Gammerman, Mark O. Stitson,Vladimir Vapnik, Volodya Vovk & Chris Watkins19 Combining Support Vector and Mathematical ProgrammingMethods for Classi cation 307Kristin P. Bennett20 Kernel Principal Component Analysis 327Bernhard Sch olkopf, Alex J. Smola & Klaus-Robert M uller\n1998/08/25 16:31\nviiReferences 353Index 373\n1998/08/25 16:31"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 30
                            }
                        ],
                        "text": "The papers(Mangasarian, 1997; Bradley et al., 1998; Bredensteiner, 1997) all contain interest-ing reviews."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 149
                            }
                        ],
                        "text": "\u2026Problem Arising in SupportVector Classi cation 147Linda Kaufman11 Making Large-Scale Support Vector Machine Learning Practical 169Thorsten Joachims12 Fast Training of Support Vector Machines Using SequentialMinimal Optimization 185John C. PlattIII Applications 20913 Support Vector Machines for\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 154
                            }
                        ],
                        "text": "The reviewhere has been solely limited to a few examples from a single family of MPM. Thereare many extensions of these methods such as those covered in (Bradley et al., 1998;Mangasarian, 1997) that can also be potentially combined with SVM."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Data mining: Overview and optimization opportunities"
            },
            "venue": {
                "fragments": [],
                "text": "Data mining: Overview and optimization opportunities"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 117
                            }
                        ],
                        "text": "The quadratic programming problems for M-SVMand -SVM were solved using the nonlinear solver implemented in MINOS 5.4(Murtagh and Saunders, 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "MINOS 5.4 user's guide"
            },
            "venue": {
                "fragments": [],
                "text": "MINOS 5.4 user's guide"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 228778895,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4c2d9e61b7f406f2ca35e63cbbb23e7cf7a85c2a",
            "isKey": false,
            "numCitedBy": 179,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-note-one-class-of-perceptrons-Vapnik",
            "title": {
                "fragments": [],
                "text": "A note one class of perceptrons"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35094809"
                        ],
                        "name": "L. Bernhardt",
                        "slug": "L.-Bernhardt",
                        "structuredName": {
                            "firstName": "Lutz",
                            "lastName": "Bernhardt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bernhardt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 169819736,
            "fieldsOfStudy": [],
            "id": "e25656f366a70cdd467141d20b5511289a02faba",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Zur Losung von Klassifizierungsaufgaben hat sich in den letzten Jahren mehr und mehr das Konzept der Bayes- oder Polynomklassifikatoren durchgesetzt (/1/, /2/)."
            },
            "slug": "Zur-Klassifizierung-Vieler-Musterklassen-mit-Bernhardt",
            "title": {
                "fragments": [],
                "text": "Zur Klassifizierung Vieler Musterklassen mit Wenigen Merkmalen"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47652868"
                        ],
                        "name": "H. Hotelling",
                        "slug": "H.-Hotelling",
                        "structuredName": {
                            "firstName": "Harold",
                            "lastName": "Hotelling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hotelling"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 144828484,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9ebb5c0d6d54707a4d6181a693b6f755ec8a45a9",
            "isKey": false,
            "numCitedBy": 8491,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Analysis-of-a-complex-of-statistical-variables-into-Hotelling",
            "title": {
                "fragments": [],
                "text": "Analysis of a complex of statistical variables into principal components."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1933
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152633671"
                        ],
                        "name": "G. McLachlan",
                        "slug": "G.-McLachlan",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "McLachlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. McLachlan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72847318"
                        ],
                        "name": "J. Schurmann",
                        "slug": "J.-Schurmann",
                        "structuredName": {
                            "firstName": "Jurgen",
                            "lastName": "Schurmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schurmann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 126019720,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d21120ddfdc9e6b52f6207f8a177052465d22de1",
            "isKey": false,
            "numCitedBy": 210,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Pattern-Classification:-A-Unified-View-of-and-McLachlan-Schurmann",
            "title": {
                "fragments": [],
                "text": "Pattern Classification: A Unified View of Statistical and Neural Approaches."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69052801"
                        ],
                        "name": "T. Watkin",
                        "slug": "T.-Watkin",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Watkin",
                            "middleNames": [
                                "L.",
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Watkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31873701"
                        ],
                        "name": "A. Rau",
                        "slug": "A.-Rau",
                        "structuredName": {
                            "firstName": "Albrecht",
                            "lastName": "Rau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Rau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144951109"
                        ],
                        "name": "Michael Biehl",
                        "slug": "Michael-Biehl",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Biehl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Biehl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 121996967,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "438bf2c47c4544064684931259305714207528d9",
            "isKey": false,
            "numCitedBy": 386,
            "numCiting": 108,
            "paperAbstract": {
                "fragments": [],
                "text": "A summary is presented of the statistical mechanical theory of learning a rule with a neural network, a rapidly advancing area which is closely related to other inverse problems frequently encountered by physicists. By emphasizing the relationship between neural networks and strongly interacting physical systems, such as spin glasses, the authors show how learning theory has provided a workshop in which to develop new, exact analytical techniques."
            },
            "slug": "THE-STATISTICAL-MECHANICS-OF-LEARNING-A-RULE-Watkin-Rau",
            "title": {
                "fragments": [],
                "text": "THE STATISTICAL-MECHANICS OF LEARNING A RULE"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "100862247"
                        ],
                        "name": "M. A. Villalobos",
                        "slug": "M.-A.-Villalobos",
                        "structuredName": {
                            "firstName": "Miguel",
                            "lastName": "Villalobos",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. A. Villalobos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122697049,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "87a5d2624d67a26b27fe21c79a4c3dae6ab1c741",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract We consider the problem of estimating a smooth function of several variables given discrete, scattered, noisy observations of its values, and given that it satisfies a family of linear inequality constraints. Constraints such as positivity over some region are included. The model is z i = f(y 1(i), \u2026, yd + e (i = 1, \u2026, n), where the e i 's are independent zero mean random variables, f is assumed to be smooth, with smoothness defined in terms of square integrability of certain derivatives, and f is known to satisfy a given set of linear inequality constraints. It is proposed that f be estimated as the minimizer, in an appropriate function space, of subject to f satisfying the constraints if they are a finite family, or satisfying a finite approximating set of constraints if they are not a finite family, where J m(f) is the thin-plate penalty functional defined by J m(f) = \u03a3\u03b11 + \u2026 + \u03b1d = m (m!/\u03b11! \u2026 \u03b1d!) \u222b \u2026 \u222b [\u03b4 m f/\u03b4y\u03b1 11 \u2026 \u03b4 y\u03b1 d d]2 dy 1 \u2026 dy d. More generally, the results apply to the model wh..."
            },
            "slug": "Inequality-Constrained-Multivariate-Smoothing-with-Villalobos-Wahba",
            "title": {
                "fragments": [],
                "text": "Inequality-Constrained Multivariate Smoothing Splines with Application to the Estimation of Posterior Probabilities"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2459584"
                        ],
                        "name": "W. Wolberg",
                        "slug": "W.-Wolberg",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Wolberg",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Wolberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747026"
                        ],
                        "name": "O. Mangasarian",
                        "slug": "O.-Mangasarian",
                        "structuredName": {
                            "firstName": "Olvi",
                            "lastName": "Mangasarian",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Mangasarian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697083"
                        ],
                        "name": "R. Setiono",
                        "slug": "R.-Setiono",
                        "structuredName": {
                            "firstName": "Rudy",
                            "lastName": "Setiono",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Setiono"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122825273,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f3151530e76cfc4b4322ef12a43c0a4329346026",
            "isKey": false,
            "numCitedBy": 312,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Pattern-Recognition-Via-Linear-Programming:-Theory-Wolberg-Mangasarian",
            "title": {
                "fragments": [],
                "text": "Pattern Recognition Via Linear Programming: Theory and Application to Medical Diagnosis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2856152"
                        ],
                        "name": "E. Parzen",
                        "slug": "E.-Parzen",
                        "structuredName": {
                            "firstName": "Emanuel",
                            "lastName": "Parzen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Parzen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123035660,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8671e29eb2e443b33b7bdbea0988a9639fc98ae2",
            "isKey": false,
            "numCitedBy": 124,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : The theory of time series is studied by probabilists (under such names as Gaussian processes and generalized processes), by statisticians (who are mainly concerned with modelling discrete parameter time series by finite parameter schemes), and by communication and control engineers (who are mainly concerned with the extraction and detection of signals in noise). The aim of this review is to outline the unifying role of reproducing kernel Hilbert spaces (RKHS) in the theory of time series. There are 13 sections (which are divided into an introduction and 4 chapters). The chapter headings are the following: Time series and RKHS; Parameter estimation and optimization; Examples of RKHS; and Probability density functionals of normal processes. (Author)"
            },
            "slug": "STATISTICAL-INFERENCE-ON-TIME-SERIES-BY-RKHS-Parzen",
            "title": {
                "fragments": [],
                "text": "STATISTICAL INFERENCE ON TIME SERIES BY RKHS METHODS."
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The aim of this review is to outline the unifying role of reproducing kernel Hilbert spaces (RKHS) in the theory of time series."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1451694966"
                        ],
                        "name": "Van Paul Yee",
                        "slug": "Van-Paul-Yee",
                        "structuredName": {
                            "firstName": "Van",
                            "lastName": "Yee",
                            "middleNames": [
                                "Paul"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Van Paul Yee"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 116888954,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e237703cdb33f6a1f1fad97ae664b37f7ac2a776",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Regularized-Radial-Basis-Function-Networks:-Theory-Yee",
            "title": {
                "fragments": [],
                "text": "Regularized Radial Basis Function Networks: Theory and Applications to Probability Estimation, Classification, and Time Series Prediction"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144631820"
                        ],
                        "name": "Y. Gordon",
                        "slug": "Y.-Gordon",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Gordon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Gordon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39282374"
                        ],
                        "name": "H. K\u00f6nig",
                        "slug": "H.-K\u00f6nig",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "K\u00f6nig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. K\u00f6nig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2134728"
                        ],
                        "name": "C. Sch\u00fctt",
                        "slug": "C.-Sch\u00fctt",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Sch\u00fctt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Sch\u00fctt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 122493370,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2b8b5fec17cded75f013634629615d0cfa32df68",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Geometric-and-probabilistic-estimates-for-entropy-Gordon-K\u00f6nig",
            "title": {
                "fragments": [],
                "text": "Geometric and probabilistic estimates for entropy and approximation numbers of operators"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691022"
                        ],
                        "name": "M. Opper",
                        "slug": "M.-Opper",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Opper",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Opper"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121715361,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c1df9fd1fafcc78474c8798ded1a5de837d51e23",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The performance of an iterative learning rule for neural networks of spin-glass-type is studied. The algorithm minimizes a cost function quadratic in the synaptic couplings. An exact expression for the time development of the cost function is derived for the case of extensively many random patterns in large networks. A learning time as a function of the storage ratio ? (number of patterns/number of spins) is calculated. It diverges as (1????)?2 as the storage ratio approaches 1."
            },
            "slug": "Learning-in-Neural-Networks:-Solvable-Dynamics-Opper",
            "title": {
                "fragments": [],
                "text": "Learning in Neural Networks: Solvable Dynamics"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "The performance of an iterative learning rule for neural networks of spin-glass-type is studied and the algorithm minimizes a cost function quadratic in the synaptic couplings."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 121176122,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "88ce531f22108f687cbb576bcb0cd660b2a694bc",
            "isKey": false,
            "numCitedBy": 539,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "On considere des procedures de lissage spline etudiees en 1978 et 1983 et leur extension a la resolution d'equations d'operateurs lineaires avec donnees bruitees"
            },
            "slug": "A-Comparison-of-GCV-and-GML-for-Choosing-the-in-the-Wahba",
            "title": {
                "fragments": [],
                "text": "A Comparison of GCV and GML for Choosing the Smoothing Parameter in the Generalized Spline Smoothing Problem"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2856152"
                        ],
                        "name": "E. Parzen",
                        "slug": "E.-Parzen",
                        "structuredName": {
                            "firstName": "Emanuel",
                            "lastName": "Parzen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Parzen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 121574866,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "728f6cd3fef3c46bd62a16f5b322ae8c8ce28259",
            "isKey": false,
            "numCitedBy": 353,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-Approach-to-Time-Series-Analysis-Parzen",
            "title": {
                "fragments": [],
                "text": "An Approach to Time Series Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1961
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11155248"
                        ],
                        "name": "J. Nash",
                        "slug": "J.-Nash",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Nash",
                            "middleNames": [
                                "F."
                            ],
                            "suffix": "Jr."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nash"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121484824,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f3604a4ec0c1b676d2d0833481d4af80fe13fc6e",
            "isKey": false,
            "numCitedBy": 1153,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-imbedding-problem-for-Riemannian-manifolds-Nash",
            "title": {
                "fragments": [],
                "text": "The imbedding problem for Riemannian manifolds"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1956
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32604641"
                        ],
                        "name": "R. Greene",
                        "slug": "R.-Greene",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Greene",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Greene"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121659272,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b840fae1c75ec96b4868d8bb4f51e94641b798cf",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Isometric-embeddings-of-Riemannian-and-manifolds-Greene",
            "title": {
                "fragments": [],
                "text": "Isometric embeddings of Riemannian and pseudo-Riemannian manifolds"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34548385"
                        ],
                        "name": "R. Prosser",
                        "slug": "R.-Prosser",
                        "structuredName": {
                            "firstName": "Reese",
                            "lastName": "Prosser",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Prosser"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 121342831,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c09952bb4273ae37c4bb673c8ffc1af25f6aee27",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-\u03f5-entropy-and-\u03f5-capacity-of-certain-channels-Prosser",
            "title": {
                "fragments": [],
                "text": "The \u03f5-entropy and \u03f5-capacity of certain time-varying channels"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1966
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49243178"
                        ],
                        "name": "Ker-Chau Li",
                        "slug": "Ker-Chau-Li",
                        "structuredName": {
                            "firstName": "Ker-Chau",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ker-Chau Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 121207622,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1a5beed74845176093ea9cd0131da73818967a02",
            "isKey": false,
            "numCitedBy": 236,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "On demontre l'optimalite asymptotique du C L de Mallow et de la validation croisee generalisee, dans le contexte de la regression ridge. On donne une application au lissage spline en regression non parametrique"
            },
            "slug": "Asymptotic-optimality-of-CL-and-generalized-in-with-Li",
            "title": {
                "fragments": [],
                "text": "Asymptotic optimality of CL and generalized cross-validation in ridge regression with application to spline smoothing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35180576"
                        ],
                        "name": "L. Bregman",
                        "slug": "L.-Bregman",
                        "structuredName": {
                            "firstName": "Lev",
                            "lastName": "Bregman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bregman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121309410,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "44a6b76e5cbc61330663d0a9f393caf91a3a1be8",
            "isKey": false,
            "numCitedBy": 2440,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-relaxation-method-of-finding-the-common-point-Bregman",
            "title": {
                "fragments": [],
                "text": "The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120958238,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c2af625662f7f0761bcc631389c281288ee275b9",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Erratum:-Spline-Interpolation-and-Smoothing-on-the-Wahba",
            "title": {
                "fragments": [],
                "text": "Erratum: Spline Interpolation and Smoothing on the Sphere"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69577892"
                        ],
                        "name": "C. Hildreth",
                        "slug": "C.-Hildreth",
                        "structuredName": {
                            "firstName": "Clifford",
                            "lastName": "Hildreth",
                            "middleNames": [
                                "George"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Hildreth"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121782835,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7c309960815ac6f73e8f438206e271159eb512a",
            "isKey": false,
            "numCitedBy": 320,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-quadratic-programming-procedure-Hildreth",
            "title": {
                "fragments": [],
                "text": "A quadratic programming procedure"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1957
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "93949743"
                        ],
                        "name": "W. Liebert",
                        "slug": "W.-Liebert",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Liebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Liebert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144902080"
                        ],
                        "name": "K. Pawelzik",
                        "slug": "K.-Pawelzik",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Pawelzik",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Pawelzik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39198624"
                        ],
                        "name": "H. Schuster",
                        "slug": "H.-Schuster",
                        "structuredName": {
                            "firstName": "Heinz",
                            "lastName": "Schuster",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Schuster"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120437862,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5182b7f577bd21a0226fd7af94bc8fc926fbd362",
            "isKey": false,
            "numCitedBy": 185,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Guided by topological considerations, a new method is introduced to obtain optimal delay coordinates for data from chaotic dynamic systems. By determining simultaneously the minimal necessary embedding dimension as well as the proper delay time we achieve optimal reconstructions of attractors. This can be demonstrated, e.g., by reliable dimension estimations from limited data series."
            },
            "slug": "Optimal-Embeddings-of-Chaotic-Attractors-from-Liebert-Pawelzik",
            "title": {
                "fragments": [],
                "text": "Optimal Embeddings of Chaotic Attractors from Topological Considerations"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A new method is introduced to obtain optimal delay coordinates for data from chaotic dynamic systems by determining simultaneously the minimal necessary embedding dimension as well as the proper delay time to achieve optimal reconstructions of attractors."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47312726"
                        ],
                        "name": "J. Mercer",
                        "slug": "J.-Mercer",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Mercer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Mercer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 121070291,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b48694cb275eba60b48026f3159373c92c1b286c",
            "isKey": false,
            "numCitedBy": 1590,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The present memoir is the outcome of an attempt to obtain the conditions under which a given symmetric and continuous function k ( s, t ) is definite, in the sense of Hilbert. At an early stage, however, it was found that the class of definite functions was too restricted to allow the determination of necessary and sufficient conditions in terms of the determinants of \u00a7 10. The discovery that this could be done for functions of positive or negative type, and the fact that almost all the theorems which are true of definite functions are, with slight modification, true of these, led finally to the abandonment of the original plan in favour of a discussion of the properties of functions belonging to the wider classes. The first part of the memoir is devoted to the definition of various terms employed, and to the re-statement of the consequences which follow from Hilbert\u2019s theorem."
            },
            "slug": "Functions-of-Positive-and-Negative-Type,-and-their-Mercer",
            "title": {
                "fragments": [],
                "text": "Functions of Positive and Negative Type, and their Connection with the Theory of Integral Equations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1909
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101397187"
                        ],
                        "name": "A. Skorokhod",
                        "slug": "A.-Skorokhod",
                        "structuredName": {
                            "firstName": "Anatoli",
                            "lastName": "Skorokhod",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Skorokhod"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103222132"
                        ],
                        "name": "M. Yadrenko",
                        "slug": "M.-Yadrenko",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Yadrenko",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Yadrenko"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 120045569,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "16b35f3d2f7ed7e5fef89704d4f7086edbc532fc",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "General questions of absolute continuity and singularity of Gaussian measures have been considered in works of Ya. Gaek [1], J. Feldman [2] and Yu. A. Rozanov 3]. However, in considering concrete Gaussian measures it is desirable to be able to answer these questions using only the defining characteristics of the corresponding processes. As is known, to solve the problem of absolute continuity and to find the density it is necessary to solve a certain operator equation, which for ordinary processes leads to a Fredholm integral equation of the first type. The existence of a solution of this equation ensures absolute continuity. But the question of the existence of solutions of such equations is very complex. Hence there arises the problem: to find conditions of absolute continuity of measures which do not involve the existence of a solution of the corresponding equations. For stationary processes, several conditions expressed in terms of correlation functions or spectral densities have been given by Rozanov [4], [3]. Other general conditions appear in the summary report of I. I. Gikhman and A. V. Skorokhod [6], as well as in the book by the same authors [7] (Chapter 7, 5). In the present paper, analogous conditions using only spectral functions and densities are found for homogeneous Gaussian fields. The authors have restricted themselves only to the case when the means of the Gaussian fields are equal to zero and the correlation functions differ. The case of identical correlation functions and distinct means is studied by M. I. Yadrenko in [8]. Combining the results of [8] with those of.this paper one can obtain conditions of absolute continuity of homogeneous fields for distinct means and correlation functions. To be especially noted is the case of isotropic Gaussian fields which are considered separately. Conditions of absolute continuity and singularity of measures corresponding to Gaussian random fields have not yet been studied sufficiently. In this connection note the works of Z. S. Zerakidze [9, [10], G. M. Molchan and Yu. I. Golosov [11]."
            },
            "slug": "On-Absolute-Continuity-of-Measures-Corresponding-to-Skorokhod-Yadrenko",
            "title": {
                "fragments": [],
                "text": "On Absolute Continuity of Measures Corresponding to Homogeneous Gaussian Fields"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49883756"
                        ],
                        "name": "D. Girard",
                        "slug": "D.-Girard",
                        "structuredName": {
                            "firstName": "Didier",
                            "lastName": "Girard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Girard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 120561555,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "eae878e367c97541c8f464f7de4ea2e0076954ff",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Asymptotic-Optimality-of-the-Fast-Randomized-of-GCV-Girard",
            "title": {
                "fragments": [],
                "text": "Asymptotic Optimality of the Fast Randomized Versions of GCV and $C_L$ in Ridge Regression and Regularization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691022"
                        ],
                        "name": "M. Opper",
                        "slug": "M.-Opper",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Opper",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Opper"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066051497"
                        ],
                        "name": "P. Kuhlmann",
                        "slug": "P.-Kuhlmann",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Kuhlmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Kuhlmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8689980"
                        ],
                        "name": "A. Mietzner",
                        "slug": "A.-Mietzner",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Mietzner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mietzner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 120876751,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2263d8060e7cc01b46b7b80454d93e0cb6a7c49b",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an approach to the statistical mechanics of feedforward neural networks which is based on counting realizable internal representations by utilizing convexity properties of the weight space. For a toy model, our method yields storage capacities based on an annealed approximation, which are in close agreement with one-step replica symmetry-breaking results obtained from a standard approach. For a single-layer perceptron, a combinatorial result for the number of realizable output combinations is recovered and generalized to fixed stabilities."
            },
            "slug": "Convexity,-internal-representations-and-the-of-Opper-Kuhlmann",
            "title": {
                "fragments": [],
                "text": "Convexity, internal representations and the statistical mechanics of neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "This work presents an approach to the statistical mechanics of feedforward neural networks which is based on counting realizable internal representations by utilizing convexity properties of the weight space, and yields storage capacities based on an annealed approximation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101750654"
                        ],
                        "name": "G. Kimeldorf",
                        "slug": "G.-Kimeldorf",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Kimeldorf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kimeldorf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 120654716,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5f75d859e750961d1d094f166fc3b564d9cfe99b",
            "isKey": false,
            "numCitedBy": 975,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : The report presents classes of prior distributions for which the Bayes' estimate of an unknown function given certain observations is a spline function. (Author)"
            },
            "slug": "A-Correspondence-Between-Bayesian-Estimation-on-and-Kimeldorf-Wahba",
            "title": {
                "fragments": [],
                "text": "A Correspondence Between Bayesian Estimation on Stochastic Processes and Smoothing by Splines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767325"
                        ],
                        "name": "Y. Censor",
                        "slug": "Y.-Censor",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Censor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Censor"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 119893124,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "18de1c759934b944e3ff63d6fb786ad7baec6441",
            "isKey": false,
            "numCitedBy": 439,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper brings together and discusses theory and applications of methods, identified and labelled as row-action methods, for linear feasibility problems (find $x \\in {\\bf R}^n $, such that $Ax \\leqq b$), linearly constrained optimization problems (minimize $f(x)$, subject to $Ax \\leqq b$) and some interval convex programming problems (minimize $f(x)$, subject to $c \\leqq Ax \\leqq b$).The main feature of row-action methods is that they are iterative procedures which, without making any changes to the original matrix A, use the rows of A, one row at a time. Such methods are important and have demonstrated effectiveness for problems with large or huge matrices which do not enjoy any detectable or usable structural pattern, apart from a high degree of sparaseness.Fields of application where row-action methods are used in various ways include image reconstruction from projection, operations research and game theory, learning theory, pattern recognition and transportation theory. A row-action method for the ..."
            },
            "slug": "Row-Action-Methods-for-Huge-and-Sparse-Systems-and-Censor",
            "title": {
                "fragments": [],
                "text": "Row-Action Methods for Huge and Sparse Systems and Their Applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49883756"
                        ],
                        "name": "D. Girard",
                        "slug": "D.-Girard",
                        "structuredName": {
                            "firstName": "Didier",
                            "lastName": "Girard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Girard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 120292248,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ef7c4fb2352d1ca06e5bbfa15210b199b6562993",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "When using nonparametric estimates of the mean curve, surface or image underlying noisy observations, the selection of smoothing parameters is generally crucial. This paper gives a theoretical comparison of the performances of generalized cross-validation (GCV) and of its fast randomized version (RGCV), as selection criteria. This is mainly done by studying the asymptotic distribution of the excess error for each selector, that is, the difference between the (data-driven) resulting average squared error (ASE) and the best possible ASE. We show here that, by using randomization, this distribution is dilated, as compared to that for CV or GCV, only by a factor always lower than 1 + 1/n R , where n R is the number of primary randomized trace estimates one uses in RGCV. We include in the compared selectors, the partial cross-validation (PCV) approach where only a fraction of all the possible leave-one-out validation tests are evaluated; so that PCV is a common practice to reduce the computational cost in many contexts. In this paper, PCV will in fact appear as quite inefficient as compared to RGCV from this computational point of view. Moreover, we show that a precise comparison (and interpretation of the gain of using n R \u2265 2) is possible in terms of equivalent (in distribution) excess errors, if PCV uses a certain percentage of the test points greater than 50%. The obtained comparisons will be seen as quite reassuring on what is sacrificed in using randomized selectors. We give rigorous results mainly for the kernel regression setting as in the previous detailed study by Hardle Hall and Marron of standard selectors, except that we do not restrict this one to an equidistant design."
            },
            "slug": "Asymptotic-comparison-of-(partial)-GCV-and-GCV-in-Girard",
            "title": {
                "fragments": [],
                "text": "Asymptotic comparison of (partial) cross-validation, GCV and randomized GCV in nonparametric regression"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2928241"
                        ],
                        "name": "M. Hutchinson",
                        "slug": "M.-Hutchinson",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Hutchinson",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hutchinson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120969358,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5537987153925c5968038dc3ed8e195a72c99d5f",
            "isKey": false,
            "numCitedBy": 599,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "An unbiased stochastic estimator of tr(I-A), where A is the influence matrix associated with the calculation of Laplacian smoothing splines, is described. The estimator is similar to one recently developed by Girard but satisfies a minimum variance criterion and does not require the simulation of a standard normal variable. It uses instead simulations of the discrete random variable which takes the values 1, -1 each with probability 1/2. Bounds on the variance of the estimator, similar to those established by Girard, are obtained using elementary methods. The estimator can be used to approximately minimize generalised cross validation (GCV) when using discretized iterative methods for fitting Laplacian smoothing splines to very large data sets. Simulated examples show that the estimated trace values, using either the estimator presented here or the estimator of Girard, perform almost as well as the exact values when applied to the minimization of GCV for n as small as a few hundred, where n is the number ..."
            },
            "slug": "A-stochastic-estimator-of-the-trace-of-the-matrix-Hutchinson",
            "title": {
                "fragments": [],
                "text": "A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2839642"
                        ],
                        "name": "B. Carl",
                        "slug": "B.-Carl",
                        "structuredName": {
                            "firstName": "Bernd",
                            "lastName": "Carl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Carl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101264613"
                        ],
                        "name": "Irmtraud Stephani",
                        "slug": "Irmtraud-Stephani",
                        "structuredName": {
                            "firstName": "Irmtraud",
                            "lastName": "Stephani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Irmtraud Stephani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 119490731,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "53bab7b5073443420d39d6622016a403d3223b40",
            "isKey": false,
            "numCitedBy": 338,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Entropy quantities 2. Approximation quantities 3. Inequalities of Bernstein-Jackson type 3. Inequalities of Berstein-Jackson type 4. A refined Riesz theory 5. Operators with values in C(X) 6. Operator theoretical methods in the local theory of Banach spaces."
            },
            "slug": "Entropy,-Compactness-and-the-Approximation-of-Carl-Stephani",
            "title": {
                "fragments": [],
                "text": "Entropy, Compactness and the Approximation of Operators"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145627013"
                        ],
                        "name": "J. Stewart",
                        "slug": "J.-Stewart",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Stewart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Stewart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 120093770,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "400ca5d045588b887ec8949662dff5436bbc2461",
            "isKey": false,
            "numCitedBy": 189,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "2 fi COS Xi + i X 6 S i n i = \u00b0i = l ' ' i = l ' Likewise it is easily verified directly that e is p.d. for real \\ , but it is not so straightforward to see that such functions as e~H e~*, and (1 4x)\" ? e p.d. These and other examples are discussed in \u00a7 3. Positive definite functions and their various analogues and generalizations have arisen in diverse parts of mathematics since the beginning of this century. They occur naturally in Fourier analysis, probability theory, operator theory, complex function-theory, moment problems, integral equations, boundary-value problems for partial differential equations, embedding problems, information theory, and other areas. Their history constitutes a good illustration of the words of Hobson [51, p. 290] : \"Not only are special results, obtained independently of one another, frequently seen to be really included in"
            },
            "slug": "Positive-definite-functions-and-generalizations,-an-Stewart",
            "title": {
                "fragments": [],
                "text": "Positive definite functions and generalizations, an historical survey"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767325"
                        ],
                        "name": "Y. Censor",
                        "slug": "Y.-Censor",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Censor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Censor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144855064"
                        ],
                        "name": "A. Lent",
                        "slug": "A.-Lent",
                        "structuredName": {
                            "firstName": "Arnold",
                            "lastName": "Lent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lent"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 119559562,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "f2f68eb6b1389d019bf05df8d7cb3e48c7896974",
            "isKey": false,
            "numCitedBy": 443,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The iterative primal-dual method of Bregman for solving linearly constrained convex programming problems, which utilizes nonorthogonal projections onto hyperplanes, is represented in a compact form, and a complete proof of convergence is given for an almost cyclic control of the method. Based on this, a new algorithm for solving interval convex programming problems, i.e., problems of the form minf(x), subject to \u03b3\u2264Ax\u2264\u03b4, is proposed. For a certain family of functionsf(x), which includes the norm \u2225x\u2225 and thex logx entropy function, convergence is proved. The present row-action method is particularly suitable for handling problems in which the matrixA is large (or huge) and sparse."
            },
            "slug": "An-iterative-row-action-method-for-interval-convex-Censor-Lent",
            "title": {
                "fragments": [],
                "text": "An iterative row-action method for interval convex programming"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The iterative primal-dual method of Bregman for solving linearly constrained convex programming problems, which utilizes nonorthogonal projections onto hyperplanes, is represented in a compact form, and a complete proof of convergence is given for an almost cyclic control of the method."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2886610"
                        ],
                        "name": "W. Gautschi",
                        "slug": "W.-Gautschi",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Gautschi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gautschi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118626484,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9dbb4bfb9f54b8641b66d454235c8eb4b68b3417",
            "isKey": false,
            "numCitedBy": 397,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This is a text in numerical analysis which is taken to mean the branch of mathematics that develops and analyzes computational methods dealing with problems arising in classical analysis, approximations theory, the theory of equations, and ordinary differential equations. The topics in this book are presented with a view towards stressing basic principles and maintaining simplicity and teachability as far as possible. Topics that require a level of technicality that goes beyond the standard of simplicity imposed are referenced in bibliographic notes at the end of each chapter. This book does not cover numerical linear algebra, nor the numerical solution of partial differential equations, as the author takes the view that these are now separate disciplines. It is intended that the student has a good background in calculus and advanced calculus and some knowledge of linear algebra, complex analysis, and differential equations."
            },
            "slug": "Numerical-analysis:-an-introduction-Gautschi",
            "title": {
                "fragments": [],
                "text": "Numerical analysis: an introduction"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20038481"
                        ],
                        "name": "B. M\u00fcller",
                        "slug": "B.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Berndt",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. M\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47160050"
                        ],
                        "name": "J. Reinhardt",
                        "slug": "J.-Reinhardt",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Reinhardt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Reinhardt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144434747"
                        ],
                        "name": "M. Strickland",
                        "slug": "M.-Strickland",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Strickland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Strickland"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118579573,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f906c7b59471ac0f9c8d761a7b57db186943d74e",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In the previous chapters we have applied the tools of statistical mechanics to determine the phase structure and equilibrium properties of the Hopfield network. The theory was based on an energy function which contained the \u201cneuron spins\u201d s i as dynamical variables with interactions based on Hebb\u2019s rule or its nonlinear modification. While this rule has an appealing simplicity it generates synaptic efficacies w ij which are far from optimal. One would like to study the storage properties of the best possible network for a given task. The question thus is: Given a set of p patterns \u03c3 i \u03bc , i = 1, ..., N; \u03bc = 1, ..., p, is there a network which has these patterns as fixed points? This is satisfied if the local field h i = \u03a3 i w ij s j points in the same direction (i.e. has the same sign) as the spin at site i so that the network is in a stationary state under the deterministic updating rule (3.5). We postulate the imbedding condition \n \n$$ \\gamma _i^\\mu \\left[ w \\right]\\,\\, \\equiv \\,\\,\\sigma _i^\\mu \\,\\sum\\limits_{j\\, = \\,1}^N {{w_{ij}}\\sigma _i^\\mu /||{w_i}||\\,\\, > \\,\\,\\kappa ,} $$ \n \n(18.1) \n \nwhere \\( ||{w_i}||\\,\\, = \\,\\,\\sqrt {{\\Sigma _j}w_{ij}^2} \\) is the Euclidian norm of the interactions coupling to neuron i. It is clear that for a growing number of patterns p it will be increasingly difficult to satisfy the set of Np equations (18.1) with a constraint \u03ba \u2265 0 needed for stability. A quantitative attack on this problem again can make use of the techniques of statistical mechanics [Ga88a, Ga88b]. Now, however, the dynamical variables of the theory are the synaptic couplings w ij . The neuron spins s i , on the other hand, are fixed to a selection of \u201cquenched\u201d patterns \u03c3 i \u03bc , which we will to assume to be random and uncorrelated.1"
            },
            "slug": "The-Space-of-Interactions-in-Neural-Networks-M\u00fcller-Reinhardt",
            "title": {
                "fragments": [],
                "text": "The Space of Interactions in Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The dynamical variables of the theory are the synaptic couplings w ij and the neuron spins s i are fixed to a selection of \u201cquenched\u201d patterns \u03c3 i \u03bc , which are to assume to be random and uncorrelated."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145698858"
                        ],
                        "name": "T. Downs",
                        "slug": "T.-Downs",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Downs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Downs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40073871"
                        ],
                        "name": "Marcus Frean",
                        "slug": "Marcus-Frean",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Frean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcus Frean"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36589297"
                        ],
                        "name": "M. Gallagher",
                        "slug": "M.-Gallagher",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Gallagher",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gallagher"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117940753,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5a4569e2a120ef410e9ca7abbb25066c05453678",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "General-cost-functions-for-support-vector-Smola-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "General cost functions for support vector regression."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118571164,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "09e6ca99220c47e5622d4272083cf0e3c4017eac",
            "isKey": false,
            "numCitedBy": 93,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Constrained-Regularization-for-Ill-Posed-Linear-in-Wahba",
            "title": {
                "fragments": [],
                "text": "Constrained Regularization for Ill Posed Linear Operator Equations, with Applications in Meteorology and Medicine."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101655384"
                        ],
                        "name": "K. Karhunen",
                        "slug": "K.-Karhunen",
                        "structuredName": {
                            "firstName": "Kari",
                            "lastName": "Karhunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Karhunen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118738283,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8ab83ad89b2dedd642246d68ded065fec63ffa79",
            "isKey": false,
            "numCitedBy": 469,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Zur-Spektraltheorie-stochastischer-prozesse-Karhunen",
            "title": {
                "fragments": [],
                "text": "Zur Spektraltheorie stochastischer prozesse"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1946
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16683211"
                        ],
                        "name": "G. Zoutendijk",
                        "slug": "G.-Zoutendijk",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Zoutendijk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Zoutendijk"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118029604,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "337358f37ea6cb781e4fda73eb7940abe6edac9c",
            "isKey": false,
            "numCitedBy": 116,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Methods-of-feasible-directions-:-a-study-in-linear-Zoutendijk",
            "title": {
                "fragments": [],
                "text": "Methods of feasible directions : a study in linear and non-linear programming"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1960
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2294095"
                        ],
                        "name": "M. Powell",
                        "slug": "M.-Powell",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Powell",
                            "middleNames": [
                                "J.",
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Powell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118224933,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c71ca26b183025b9f39f940f5e730f2c9a64e414",
            "isKey": false,
            "numCitedBy": 1426,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Radial-basis-functions-for-multivariable-a-review-Powell",
            "title": {
                "fragments": [],
                "text": "Radial basis functions for multivariable interpolation: a review"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2262508"
                        ],
                        "name": "J. Simonoff",
                        "slug": "J.-Simonoff",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Simonoff",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Simonoff"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118542831,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b61e3381c76dbf0ab5c9e864451dab5b2a7b235f",
            "isKey": false,
            "numCitedBy": 1100,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Exploring and identifying structure is even more important for multivariate data than univariate data, given the difficulties in graphically presenting multivariate data and the comparative lack of parametric models to represent it. Unfortunately, such exploration is also inherently more difficult."
            },
            "slug": "Multivariate-Density-Estimation-Simonoff",
            "title": {
                "fragments": [],
                "text": "Multivariate Density Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Exploring and identifying structure is even more important for multivariate data than univariate data, given the difficulties in graphically presenting multivariateData and the comparative lack of parametric models to represent it."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101173505"
                        ],
                        "name": "A. Pietsch",
                        "slug": "A.-Pietsch",
                        "structuredName": {
                            "firstName": "Albrecht",
                            "lastName": "Pietsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pietsch"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 115636909,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "03044bc26fddb7cfe46a28d22927a57b4e5308b6",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Eigenvalue-distribution-of-compact-operators-Pietsch",
            "title": {
                "fragments": [],
                "text": "Eigenvalue distribution of compact operators"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2734323"
                        ],
                        "name": "Y. Sawano",
                        "slug": "Y.-Sawano",
                        "structuredName": {
                            "firstName": "Yoshihiro",
                            "lastName": "Sawano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Sawano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087748"
                        ],
                        "name": "S. Saitoh",
                        "slug": "S.-Saitoh",
                        "structuredName": {
                            "firstName": "Saburou",
                            "lastName": "Saitoh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Saitoh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117252811,
            "fieldsOfStudy": [],
            "id": "636b46471adea4916ec1b2e38c8e8265218f6952",
            "isKey": false,
            "numCitedBy": 616,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Theory-of-Reproducing-Kernels-and-Its-Applications-Sawano-Saitoh",
            "title": {
                "fragments": [],
                "text": "Theory of Reproducing Kernels and Its Applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 116740162,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "238cb4ee992a72f6da7bd9b73b641f22b64ce1dc",
            "isKey": false,
            "numCitedBy": 548,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY Spline and generalized spline smoothing is shown to be equivalent to Bayesian estimation with a partially improper prior. This result supports the idea that spline smoothing is a natural solution to the regression problem when one is given a set of regression functions but one also wants to hedge against the possibility that the true model is not exactly in the span of the given regression functions. A natural measure of the deviation of the true model from the span of the regression functions comes out of the spline theory in a natural way. An appropriate value of this measure can be estimated from the data and used to constrain the estimated model to have the estimated deviation. Some convergence results and computational tricks are also discussed."
            },
            "slug": "Improper-Priors,-Spline-Smoothing-and-the-Problem-Wahba",
            "title": {
                "fragments": [],
                "text": "Improper Priors, Spline Smoothing and the Problem of Guarding Against Model Errors in Regression"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 115205884,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7cabbdf6a7288d15e26fa6ea504009bab3d1edf4",
            "isKey": false,
            "numCitedBy": 1137,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Pattern-recognition-using-generalized-portrait-Vapnik",
            "title": {
                "fragments": [],
                "text": "Pattern recognition using generalized portrait method"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1963
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145720948"
                        ],
                        "name": "Chong Gu",
                        "slug": "Chong-Gu",
                        "structuredName": {
                            "firstName": "Chong",
                            "lastName": "Gu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chong Gu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 115370290,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "fe73f9d7e0454e5d9e740c1043f33e8b0c6dfa1e",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider fitting multivariate models to response data based on analysis-of-variance (ANOVA)-like decompositions of functions of several variables, f(t 1 ,..., t d )=C+\u03a3 \u03b1 f \u03b1 (t \u03b1 )+\u03a3 \u03b1<\u03b2 f \u03b1\u03b2 (t \u03b1 , t \u03b2 )+.... A theory for fitting (some components of) these models with polynomial smoothing splines exists when each t \u03b1 is in a subset of the real line. In this case the various estimated components turn out to be certain tensor sums and products of polynomial splines. This approach may not be natural when one or more of the \u00abvariables\u00bb are geographic, in particular where nature does not know north from east"
            },
            "slug": "Semiparametric-Analysis-of-Variance-with-Tensor-Gu-Wahba",
            "title": {
                "fragments": [],
                "text": "Semiparametric Analysis of Variance with Tensor Product Thin Plate Splines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4465945"
                        ],
                        "name": "C. Beenakker",
                        "slug": "C.-Beenakker",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Beenakker",
                            "middleNames": [
                                "W.",
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Beenakker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 107646899,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "596a7d2c0b6aff1990b8c9c48e9816fd792040a0",
            "isKey": false,
            "numCitedBy": 201,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Supersymmetry-in-disorder-and-chaos-Beenakker",
            "title": {
                "fragments": [],
                "text": "Supersymmetry in disorder and chaos"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2837737"
                        ],
                        "name": "M. Talagrand",
                        "slug": "M.-Talagrand",
                        "structuredName": {
                            "firstName": "Michel",
                            "lastName": "Talagrand",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Talagrand"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121660065,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b61b4725a9209540711e5203c7c221157c3bbf1e",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We give simple proofs of previous characterizations of Glivenko-Cantelli classes."
            },
            "slug": "The-Glivenko-Cantelli-problem,-ten-years-later-Talagrand",
            "title": {
                "fragments": [],
                "text": "The Glivenko-Cantelli problem, ten years later"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059566454"
                        ],
                        "name": "G. Deco",
                        "slug": "G.-Deco",
                        "structuredName": {
                            "firstName": "Gustavo",
                            "lastName": "Deco",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Deco"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768393"
                        ],
                        "name": "B. Sch\u00fcrmann",
                        "slug": "B.-Sch\u00fcrmann",
                        "structuredName": {
                            "firstName": "Bernd",
                            "lastName": "Sch\u00fcrmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00fcrmann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62281135,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a870487587ddd8527392e4152bcafa6eca9eab9",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Dynamic-modeling-of-chaotic-time-series-by-neural-Deco-Sch\u00fcrmann",
            "title": {
                "fragments": [],
                "text": "Dynamic modeling of chaotic time series by neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "COLT 1997"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145298005"
                        ],
                        "name": "Catherine Blake",
                        "slug": "Catherine-Blake",
                        "structuredName": {
                            "firstName": "Catherine",
                            "lastName": "Blake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Catherine Blake"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62622768,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e068be31ded63600aea068eacd12931efd2a1029",
            "isKey": false,
            "numCitedBy": 13446,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "UCI-Repository-of-machine-learning-databases-Blake",
            "title": {
                "fragments": [],
                "text": "UCI Repository of machine learning databases"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123962567"
                        ],
                        "name": "M. C. Seiler",
                        "slug": "M.-C.-Seiler",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Seiler",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. C. Seiler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50678756"
                        ],
                        "name": "F. A. Seiler",
                        "slug": "F.-A.-Seiler",
                        "structuredName": {
                            "firstName": "Fritz",
                            "lastName": "Seiler",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. A. Seiler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62717952,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e980fbf251ecb28ba85eb092fc66ce284bb63be",
            "isKey": false,
            "numCitedBy": 13115,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Numerical-Recipes-in-C:-The-Art-of-Scientific-Seiler-Seiler",
            "title": {
                "fragments": [],
                "text": "Numerical Recipes in C: The Art of Scientific Computing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145740789"
                        ],
                        "name": "F. Glover",
                        "slug": "F.-Glover",
                        "structuredName": {
                            "firstName": "Fred",
                            "lastName": "Glover",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Glover"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 77
                            }
                        ],
                        "text": "In addition,there are wide classes of totally unrelated MPM approaches, e.g. Glover (1990);Gochet et al. (1997), that also can be potentially synthesized with SVM. Omissionof any method from this paper should not be used as an indication of the qualityof the method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62709938,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "420a6287046556d255285f66e501909a7d544099",
            "isKey": false,
            "numCitedBy": 241,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Discriminant analysis is an important tool for practical problem solving. Classical statistical applications have been joined recently by applications in the fields of management science and artificial intelligence. In a departure from the methodology of statistics, a series of proposals have appeared for capturing the goals of discriminant analysis in a collection of linear programming formulations. The evolution of these formulations has brought advances that have removed a number of initial shortcomings and deepened our understanding of how these models differ in essential ways from other familiar classes of LP formulations. We will demonstrate, however, that the full power of the LP discriminant analysis models has not been achieved, due to a previously undetected distortion that inhibits the quality of solutions generated. The purpose of this paper is to show how to eliminate this distortion and thereby increase the scope and flexibility of these models. We additionally show how these outcomes open the door to special model manipulations and simplifications, including the use of a successive goal method for establishing a series of conditional objectives to achieve improved discrimination."
            },
            "slug": "IMPROVED-LINEAR-PROGRAMMING-MODELS-FOR-DISCRIMINANT-Glover",
            "title": {
                "fragments": [],
                "text": "IMPROVED LINEAR PROGRAMMING MODELS FOR DISCRIMINANT ANALYSIS"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown how to eliminate a previously undetected distortion and thereby increase the scope and flexibility of the LP discriminant analysis models, including the use of a successive goal method for establishing a series of conditional objectives to achieve improved discrimination."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "115790757"
                        ],
                        "name": "A. Kolmogorov",
                        "slug": "A.-Kolmogorov",
                        "structuredName": {
                            "firstName": "Andrey",
                            "lastName": "Kolmogorov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kolmogorov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145580592"
                        ],
                        "name": "S. V. Fomin",
                        "slug": "S.-V.-Fomin",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Fomin",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. V. Fomin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62679408,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "7b7ae57c9380fe4248743678c5a9e8360e2849bf",
            "isKey": false,
            "numCitedBy": 1261,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Introductory real analysis , Introductory real analysis , \u06a9\u062a\u0627\u0628\u062e\u0627\u0646\u0647 \u062f\u06cc\u062c\u06cc\u062a\u0627\u0644 \u062c\u0646\u062f\u06cc \u0634\u0627\u067e\u0648\u0631 \u0627\u0647\u0648\u0627\u0632"
            },
            "slug": "Introductory-Real-Analysis-Kolmogorov-Fomin",
            "title": {
                "fragments": [],
                "text": "Introductory Real Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2319833"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 207975157,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "56623a496727d5c71491850e04512ddf4152b487",
            "isKey": false,
            "numCitedBy": 4468,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Beyond-Regression-:-\"New-Tools-for-Prediction-and-Werbos",
            "title": {
                "fragments": [],
                "text": "Beyond Regression : \"New Tools for Prediction and Analysis in the Behavioral Sciences"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61096945,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3941c2eac34dea5ae469a073ab61a4c7b0008579",
            "isKey": false,
            "numCitedBy": 633,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Text-categorization-with-support-vector-machines-Joachims",
            "title": {
                "fragments": [],
                "text": "Text categorization with support vector machines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "79783680"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2099637865"
                        ],
                        "name": "Mozer",
                        "slug": "Mozer",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Mozer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mozer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054400760"
                        ],
                        "name": "M. Jordan",
                        "slug": "M.-Jordan",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Jordan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2214848"
                        ],
                        "name": "T. Petsche",
                        "slug": "T.-Petsche",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Petsche",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Petsche"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60518954,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "271c040ea880abc2470f72690ed89bc3d8a11a2c",
            "isKey": false,
            "numCitedBy": 213,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Improving-the-accuracy-and-speed-of-support-vector-Burges-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "Improving the accuracy and speed of support vector learning machines"
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 1997"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2545803"
                        ],
                        "name": "M. Aizerman",
                        "slug": "M.-Aizerman",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Aizerman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Aizerman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60493317,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3caf34c1c86633b6e80dca29e3cb2b6367a0f93",
            "isKey": false,
            "numCitedBy": 1692,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Theoretical-Foundations-of-the-Potential-Function-Aizerman",
            "title": {
                "fragments": [],
                "text": "Theoretical Foundations of the Potential Function Method in Pattern Recognition Learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1905708"
                        ],
                        "name": "I. Nagayama",
                        "slug": "I.-Nagayama",
                        "structuredName": {
                            "firstName": "Itaru",
                            "lastName": "Nagayama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Nagayama"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34597350"
                        ],
                        "name": "N. Akamatsu",
                        "slug": "N.-Akamatsu",
                        "structuredName": {
                            "firstName": "Norio",
                            "lastName": "Akamatsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Akamatsu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60137345,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b793e07e7a0afcadca9a3f68c35bee6fbbd6bd06",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Approximation-of-Chaotic-Behavior-by-Using-Neural-Nagayama-Akamatsu",
            "title": {
                "fragments": [],
                "text": "Approximation of Chaotic Behavior by Using Neural Network (Special Issue on Neurocomputing)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66957615"
                        ],
                        "name": "R. J. Schalko",
                        "slug": "R.-J.-Schalko",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Schalko",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. J. Schalko"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58770897,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a995e33e958dd7dc801b12e029154ced1b8fc9ca",
            "isKey": false,
            "numCitedBy": 238,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Digital-Image-Processing-and-Computer-Vision-Schalko",
            "title": {
                "fragments": [],
                "text": "Digital Image Processing and Computer Vision"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2928241"
                        ],
                        "name": "M. Hutchinson",
                        "slug": "M.-Hutchinson",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Hutchinson",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hutchinson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2077386969"
                        ],
                        "name": "F. Hoog",
                        "slug": "F.-Hoog",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Hoog",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Hoog"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 55107772,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3bea0b7a2740c8f53bb9d64402e8c78998566731",
            "isKey": false,
            "numCitedBy": 231,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "SummaryA procedure for calculating the trace of the influence matrix associated with a polynomial smoothing spline of degree2m\u22121 fitted ton distinct, not necessarily equally spaced or uniformly weighted, data points is presented. The procedure requires orderm2n operations and therefore permits efficient orderm2n calculation of statistics associated with a polynomial smoothing spline, including the generalized cross validation. The method is a significant improvement over an existing method which requires ordern3 operations."
            },
            "slug": "Smoothing-noisy-data-with-spline-functions-Hutchinson-Hoog",
            "title": {
                "fragments": [],
                "text": "Smoothing noisy data with spline functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144497046"
                        ],
                        "name": "N. Nilsson",
                        "slug": "N.-Nilsson",
                        "structuredName": {
                            "firstName": "Nils",
                            "lastName": "Nilsson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Nilsson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 54132942,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d725045ed29d69b7a503896841ef637383376043",
            "isKey": false,
            "numCitedBy": 445,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-Machines:-Foundations-of-Trainable-Systems-Nilsson",
            "title": {
                "fragments": [],
                "text": "Learning Machines: Foundations of Trainable Pattern-Classifying Systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20038481"
                        ],
                        "name": "B. M\u00fcller",
                        "slug": "B.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Berndt",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. M\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47160050"
                        ],
                        "name": "J. Reinhardt",
                        "slug": "J.-Reinhardt",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Reinhardt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Reinhardt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 53937129,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cea39a96c4ed375d0b81ca5908d2eabc2d15fd5a",
            "isKey": false,
            "numCitedBy": 542,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The concepts of neural-network models and techniques of parallel distributed processing are comprehensively presented in a three-step approach. After a brief overview of the neural structure of the brain and the history of neural-network modelling, the reader is introduced to \"neural\" information processing, such as associative memory, perceptrons, feature-sensitive networks, learning strategies and practical applications. Part 2 covers more advanced subjects such as spin glasses, the mean-field theory of the Hopfield model, and the space of interactions in neural networks. The self-contained final part discusses seven programmes that provide practical demonstrations of neural-network models and their learning strategies. Software is included with the text on a 5 1/4-inch MS-DOS diskette and can be run using Borland's TURBO-C 2.0 compiler, the Microsoft C compiler (5.0), or compatible compilers."
            },
            "slug": "Neural-networks:-an-introduction-M\u00fcller-Reinhardt",
            "title": {
                "fragments": [],
                "text": "Neural networks: an introduction"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "The concepts of neural-network models and techniques of parallel distributed processing are comprehensively presented in a three-step approach and the reader is introduced to \"neural\" information processing, such as associative memory, perceptrons, feature-sensitive networks, learning strategies and practical applications."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145319478"
                        ],
                        "name": "Michael J. Jones",
                        "slug": "Michael-J.-Jones",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jones",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 49743910,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "eae2430d9a984120bf511655a03c15089b007499",
            "isKey": false,
            "numCitedBy": 1365,
            "numCiting": 198,
            "paperAbstract": {
                "fragments": [],
                "text": "We had previously shown that regularization principles lead to approximation schemes that are equivalent to networks with one layer of hidden units, called regularization networks. In particular, standard smoothness functionals lead to a subclass of regularization networks, the well known radial basis functions approximation schemes. This paper shows that regularization networks encompass a much broader range of approximation schemes, including many of the popular general additive models and some of the neural networks. In particular, we introduce new classes of smoothness functionals that lead to different classes of basis functions. Additive splines as well as some tensor product splines can be obtained from appropriate classes of smoothness functionals. Furthermore, the same generalization that extends radial basis functions (RBF) to hyper basis functions (HBF) also leads from additive models to ridge approximation models, containing as special cases Breiman's hinge functions, some forms of projection pursuit regression, and several types of neural networks. We propose to use the term generalized regularization networks for this broad class of approximation schemes that follow from an extension of regularization. In the probabilistic interpretation of regularization, the different classes of basis functions correspond to different classes of prior probabilities on the approximating function spaces, and therefore to different types of smoothness assumptions. In summary, different multilayer networks with one hidden layer, which we collectively call generalized regularization networks, correspond to different classes of priors and associated smoothness functionals in a classical regularization principle. Three broad classes are (1) radial basis functions that can be generalized to hyper basis functions, (2) some tensor product splines, and (3) additive splines that can be generalized to schemes of the type of ridge approximation, hinge functions, and several perceptron-like neural networks with one hidden layer."
            },
            "slug": "Regularization-Theory-and-Neural-Networks-Girosi-Jones",
            "title": {
                "fragments": [],
                "text": "Regularization Theory and Neural Networks Architectures"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "This paper shows that regularization networks encompass a much broader range of approximation schemes, including many of the popular general additive models and some of the neural networks, and introduces new classes of smoothness functionals that lead to different classes of basis functions."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067703783"
                        ],
                        "name": "Brown",
                        "slug": "Brown",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Brown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2080524370"
                        ],
                        "name": "Bryant",
                        "slug": "Bryant",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Bryant",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryant"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122495506"
                        ],
                        "name": "Abarbanel",
                        "slug": "Abarbanel",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Abarbanel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abarbanel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 23163670,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9a489145ab9a105dda1166d4bc48c0430ead6354",
            "isKey": false,
            "numCitedBy": 332,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper examines the problem of accurately determining, from an observed time series, the Liapunov exponents for the dynamical system generating the data. It is shown that, even with very large data sets, it is clearly advantageous to utilize local neighborhood-to-neighborhood mappings with higher-order Taylor series rather than just local linear maps. This procedure is demonstrated on the Henon and Ikeda maps of the plane itself, the Lorenz system of three ordinary differential equations, and the Mackey-Glass delay differential equation."
            },
            "slug": "Computing-the-Lyapunov-spectrum-of-a-dynamical-from-Brown-Bryant",
            "title": {
                "fragments": [],
                "text": "Computing the Lyapunov spectrum of a dynamical system from an observed time series."
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is shown that, even with very large data sets, it is clearly advantageous to utilize local neighborhood-to-neighborhood mappings with higher-order Taylor series rather than just local linear maps."
            },
            "venue": {
                "fragments": [],
                "text": "Physical review. A, Atomic, molecular, and optical physics"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102552065"
                        ],
                        "name": "W. D. Cairns",
                        "slug": "W.-D.-Cairns",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Cairns",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. D. Cairns"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 127369,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "905ded91783665b05e9ec5a6ce415e0da6e84808",
            "isKey": false,
            "numCitedBy": 236,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "s for the workshop and invited addresses are listed first, in chronological order, followed by faculty and graduate student abstracts, alphabetized by submitting presenter\u2019s last name. Student presentation abstracts follow, with student poster abstracts at the end (all alphabetized the same way)."
            },
            "slug": "THE-MATHEMATICAL-ASSOCIATION-OF-AMERICA.-Cairns",
            "title": {
                "fragments": [],
                "text": "THE MATHEMATICAL ASSOCIATION OF AMERICA."
            },
            "tldr": {
                "abstractSimilarityScore": 34,
                "text": "Student presentation abstracts follow, with student poster abstracts at the end (all alphabetized the same way)."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1918
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "17328656"
                        ],
                        "name": "A. Roy",
                        "slug": "A.-Roy",
                        "structuredName": {
                            "firstName": "Asim",
                            "lastName": "Roy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Roy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145033054"
                        ],
                        "name": "S. Mukhopadhyay",
                        "slug": "S.-Mukhopadhyay",
                        "structuredName": {
                            "firstName": "Somnath",
                            "lastName": "Mukhopadhyay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mukhopadhyay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6451782,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b91a9dc7ef83053c2ef251f3cf9060213f64c2fb",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an algorithm for constructing and training a class of higher-order perceptrons for classification problems. The method uses linear programming models to construct and train the net. Its polynomial time complexity is proven and computational results are provided for several well-known problems. In all cases, very small nets were created compared to those reported in other computational studies."
            },
            "slug": "Iterative-generation-of-higher-order-nets-in-time-Roy-Mukhopadhyay",
            "title": {
                "fragments": [],
                "text": "Iterative generation of higher-order nets in polynomial time using linear programming"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The method uses linear programming models to construct and train the net and its polynomial time complexity is proven and computational results are provided for several well-known problems."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "114379909"
                        ],
                        "name": "Schouten",
                        "slug": "Schouten",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Schouten",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Schouten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121015391"
                        ],
                        "name": "Takens",
                        "slug": "Takens",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Takens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Takens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29974437"
                        ],
                        "name": "van den Bleek CM",
                        "slug": "van-den-Bleek-CM",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "van den Bleek CM",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "van den Bleek CM"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 28816567,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "f14b8e43c445664f52cdee47564e602c8505c78e",
            "isKey": false,
            "numCitedBy": 194,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A simple method is proposed to estimate the correlation dimension of a noisy chaotic attractor. The method is based on the observation that the noise induces a bias in the observed distances of trajectories, which tend to appear farther apart than they are. Under the assumption of noise being strictly bounded in amplitude, this leads to a rescaling of interpoint distances on the attractor. A correlation integral function is obtained that accounts for this effect of noise. The applicability of the method is illustrated with two examples, viz., the Lorenz attractor with additive noise and experimental time series of pressure fluctuation data measured in gas-solid fluidized beds."
            },
            "slug": "Estimation-of-the-dimension-of-a-noisy-attractor.-Schouten-Takens",
            "title": {
                "fragments": [],
                "text": "Estimation of the dimension of a noisy attractor."
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A simple method is proposed to estimate the correlation dimension of a noisy chaotic attractor based on the observation that the noise induces a bias in the observed distances of trajectories, which tend to appear farther apart than they are."
            },
            "venue": {
                "fragments": [],
                "text": "Physical review. E, Statistical physics, plasmas, fluids, and related interdisciplinary topics"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40805799"
                        ],
                        "name": "J. W. Humberston",
                        "slug": "J.-W.-Humberston",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Humberston",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. W. Humberston"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5482999,
            "fieldsOfStudy": [],
            "id": "f2dbab1960529561c9432600694607d59d6db694",
            "isKey": false,
            "numCitedBy": 8102,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Classical-mechanics-Humberston",
            "title": {
                "fragments": [],
                "text": "Classical mechanics"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 52810328,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1061ff8a216a8d00f5f189d7ea593c6f0703b771",
            "isKey": false,
            "numCitedBy": 511,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A Support Vector Machine SVM is a uni versal learning machine whose decision sur face is parameterized by a set of support vec tors and by a set of corresponding weights An SVM is also characterized by a kernel function Choice of the kernel determines whether the resulting SVM is a polynomial classi er a two layer neural network a ra dial basis function machine or some other learning machine SVMs are currently considerably slower in test phase than other approaches with sim ilar generalization performance To address this we present a general method to signif icantly decrease the complexity of the deci sion rule obtained using an SVM The pro posed method computes an approximation to the decision rule in terms of a reduced set of vectors These reduced set vectors are not support vectors and can in some cases be computed analytically We give ex perimental results for three pattern recogni tion problems The results show that the method can decrease the computational com plexity of the decision rule by a factor of ten with no loss in generalization perfor mance making the SVM test speed com petitive with that of other methods Fur ther the method allows the generalization performance complexity trade o to be di rectly controlled The proposed method is not speci c to pattern recognition and can be applied to any problem where the Sup port Vector algorithm is used for example regression INTRODUCTION SUPPORT VECTOR MACHINES Consider a two class classi er for which the decision rule takes the form"
            },
            "slug": "Simplified-Support-Vector-Decision-Rules-Burges",
            "title": {
                "fragments": [],
                "text": "Simplified Support Vector Decision Rules"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The results show that the method can decrease the computational complexity of the decision rule by a factor of ten with no loss in generalization perfor mance making the SVM test speed com petitive with that of other methods."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 201
                            }
                        ],
                        "text": "2 Two MPM Methods for Classi cation Mangasarian's Multisurface Method of Pattern Recognition (Mangasiarian, 1965; Mangasarian, 1968) is very similar in derivation to the Generalized Portrait Method of Vapnik and Chervonenkis (1974). Mangasarian proposed nding a linear discriminant for linearly separable problems by solving the following optimization problem: max w; ; subject to w xi ; i 2 A1 w xj ; j 2 A2 kwk = 1 (19."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 28637672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "385197d4c02593e2823c71e4f90a0993b703620e",
            "isKey": false,
            "numCitedBy": 26320,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "slug": "Statistical-learning-theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "Statistical learning theory"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48491389"
                        ],
                        "name": "L. Gurvits",
                        "slug": "L.-Gurvits",
                        "structuredName": {
                            "firstName": "Leonid",
                            "lastName": "Gurvits",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gurvits"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 12769183,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "cf61dc7491f61c45d5833901a15e04409ff57799",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-note-on-a-scale-sensitive-dimension-of-linear-in-Gurvits",
            "title": {
                "fragments": [],
                "text": "A note on a scale-sensitive dimension of linear bounded functionals in Banach spaces"
            },
            "venue": {
                "fragments": [],
                "text": "Theor. Comput. Sci."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145016534"
                        ],
                        "name": "J. Moody",
                        "slug": "J.-Moody",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Moody",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Moody"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2319258"
                        ],
                        "name": "C. Darken",
                        "slug": "C.-Darken",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Darken",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Darken"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 31251383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e7c4f513f24c3b82a1138b9f22ed87ed00cbe76",
            "isKey": false,
            "numCitedBy": 4527,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a network architecture which uses a single internal layer of locally-tuned processing units to learn both classification tasks and real-valued function approximations (Moody and Darken 1988). We consider training such networks in a completely supervised manner, but abandon this approach in favor of a more computationally efficient hybrid learning method which combines self-organized and supervised learning. Our networks learn faster than backpropagation for two reasons: the local representations ensure that only a few units respond to any given input, thus reducing computational overhead, and the hybrid learning rules are linear rather than nonlinear, thus leading to faster convergence. Unlike many existing methods for data analysis, our network architecture and learning rules are truly adaptive and are thus appropriate for real-time use."
            },
            "slug": "Fast-Learning-in-Networks-of-Locally-Tuned-Units-Moody-Darken",
            "title": {
                "fragments": [],
                "text": "Fast Learning in Networks of Locally-Tuned Processing Units"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This work proposes a network architecture which uses a single internal layer of locally-tuned processing units to learn both classification tasks and real-valued function approximations (Moody and Darken 1988)."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37274089"
                        ],
                        "name": "D. Henderson",
                        "slug": "D.-Henderson",
                        "structuredName": {
                            "firstName": "Donnie",
                            "lastName": "Henderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Henderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2799635"
                        ],
                        "name": "R. Howard",
                        "slug": "R.-Howard",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Howard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Howard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34859193"
                        ],
                        "name": "W. Hubbard",
                        "slug": "W.-Hubbard",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "Hubbard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Hubbard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041866"
                        ],
                        "name": "L. Jackel",
                        "slug": "L.-Jackel",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Jackel",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jackel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 41312633,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8e8f3c8d4418c8d62e306538c9c1292635e9d27",
            "isKey": false,
            "numCitedBy": 7828,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification."
            },
            "slug": "Backpropagation-Applied-to-Handwritten-Zip-Code-LeCun-Boser",
            "title": {
                "fragments": [],
                "text": "Backpropagation Applied to Handwritten Zip Code Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This paper demonstrates how constraints from the task domain can be integrated into a backpropagation network through the architecture of the network, successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1915481"
                        ],
                        "name": "W. Gochet",
                        "slug": "W.-Gochet",
                        "structuredName": {
                            "firstName": "Willy",
                            "lastName": "Gochet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gochet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3549175"
                        ],
                        "name": "A. Stam",
                        "slug": "A.-Stam",
                        "structuredName": {
                            "firstName": "Antonie",
                            "lastName": "Stam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143725377"
                        ],
                        "name": "V. Srinivasan",
                        "slug": "V.-Srinivasan",
                        "structuredName": {
                            "firstName": "Vivek",
                            "lastName": "Srinivasan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Srinivasan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144336045"
                        ],
                        "name": "Shaoxiang Chen",
                        "slug": "Shaoxiang-Chen",
                        "structuredName": {
                            "firstName": "Shaoxiang",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoxiang Chen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 91
                            }
                        ],
                        "text": "In addition,there are wide classes of totally unrelated MPM approaches, e.g. Glover (1990);Gochet et al. (1997), that also can be potentially synthesized with SVM. Omissionof any method from this paper should not be used as an indication of the qualityof the method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 30113666,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science",
                "Business"
            ],
            "id": "1bbbb60d272f5e0f6578754134aa36c6b877917e",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we introduce a nonparametric linear programming formulation for the general multigroup classification problem. Previous research using linear programming formulations has either been limited to the two-group case, or required complicated constraints and many zero-one variables. We develop general properties of our multigroup formulation and illustrate its use with several small example problems and previously published real data sets. A comparative analysis on the real data sets shows that our formulation may offer an interesting robust alternative to parametric statistical formulations for the multigroup discriminant problem."
            },
            "slug": "Multigroup-Discriminant-Analysis-Using-Linear-Gochet-Stam",
            "title": {
                "fragments": [],
                "text": "Multigroup Discriminant Analysis Using Linear Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "A comparative analysis on the real data sets shows that the nonparametric linear programming formulation introduced in this paper may offer an interesting robust alternative to parametric statistical formulations for the multigroup discriminant problem."
            },
            "venue": {
                "fragments": [],
                "text": "Oper. Res."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "17328656"
                        ],
                        "name": "A. Roy",
                        "slug": "A.-Roy",
                        "structuredName": {
                            "firstName": "Asim",
                            "lastName": "Roy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Roy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123798980"
                        ],
                        "name": "Lark-Sang Kim",
                        "slug": "Lark-Sang-Kim",
                        "structuredName": {
                            "firstName": "Lark-Sang",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lark-Sang Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145033054"
                        ],
                        "name": "S. Mukhopadhyay",
                        "slug": "S.-Mukhopadhyay",
                        "structuredName": {
                            "firstName": "Somnath",
                            "lastName": "Mukhopadhyay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mukhopadhyay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 152
                            }
                        ],
                        "text": "Note that there are some MPM that doperform nonlinear mappings into higher dimensional space, most notably the poly-nomial neural network approaches of Roy et al. (1993, 1995); Roy and Mukhopad-hyay (1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026Vapnik, Volodya Vovk & Chris Watkins19 Combining Support Vector and Mathematical ProgrammingMethods for Classi cation 307Kristin P. Bennett20 Kernel Principal Component Analysis 327Bernhard Sch olkopf, Alex J. Smola & Klaus-Robert M uller\n1998/08/25 16:31\nviiReferences 353Index 373\n1998/08/25 16:31"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2459964,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9c58c5c3ffd078bfbcd696adc6fb28cfa1071cfa",
            "isKey": false,
            "numCitedBy": 100,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-polynomial-time-algorithm-for-the-construction-of-Roy-Kim",
            "title": {
                "fragments": [],
                "text": "A polynomial time algorithm for the construction and training of a class of multilayer perceptrons"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727797"
                        ],
                        "name": "S. Chen",
                        "slug": "S.-Chen",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Chen",
                            "middleNames": [
                                "Saobing"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145621255"
                        ],
                        "name": "M. Saunders",
                        "slug": "M.-Saunders",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Saunders",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Saunders"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2429822,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9af121fbed84c3484ab86df8f17f1f198ed790a0",
            "isKey": false,
            "numCitedBy": 9732,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "The time-frequency and time-scale communities have recently developed a large number of overcomplete waveform dictionaries --- stationary wavelets, wavelet packets, cosine packets, chirplets, and warplets, to name a few. Decomposition into overcomplete systems is not unique, and several methods for decomposition have been proposed, including the method of frames (MOF), Matching pursuit (MP), and, for special dictionaries, the best orthogonal basis (BOB). \nBasis Pursuit (BP) is a principle for decomposing a signal into an \"optimal\" superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions. We give examples exhibiting several advantages over MOF, MP, and BOB, including better sparsity and superresolution. BP has interesting relations to ideas in areas as diverse as ill-posed problems, in abstract harmonic analysis, total variation denoising, and multiscale edge denoising. \nBP in highly overcomplete dictionaries leads to large-scale optimization problems. With signals of length 8192 and a wavelet packet dictionary, one gets an equivalent linear program of size 8192 by 212,992. Such problems can be attacked successfully only because of recent advances in linear programming by interior-point methods. We obtain reasonable success with a primal-dual logarithmic barrier method and conjugate-gradient solver."
            },
            "slug": "Atomic-Decomposition-by-Basis-Pursuit-Chen-Donoho",
            "title": {
                "fragments": [],
                "text": "Atomic Decomposition by Basis Pursuit"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Basis Pursuit (BP) is a principle for decomposing a signal into an \"optimal\" superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Sci. Comput."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2839642"
                        ],
                        "name": "B. Carl",
                        "slug": "B.-Carl",
                        "structuredName": {
                            "firstName": "Bernd",
                            "lastName": "Carl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Carl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 120053955,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2f97eddf5b264524c32778cbb3f0c4ee0cb42a60",
            "isKey": false,
            "numCitedBy": 124,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "\u00a9 Annales de l\u2019institut Fourier, 1985, tous droits r\u00e9serv\u00e9s. L\u2019acc\u00e8s aux archives de la revue \u00ab Annales de l\u2019institut Fourier \u00bb (http://annalif.ujf-grenoble.fr/) implique l\u2019accord avec les conditions g\u00e9n\u00e9rales d\u2019utilisation (http://www.numdam.org/legal.php). Toute utilisation commerciale ou impression syst\u00e9matique est constitutive d\u2019une infraction p\u00e9nale. Toute copie ou impression de ce fichier doit contenir la pr\u00e9sente mention de copyright."
            },
            "slug": "Inequalities-of-Bernstein-Jackson-type-and-the-of-Carl",
            "title": {
                "fragments": [],
                "text": "Inequalities of Bernstein-Jackson-type and the degree of compactness of operators in Banach spaces"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703769"
                        ],
                        "name": "J. Karhunen",
                        "slug": "J.-Karhunen",
                        "structuredName": {
                            "firstName": "Juha",
                            "lastName": "Karhunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Karhunen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768650"
                        ],
                        "name": "J. Joutsensalo",
                        "slug": "J.-Joutsensalo",
                        "structuredName": {
                            "firstName": "Jyrki",
                            "lastName": "Joutsensalo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Joutsensalo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1578694,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "993ebaefaa0bc8cc201d9e2f5cfef346cb8881b9",
            "isKey": false,
            "numCitedBy": 299,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Generalizations-of-principal-component-analysis,-Karhunen-Joutsensalo",
            "title": {
                "fragments": [],
                "text": "Generalizations of principal component analysis, optimization problems, and neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48107116"
                        ],
                        "name": "M. Mackey",
                        "slug": "M.-Mackey",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Mackey",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mackey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145548576"
                        ],
                        "name": "L. Glass",
                        "slug": "L.-Glass",
                        "structuredName": {
                            "firstName": "Leon",
                            "lastName": "Glass",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Glass"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 42039623,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "e39c17da0a3a0e7f709ef3e785c912df5cf386df",
            "isKey": false,
            "numCitedBy": 3641,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "First-order nonlinear differential-delay equations describing physiological control systems are studied. The equations display a broad diversity of dynamical behavior including limit cycle oscillations, with a variety of wave forms, and apparently aperiodic or \"chaotic\" solutions. These results are discussed in relation to dynamical respiratory and hematopoietic diseases."
            },
            "slug": "Oscillation-and-chaos-in-physiological-control-Mackey-Glass",
            "title": {
                "fragments": [],
                "text": "Oscillation and chaos in physiological control systems."
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "First-order nonlinear differential-delay equations describing physiological control systems displaying a broad diversity of dynamical behavior including limit cycle oscillations, with a variety of wave forms, and apparently aperiodic or \"chaotic\" solutions are studied."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3267844"
                        ],
                        "name": "K. Schittkowski",
                        "slug": "K.-Schittkowski",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Schittkowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Schittkowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2907488"
                        ],
                        "name": "Christian Zillober",
                        "slug": "Christian-Zillober",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Zillober",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Zillober"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 44060508,
            "fieldsOfStudy": [],
            "id": "d4143c46910f249bedbdc37caf88e4c292124c08",
            "isKey": false,
            "numCitedBy": 6352,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "NONLINEAR-PROGRAMMING-Schittkowski-Zillober",
            "title": {
                "fragments": [],
                "text": "NONLINEAR PROGRAMMING"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2024710"
                        ],
                        "name": "A. Weigend",
                        "slug": "A.-Weigend",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Weigend",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Weigend"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688083"
                        ],
                        "name": "N. Gershenfeld",
                        "slug": "N.-Gershenfeld",
                        "structuredName": {
                            "firstName": "Neil",
                            "lastName": "Gershenfeld",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Gershenfeld"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 26996169,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "85f5b8e3e0b2fb868528349e5032b0c2d20c7a34",
            "isKey": false,
            "numCitedBy": 1882,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Make more knowledge even in less time every day. You may not always spend your time and money to go abroad and get the experience and knowledge by yourself. Reading is a good alternative to do in getting this desirable knowledge and experience. You may gain many things from experiencing directly, but of course it will spend much money. So here, by reading time series prediction forecasting the future and understanding the past, you can take more advantages with limited budget."
            },
            "slug": "Time-Series-Prediction:-Forecasting-the-Future-and-Weigend-Gershenfeld",
            "title": {
                "fragments": [],
                "text": "Time Series Prediction: Forecasting the Future and Understanding the Past"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "By reading time series prediction forecasting the future and understanding the past, you can take more advantages with limited budget."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684821"
                        ],
                        "name": "P. Olver",
                        "slug": "P.-Olver",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Olver",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Olver"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117822940,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "65c10b4fb95dd2860a6eb2e43c88cb9b30dc8d51",
            "isKey": false,
            "numCitedBy": 6656,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1 Introduction to Lie Groups.- 1.1. Manifolds.- Change of Coordinates.- Maps Between Manifolds.- The Maximal Rank Condition.- Submanifolds.- Regular Submanifolds.- Implicit Submanifolds.- Curves and Connectedness.- 1.2. Lie Groups.- Lie Subgroups.- Local Lie Groups.- Local Transformation Groups.- Orbits.- 1.3. Vector Fields.- Flows.- Action on Functions.- Differentials.- Lie Brackets.- Tangent Spaces and Vectors Fields on Submanifolds.- Frobenius' Theorem.- 1.4. Lie Algebras.- One-Parameter Subgroups.- Subalgebras.- The Exponential Map.- Lie Algebras of Local Lie Groups.- Structure Constants.- Commutator Tables.- Infinitesimal Group Actions.- 1.5. Differential Forms.- Pull-Back and Change of Coordinates.- Interior Products.- The Differential.- The de Rham Complex.- Lie Derivatives.- Homotopy Operators.- Integration and Stokes' Theorem.- Notes.- Exercises.- 2 Symmetry Groups of Differential Equations.- 2.1. Symmetries of Algebraic Equations.- Invariant Subsets.- Invariant Functions.- Infinitesimal Invariance.- Local Invariance.- Invariants and Functional Dependence.- Methods for Constructing Invariants.- 2.2. Groups and Differential Equations.- 2.3. Prolongation.- Systems of Differential Equations.- Prolongation of Group Actions.- Invariance of Differential Equations.- Prolongation of Vector Fields.- Infinitesimal Invariance.- The Prolongation Formula.- Total Derivatives.- The General Prolongation Formula.- Properties of Prolonged Vector Fields.- Characteristics of Symmetries.- 2.4. Calculation of Symmetry Groups.- 2.5. Integration of Ordinary Differential Equations.- First Order Equations.- Higher Order Equations.- Differential Invariants.- Multi-parameter Symmetry Groups.- Solvable Groups.- Systems of Ordinary Differential Equations.- 2.6. Nondegeneracy Conditions for Differential Equations.- Local Solvability.- In variance Criteria.- The Cauchy-Kovalevskaya Theorem.- Characteristics.- Normal Systems.- Prolongation of Differential Equations.- Notes.- Exercises.- 3 Group-Invariant Solutions.- 3.1. Construction of Group-Invariant Solutions.- 3.2. Examples of Group-Invariant Solutions.- 3.3. Classification of Group-Invariant Solutions.- The Adjoint Representation.- Classification of Subgroups and Subalgebras.- Classification of Group-Invariant Solutions.- 3.4. Quotient Manifolds.- Dimensional Analysis.- 3.5. Group-Invariant Prolongations and Reduction.- Extended Jet Bundles.- Differential Equations.- Group Actions.- The Invariant Jet Space.- Connection with the Quotient Manifold.- The Reduced Equation.- Local Coordinates.- Notes.- Exercises.- 4 Symmetry Groups and Conservation Laws.- 4.1. The Calculus of Variations.- The Variational Derivative.- Null Lagrangians and Divergences.- Invariance of the Euler Operator.- 4.2. Variational Symmetries.- Infinitesimal Criterion of Invariance.- Symmetries of the Euler-Lagrange Equations.- Reduction of Order.- 4.3. Conservation Laws.- Trivial Conservation Laws.- Characteristics of Conservation Laws.- 4.4. Noether's Theorem.- Divergence Symmetries.- Notes.- Exercises.- 5 Generalized Symmetries.- 5.1. Generalized Symmetries of Differential Equations.- Differential Functions.- Generalized Vector Fields.- Evolutionary Vector Fields.- Equivalence and Trivial Symmetries.- Computation of Generalized Symmetries.- Group Transformations.- Symmetries and Prolongations.- The Lie Bracket.- Evolution Equations.- 5.2. Recursion Operators, Master Symmetries and Formal Symmetries.- Frechet Derivatives.- Lie Derivatives of Differential Operators.- Criteria for Recursion Operators.- The Korteweg-de Vries Equation.- Master Symmetries.- Pseudo-differential Operators.- Formal Symmetries.- 5.3. Generalized Symmetries and Conservation Laws.- Adjoints of Differential Operators.- Characteristics of Conservation Laws.- Variational Symmetries.- Group Transformations.- Noether's Theorem.- Self-adjoint Linear Systems.- Action of Symmetries on Conservation Laws.- Abnormal Systems and Noether's Second Theorem.- Formal Symmetries and Conservation Laws.- 5.4. The Variational Complex.- The D-Complex.- Vertical Forms.- Total Derivatives of Vertical Forms.- Functionals and Functional Forms.- The Variational Differential.- Higher Euler Operators.- The Total Homotopy Operator.- Notes.- Exercises.- 6 Finite-Dimensional Hamiltonian Systems.- 6.1. Poisson Brackets.- Hamiltonian Vector Fields.- The Structure Functions.- The Lie-Poisson Structure.- 6.2. Symplectic Structures and Foliations.- The Correspondence Between One-Forms and Vector Fields.- Rank of a Poisson Structure.- Symplectic Manifolds.- Maps Between Poisson Manifolds.- Poisson Submanifolds.- Darboux' Theorem.- The Co-adjoint Representation.- 6.3. Symmetries, First Integrals and Reduction of Order.- First Integrals.- Hamiltonian Symmetry Groups.- Reduction of Order in Hamiltonian Systems.- Reduction Using Multi-parameter Groups.- Hamiltonian Transformation Groups.- The Momentum Map.- Notes.- Exercises.- 7 Hamiltonian Methods for Evolution Equations.- 7.1. Poisson Brackets.- The Jacobi Identity.- Functional Multi-vectors.- 7.2. Symmetries and Conservation Laws.- Distinguished Functionals.- Lie Brackets.- Conservation Laws.- 7.3. Bi-Hamiltonian Systems.- Recursion Operators.- Notes.- Exercises.- References.- Symbol Index.- Author Index."
            },
            "slug": "Applications-of-lie-groups-to-differential-Olver",
            "title": {
                "fragments": [],
                "text": "Applications of lie groups to differential equations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2459584"
                        ],
                        "name": "W. Wolberg",
                        "slug": "W.-Wolberg",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Wolberg",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Wolberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747026"
                        ],
                        "name": "O. Mangasarian",
                        "slug": "O.-Mangasarian",
                        "structuredName": {
                            "firstName": "Olvi",
                            "lastName": "Mangasarian",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Mangasarian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3064517,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "50537a16eb18e2bc4971165258cba7a071f38cb7",
            "isKey": false,
            "numCitedBy": 1000,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Multisurface pattern separation is a mathematical method for distinguishing between elements of two pattern sets. Each element of the pattern sets is comprised of various scalar observations. In this paper, we use the diagnosis of breast cytology to demonstrate the applicability of this method to medical diagnosis and decision making. Each of 11 cytological characteristics of breast fine-needle aspirates reported to differ between benign and malignant samples was graded 1 to 10 at the time of sample collection. Nine characteristics were found to differ significantly between benign and malignant samples. Mathematically, these values for each sample were represented by a point in a nine-dimensional space of real variables. Benign points were separated from malignant ones by planes determined by linear programming. Correct separation was accomplished in 369 of 370 samples (201 benign and 169 malignant). In the one misclassified malignant case, the fine-needle aspirate cytology was so definitely benign and the cytology of the excised cancer so definitely malignant that we believe the tumor was missed on aspiration. Our mathematical method is applicable to other medical diagnostic and decision-making problems."
            },
            "slug": "Multisurface-method-of-pattern-separation-for-to-Wolberg-Mangasarian",
            "title": {
                "fragments": [],
                "text": "Multisurface method of pattern separation for medical diagnosis applied to breast cytology."
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The diagnosis of breast cytology is used to demonstrate the applicability ofMultisurface pattern separation to medical diagnosis and decision making and it is found that this mathematical method is applicable to other medical diagnostic and decision-making problems."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101617171"
                        ],
                        "name": "M. Hamermesh",
                        "slug": "M.-Hamermesh",
                        "structuredName": {
                            "firstName": "Morton",
                            "lastName": "Hamermesh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hamermesh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49637194"
                        ],
                        "name": "A. Mullin",
                        "slug": "A.-Mullin",
                        "structuredName": {
                            "firstName": "Albert",
                            "lastName": "Mullin",
                            "middleNames": [
                                "A."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mullin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122768784,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e2317c866de5b580b8e914d5871b6106a2eb3a8f",
            "isKey": false,
            "numCitedBy": 1363,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Group-Theory-and-its-Applications-to-Physical-Hamermesh-Mullin",
            "title": {
                "fragments": [],
                "text": "Group Theory and its Applications to Physical Problems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107346474"
                        ],
                        "name": "S. S. Rao",
                        "slug": "S.-S.-Rao",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Rao",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. S. Rao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145465616"
                        ],
                        "name": "R. C. Desai",
                        "slug": "R.-C.-Desai",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Desai",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Desai"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 569908,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "93ec9bad267bb1e072cdc9bf6b9ab6a6f2ab3c10",
            "isKey": false,
            "numCitedBy": 1127,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction. Chapter 1: Introduction to mental ray. What Is mental ray? Why Use mental ray? The Structure of mental ray. mental ray Integration. Command-Line Rendering and the Stand-Alone Renderer. mental ray Shaders and Shader Libraries. Indirect Illumination. Chapter 2: Rendering Algorithms. Introduction to Synthetic Lighting. Rendering under the Hood. mental ray Rendering Algorithms. Scanline Rendering in Depth. Raytrace Rendering in Depth. Hardware Rendering. Chapter 3: mental ray Output. mental ray Data Types. The Frame Buffer. Frame Buffer Options. mental ray Cameras. Output Statements. Chapter 4: Camera Fundamentals. Camera Basics and Aspect Ratios. Camera Lenses. Host Application Settings. Chapter 5: Quality Control. Sampling and Filtering in Host Applications. Raytrace Acceleration. Diagnostic and BSP Fine-Tuning. Chapter 6: Lights and Soft Shadows. mental ray Lights. Area Lights. Host Application Settings. Light Profiles. Chapter 7: Shadow Algorithms. Shadow Algorithms. Raytrace Shadows. Depth-Based Shadows. Stand-Alone and Host Settings. Chapter 8: Motion Blur. mental ray Motion Blur. Motion-Blur Options. Motion-Blur Render Algorithms. Host Settings. Chapter 9: The Fundamentals of Light and Shading Models. The Fundamentals of Light. Light Transport and Shading Models. mental ray Shaders. Chapter 10: mental ray Shaders and Shader Trees. Installing Custom Shaders. DGS and Dielectric Shading Models. Glossy Reflection and Refraction Shaders. Brushed Metals with the Glossy and Anisotropic Shaders. The Architectural (mia) Material. Chapter 11: mental ray Textures and Projections. Texture Space and Projections. mental ray Bump Mapping. mental ray Projection and Remapping Shaders. Host Application Settings. Memory Mapping, Pyramid Images, and Image Filtering. Chapter 12: Indirect Illumination. mental ray Indirect Illumination. Photon Shaders and Photon-Casting Lights. Indirect Illumination Options and Fine-Tuning. Participating Media (PM) Effects. Chapter 13: Final Gather and Ambient Occlusion. Final Gather Fundamentals. Final Gather Options and Techniques. Advanced Final Gather Techniques. Ambient Occlusion. Chapter 14: Subsurface Scattering. Advanced Shading Models. Nonphysical Subsurface Scattering. An Advanced Shader Tree. Physical Subsurface Scattering. Appendix: About the Companion CD. Index."
            },
            "slug": "Optimization-Theory-and-Applications-Rao-Desai",
            "title": {
                "fragments": [],
                "text": "Optimization Theory and Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This book discusses the construction of mental ray, a model for synthetic lighting, and some of the techniques used to design and implement such models."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Systems, Man, and Cybernetics"
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145833095"
                        ],
                        "name": "S. Kothari",
                        "slug": "S.-Kothari",
                        "structuredName": {
                            "firstName": "Suresh",
                            "lastName": "Kothari",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kothari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681982"
                        ],
                        "name": "H. Oh",
                        "slug": "H.-Oh",
                        "structuredName": {
                            "firstName": "Heekuck",
                            "lastName": "Oh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Oh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 177751,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbc0a468ab103ae29717703d4aa9f682f6a2b664",
            "isKey": false,
            "numCitedBy": 15336,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-Networks-for-Pattern-Recognition-Kothari-Oh",
            "title": {
                "fragments": [],
                "text": "Neural Networks for Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Adv. Comput."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2076815081"
                        ],
                        "name": "Seung",
                        "slug": "Seung",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Seung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30053069"
                        ],
                        "name": "Sompolinsky",
                        "slug": "Sompolinsky",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Sompolinsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sompolinsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30093773"
                        ],
                        "name": "Tishby",
                        "slug": "Tishby",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Tishby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tishby"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7394722,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2498a4e1755f047accc06a6e0fab0b0eb1b37ae0",
            "isKey": false,
            "numCitedBy": 393,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "number of examples P used for training. The theory implies that, for a reduction in eg that remains finite in the large-N limit, P should generally scale as nN, where N is the number of independently adjustable weights in the network. We show that for smooth networks, i.e., those with continuously varying weights and smooth transfer functions, the generalization curve asymptotically obeys an inverse power law. In contrast, for nonsmooth networks other behaviors can appear, depending on the nature of the nonlinearities as well as the realizability of the rule. In particular, a discontinuous learning transition from a state of poor to a state of perfect generalization can occur in nonsmooth networks learning realizable rules. We illustrate both gradual and continuous learning with a detailed analytical and numerical study of several single-layer perceptron models. Comparing with the exact replica theory of perceptron learning, we find that for realizable rules the high-temperature and annealed theories provide very good approximations to the generalization performance. Assuming this to hold for multilayer networks as well, we propose a classification of possible asymptotic forms of learning curves in general realizable models. For unrealizable rules we find that the above approximations fail in general to predict correctly the shapes of the generalization curves. Another indication of the important role of quenched disorder for unrealizable rules is that the generalization error is not necessarily a monotonically increasing function of temperature. Also, unrealizable rules can possess genuine spin-glass phases indicative of degenerate minima separated by high barriers."
            },
            "slug": "Statistical-mechanics-of-learning-from-examples.-Seung-Sompolinsky",
            "title": {
                "fragments": [],
                "text": "Statistical mechanics of learning from examples."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that for smooth networks, i.e., those with continuously varying weights and smooth transfer functions, the generalization curve asymptotically obeys an inverse power law, while for nonsmooth networks other behaviors can appear, depending on the nature of the nonlinearities as well as the realizability of the rule."
            },
            "venue": {
                "fragments": [],
                "text": "Physical review. A, Atomic, molecular, and optical physics"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30071716"
                        ],
                        "name": "Kennel",
                        "slug": "Kennel",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Kennel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kennel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067703783"
                        ],
                        "name": "Brown",
                        "slug": "Brown",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Brown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122495506"
                        ],
                        "name": "Abarbanel",
                        "slug": "Abarbanel",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Abarbanel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abarbanel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 35354878,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2f8b034ac986d4007fa4801312ffbb2dbcb556c2",
            "isKey": false,
            "numCitedBy": 3152,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "We examine the issue of determining an acceptable minimum embedding dimension by looking at the behavior of near neighbors under changes in the embedding dimension from d\\ensuremath{\\rightarrow}d+1. When the number of nearest neighbors arising through projection is zero in dimension ${\\mathit{d}}_{\\mathit{E}}$, the attractor has been unfolded in this dimension. The precise determination of ${\\mathit{d}}_{\\mathit{E}}$ is clouded by ``noise,'' and we examine the manner in which noise changes the determination of ${\\mathit{d}}_{\\mathit{E}}$. Our criterion also indicates the error one makes by choosing an embedding dimension smaller than ${\\mathit{d}}_{\\mathit{E}}$. This knowledge may be useful in the practical analysis of observed time series."
            },
            "slug": "Determining-embedding-dimension-for-phase-space-a-Kennel-Brown",
            "title": {
                "fragments": [],
                "text": "Determining embedding dimension for phase-space reconstruction using a geometrical construction."
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "The issue of determining an acceptable minimum embedding dimension is examined by looking at the behavior of near neighbors under changes in the embedding dimensions from d\\ensuremath{\\rightarrow}d+1 by examining the manner in which noise changes the determination of ${\\mathit{d}}_{\\math it{E}}$."
            },
            "venue": {
                "fragments": [],
                "text": "Physical review. A, Atomic, molecular, and optical physics"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145305161"
                        ],
                        "name": "W. Boothby",
                        "slug": "W.-Boothby",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Boothby",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Boothby"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 120912603,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4cb66cd29535993bbf851dc4f35d5b328b687018",
            "isKey": false,
            "numCitedBy": 1938,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-introduction-to-differentiable-manifolds-and-Boothby",
            "title": {
                "fragments": [],
                "text": "An introduction to differentiable manifolds and Riemannian geometry"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3125657"
                        ],
                        "name": "Phil Knirsch",
                        "slug": "Phil-Knirsch",
                        "structuredName": {
                            "firstName": "Phil",
                            "lastName": "Knirsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Phil Knirsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 35364433,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3be7657c39341bf88e060a7b071a30e75c217c6f",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Kernel-based learning methods provide their solutions as expansions in terms of a kernel. We consider the problem of reducing the computational complexity of evaluating these expansions by approximating them using fewer terms. As a by-product, we point out a connection between clustering and approximation in reproducing kernel Hilbert spaces generated by a particular class of kernels."
            },
            "slug": "Fast-Approximation-of-Support-Vector-Kernel-and-an-Sch\u00f6lkopf-Smola",
            "title": {
                "fragments": [],
                "text": "Fast Approximation of Support Vector Kernel Expansions, and an Interpretation of Clustering as Approximation in Feature Spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This work considers the problem of reducing the computational complexity of evaluating these expansions by approximating them using fewer terms and points out a connection between clustering and approximation in reproducing kernel Hilbert spaces generated by a particular class of kernels."
            },
            "venue": {
                "fragments": [],
                "text": "DAGM-Symposium"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14892653,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "089a76dbc62a06ad30ae1925530e8733e850268e",
            "isKey": false,
            "numCitedBy": 3701,
            "numCiting": 96,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of the approximation of nonlinear mapping, (especially continuous mappings) is considered. Regularization theory and a theoretical framework for approximation (based on regularization techniques) that leads to a class of three-layer networks called regularization networks are discussed. Regularization networks are mathematically related to the radial basis functions, mainly used for strict interpolation tasks. Learning as approximation and learning as hypersurface reconstruction are discussed. Two extensions of the regularization approach are presented, along with the approach's corrections to splines, regularization, Bayes formulation, and clustering. The theory of regularization networks is generalized to a formulation that includes task-dependent clustering and dimensionality reduction. Applications of regularization networks are discussed. >"
            },
            "slug": "Networks-for-approximation-and-learning-Poggio-Girosi",
            "title": {
                "fragments": [],
                "text": "Networks for approximation and learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34641136"
                        ],
                        "name": "J. Bunch",
                        "slug": "J.-Bunch",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Bunch",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bunch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10706913"
                        ],
                        "name": "L. Kaufman",
                        "slug": "L.-Kaufman",
                        "structuredName": {
                            "firstName": "Linda",
                            "lastName": "Kaufman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Kaufman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11913688,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "53bcb63d1b02d795bc170d5a6b690aa1807ce403",
            "isKey": false,
            "numCitedBy": 363,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Several decompositions ofsymmetric matrices for calculating inertia and solving systems of linear equations are discussed. New partial pivoting strategies for decomposing symmetric matrices are introduced and analyzed."
            },
            "slug": "Some-stable-methods-for-calculating-inertia-and-Bunch-Kaufman",
            "title": {
                "fragments": [],
                "text": "Some stable methods for calculating inertia and solving symmetric linear systems"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "Several decompositions of symmetry matrices for calculating inertia and solving systems of linear equations are discussed and new partial pivoting strategies for decomposing symmetric matrices are introduced and analyzed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744288"
                        ],
                        "name": "P. Gill",
                        "slug": "P.-Gill",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Gill",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143873253"
                        ],
                        "name": "W. Murray",
                        "slug": "W.-Murray",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Murray",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Murray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685738"
                        ],
                        "name": "M. H. Wright",
                        "slug": "M.-H.-Wright",
                        "structuredName": {
                            "firstName": "Margaret",
                            "lastName": "Wright",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. H. Wright"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 20611582,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8d9abd1c078573188b13d36c1b1efb7cb2fa865",
            "isKey": false,
            "numCitedBy": 7626,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Practical Optimization MethodsFree eBook: Practical Aspects of Structural Optimization [1701.01450] Practical optimization for hybrid quantum Practical Optimization | 4e70c9cf5faf796993a61adc9f46f2c5Acces PDF Practical OptimizationPractical Bayesian Optimization of Machine Learning Particle Swarm Optimization (PSO) An Overview Practical Issues Optimization Algorithms in Physics Practical Mathematical Optimization Universit T BremenA Practical Price Optimization Approach for Omnichannel A Gentle Introduction to Stochastic Optimization AlgorithmsApplied Sciences | Free Full-Text | Evolutionary 0387986316 Practical Optimization Methods: with A Lecture on Model Predictive ControlPractical Optimization : Algorithms and Engineering Wiley Series in Discrete Mathematics and Optimization Ser PRACTICAL OPTIMIZATION uCozEvolutionary practical optimization | DeepDyveA Practical Guide To Hyperparameter Optimization.Blood platelet production: a novel approach for practical [PDF] Practical Bilevel Optimization Download and Read Stability and Sample-based Approximations of Composite Practical portfolio optimization in Python (2/3) machine (PDF) Practical Financial Optimization. Decision making A Multiobjective Optimization Model for Prevention and Particle swarm optimization WikipediaPractical Methods Of Optimization|RPractical Portfolio Optimization London Business SchoolBao: Making Learned Query Optimization PracticalApache Spark Core Practical Optimization DatabricksPractical Methods of Optimization by R. FletcherChapter 11 Nonlinear Optimization Examples4.7 Applied Optimization Problems \u2013 Calculus Volume 1Practical bayesian optimization using Goptuna | by Masashi Practical Optimization Methods For 4th Generation Cellular Facility location problems \u2014 Mathematical Optimization Practical optimization (2004 edition) | Open Library[J726.Ebook] PDF Download Practical Optimization of Multi-objective Exploration for Practical Optimization Practical Optimization: a Gentle Introduction has moved!?Practical Rod Pumping Optimization on Apple Books(PDF) Practical Optimization with MATLAB The Free StudyPractical portfolio optimization in Python (3/3) code (PDF) Practical, Fast and Robust Point Cloud Registration Numerical Optimization Stanford UniversityPractical Optimization Methods with Mathematica ApplicationsPractical Optimization | 4e70c9cf5faf796993a61adc9f46f2c5Search Engine Optimization: Practical Marketing TechniquesLagout.orgMeter Placement in Active Distribution System using Manual: Practical guide to optimization for mobiles Unity"
            },
            "slug": "Practical-optimization-Gill-Murray",
            "title": {
                "fragments": [],
                "text": "Practical optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This ebook Practical Optimization by Philip E. Gill is presented in pdf format and the full version of this ebook in DjVu, ePub, doc, txt, PDF forms is presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69924093"
                        ],
                        "name": "S. Hyakin",
                        "slug": "S.-Hyakin",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Hyakin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hyakin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60577818,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "045310b06e8a3983a363a118cc9dcc3f292970b4",
            "isKey": false,
            "numCitedBy": 9869,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Simon Haykin Neural Networks A Comprehensive Foundation. Neural Networks A Comprehensive Foundation Simon S. Neural Networks A Comprehensive Foundation Simon S. Neural Networks A Comprehensive Foundation. Neural Networks Association for Computing Machinery. Book Review Neural Networks A Comprehensive Foundation. Neural Networks A Comprehensive Foundation Pearson. Neural networks a comprehensive foundation. Neural Networks a Comprehensive Foundation AbeBooks. Neural networks a comprehensive foundation solutions. cdn preterhuman net. Neural Networks A Comprehensive Foundation Goodreads. Neural Networks A Comprehensive Foundation Amazon it. Neural Networks A Comprehensive Foundation Amazon co uk. Neural Networks A Comprehensive Foundation 3rd Edition. Neural Networks A Comprehensive Foundation Simon. Neural Networks A Comprehensive Foundation amazon com. Neural networks a comprehensive foundation Academia edu. Neural Networks A Comprehensive Foundation Amazon. neural networks a comprehensive foundation simon haykin. Simon Haykin Neural Networks A Comprehensive Foundation. Neural Networks A comprehensive Foundation 2 ed. Simon haykin neural networks a comprehensive foundation pdf. Buy Neural Networks A Comprehensive Foundation Book. Neural networks a comprehensive foundation 2e book. Neural Networks A Comprehensive Foundation. NEURAL NETWORKS A COMPREHENSIVE FOUNDATION SIMON. Neural Networks a Comprehensive Foundation by Haykin Simon. Neural Networks A Comprehensive Foundation pdf PDF Drive. Neural Networks A Comprehensive Foundation amazon ca. Simon Haykin Neural Networks A Comprehensive Foundation. NEURAL NETWORKS A Comprehensive Foundation PDF. Neural Networks A Comprehensive Foundation pdf PDF Drive. Neural Networks A Comprehensive Foundation by Haykin. Neural Networks A Comprehensive Foundation 3rd Edition. Neural Networks A Comprehensive Foundation Simon S. Neural Networks A Comprehensive Foundation. Neural networks a comprehensive foundation Book 1994. Neural Networks A Comprehensive Foundation 2nd Edition. Neural Networks A Comprehensive Foundation S S Haykin. Neural Networks A Comprehensive Foundation International. Neural Networks A Comprehensive Foundation 2 e Pearson. Download Neural Networks A Comprehensive Foundation 2Nd. Neural Networks A comprehensive foundation Aalto"
            },
            "slug": "Neural-Networks:-A-Comprehensive-Foundation-Hyakin",
            "title": {
                "fragments": [],
                "text": "Neural Networks: A Comprehensive Foundation"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "Simon Haykin Neural Networks A Comprehensive Foundation Simon S. Haykin neural networks a comprehensive foundation pdf PDF Drive."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12946615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b07ce649d6f6eb636872527104b0209d3edc8188",
            "isKey": false,
            "numCitedBy": 16925,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Provides a unified, comprehensive and up-to-date treatment of both statistical and descriptive methods for pattern recognition. The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "slug": "Pattern-classification-and-scene-analysis-Duda-Hart",
            "title": {
                "fragments": [],
                "text": "Pattern classification and scene analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "venue": {
                "fragments": [],
                "text": "A Wiley-Interscience publication"
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 240
                            }
                        ],
                        "text": "Recent results on applying learn-ing theory to decision trees show that there is a tradeo between the structuralcomplexity of a tree, i.e. the depth and number of nodes, and the complexity of thedecisions that are used (Golea et al., 1998; Shawe-Taylor and Cristianini, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 131
                            }
                        ],
                        "text": "Wealso know that for a given tree structure and empirical risk, decisions with largermargins should produce better generalization (Shawe-Taylor and Cristianini, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17633086,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9d04e24bb9bd5050541f7b537ba9ecbfb098e57",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Perceptron Decision Trees (also known as Linear Machine DTs, etc.) are analysed in order that data-dependent Structural Risk Minimisation can be applied. Data-dependent analysis is performed which indicates that choosing the maximal margin hyperplanes at the decision nodes will improve the generalization. The analysis uses a novel technique to bound the generalization error in terms of the margins at individual nodes. Experiments performed on real data sets confirm the validity of the approach."
            },
            "slug": "Data-Dependent-Structural-Risk-Minimization-for-Shawe-Taylor-Cristianini",
            "title": {
                "fragments": [],
                "text": "Data-Dependent Structural Risk Minimization for Perceptron Decision Trees"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "Perceptron Decision Trees (also known as Linear Machine DTs, etc.) are analysed in order that data-dependent Structural Risk Minimisation can be applied and it is indicated that choosing the maximal margin hyperplanes at the decision nodes will improve the generalization."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16577977,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "3e00dd12caea7c4dab1633a35d1da3cb2e76b420",
            "isKey": false,
            "numCitedBy": 2357,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "A simple linear neuron model with constrained Hebbian-type synaptic modification is analyzed and a new class of unconstrained learning rules is derived. It is shown that the model neuron tends to extract the principal component from a stationary input vector sequence."
            },
            "slug": "Simplified-neuron-model-as-a-principal-component-Oja",
            "title": {
                "fragments": [],
                "text": "Simplified neuron model as a principal component analyzer"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A simple linear neuron model with constrained Hebbian-type synaptic modification is analyzed and a new class of unconstrained learning rules is derived."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of mathematical biology"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35224059"
                        ],
                        "name": "S. Golowich",
                        "slug": "S.-Golowich",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Golowich",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Golowich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 19196574,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43ffa2c1a06a76e58a333f2e7d0bd498b24365ca",
            "isKey": false,
            "numCitedBy": 2601,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "The Support Vector (SV) method was recently proposed for estimating regressions, constructing multidimensional splines, and solving linear operator equations [Vapnik, 1995]. In this presentation we report results of applying the SV method to these problems."
            },
            "slug": "Support-Vector-Method-for-Function-Approximation,-Vapnik-Golowich",
            "title": {
                "fragments": [],
                "text": "Support Vector Method for Function Approximation, Regression Estimation and Signal Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This presentation reports results of applying the Support Vector method to problems of estimating regressions, constructing multidimensional splines, and solving linear operator equations."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11382731,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8314dda1ec43ce57ff877f8f02ed89acb68ca035",
            "isKey": false,
            "numCitedBy": 581,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Memory-based classification algorithms such as radial basis functions or K-nearest neighbors typically rely on simple distances (Euclidean, dot product...), which are not particularly meaningful on pattern vectors. More complex, better suited distance measures are often expensive and rather ad-hoc (elastic matching, deformable templates). We propose a new distance measure which (a) can be made locally invariant to any set of transformations of the input and (b) can be computed efficiently. We tested the method on large handwritten character databases provided by the Post Office and the NIST. Using invariances with respect to translation, rotation, scaling, shearing and line thickness, the method consistently outperformed all other systems tested on the same databases."
            },
            "slug": "Efficient-Pattern-Recognition-Using-a-New-Distance-Simard-LeCun",
            "title": {
                "fragments": [],
                "text": "Efficient Pattern Recognition Using a New Transformation Distance"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new distance measure which can be made locally invariant to any set of transformations of the input and can be computed efficiently is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786646"
                        ],
                        "name": "J. Eckmann",
                        "slug": "J.-Eckmann",
                        "structuredName": {
                            "firstName": "Jean-Pierre",
                            "lastName": "Eckmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Eckmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2707351"
                        ],
                        "name": "D. Ruelle",
                        "slug": "D.-Ruelle",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Ruelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ruelle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18330392,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "3a7800c2800aee5fc34f6a6b68f0402ae95fd474",
            "isKey": false,
            "numCitedBy": 4336,
            "numCiting": 174,
            "paperAbstract": {
                "fragments": [],
                "text": "Physical and numerical experiments show that deterministic noise, or chaos, is ubiquitous. While a good understanding of the onset of chaos has been achieved, using as a mathematical tool the geometric theory of differentiable dynamical systems, moderately excited chaotic systems require new tools, which are provided by the ergodic theory of dynamical systems. This theory has reached a stage where fruitful contact and exchange with physical experiments has become widespread. The present review is an account of the main mathematical ideas and their concrete implementation in analyzing experiments. The main subjects are the theory of dimensions (number of excited degrees of freedom), entropy (production of information), and characteristic exponents (describing sensitivity to initial conditions). The relations between these quantities, as well as their experimental determination, are discussed. The systematic investigation of these quantities provides us for the first time with a reasonable understanding of dynamical systems, excited well beyond the quasiperiodic regimes. This is another step towards understanding highly turbulent fluids."
            },
            "slug": "Ergodic-theory-of-chaos-and-strange-attractors-Eckmann-Ruelle",
            "title": {
                "fragments": [],
                "text": "Ergodic theory of chaos and strange attractors"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6635519,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4abd4e51705e74f1739bd3a1e47ac10e45f6468b",
            "isKey": false,
            "numCitedBy": 1170,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning an input-output mapping from a set of examples, of the type that many neural networks have been constructed to perform, can be regarded as synthesizing an approximation of a multidimensional function (that is, solving the problem of hypersurface reconstruction). From this point of view, this form of learning is closely related to classical approximation techniques, such as generalized splines and regularization theory. A theory is reported that shows the equivalence between regularization and a class of three-layer networks called regularization networks or hyper basis functions. These networks are not only equivalent to generalized splines but are also closely related to the classical radial basis functions used for interpolation tasks and to several pattern recognition and neural network algorithms. They also have an interesting interpretation in terms of prototypes that are synthesized and optimally combined during the learning stage."
            },
            "slug": "Regularization-Algorithms-for-Learning-That-Are-to-Poggio-Girosi",
            "title": {
                "fragments": [],
                "text": "Regularization Algorithms for Learning That Are Equivalent to Multilayer Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A theory is reported that shows the equivalence between regularization and a class of three-layer networks called regularization networks or hyper basis functions."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2637505"
                        ],
                        "name": "D. Cox",
                        "slug": "D.-Cox",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "Cox",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1423299574"
                        ],
                        "name": "F. O\u2019Sullivan",
                        "slug": "F.-O\u2019Sullivan",
                        "structuredName": {
                            "firstName": "Finbarr",
                            "lastName": "O\u2019Sullivan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. O\u2019Sullivan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 120908503,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5bf249445466ac041694525d8969d7879cd259ce",
            "isKey": false,
            "numCitedBy": 225,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "A general approach to the first order asymptotic analysis ofpenalized likelihood and related estimators is described. The method gives expansions for the systematic and random error. Asymptotic convergence rates in a family of spectral norms are obtained. The theory applies to a broad range of function estimation prob~erns including non\"paxametric. dellSity, hazard and generalized regression curve estimation. Some examples are provided. AMS 1980 subject classifications. Primary, 62-G05, Secondary, 62J05, 41-A35, 41-A25, 47-A53, 45-LlO, 45-M05."
            },
            "slug": "Asymptotic-Analysis-of-Penalized-Likelihood-and-Cox-O\u2019Sullivan",
            "title": {
                "fragments": [],
                "text": "Asymptotic Analysis of Penalized Likelihood and Related Estimators"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746242"
                        ],
                        "name": "S. Mallat",
                        "slug": "S.-Mallat",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Mallat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mallat"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 122523543,
            "fieldsOfStudy": [
                "Geology",
                "Computer Science"
            ],
            "id": "d30e8f3e565d4a9df831875c383687507606d4f0",
            "isKey": false,
            "numCitedBy": 17946,
            "numCiting": 381,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-wavelet-tour-of-signal-processing-Mallat",
            "title": {
                "fragments": [],
                "text": "A wavelet tour of signal processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6082464,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d27c7569fdbcbb57ff511f5293e32b547acca7b3",
            "isKey": false,
            "numCitedBy": 572,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "This article shows a relationship between two different approximation techniques: the support vector machines (SVM), proposed by V. Vapnik (1995) and a sparse approximation scheme that resembles the basis pursuit denoising algorithm (Chen, 1995; Chen, Donoho, & Saunders, 1995). SVM is a technique that can be derived from the structural risk minimization principle (Vapnik, 1982) and can be used to estimate the parameters of several different approximation schemes, including radial basis functions, algebraic and trigonometric polynomials, B-splines, and some forms of multilayer perceptrons. Basis pursuit denoising is a sparse approximation technique in which a function is reconstructed by using a small number of basis functions chosen from a large set (the dictionary). We show that if the data are noiseless, the modified version of basis pursuit denoising proposed in this article is equivalent to SVM in the following sense: if applied to the same data set, the two techniques give the same solution, which is obtained by solving the same quadratic programming problem. In the appendix, we present a derivation of the SVM technique in the framework of regularization theory, rather than statistical learning theory, establishing a connection between SVM, sparse approximation, and regularization theory."
            },
            "slug": "An-Equivalence-Between-Sparse-Approximation-and-Girosi",
            "title": {
                "fragments": [],
                "text": "An Equivalence Between Sparse Approximation and Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "If the data are noiseless, the modified version of basis pursuit denoising proposed in this article is equivalent to SVM in the following sense: if applied to the same data set, the two techniques give the same solution, which is obtained by solving the same quadratic programming problem."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781874"
                        ],
                        "name": "E. Osuna",
                        "slug": "E.-Osuna",
                        "structuredName": {
                            "firstName": "Edgar",
                            "lastName": "Osuna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Osuna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771659"
                        ],
                        "name": "R. Freund",
                        "slug": "R.-Freund",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Freund",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2845602,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9008cdacbdcff8a218a6928e94fe7c6dfc237b24",
            "isKey": false,
            "numCitedBy": 2841,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the application of Support Vector Machines (SVMs) in computer vision. SVM is a learning technique developed by V. Vapnik and his team (AT&T Bell Labs., 1985) that can be seen as a new method for training polynomial, neural network, or Radial Basis Functions classifiers. The decision surfaces are found by solving a linearly constrained quadratic programming problem. This optimization problem is challenging because the quadratic form is completely dense and the memory requirements grow with the square of the number of data points. We present a decomposition algorithm that guarantees global optimality, and can be used to train SVM's over very large data sets. The main idea behind the decomposition is the iterative solution of sub-problems and the evaluation of optimality conditions which are used both to generate improved iterative values, and also establish the stopping criteria for the algorithm. We present experimental results of our implementation of SVM, and demonstrate the feasibility of our approach on a face detection problem that involves a data set of 50,000 data points."
            },
            "slug": "Training-support-vector-machines:-an-application-to-Osuna-Freund",
            "title": {
                "fragments": [],
                "text": "Training support vector machines: an application to face detection"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A decomposition algorithm that guarantees global optimality, and can be used to train SVM's over very large data sets is presented, and the feasibility of the approach on a face detection problem that involves a data set of 50,000 data points is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6298480,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a769ec3a8fb442548beeafa9b5e0d71661b195ac",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we initiate an investigation of generalizations of the Probably Approximately Correct (PAC) learning model that attempt to signi cantly weaken the target function assumptions. The ultimate goal in this direction is informally termed agnostic learning, in which we make virtually no assumptions on the target function. The name derives from the fact that as designers of learning algorithms, we give up the belief that Nature (as represented by the target function) has a simple or succinct explanation. We give a number of positive and negative results that provide an initial outline of the possibilities for agnostic learning. Our results include hardness results for the most obvious generalization of the PAC model to an agnostic setting, an e cient and general agnostic learning method based on dynamic programming, relationships between loss functions for agnostic learning, and an algorithm for a learning problem that involves hidden variables."
            },
            "slug": "Toward-Eecient-Agnostic-Learning-Schapire",
            "title": {
                "fragments": [],
                "text": "Toward Eecient Agnostic Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Results include hardness results for the most obvious generalization of the PAC model to an agnostic setting, an e cient and general agnostic learning method based on dynamic programming, relationships between loss functions for agnosticLearning, and an algorithm for a learning problem that involves hidden variables."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704699"
                        ],
                        "name": "M. Pontil",
                        "slug": "M.-Pontil",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Pontil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pontil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716824"
                        ],
                        "name": "A. Verri",
                        "slug": "A.-Verri",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Verri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Verri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6796001,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "0ebe1584dab15c93dbebe8f5bc61a02972062d86",
            "isKey": false,
            "numCitedBy": 214,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Support vector machines (SVMs) perform pattern recognition between two point classes by finding a decision surface determined by certain points of the training set, termed support vectors (SV). This surface, which in some feature space of possibly infinite dimension can be regarded as a hyperplane, is obtained from the solution of a problem of quadratic programming that depends on a regularization parameter. In this article, we study some mathematical properties of support vectors and show that the decision surface can be written as the sum of two orthogonal terms, the first depending on only the margin vectors (which are SVs lying on the margin), the second proportional to the regularization parameter. For almost all values of the parameter, this enables us to predict how the decision surface varies for small parameter changes. In the special but important case of feature space of finite dimension m, we also show that there are at most m + 1 margin vectors and observe that m + 1 SVs are usually sufficient to determine the decision surface fully. For relatively small m, this latter result leads to a consistent reduction of the SV number."
            },
            "slug": "Properties-of-Support-Vector-Machines-Pontil-Verri",
            "title": {
                "fragments": [],
                "text": "Properties of Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that the decision surface can be written as the sum of two orthogonal terms, the first depending on only the margin vectors (which are SVs lying on the margin), the second proportional to the regularization parameter for almost all values of the parameter."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143957317"
                        ],
                        "name": "R. C. Williamson",
                        "slug": "R.-C.-Williamson",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Williamson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Williamson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145004630"
                        ],
                        "name": "M. Anthony",
                        "slug": "M.-Anthony",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Anthony",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Anthony"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5280896,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "67f9e3de2fb39f051ef23b8fbed6d72de7b02900",
            "isKey": false,
            "numCitedBy": 105,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper introduces a framework for studying structural risk minimisation. The model views structural risk minimisation in a PAC context. It then considers the more general case when the hierarchy of classes is chosen in response to the data. This theoretically explains the impressive performance of the maximal margin hyperplane algorithm of Vapnik. It may also provide a general technique for exploitingserendipitous simplicity in observed data to obtain better prediction accuracy from small training sets."
            },
            "slug": "A-framework-for-structural-risk-minimisation-Shawe-Taylor-Bartlett",
            "title": {
                "fragments": [],
                "text": "A framework for structural risk minimisation"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "The paper introduces a framework for studying structural risk minimisation in a PAC context and considers the more general case when the hierarchy of classes is chosen in response to the data."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '96"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2976216"
                        ],
                        "name": "P. Sykacek",
                        "slug": "P.-Sykacek",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Sykacek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Sykacek"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 9034881,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "85164f08f15e54f5fc520fe692c105eb1b989292",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Bayesian algorithms for Neural Networks are known to produce classiiers which are very resistent to overrt-ting. It is often claimed that one of the main distinctive features of Bayesian Learning Algorithms is that they don't simply output one hypothesis, but rather an entire distribution of probability over an hypothesis set: the Bayes posterior. An alternative perspective is that they output a linear combination of classiiers, whose coeecients are given by Bayes theorem. One of the concepts used to deal with thresholded convex combinations is th\u00e8margin' of the hyperplane with respect to the training sample, which is correlated to the predictive power of the hypothesis itself. We provide a novel theoretical analysis of such clas-siiers, based on Data-Dependent VC theory, proving that they can be expected to be large margin hyper-planes in a Hilbert space. We then present experimental evidence that the predictions of our model are correct , i.e. that bayesian classifers really nd hypotheses which have large margin on the training examples. This not only explains the remarkable resistance to overrtting exhibited by such classiiers, but also co-locates them in the same class of other systems, like Support Vector machines and Adaboost, which have a similar performance."
            },
            "slug": "Bayesian-Classifiers-Are-Large-Margin-Hyperplanes-a-Cristianini-Shawe-Taylor",
            "title": {
                "fragments": [],
                "text": "Bayesian Classifiers Are Large Margin Hyperplanes in a Hilbert Space"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A novel theoretical analysis of classiiers of Bayesian algorithms for Neural Networks, based on Data-Dependent VC theory, proves that they can be expected to be large margin hyper-planes in a Hilbert space, and presents experimental evidence that the predictions of the model are correct."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1390055667"
                        ],
                        "name": "Dr. M. G. Worster",
                        "slug": "Dr.-M.-G.-Worster",
                        "structuredName": {
                            "firstName": "Dr.",
                            "lastName": "Worster",
                            "middleNames": [
                                "M.",
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dr. M. G. Worster"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 4079036,
            "fieldsOfStudy": [
                "Education",
                "Physics"
            ],
            "id": "757dd5fbc67d8f2591eb2077180af74c1797fcd1",
            "isKey": false,
            "numCitedBy": 2427,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The following people have maintained these notes."
            },
            "slug": "Methods-of-Mathematical-Physics-Worster",
            "title": {
                "fragments": [],
                "text": "Methods of Mathematical Physics"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1947
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6530745,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7abda1941534d3bb558dd959025d67f1df526303",
            "isKey": false,
            "numCitedBy": 792,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Three Bayesian ideas are presented for supervised adaptive classifiers. First, it is argued that the output of a classifier should be obtained by marginalizing over the posterior distribution of the parameters; a simple approximation to this integral is proposed and demonstrated. This involves a \"moderation\" of the most probable classifier's outputs, and yields improved performance. Second, it is demonstrated that the Bayesian framework for model comparison described for regression models in MacKay (1992a,b) can also be applied to classification problems. This framework successfully chooses the magnitude of weight decay terms, and ranks solutions found using different numbers of hidden units. Third, an information-based data selection criterion is derived and demonstrated within this framework."
            },
            "slug": "The-Evidence-Framework-Applied-to-Classification-Mackay",
            "title": {
                "fragments": [],
                "text": "The Evidence Framework Applied to Classification Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is demonstrated that the Bayesian framework for model comparison described for regression models in MacKay (1992a,b) can also be applied to classification problems and an information-based data selection criterion is derived and demonstrated within this framework."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18251470,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "445ad69010658097fc317f7b83f1198179eebae8",
            "isKey": false,
            "numCitedBy": 1840,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper develops the separating capacities of families of nonlinear decision surfaces by a direct application of a theorem in classical combinatorial geometry. It is shown that a family of surfaces having d degrees of freedom has a natural separating capacity of 2d pattern vectors, thus extending and unifying results of Winder and others on the pattern-separating capacity of hyperplanes. Applying these ideas to the vertices of a binary n-cube yields bounds on the number of spherically, quadratically, and, in general, nonlinearly separable Boolean functions of n variables. It is shown that the set of all surfaces which separate a dichotomy of an infinite, random, separable set of pattern vectors can be characterized, on the average, by a subset of only 2d extreme pattern vectors. In addition, the problem of generalizing the classifications on a labeled set of pattern points to the classification of a new point is defined, and it is found that the probability of ambiguous generalization is large unless the number of training patterns exceeds the capacity of the set of separating surfaces."
            },
            "slug": "Geometrical-and-Statistical-Properties-of-Systems-Cover",
            "title": {
                "fragments": [],
                "text": "Geometrical and Statistical Properties of Systems of Linear Inequalities with Applications in Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that a family of surfaces having d degrees of freedom has a natural separating capacity of 2d pattern vectors, thus extending and unifying results of Winder and others on the pattern-separating capacity of hyperplanes."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Electron. Comput."
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052864185"
                        ],
                        "name": "P. Bradley",
                        "slug": "P.-Bradley",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Bradley",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bradley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747026"
                        ],
                        "name": "O. Mangasarian",
                        "slug": "O.-Mangasarian",
                        "structuredName": {
                            "firstName": "Olvi",
                            "lastName": "Mangasarian",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Mangasarian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2562282"
                        ],
                        "name": "W. N. Street",
                        "slug": "W.-N.-Street",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Street",
                            "middleNames": [
                                "Nick"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. N. Street"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 206
                            }
                        ],
                        "text": "\u2026constructingthe best linear discriminant using the minimum number of attributes; and misclas-si cation minimization: explicitly minimizing the number of points misclassi ed(Bradley and Mangasarian, 1998a; Bradley et al., 1995; Bredensteiner and Ben-nett, 1997; Bennett and Bredensteiner, 1997)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2661774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7deeb7b082d3def4b0eb15d879ea576b0a2d9b97",
            "isKey": false,
            "numCitedBy": 218,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of discriminating between two finite point sets in n-dimensional feature space by a separating plane that utilizes as few of the features as possible is formulated as a mathematical program with a parametric objective function and linear constraints. The step function that appears in the objective function can be approximated by a sigmoid or by a concave exponential on the nonnegative real line, or it can be treated exactly by considering the equivalent linear program with equilibrium constraints. Computational tests of these three approaches on publicly available real-world databases have been carried out and compared with an adaptation of the optimal brain damage method for reducing neural network complexity. One feature selection algorithm via concave minimization reduced cross-validation error on a cancer prognosis database by 35.4% while reducing problem features from 32 to 4."
            },
            "slug": "Feature-Selection-via-Mathematical-Programming-Bradley-Mangasarian",
            "title": {
                "fragments": [],
                "text": "Feature Selection via Mathematical Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Computational tests of three approaches to feature selection algorithm via concave minimization on publicly available real-world databases have been carried out and compared with an adaptation of the optimal brain damage method for reducing neural network complexity."
            },
            "venue": {
                "fragments": [],
                "text": "INFORMS J. Comput."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2096974192"
                        ],
                        "name": "lawa Kanas",
                        "slug": "lawa-Kanas",
                        "structuredName": {
                            "firstName": "lawa",
                            "lastName": "Kanas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "lawa Kanas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2301671"
                        ],
                        "name": "A. Lecko",
                        "slug": "A.-Lecko",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Lecko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lecko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3221860"
                        ],
                        "name": "M. Startek",
                        "slug": "M.-Startek",
                        "structuredName": {
                            "firstName": "Mariusz",
                            "lastName": "Startek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Startek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102285934"
                        ],
                        "name": "S. Kanas",
                        "slug": "S.-Kanas",
                        "structuredName": {
                            "firstName": "Stanis",
                            "lastName": "Kanas",
                            "middleNames": [
                                "lawa"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kanas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5203442,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "041003a0719277d0957cfc1af92b0ae9b147d880",
            "isKey": false,
            "numCitedBy": 580,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "The papers [3], [7], [2], [1], [5], [6], and [4] provide the notation and terminology for this paper. We consider metric structures which are systems \u3008 a carrier, a distance \u3009 where the carrier is a non-empty set and the distance is a function from [: the carrier, the carrier :] into . In the sequel M will be a metric structure. Let us consider M . A point of M is an element of the carrier of M . Next we state a proposition (1) For every element x of the carrier of M holds x is a point of M . Let us consider M , and let a, b be elements of the carrier of M . The functor \u03c1(a, b) yielding a real number, is defined by: \u03c1(a, b) = (the distance of M)(a, b). We now state a proposition (2) For all elements x, y of the carrier of M holds \u03c1(x, y) = (the distance of M)(x, y)."
            },
            "slug": "Metric-Spaces-Kanas-Lecko",
            "title": {
                "fragments": [],
                "text": "Metric Spaces"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6884486,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "28667c276ba78ab1d855064d5456d50d9932b775",
            "isKey": false,
            "numCitedBy": 685,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "The main aim of this paper is to provide a tutorial on regression with Gaussian processes. We start from Bayesian linear regression, and show how by a change of viewpoint one can see this method as a Gaussian process predictor based on priors over functions, rather than on priors over parameters. This leads in to a more general discussion of Gaussian processes in section 4. Section 5 deals with further issues, including hierarchical modelling and the setting of the parameters that control the Gaussian process, the covariance functions for neural network models and the use of Gaussian processes in classification problems."
            },
            "slug": "Prediction-with-Gaussian-Processes:-From-Linear-to-Williams",
            "title": {
                "fragments": [],
                "text": "Prediction with Gaussian Processes: From Linear Regression to Linear Prediction and Beyond"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "The main aim of this paper is to provide a tutorial on regression with Gaussian processes, starting from Bayesian linear regression, and showing how by a change of viewpoint one can see this method as a Gaussian process predictor based on priors over functions, rather than on prior over parameters."
            },
            "venue": {
                "fragments": [],
                "text": "Learning in Graphical Models"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712211"
                        ],
                        "name": "G. Golub",
                        "slug": "G.-Golub",
                        "structuredName": {
                            "firstName": "Gene",
                            "lastName": "Golub",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Golub"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2139541"
                        ],
                        "name": "U. V. Matt",
                        "slug": "U.-V.-Matt",
                        "structuredName": {
                            "firstName": "Urs",
                            "lastName": "Matt",
                            "middleNames": [
                                "von"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. V. Matt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121361085,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "60a0ac2427dd8540908d1a675eca3a1c23c8fe33",
            "isKey": false,
            "numCitedBy": 206,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Although generalized cross-validation is a popular tool for calculating a regularization parameter, it has been rarely applied to large-scale problems until recently. A major difficulty lies in the evaluation of the cross-validation function that requires the calculation of the trace of an inverse matrix. In the last few years stochastic trace estimators have been proposed to alleviate this problem. This article demonstrates numerical approximation techniques that further reduce the computational complexity. The new approach employs Gauss quadrature to compute lower and upper bounds on the cross-validation function. It only requires the operator form of the system matrix\u2014that is, a subroutine to evaluate matrix-vector products. Thus, the factorization of large matrices can be avoided. The new approach has been implemented in MATLAB. Numerical experiments confirm the remarkable accuracy of the stochastic trace estimator. Regularization parameters are computed for ill-posed problems with 100, 1,000..."
            },
            "slug": "Generalized-cross-validation-for-large-scale-Golub-Matt",
            "title": {
                "fragments": [],
                "text": "Generalized cross-validation for large scale problems"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This article demonstrates numerical approximation techniques that further reduce the computational complexity of generalized cross-validation and employs Gauss quadrature to compute lower and upper bounds on the cross- validation function."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2022386739"
                        ],
                        "name": "Peter Barlett",
                        "slug": "Peter-Barlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Barlett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Barlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740222"
                        ],
                        "name": "Wee Sun Lee",
                        "slug": "Wee-Sun-Lee",
                        "structuredName": {
                            "firstName": "Wee",
                            "lastName": "Lee",
                            "middleNames": [
                                "Sun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wee Sun Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 573509,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d19272112b50547614479a0c409fca66e3b05f7",
            "isKey": false,
            "numCitedBy": 2844,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the surprising recurring phenomena observed in experiments with boosting is that the test error of the generated classifier usually does not increase as its size becomes very large, and often is observed to decrease even after the training error reaches zero. In this paper, we show that this phenomenon is related to the distribution of margins of the training examples with respect to the generated voting classification rule, where the margin of an example is simply the difference between the number of correct votes and the maximum number of votes received by any incorrect label. We show that techniques used in the analysis of Vapnik's support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error. We also show theoretically and experimentally that boosting is especially effective at increasing the margins of the training examples. Finally, we compare our explanation to those based on the bias-variance"
            },
            "slug": "Boosting-the-margin:-A-new-explanation-for-the-of-Schapire-Freund",
            "title": {
                "fragments": [],
                "text": "Boosting the margin: A new explanation for the effectiveness of voting methods"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that techniques used in the analysis of Vapnik's support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704341"
                        ],
                        "name": "E. Amaldi",
                        "slug": "E.-Amaldi",
                        "structuredName": {
                            "firstName": "Edoardo",
                            "lastName": "Amaldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Amaldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747198"
                        ],
                        "name": "V. Kann",
                        "slug": "V.-Kann",
                        "structuredName": {
                            "firstName": "Viggo",
                            "lastName": "Kann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Kann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026Vapnik, Volodya Vovk & Chris Watkins19 Combining Support Vector and Mathematical ProgrammingMethods for Classi cation 307Kristin P. Bennett20 Kernel Principal Component Analysis 327Bernhard Sch olkopf, Alex J. Smola & Klaus-Robert M uller\n1998/08/25 16:31\nviiReferences 353Index 373\n1998/08/25 16:31"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 26
                            }
                        ],
                        "text": "Both areNP-Hard problems (Amaldi and Kann, 1998, 1995), but approximate answers maybe found using nonconvex optimization techniques."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15938512,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "61a65e0d94160135b96cf7b484344449b53f621d",
            "isKey": false,
            "numCitedBy": 206,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Complexity-and-Approximability-of-Finding-of-Amaldi-Kann",
            "title": {
                "fragments": [],
                "text": "The Complexity and Approximability of Finding Maximum Feasible Subsystems of Linear Relations"
            },
            "venue": {
                "fragments": [],
                "text": "Theor. Comput. Sci."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6636078,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7ec8029e5855b6efbac161488a2e68f83298091c",
            "isKey": false,
            "numCitedBy": 650,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We report a novel possibility for extracting a small subset of a data base which contains all the information necessary to solve a given classification task: using the Support Vector Algorithm to train three different types of handwritten digit classifiers, we observed that these types of classifiers construct their decision surface from strongly overlapping small (\u2248 4%) subsets of the data base. This finding opens up the possibility of compressing data bases significantly by disposing of the data which is not important for the solution of a given task. \n \nIn addition, we show that the theory allows us to predict the classifier that will have the best generalization ability, based solely on performance on the training set and characteristics of the learning machines. This finding is important for cases where the amount of available data is limited."
            },
            "slug": "Extracting-Support-Data-for-a-Given-Task-Sch\u00f6lkopf-Burges",
            "title": {
                "fragments": [],
                "text": "Extracting Support Data for a Given Task"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is observed that three different types of handwritten digit classifiers construct their decision surface from strongly overlapping small subsets of the data base, which opens up the possibility of compressing data bases significantly by disposing of theData which is not important for the solution of a given task."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15883988,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0f2433c088591d265891231f1c22424047f1bc1",
            "isKey": false,
            "numCitedBy": 254,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks. The framework makes possible: (1) objective comparisons between solutions using alternative network architectures; (2) objective stopping rules for deletion of weights; (3) objective choice of magnitude and type of weight decay terms or additive regularisers (for penalising large weights, etc.); (4) a measure of the e ective number of well{determined parameters in a model; (5) quanti ed estimates of the error bars on network parameters and on network output; (6) objective comparisons with alternative learning and interpolation models such as splines and radial basis functions. The Bayesian `evidence' automatically embodies `Occam's razor,' penalising over{ exible and over{complex architectures. The Bayesian approach helps detect poor underlying assumptions in learning models. For learning models well{ matched to a problem, a good correlation between generalisation ability and the Bayesian evidence is obtained."
            },
            "slug": "A-Practical-Bayesian-Framework-for-Backprop-Mackay",
            "title": {
                "fragments": [],
                "text": "A Practical Bayesian Framework for Backprop Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks and a good correlation between generalisation ability and the Bayesian evidence is obtained."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1986103"
                        ],
                        "name": "R. Vanderbei",
                        "slug": "R.-Vanderbei",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Vanderbei",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Vanderbei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 80868,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "ff9b1158ff89903e7d8e21ebad37d0906bdf3cf0",
            "isKey": false,
            "numCitedBy": 550,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a software package, called LOQO, which implements a primal-dual interior-point method for general nonlinear programming. We focus in this paper mainly on the algorithm as it applies to linear and quadratic programming with only brief mention of the extensions to convex and general nonlinear programming, since a detailed paper describing these extensions was published recently elsewhere. In particular, we emphasize the importance of establishing and maintaining symmetric quasidefiniteness of the reduced KKT system. We show that the industry standard MPS format can be nicely formulated in such a way to provide quasidefiniteness. Computational results are included for a variety of linear and quadratic programming problems."
            },
            "slug": "LOQO:an-interior-point-code-for-quadratic-Vanderbei",
            "title": {
                "fragments": [],
                "text": "LOQO:an interior point code for quadratic programming"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This paper describes a software package, called LOQO, which implements a primal-dual interior-point method for general nonlinear programming, and shows that the industry standard MPS format can be nicely formulated in such a way to provide quasidefiniteness."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050470845"
                        ],
                        "name": "H. Drucker",
                        "slug": "H.-Drucker",
                        "structuredName": {
                            "firstName": "Harris",
                            "lastName": "Drucker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Drucker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10706913"
                        ],
                        "name": "L. Kaufman",
                        "slug": "L.-Kaufman",
                        "structuredName": {
                            "firstName": "Linda",
                            "lastName": "Kaufman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Kaufman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 743542,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e52fb14e4beccc5e88a33c1fe5c7d6e780831ae1",
            "isKey": false,
            "numCitedBy": 3695,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "A new regression technique based on Vapnik's concept of support vectors is introduced. We compare support vector regression (SVR) with a committee regression technique (bagging) based on regression trees and ridge regression done in feature space. On the basis of these experiments, it is expected that SVR will have advantages in high dimensionality space because SVR optimization does not depend on the dimensionality of the input space."
            },
            "slug": "Support-Vector-Regression-Machines-Drucker-Burges",
            "title": {
                "fragments": [],
                "text": "Support Vector Regression Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "This work compares support vector regression (SVR) with a committee regression technique (bagging) based on regression trees and ridge regression done in feature space and expects that SVR will have advantages in high dimensionality space because SVR optimization does not depend on the dimensionality of the input space."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3224336"
                        ],
                        "name": "L. Niklasson",
                        "slug": "L.-Niklasson",
                        "structuredName": {
                            "firstName": "Lars",
                            "lastName": "Niklasson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Niklasson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144867541"
                        ],
                        "name": "M. Bod\u00e9n",
                        "slug": "M.-Bod\u00e9n",
                        "structuredName": {
                            "firstName": "Mikael",
                            "lastName": "Bod\u00e9n",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bod\u00e9n"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2491309"
                        ],
                        "name": "T. Ziemke",
                        "slug": "T.-Ziemke",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Ziemke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ziemke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18411295,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8fcfa240acf5d462793d708bb85872182211caf8",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The concept of Support Vector Regression is extended to a more general class of convex cost functions. It is shown how the resulting convex constrained optimization problems can be efficiently solved by a Primal-Dual Interior Point path following method. Both computational feasibility and improvement of estimation is demonstrated in the experiments."
            },
            "slug": "Convex-Cost-Functions-for-Support-Vector-Regression-Smola-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "Convex Cost Functions for Support Vector Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "The concept of Support Vector Regression is extended and it is shown how the resulting convex constrained optimization problems can be efficiently solved by a Primal-Dual Interior Point path following method."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2459012"
                        ],
                        "name": "S. Mika",
                        "slug": "S.-Mika",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Mika",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mika"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2414086"
                        ],
                        "name": "G. R\u00e4tsch",
                        "slug": "G.-R\u00e4tsch",
                        "structuredName": {
                            "firstName": "Gunnar",
                            "lastName": "R\u00e4tsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. R\u00e4tsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 11828650,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7b6b146566b4c55ec0af9589456f4745c8ce3e38",
            "isKey": false,
            "numCitedBy": 128,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Algorithms based on Mercer kernels construct their solutions in terms of expansions in a high-dimensional feature space F. Previous work has shown that all algorithms which can be formulated in terms of dot products in F can be performed using a kernel without explicitly working in F. The list of such algorithms includes support vector machines and nonlinear kernel principal component extraction. So far, however, it did not include the reconstruction of patterns from their largest nonlinear principal components, a technique which is common practice in linear principal component analysis."
            },
            "slug": "Kernel-PCA-pattern-reconstruction-via-approximate-Sch\u00f6lkopf-Mika",
            "title": {
                "fragments": [],
                "text": "Kernel PCA pattern reconstruction via approximate pre-images."
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work has shown that the reconstruction of patterns from their largest nonlinear principal components, a technique which is common practice in linear principal component analysis, can be performed using a kernel without explicitly working in F."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143957317"
                        ],
                        "name": "R. C. Williamson",
                        "slug": "R.-C.-Williamson",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Williamson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Williamson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 777816,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ee177aacf6b3697d079579ce558cdb2ee58cee39",
            "isKey": false,
            "numCitedBy": 192,
            "numCiting": 104,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive new bounds for the generalization error of kernel machines, such as support vector machines and related regularization networks by obtaining new bounds on their covering numbers. The proofs make use of a viewpoint that is apparently novel in the field of statistical learning theory. The hypothesis class is described in terms of a linear operator mapping from a possibly infinite-dimensional unit ball in feature space into a finite-dimensional space. The covering numbers of the class are then determined via the entropy numbers of the operator. These numbers, which characterize the degree of compactness of the operator can be bounded in terms of the eigenvalues of an integral operator induced by the kernel function used by the machine. As a consequence, we are able to theoretically explain the effect of the choice of kernel function on the generalization performance of support vector machines."
            },
            "slug": "Generalization-performance-of-regularization-and-of-Williamson-Smola",
            "title": {
                "fragments": [],
                "text": "Generalization performance of regularization networks and support vector machines via entropy numbers of compact operators"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "New bounds for the generalization error of kernel machines, such as support vector machines and related regularization networks, are derived by obtaining new bounds on their covering numbers by using the eigenvalues of an integral operator induced by the kernel function used by the machine."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3472959"
                        ],
                        "name": "C. Rasmussen",
                        "slug": "C.-Rasmussen",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Rasmussen",
                            "middleNames": [
                                "Edward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rasmussen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16685561,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f49a73c42be6dbd851af4599d9911ea1d6ac7f4",
            "isKey": false,
            "numCitedBy": 495,
            "numCiting": 160,
            "paperAbstract": {
                "fragments": [],
                "text": "This thesis develops two Bayesian learning methods relying on Gaussian processes and a rigorous statistical approach for evaluating such methods. In these experimental designs the sources of uncertainty in the estimated generalisation performances due to both variation in training and test sets are accounted for. The framework allows for estimation of generalisation performance as well as statistical tests of significance for pairwise comparisons. Two experimental designs are recommended and supported by the DELVE software environment. \nTwo new non-parametric Bayesian learning methods relying on Gaussian process priors over functions are developed. These priors are controlled by hyperparameters which set the characteristic length scale for each input dimension. In the simplest method, these parameters are fit from the data using optimization. In the second, fully Bayesian method, a Markov chain Monte Carlo technique is used to integrate over the hyperparameters. One advantage of these Gaussian process methods is that the priors and hyperparameters of the trained models are easy to interpret. \nThe Gaussian process methods are benchmarked against several other methods, on regression tasks using both real data and data generated from realistic simulations. The experiments show that small datasets are unsuitable for benchmarking purposes because the uncertainties in performance measurements are large. A second set of experiments provide strong evidence that the bagging procedure is advantageous for the Multivariate Adaptive Regression Splines (MARS) method. \nThe simulated datasets have controlled characteristics which make them useful for understanding the relationship between properties of the dataset and the performance of different methods. The dependency of the performance on available computation time is also investigated. It is shown that a Bayesian approach to learning in multi-layer perceptron neural networks achieves better performance than the commonly used early stopping procedure, even for reasonably short amounts of computation time. The Gaussian process methods are shown to consistently outperform the more conventional methods."
            },
            "slug": "Evaluation-of-gaussian-processes-and-other-methods-Rasmussen",
            "title": {
                "fragments": [],
                "text": "Evaluation of gaussian processes and other methods for non-linear regression"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that a Bayesian approach to learning in multi-layer perceptron neural networks achieves better performance than the commonly used early stopping procedure, even for reasonably short amounts of computation time."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2633352"
                        ],
                        "name": "J. Kohlmorgen",
                        "slug": "J.-Kohlmorgen",
                        "structuredName": {
                            "firstName": "Jens",
                            "lastName": "Kohlmorgen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kohlmorgen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144902080"
                        ],
                        "name": "K. Pawelzik",
                        "slug": "K.-Pawelzik",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Pawelzik",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Pawelzik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8000182,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "429363cd377f880838b6b6589ab5ab02a4b726ae",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for the analysis of nonstationary time series with multiple operating modes. In particular, it is possible to detect and to model both a switching of the dynamics and a less abrupt, time consuming drift from one mode to another. This is achieved in two steps. First, an unsupervised training method provides prediction experts for the inherent dynamical modes. Then, the trained experts are used in a hidden Markov model that allows to model drifts. An application to physiological wake/sleep data demonstrates that analysis and modeling of real-world time series can be improved when the drift paradigm is taken into account."
            },
            "slug": "Analysis-of-Drifting-Dynamics-with-Neural-Network-Kohlmorgen-M\u00fcller",
            "title": {
                "fragments": [],
                "text": "Analysis of Drifting Dynamics with Neural Network Hidden Markov Models"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is possible to detect and to model both a switching of the dynamics and a less abrupt, time consuming drift from one mode to another in nonstationary time series with multiple operating modes."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3463572"
                        ],
                        "name": "M. O. Stitson",
                        "slug": "M.-O.-Stitson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Stitson",
                            "middleNames": [
                                "Oliver"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. O. Stitson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793317"
                        ],
                        "name": "A. Gammerman",
                        "slug": "A.-Gammerman",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Gammerman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gammerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145675281"
                        ],
                        "name": "V. Vovk",
                        "slug": "V.-Vovk",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vovk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vovk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4562073"
                        ],
                        "name": "C. Watkins",
                        "slug": "C.-Watkins",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Watkins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Watkins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 118468304,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "221663c3ec94babfaa0754a00d92ff69e2a4424a",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Support Vector Machines using ANOVA Decomposition Kernels (SVAD) [Vapng] are a way of imposing a structure on multi-dimensional kernels which are generated as the tensor product of one-dimensional kernels. This gives more accurate control over the capacity of the learning machine (VCdimension). SVAD uses ideas from ANOVA decomposition methods and extends them to generate kernels which directly implement these ideas. SVAD is used with spline kernels and results show that SVAD performs better than the respective non ANOVA decomposition kernel. The Boston housing data set from UCI has been tested on Bagging [Bre94] and Support Vector methods before [DBK97] and these results are compared to the SVAD method."
            },
            "slug": "Support-vector-regression-with-ANOVA-decomposition-Stitson-Gammerman",
            "title": {
                "fragments": [],
                "text": "Support vector regression with ANOVA decomposition kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "Support Vector Machines using ANOVA Decomposition Kernels (SVAD) is used with spline kernels and results show that SVAD performs better than the respective non ANOVA decomposition kernel."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40770452"
                        ],
                        "name": "L. Goddard",
                        "slug": "L.-Goddard",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Goddard",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Goddard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4145017,
            "fieldsOfStudy": [
                "Political Science"
            ],
            "id": "b34dcf8fbef631ea4419795fe66756de02fe7e26",
            "isKey": false,
            "numCitedBy": 3093,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Information TheoryPapers read at a Symposium on Information Theory held at the Royal Institution, London, August 29th to September 2nd, 1960. Edited by Colin Cherry. Pp. xi + 476. (London: Butterworth and Co. (Publishers), Ltd., 1961.) 95s."
            },
            "slug": "Information-Theory-Goddard",
            "title": {
                "fragments": [],
                "text": "Information Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "117055543"
                        ],
                        "name": "M\u00fcller K-R Burges C Smola Aj",
                        "slug": "M\u00fcller-K-R-Burges-C-Smola-Aj",
                        "structuredName": {
                            "firstName": "M\u00fcller",
                            "lastName": "Smola Aj",
                            "middleNames": [
                                "K-R",
                                "Burges",
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M\u00fcller K-R Burges C Smola Aj"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102944324"
                        ],
                        "name": "Vapnik",
                        "slug": "Vapnik",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Vapnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vapnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145698858"
                        ],
                        "name": "T. Downs",
                        "slug": "T.-Downs",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Downs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Downs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40073871"
                        ],
                        "name": "Marcus Frean",
                        "slug": "Marcus-Frean",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Frean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcus Frean"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36589297"
                        ],
                        "name": "M. Gallagher",
                        "slug": "M.-Gallagher",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Gallagher",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gallagher"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18522889,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "caebbd9fd6db26de0afb01ec8a39c4c63c28e399",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "The last years have witnessed an increasing interest in Support Vector (SV) machines, which use Mercer kernels for eeciently performing computations in high-dimensional spaces. In pattern recognition, the SV algorithm constructs nonlinear decision functions by training a classiier to perform a linear separation in some high-dimensional space which is nonlinearly related to input space. Recently, we have developed a technique for Nonlinear Principal Component Analysis (Kernel PCA) based on the same types of kernels. This way, we can for instance eeciently extract polynomial features of arbitrary order by computing projections onto principal components in the space of all products of n pixels of images. We explain the idea of Mercer kernels and associated feature spaces, and describe connections to the theory of reproducing kernels and to regularization theory, followed by an overview of the above algorithms employing these kernels."
            },
            "slug": "Support-Vector-methods-in-learning-and-feature-Sch\u00f6lkopf-Aj",
            "title": {
                "fragments": [],
                "text": "Support Vector methods in learning and feature extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The idea of Mercer kernels and associated feature spaces are explained, connections to the theory of reproducing kernels and to regularization theory are described, and a technique for Nonlinear Principal Component Analysis (Kernel PCA) based on the same types of kernels is developed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057622896"
                        ],
                        "name": "Dong Xiang",
                        "slug": "Dong-Xiang",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Xiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Xiang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 116101450,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71c8a0da0c7450cf37ffb39b56507f677eed66ae",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "1996 i Acknowledgements I would like to express my deepest gratitude to my advisor, Professor Grace Wahba, for her invaluable advice during the course of this dissertation. I would also like to thank the members of my nal exam committee, ProfessorsMadison for its nancial support and excellent computer facilities. Finally, I would like to convey my sincerest regards to my parents and sister for their love and support. ii Abstract We consider the application of the smoothing spline to the generalized linear model in large data set situations. First we derive a Generalized Approximate Cross Validation function (GACV), which is an approximate leave-out-one cross validation function used to choose smoothing parameters. In order to apply the GACV function to a large data set situation, we propose a corresponding randomized version of it. To reduce the computational intensity of calculating the smoothing spline estimate, we suggest an approximate solution and a clustering method to choose a subset of the basis functions. Combining randomized GACV with this approximate solution, we apply it to binary response data from the Wisconsin Epidemiological Study of Diabetic Retinopathy in order to establish the accuracy of the model when applied to a large data set. iii Contents Acknowledgements i Abstract ii 1 Introduction 1"
            },
            "slug": "Model-Fitting-and-Testing-for-Non-Gaussian-Data-Xiang",
            "title": {
                "fragments": [],
                "text": "Model Fitting and Testing for Non-Gaussian Data with Large Data Sets"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This dissertation applies the Generalized Approximate Cross Validation function (GACV) to binary response data from the Wisconsin Epidemiological Study of Diabetic Retinopathy in order to establish the accuracy of the model when applied to a large data set."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2647026"
                        ],
                        "name": "A. Blumer",
                        "slug": "A.-Blumer",
                        "structuredName": {
                            "firstName": "Anselm",
                            "lastName": "Blumer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blumer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683946"
                        ],
                        "name": "A. Ehrenfeucht",
                        "slug": "A.-Ehrenfeucht",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Ehrenfeucht",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ehrenfeucht"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1138467,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0b8fa3496283d4d808fba9ff62d5f024bcf23be",
            "isKey": false,
            "numCitedBy": 1909,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": "Valiant's learnability model is extended to learning classes of concepts defined by regions in Euclidean space En. The methods in this paper lead to a unified treatment of some of Valiant's results, along with previous results on distribution-free convergence of certain pattern recognition algorithms. It is shown that the essential condition for distribution-free learnability is finiteness of the Vapnik-Chervonenkis dimension, a simple combinatorial parameter of the class of concepts to be learned. Using this parameter, the complexity and closure properties of learnable classes are analyzed, and the necessary and sufficient conditions are provided for feasible learnability."
            },
            "slug": "Learnability-and-the-Vapnik-Chervonenkis-dimension-Blumer-Ehrenfeucht",
            "title": {
                "fragments": [],
                "text": "Learnability and the Vapnik-Chervonenkis dimension"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper shows that the essential condition for distribution-free learnability is finiteness of the Vapnik-Chervonenkis dimension, a simple combinatorial parameter of the class of concepts to be learned."
            },
            "venue": {
                "fragments": [],
                "text": "JACM"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1644344103"
                        ],
                        "name": "J. C. BurgesChristopher",
                        "slug": "J.-C.-BurgesChristopher",
                        "structuredName": {
                            "firstName": "J",
                            "lastName": "BurgesChristopher",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. C. BurgesChristopher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 215966761,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6716697767fc601efc7690f40820d9ea7a7bf57c",
            "isKey": false,
            "numCitedBy": 13527,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, w..."
            },
            "slug": "A-Tutorial-on-Support-Vector-Machines-for-Pattern-BurgesChristopher",
            "title": {
                "fragments": [],
                "text": "A Tutorial on Support Vector Machines for Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This tutorial starts with an overview of the concepts of VC dimension and structural risk minimization and describes linear Support Vector Machines (SVMs) for separable and non-separable data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041866"
                        ],
                        "name": "L. Jackel",
                        "slug": "L.-Jackel",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Jackel",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jackel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122774836"
                        ],
                        "name": "A. Brunot",
                        "slug": "A.-Brunot",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Brunot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Brunot"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152547641"
                        ],
                        "name": "Corinna Cortes",
                        "slug": "Corinna-Cortes",
                        "structuredName": {
                            "firstName": "Corinna",
                            "lastName": "Cortes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Corinna Cortes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050470845"
                        ],
                        "name": "H. Drucker",
                        "slug": "H.-Drucker",
                        "structuredName": {
                            "firstName": "Harris",
                            "lastName": "Drucker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Drucker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145636949"
                        ],
                        "name": "Urs Muller",
                        "slug": "Urs-Muller",
                        "structuredName": {
                            "firstName": "Urs",
                            "lastName": "Muller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Urs Muller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "97914531"
                        ],
                        "name": "E. Sackinger",
                        "slug": "E.-Sackinger",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "Sackinger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Sackinger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 11259076,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d50dce749321301f0104689f2dc582303a83be65",
            "isKey": false,
            "numCitedBy": 631,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "COMPARISON OF LEARNINGALGORITHMS FOR HANDWRITTEN DIGITRECOGNITIONY. LeCun, L. Jackel, L. Bottou, A. Brunot, C. Cortes,J. Denker, H. Drucker, I. Guyon, U. M \u007fuller,E. S\u007fackinger, P. Simard, and V. VapnikBell Lab oratories, Holmdel, NJ 07733, USAEmail: yann@research.att.comAbstractThis pap er compares the p erformance of several classi er algorithmson a standard database of handwritten digits. We consider not only rawaccuracy, but also rejection, training time, recognition time, and memoryrequirements.1"
            },
            "slug": "Comparison-of-learning-algorithms-for-handwritten-LeCun-Jackel",
            "title": {
                "fragments": [],
                "text": "Comparison of learning algorithms for handwritten digit recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This comparison of several learning algorithms for handwritten digits considers not only raw accuracy, but also rejection, training time, recognition time, and memory requirements."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041866"
                        ],
                        "name": "L. Jackel",
                        "slug": "L.-Jackel",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Jackel",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jackel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122774836"
                        ],
                        "name": "A. Brunot",
                        "slug": "A.-Brunot",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Brunot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Brunot"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152547641"
                        ],
                        "name": "Corinna Cortes",
                        "slug": "Corinna-Cortes",
                        "structuredName": {
                            "firstName": "Corinna",
                            "lastName": "Cortes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Corinna Cortes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050470845"
                        ],
                        "name": "H. Drucker",
                        "slug": "H.-Drucker",
                        "structuredName": {
                            "firstName": "Harris",
                            "lastName": "Drucker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Drucker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145636949"
                        ],
                        "name": "Urs Muller",
                        "slug": "Urs-Muller",
                        "structuredName": {
                            "firstName": "Urs",
                            "lastName": "Muller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Urs Muller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "97914531"
                        ],
                        "name": "E. Sackinger",
                        "slug": "E.-Sackinger",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "Sackinger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Sackinger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 11259076,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d50dce749321301f0104689f2dc582303a83be65",
            "isKey": false,
            "numCitedBy": 631,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "COMPARISON OF LEARNINGALGORITHMS FOR HANDWRITTEN DIGITRECOGNITIONY. LeCun, L. Jackel, L. Bottou, A. Brunot, C. Cortes,J. Denker, H. Drucker, I. Guyon, U. M \u007fuller,E. S\u007fackinger, P. Simard, and V. VapnikBell Lab oratories, Holmdel, NJ 07733, USAEmail: yann@research.att.comAbstractThis pap er compares the p erformance of several classi er algorithmson a standard database of handwritten digits. We consider not only rawaccuracy, but also rejection, training time, recognition time, and memoryrequirements.1"
            },
            "slug": "Comparison-of-learning-algorithms-for-handwritten-LeCun-Jackel",
            "title": {
                "fragments": [],
                "text": "Comparison of learning algorithms for handwritten digit recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This comparison of several learning algorithms for handwritten digits considers not only raw accuracy, but also rejection, training time, recognition time, and memory requirements."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 125709427,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6aa8f88d48a80069ec38c6ddcfe1e82d7d83b57d",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "We review a general approach to multivariate function estimation based on optimal approximation and smoothing in reproducing kernel spaces. Learning with radial basis functions (including thin plate splines) is an important special case. In this context we describe a test for linearity of a functional iteration, and a general class of model-building methods based on sums and products of reproducing kernels. We describe the use of (radial) basis functions in the context of regularization, for use with very large data sets. We generalize some of the results from the estimation of real\u00ad valued functions to the estimation of vector-valued functions. Finally, we generalize from the estimation of vector-valued functions to the estimation of function-valued functions on arbitrary index sets, thereby proposing a theory of regularized estimates of nonlinear operators."
            },
            "slug": "Multivariate-Function-and-Operator-Estimation,-on-Wahba",
            "title": {
                "fragments": [],
                "text": "Multivariate Function and Operator Estimation, Based on Smoothing Splines and Reproducing Kernels"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 685382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "015999a72c70a960e59c51078b09c8f672af0d2c",
            "isKey": false,
            "numCitedBy": 1198,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Sample complexity results from computational learning theory, when applied to neural network learning for pattern classification problems, suggest that for good generalization performance the number of training examples should grow at least linearly with the number of adjustable parameters in the network. Results in this paper show that if a large neural network is used for a pattern classification problem and the learning algorithm finds a network with small weights that has small squared error on the training patterns, then the generalization performance depends on the size of the weights rather than the number of weights. For example, consider a two-layer feedforward network of sigmoid units, in which the sum of the magnitudes of the weights associated with each unit is bounded by A and the input dimension is n. We show that the misclassification probability is no more than a certain error estimate (that is related to squared error on the training set) plus A/sup 3/ /spl radic/((log n)/m) (ignoring log A and log m factors), where m is the number of training patterns. This may explain the generalization performance of neural networks, particularly when the number of training examples is considerably smaller than the number of weights. It also supports heuristics (such as weight decay and early stopping) that attempt to keep the weights small during training. The proof techniques appear to be useful for the analysis of other pattern classifiers: when the input domain is a totally bounded metric space, we use the same approach to give upper bounds on misclassification probability for classifiers with decision boundaries that are far from the training examples."
            },
            "slug": "The-Sample-Complexity-of-Pattern-Classification-The-Bartlett",
            "title": {
                "fragments": [],
                "text": "The Sample Complexity of Pattern Classification with Neural Networks: The Size of the Weights is More Important than the Size of the Network"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "Results in this paper show that if a large neural network is used for a pattern classification problem and the learning algorithm finds a network with small weights that has small squared error on the training patterns, then the generalization performance depends on the size of the weights rather than the number of weights."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144007105"
                        ],
                        "name": "Philip M. Long",
                        "slug": "Philip-M.-Long",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Long",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip M. Long"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143957317"
                        ],
                        "name": "R. C. Williamson",
                        "slug": "R.-C.-Williamson",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Williamson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Williamson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2551295,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "fc48a9403b5d01a2d1724d4e04218a4a9b78cb3a",
            "isKey": false,
            "numCitedBy": 149,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of learning real-valued functions from random examples when the function values are corrupted with noise. With mild conditions on independent observation noise, we provide characterizations of the learnability of a real-valued function class in terms of a generalization of the Vapnik-Chervonenkis dimension, the fat shattering function, introduced by Kearns and Schapire. We show that, given some restrictions on the noise, a function class is learnable in our model if and only if its fat-shattering function is finite. With different (also quite mild) restrictions, satisfied for example by gaussian noise, we show that a function class is learnable from polynomially many examples if and only if its fat-shattering function grows polynomially. We prove analogous results in an agnostic setting, where there is no assumption of an underlying function class."
            },
            "slug": "Fat-shattering-and-the-learnability-of-real-valued-Bartlett-Long",
            "title": {
                "fragments": [],
                "text": "Fat-shattering and the learnability of real-valued functions"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that, given some restrictions on the noise, a function class is learnable in this model if and only if its fat-shattering function is finite, and analogous results in an agnostic setting, where there is no assumption of an underlying function class."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '94"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2427083,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "40212e9474c3ddf3d8c6ffd13dd3211ec9406c49",
            "isKey": false,
            "numCitedBy": 8600,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper explores the use of Support Vector Machines (SVMs) for learning text classifiers from examples. It analyzes the particular properties of learning with text data and identifies why SVMs are appropriate for this task. Empirical results support the theoretical findings. SVMs achieve substantial improvements over the currently best performing methods and behave robustly over a variety of different learning tasks. Furthermore they are fully automatic, eliminating the need for manual parameter tuning."
            },
            "slug": "Text-Categorization-with-Support-Vector-Machines:-Joachims",
            "title": {
                "fragments": [],
                "text": "Text Categorization with Support Vector Machines: Learning with Many Relevant Features"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "This paper explores the use of Support Vector Machines for learning text classifiers from examples and analyzes the particular properties of learning with text data and identifies why SVMs are appropriate for this task."
            },
            "venue": {
                "fragments": [],
                "text": "ECML"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152597562"
                        ],
                        "name": "Gunnar R\u00e4tsch",
                        "slug": "Gunnar-R\u00e4tsch",
                        "structuredName": {
                            "firstName": "Gunnar",
                            "lastName": "R\u00e4tsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gunnar R\u00e4tsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2633352"
                        ],
                        "name": "J. Kohlmorgen",
                        "slug": "J.-Kohlmorgen",
                        "structuredName": {
                            "firstName": "Jens",
                            "lastName": "Kohlmorgen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kohlmorgen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5398743,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f43840dc1638a18eb6178f1060dc5f41af1c5ac7",
            "isKey": false,
            "numCitedBy": 975,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Support Vector Machines are used for time series prediction and compared to radial basis function networks. We make use of two different cost functions for Support Vectors: training with (i) an e insensitive loss and (ii) Huber's robust loss function and discuss how to choose the regularization parameters in these models. Two applications are considered: data from (a) a noisy (normal and uniform noise) Mackey Glass equation and (b) the Santa Fe competition (set D). In both cases Support Vector Machines show an excellent performance. In case (b) the Support Vector approach improves the best known result on the benchmark by a factor of 29%."
            },
            "slug": "Predicting-Time-Series-with-Support-Vector-Machines-M\u00fcller-Smola",
            "title": {
                "fragments": [],
                "text": "Predicting Time Series with Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Two different cost functions for Support Vectors are made use: training with an e insensitive loss and Huber's robust loss function and how to choose the regularization parameters in these models are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "1998/08/25 16:31"
                    },
                    "intents": []
                }
            ],
            "corpusId": 15109515,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "51c1519a57a65351a713a3d74f8d477105df0ec3",
            "isKey": false,
            "numCitedBy": 352,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We explore methods for incorporating prior knowledge about a problem at hand in Support Vector learning machines. We show that both invariances under group transformations and prior knowledge about locality in images can be incorporated by constructing appropriate kernel functions."
            },
            "slug": "Prior-Knowledge-in-Support-Vector-Kernels-Sch\u00f6lkopf-Simard",
            "title": {
                "fragments": [],
                "text": "Prior Knowledge in Support Vector Kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "It is shown that both invariances under group transformations and prior knowledge about locality in images can be incorporated by constructing appropriate kernel functions by exploring methods for incorporating prior knowledge in Support Vector learning machines."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053520352"
                        ],
                        "name": "M. Kirby",
                        "slug": "M.-Kirby",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kirby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kirby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49555086"
                        ],
                        "name": "L. Sirovich",
                        "slug": "L.-Sirovich",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Sirovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Sirovich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 570648,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66d75a5fe9e1b6511c5135d68e9ce8c0da5a7374",
            "isKey": false,
            "numCitedBy": 2852,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of natural symmetries (mirror images) in a well-defined family of patterns (human faces) is discussed within the framework of the Karhunen-Loeve expansion. This results in an extension of the data and imposes even and odd symmetry on the eigenfunctions of the covariance matrix, without increasing the complexity of the calculation. The resulting approximation of faces projected from outside of the data set onto this optimal basis is improved on average. >"
            },
            "slug": "Application-of-the-Karhunen-Loeve-Procedure-for-the-Kirby-Sirovich",
            "title": {
                "fragments": [],
                "text": "Application of the Karhunen-Loeve Procedure for the Characterization of Human Faces"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "The use of natural symmetries (mirror images) in a well-defined family of patterns (human faces) is discussed within the framework of the Karhunen-Loeve expansion, which results in an extension of the data and imposes even and odd symmetry on the eigenfunctions of the covariance matrix."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40435910"
                        ],
                        "name": "Chaogang Xiang",
                        "slug": "Chaogang-Xiang",
                        "structuredName": {
                            "firstName": "Chaogang",
                            "lastName": "Xiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chaogang Xiang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14944393,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9ecac0591ad14ce1eb04af2438df764e2c46c228",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the use of smoothing splines in generalized additive models with binary responses in the large data set situation. Xiang and Wahba (1996) proposed using the Generalized Approximate Cross Validation (GACV) function as a method to choose (multiple) smoothing parameters in the binary data case and demonstrated through simulation that the GACV method compares well to existing iterative methods, as judged by the Kullback-Leibler distance of the estimate to the true function being tted. However, the calculation of the GACV function involves solving an n by n linear system, where n is the sample size. As the sample size increases, the calculation becomes numerically unstable and infeasible. To reduce these computational problems we propose a randomized version of the GACV function, which is numerically stable. Furthermore, we use a clustering algorithm to choose a set of basis functions with which to approximate the exact additive smoothing spline estimate, which has a basis function for every data point. Combining these two approaches, we are able to extend smoothing spline methods in the binary response case to much larger data sets without sacriicing much accuracy."
            },
            "slug": "Approximate-Smoothing-Spline-Methods-for-Large-in-Xiang",
            "title": {
                "fragments": [],
                "text": "Approximate Smoothing Spline Methods for Large DataSets in the Binary"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A randomized version of the GACV function is proposed, which is numerically stable and uses a clustering algorithm to choose a set of basis functions with which to approximate the exact additive smoothing spline estimate, which has a basis function for every data point."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 11652139,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0366ce5be03f003f8b28078f8e154a79baa80987",
            "isKey": false,
            "numCitedBy": 322,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract. We present a kernel-based framework for pattern recognition, regression estimation, function approximation, and multiple operator inversion. Adopting a regularization-theoretic framework, the above are formulated as constrained optimization problems. Previous approaches such as ridge regression, support vector methods, and regularization networks are included as special cases. We show connections between the cost function and some properties up to now believed to apply to support vector machines only. For appropriately chosen cost functions, the optimal solution of all the problems described above can be found by solving a simple quadratic programming problem."
            },
            "slug": "On-a-Kernel-Based-Method-for-Pattern-Recognition,-Smola-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "On a Kernel-Based Method for Pattern Recognition, Regression, Approximation, and Operator Inversion"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A kernel-based framework for pattern recognition, regression estimation, function approximation, and multiple operator inversion is presented, adopting a regularization-theoretic framework."
            },
            "venue": {
                "fragments": [],
                "text": "Algorithmica"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2155551370"
                        ],
                        "name": "Shaobing Chen",
                        "slug": "Shaobing-Chen",
                        "structuredName": {
                            "firstName": "Shaobing",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaobing Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122514140"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Donoho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 96447294,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "30f3567eeb13079a1a02ac1342f610cbd95df3bc",
            "isKey": false,
            "numCitedBy": 654,
            "numCiting": 96,
            "paperAbstract": {
                "fragments": [],
                "text": "The time-frequency and time-scale communities have recently developed an enormous number of over-complete signal dictionaries, wavelets, wavelet packets, cosine packets, Wilson bases, chirplets, warped bases, and hyperbolic cross bases being a few examples. Basis pursuit is a technique for decomposing a signal into an \"optimal\" superposition of dictionary elements. The optimization criterion is the l/sup 1/ norm of coefficients. The method has several advantages over matching pursuit and best ortho basis, including super-resolution and stability.<<ETX>>"
            },
            "slug": "Basis-pursuit-Chen-Donoho",
            "title": {
                "fragments": [],
                "text": "Basis pursuit"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Basis pursuit is a technique for decomposing a signal into an \"optimal\" superposition of dictionary elements, which has several advantages over matching pursuit and best ortho basis, including super-resolution and stability."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1994 28th Asilomar Conference on Signals, Systems and Computers"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12154028,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7020c2455b74deda5af696248cc41c53e32c00e2",
            "isKey": false,
            "numCitedBy": 229,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Large VC-dimension classifiers can learn difficult tasks, but are usually impractical because they generalize well only if they are trained with huge quantities of data. In this paper we show that even high-order polynomial classifiers in high dimensional spaces can be trained with a small amount of training data and yet generalize better than classifiers with a smaller VC-dimension. This is achieved with a maximum margin algorithm (the Generalized Portrait). The technique is applicable to a wide variety of classifiers, including Perceptrons, polynomial classifiers (sigma-pi unit networks) and Radial Basis Functions. The effective number of parameters is adjusted automatically by the training algorithm to match the complexity of the problem. It is shown to equal the number of those training patterns which are closest patterns to the decision boundary (supporting patterns). Bounds on the generalization error and the speed of convergence of the algorithm are given. Experimental results on handwritten digit recognition demonstrate good generalization compared to other algorithms."
            },
            "slug": "Automatic-Capacity-Tuning-of-Very-Large-Classifiers-Guyon-Boser",
            "title": {
                "fragments": [],
                "text": "Automatic Capacity Tuning of Very Large VC-Dimension Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that even high-order polynomial classifiers in high dimensional spaces can be trained with a small amount of training data and yet generalize better than classifiers with a smaller VC-dimension."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690395"
                        ],
                        "name": "M. Bierlaire",
                        "slug": "M.-Bierlaire",
                        "structuredName": {
                            "firstName": "Michel",
                            "lastName": "Bierlaire",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bierlaire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145758858"
                        ],
                        "name": "P. Toint",
                        "slug": "P.-Toint",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Toint",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Toint"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3149610"
                        ],
                        "name": "D. Tuyttens",
                        "slug": "D.-Tuyttens",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Tuyttens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Tuyttens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16541311,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23f9af4c766906a8bcbde9faafc6470a4a34db76",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-iterative-algorithms-for-linear-least-squares-Bierlaire-Toint",
            "title": {
                "fragments": [],
                "text": "On iterative algorithms for linear least squares problems with bound constraints"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8992604"
                        ],
                        "name": "E. Levin",
                        "slug": "E.-Levin",
                        "structuredName": {
                            "firstName": "Esther",
                            "lastName": "Levin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Levin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207597853,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "899defb6a100af509547b8d74bb626533ee87da4",
            "isKey": false,
            "numCitedBy": 351,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "A method for measuring the capacity of learning machines is described. The method is based on fitting a theoretically derived function to empirical measurements of the maximal difference between the error rates on two separate data sets of varying sizes. Experimental measurements of the capacity of various types of linear classifiers are presented."
            },
            "slug": "Measuring-the-VC-Dimension-of-a-Learning-Machine-Vapnik-Levin",
            "title": {
                "fragments": [],
                "text": "Measuring the VC-Dimension of a Learning Machine"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A method for measuring the capacity of learning machines is described, based on fitting a theoretically derived function to empirical measurements of the maximal difference between the error rates on two separate data sets of varying sizes."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101417459"
                        ],
                        "name": "A. Wolf",
                        "slug": "A.-Wolf",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Wolf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Wolf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143645092"
                        ],
                        "name": "J. Swift",
                        "slug": "J.-Swift",
                        "structuredName": {
                            "firstName": "Jack",
                            "lastName": "Swift",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Swift"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2421446"
                        ],
                        "name": "H. Swinney",
                        "slug": "H.-Swinney",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Swinney",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Swinney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "65757961"
                        ],
                        "name": "J. A. Vastano",
                        "slug": "J.-A.-Vastano",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Vastano",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. A. Vastano"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14411384,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "f2a370448285cdba3e7c634f072e47a254698ae6",
            "isKey": false,
            "numCitedBy": 7548,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Determining-Lyapunov-exponents-from-a-time-series-Wolf-Swift",
            "title": {
                "fragments": [],
                "text": "Determining Lyapunov exponents from a time series"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052864185"
                        ],
                        "name": "P. Bradley",
                        "slug": "P.-Bradley",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Bradley",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bradley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747026"
                        ],
                        "name": "O. Mangasarian",
                        "slug": "O.-Mangasarian",
                        "structuredName": {
                            "firstName": "Olvi",
                            "lastName": "Mangasarian",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Mangasarian"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 174
                            }
                        ],
                        "text": "\u2026constructingthe best linear discriminant using the minimum number of attributes; and misclas-si cation minimization: explicitly minimizing the number of points misclassi ed(Bradley and Mangasarian, 1998a; Bradley et al., 1995; Bredensteiner and Ben-nett, 1997; Bennett and Bredensteiner, 1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 256,
                                "start": 226
                            }
                        ],
                        "text": "The greater e ectiveness of lin-ear versus quadratic programming algorithms is de nitely true for general-purposesolvers but optimization methods adapted to SVM problem structure such as theones discussed in this book and in (Bradley and Mangasarian, 1998b) may helpalleviate this di erence."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 129
                            }
                        ],
                        "text": "RLP with 1-norm capacity control has been investigated in several papers (Ben-nett and Bredensteiner, 1998; Bredensteiner, 1997; Bradley and Mangasarian,1998a; Bennett et al., 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17784771,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "557aeb897fa4a6b23bd610f0a7ee9e2f5dfd8316",
            "isKey": false,
            "numCitedBy": 182,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "A linear support vector machine formulation is used to generate a fast, finitely-terminating linear-programming algorithm for discriminating between two massive sets in n-dimen-sional space, where the number of points can be orders of magnitude larger than n. The algorithm creates a succession of sufficiently small linear programs that separate chunks of the data at a time. The key idea is that a small number of support vectors, corresponding to linear programming constraints with positive dual variables, are carried over between the successive small linear programs, each of which containing a chunk of the data. We prove that this procedure is monotonic and terminates in a finite number of steps at an exact solution that leads to an optimal separating plane for the entire dataset. Numerical results on fully dense publicly available datasets, numbering 20,000 to 1 million points in 32-dimensional space, confirm the theoretical results and demonstrate the ability to handle very large problems"
            },
            "slug": "Massive-data-discrimination-via-linear-support-Bradley-Mangasarian",
            "title": {
                "fragments": [],
                "text": "Massive data discrimination via linear support vector machines"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Numerical results on fully dense publicly available datasets, numbering 20,000 to 1 million points in 32-dimensional space, confirm the theoretical results and demonstrate the ability to handle very large problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6674407,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3f600e6c6cf93e78c9e6e690443d6d22c4bf18b9",
            "isKey": false,
            "numCitedBy": 7880,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method for performing a nonlinear form of principal component analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in high-dimensional feature spaces, related to input space by some nonlinear mapfor instance, the space of all possible five-pixel products in 16 16 images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition."
            },
            "slug": "Nonlinear-Component-Analysis-as-a-Kernel-Eigenvalue-Sch\u00f6lkopf-Smola",
            "title": {
                "fragments": [],
                "text": "Nonlinear Component Analysis as a Kernel Eigenvalue Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A new method for performing a nonlinear form of principal component analysis by the use of integral operator kernel functions is proposed and experimental results on polynomial feature extraction for pattern recognition are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057622896"
                        ],
                        "name": "Dong Xiang",
                        "slug": "Dong-Xiang",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Xiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Xiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15445093,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "cd5da304a19b9afdd35a4f0a70cc0e164fc4b357",
            "isKey": false,
            "numCitedBy": 133,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a Generalized Approximate Cross Validation (GACV) function for estimating the smoothing parameter in the penalized log likeli- hood regression problem with non-Gaussian data. This GACV is obtained by, first, obtaining an approximation to the leaving-out-one function based on the negative log likelihood, and then, in a step reminiscent of that used to get from leaving-out- one cross validation to GCV in the Gaussian case, we replace diagonal elements of certain matrices by 1/n times the trace. A numerical simulation with Bernoulli data is used to compare the smoothing parameter \u03bb chosen by this approximation procedure with the \u03bb chosen from the two most often used algorithms based on the generalized cross validation procedure (O'Sullivan et al. (1986), Gu (1990, 1992)). In the examples here, the GACV estimate produces a better fit of the truth in term of minimizing the Kullback-Leibler distance. Figures suggest that the GACV curve may be an approximately unbiased estimate of the Kullback-Leibler distance in the Bernoulli data case; however, a theoretical proof is yet to be found."
            },
            "slug": "A-GENERALIZED-APPROXIMATE-CROSS-VALIDATION-FOR-WITH-Xiang-Wahba",
            "title": {
                "fragments": [],
                "text": "A GENERALIZED APPROXIMATE CROSS VALIDATION FOR SMOOTHING SPLINES WITH NON-GAUSSIAN DATA"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A Generalized Approximate Cross Validation function for estimating the smoothing parameter in the penalized log likeli- hood regression problem with non-Gaussian data and suggests that the GACV curve may be an approximately unbiased estimate of the Kullback-Leibler distance in the Bernoulli data case."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145675811"
                        ],
                        "name": "Sayan Mukherjee",
                        "slug": "Sayan-Mukherjee",
                        "structuredName": {
                            "firstName": "Sayan",
                            "lastName": "Mukherjee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sayan Mukherjee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781874"
                        ],
                        "name": "E. Osuna",
                        "slug": "E.-Osuna",
                        "structuredName": {
                            "firstName": "Edgar",
                            "lastName": "Osuna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Osuna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16950792,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d81512c6c2582fa91fe151efdaf80a867f66d12a",
            "isKey": false,
            "numCitedBy": 548,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel method for regression has been recently proposed by Vapnik et al. (1995, 1996). The technique, called support vector machine (SVM), is very well founded from the mathematical point of view and seems to provide a new insight in function approximation. We implemented the SVM and tested it on a database of chaotic time series previously used to compare the performances of different approximation techniques, including polynomial and rational approximation, local polynomial techniques, radial basis functions, and neural networks. The SVM performs better than the other approaches. We also study, for a particular time series, the variability in performance with respect to the few free parameters of SVM."
            },
            "slug": "Nonlinear-prediction-of-chaotic-time-series-using-Mukherjee-Osuna",
            "title": {
                "fragments": [],
                "text": "Nonlinear prediction of chaotic time series using support vector machines"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The SVM is implemented and tested on a database of chaotic time series previously used to compare the performances of different approximation techniques, including polynomial and rational approximation, localPolynomial techniques, radial basis functions, and neural networks; the SVM performs better than the other approaches."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks for Signal Processing VII. Proceedings of the 1997 IEEE Signal Processing Society Workshop"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2647543"
                        ],
                        "name": "Erin J. Bredensteiner",
                        "slug": "Erin-J.-Bredensteiner",
                        "structuredName": {
                            "firstName": "Erin",
                            "lastName": "Bredensteiner",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Erin J. Bredensteiner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145728220"
                        ],
                        "name": "K. Bennett",
                        "slug": "K.-Bennett",
                        "structuredName": {
                            "firstName": "Kristin",
                            "lastName": "Bennett",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Bennett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15871727,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "547f61c0571ecd771ed171c8d9602c916c0a2525",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Decision trees for classification can be constructed using mathematical programming. Within decision tree algorithms, the feature minimization problem is to construct accurate decisions using as few features or attributes within each decision as possible. Feature minimization is an important aspect of data mining since it helps identify what attributes are important and helps produce accurate and interpretable decision trees. In feature minimization with bounded accuracy, we minimize the number of features using a given misclassification error tolerance. This problem can be formulated as a parametric bilinear program and is shown to be NP-complete. A parametric FrankWolfe method is used to solve the bilinear subproblems. The resulting minimization algorithm produces more compact, accurate, and interpretable trees. This procedure can be applied to many different error functions. Formulations and results for two error functions are given. One method, FM RLP-P, dramatically reduced the number of features of one dataset from 147 to 2 while maintaining an 83.6% testing accuracy. Computational results compare favorably with the standard univariate decision tree method, C4.5, as well as with linear programming methods of tree construction."
            },
            "slug": "Feature-minimization-within-decision-trees-Bredensteiner-Bennett",
            "title": {
                "fragments": [],
                "text": "Feature minimization within decision trees"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work reduces the number of features of one dataset from 147 to 2 while maintaining an 83.6% testing accuracy and shows that the resulting minimization algorithm produces more compact, accurate, and interpretable trees."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145004630"
                        ],
                        "name": "M. Anthony",
                        "slug": "M.-Anthony",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Anthony",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Anthony"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13604547,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb5c2a4822eeb6fd8c5ac00633d81c7a2c353430",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 137,
            "paperAbstract": {
                "fragments": [],
                "text": "There are a number of mathematical approaches to the study of learning and generalization in arti cial neural networks. Here we survey the `probably approximately correct' (PAC) model of learning and some of its variants. These models provide a probabilistic framework for the discussion of generalization and learning. This survey concentrates on the sample complexity questions in these models; that is, the emphasis is on how many examples should be used for training. Computational complexity considerations are brie y discussed for the basic PAC model. Throughout, the importance of the Vapnik-Chervonenkis dimension is highlighted. Particular attention is devoted to describing how the probabilistic models apply in the context of neural network learning, both for networks with binary-valued output and for networks with real-valued output."
            },
            "slug": "Probabilistic-Analysis-of-Learning-in-Artificial-Anthony",
            "title": {
                "fragments": [],
                "text": "Probabilistic Analysis of Learning in Artificial Neural Networks: The PAC Model and its Variants"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A survey of the `probably approximately correct' (PAC) model of learning and some of its variants, which provide a probabilistic framework for the discussion of generalization and learning, focuses on the sample complexity questions in these models."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38817267"
                        ],
                        "name": "K. Sung",
                        "slug": "K.-Sung",
                        "structuredName": {
                            "firstName": "Kah",
                            "lastName": "Sung",
                            "middleNames": [
                                "Kay"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1900499,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4a422669ec9b6a60b05d2d2595314008a5fb419",
            "isKey": false,
            "numCitedBy": 1313,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "The support vector (SV) machine is a novel type of learning machine, based on statistical learning theory, which contains polynomial classifiers, neural networks, and radial basis function (RBF) networks as special cases. In the RBF case, the SV algorithm automatically determines centers, weights, and threshold that minimize an upper bound on the expected test error. The present study is devoted to an experimental comparison of these machines with a classical approach, where the centers are determined by X-means clustering, and the weights are computed using error backpropagation. We consider three machines, namely, a classical RBF machine, an SV machine with Gaussian kernel, and a hybrid system with the centers determined by the SV method and the weights trained by error backpropagation. Our results show that on the United States postal service database of handwritten digits, the SV machine achieves the highest recognition accuracy, followed by the hybrid system. The SV approach is thus not only theoretically well-founded but also superior in a practical application."
            },
            "slug": "Comparing-support-vector-machines-with-Gaussian-to-Sch\u00f6lkopf-Sung",
            "title": {
                "fragments": [],
                "text": "Comparing support vector machines with Gaussian kernels to radial basis function classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The results show that on the United States postal service database of handwritten digits, the SV machine achieves the highest recognition accuracy, followed by the hybrid system, and the SV approach is thus not only theoretically well-founded but also superior in a practical application."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7831590,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1ad15c08556c8f8e3739703857ea01077ce738c5",
            "isKey": false,
            "numCitedBy": 2054,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method for performing a nonlinear form of Principal Component Analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in highdimensional feature spaces, related to input space by some nonlinear map; for instance the space of all possible d-pixel products in images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition."
            },
            "slug": "Kernel-Principal-Component-Analysis-Sch\u00f6lkopf-Smola",
            "title": {
                "fragments": [],
                "text": "Kernel Principal Component Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A new method for performing a nonlinear form of Principal Component Analysis by the use of integral operator kernel functions is proposed and experimental results on polynomial feature extraction for pattern recognition are presented."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145328018"
                        ],
                        "name": "M. Oren",
                        "slug": "M.-Oren",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Oren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Oren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145030811"
                        ],
                        "name": "C. Papageorgiou",
                        "slug": "C.-Papageorgiou",
                        "structuredName": {
                            "firstName": "Constantine",
                            "lastName": "Papageorgiou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Papageorgiou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46597039"
                        ],
                        "name": "P. Sinha",
                        "slug": "P.-Sinha",
                        "structuredName": {
                            "firstName": "Pawan",
                            "lastName": "Sinha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Sinha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781874"
                        ],
                        "name": "E. Osuna",
                        "slug": "E.-Osuna",
                        "structuredName": {
                            "firstName": "Edgar",
                            "lastName": "Osuna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Osuna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7967646,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd0534a87e09b3d64b7e7462e2684c60c9aca1f5",
            "isKey": false,
            "numCitedBy": 837,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a trainable object detection architecture that is applied to detecting people in static images of cluttered scenes. This problem poses several challenges. People are highly non-rigid objects with a high degree of variability in size, shape, color, and texture. Unlike previous approaches, this system learns from examples and does not rely on any a priori (hand-crafted) models or on motion. The detection technique is based on the novel idea of the wavelet template that defines the shape of an object in terms of a subset of the wavelet coefficients of the image. It is invariant to changes in color and texture and can be used to robustly define a rich and complex class of objects such as people. We show how the invariant properties and computational efficiency of the wavelet template make it an effective tool for object detection."
            },
            "slug": "Pedestrian-detection-using-wavelet-templates-Oren-Papageorgiou",
            "title": {
                "fragments": [],
                "text": "Pedestrian detection using wavelet templates"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "This paper presents a trainable object detection architecture that is applied to detecting people in static images of cluttered scenes and shows how the invariant properties and computational efficiency of the wavelet template make it an effective tool for object detection."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734327"
                        ],
                        "name": "N. Alon",
                        "slug": "N.-Alon",
                        "structuredName": {
                            "firstName": "Noga",
                            "lastName": "Alon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Alon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1409749316"
                        ],
                        "name": "S. Ben-David",
                        "slug": "S.-Ben-David",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Ben-David",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ben-David"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388387856"
                        ],
                        "name": "N. Cesa-Bianchi",
                        "slug": "N.-Cesa-Bianchi",
                        "structuredName": {
                            "firstName": "Nicol\u00f2",
                            "lastName": "Cesa-Bianchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cesa-Bianchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8347198,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e07c3df9d8d53c3be8cb9e982da98a4471322d90",
            "isKey": false,
            "numCitedBy": 411,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "Learnability in Valiant's PAC learning model has been shown to be strongly related to the existence of uniform laws of large numbers. These laws define a distribution-free convergence property of means to expectations uniformly over classes of random variables. Classes of real-valued functions enjoying such a property are also known as uniform Gliveako-Cantelli classes. In this paper we prove, through a generalization of Sauer's lemma that may be interesting in its own right, a new characterization of uniform Glivenko-Cantelli classes. Our characterization yields Dudley, Gine, and Zinn's previous characterization as a corollary. Furthermore, it is the first based on a simple combinatorial quantity generalizing the Vapnik-Chervonenkis dimension. We apply this result to characterize PAC learnability in the statistical regression framework of probabilistic concepts, solving an open problem posed by Kearns and Schapire. Our characterization shows that the accuracy parameter plays a crucial role in determining the effective complexity of the learner's hypothesis class.<<ETX>>"
            },
            "slug": "Scale-sensitive-dimensions,-uniform-convergence,-Alon-Ben-David",
            "title": {
                "fragments": [],
                "text": "Scale-sensitive dimensions, uniform convergence, and learnability"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A characterization of learnability in the probabilistic concept model, solving an open problem posed by Kearns and Schapire, and shows that the accuracy parameter plays a crucial role in determining the effective complexity of the learner's hypothesis class."
            },
            "venue": {
                "fragments": [],
                "text": "JACM"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109729302"
                        ],
                        "name": "Donald R. Johnson",
                        "slug": "Donald-R.-Johnson",
                        "structuredName": {
                            "firstName": "Donald R.",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donald R. Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143750989"
                        ],
                        "name": "F. Gao",
                        "slug": "F.-Gao",
                        "structuredName": {
                            "firstName": "Feng",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47782928"
                        ],
                        "name": "J. Gong",
                        "slug": "J.-Gong",
                        "structuredName": {
                            "firstName": "Jianjian",
                            "lastName": "Gong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gong"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17919171,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "eac71efc12d7e78aea23288e8fc47c27a140f2a9",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 90,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract In variational data assimilation, optimal ingestion of the observational data, and optimal use of prior physical and statistical information involve the choice of numerous weighting, smoothing, and tuning parameters that control the filtering and merging of diverse sources of information. Generally these weights must be obtained from a partial and imperfect understanding of various sources of errors and are frequently chosen by a combination of historical information, physical reasoning, and trial and error. Generalized cross validation (GCV) has long been one of the methods of choice for choosing certain tuning, smoothing, regularization parameters in ill-posed inverse problems, smoothing, and filtering problems. In theory, it is well suited for the adaptive choice of certain parameters that occur in variational objective analysis and for data assimilation problems that are mathematically equivalent to variational problems. The main drawback of the use of GCV in data assimilation problems was th..."
            },
            "slug": "Adaptive-Tuning-of-Numerical-Weather-Prediction-GCV-Wahba-Johnson",
            "title": {
                "fragments": [],
                "text": "Adaptive Tuning of Numerical Weather Prediction Models: Randomized GCV in Three- and Four-Dimensional Data Assimilation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work has shown that generalized cross validation is well suited for the adaptive choice of certain parameters that occur in variational objective analysis and for data assimilation problems that are mathematically equivalent to variational problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 9756494,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "55590f229e23a8e67af7d6d36f7456a595c251d1",
            "isKey": false,
            "numCitedBy": 318,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Developed only recently, support vector learning machines achieve high generalization ability by minimizing a bound on the expected test error; however, so far there existed no way of adding knowledge about invariances of a classification problem at hand. We present a method of incorporating prior knowledge about transformation invariances by applying transformations to support vectors, the training examples most critical for determining the classification boundary."
            },
            "slug": "Incorporating-Invariances-in-Support-Vector-Sch\u00f6lkopf-Burges",
            "title": {
                "fragments": [],
                "text": "Incorporating Invariances in Support Vector Learning Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This work presents a method of incorporating prior knowledge about transformation invariances by applying transformations to support vectors, the training examples most critical for determining the classification boundary."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47782928"
                        ],
                        "name": "J. Gong",
                        "slug": "J.-Gong",
                        "structuredName": {
                            "firstName": "Jianjian",
                            "lastName": "Gong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109729302"
                        ],
                        "name": "Donald R. Johnson",
                        "slug": "Donald-R.-Johnson",
                        "structuredName": {
                            "firstName": "Donald R.",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donald R. Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41230942"
                        ],
                        "name": "J. Tribbia",
                        "slug": "J.-Tribbia",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Tribbia",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tribbia"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17034232,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "1b30352e44072f01614fff312371d663d71bbf70",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 104,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract In Wahba et al. it was shown how the randomized trace method could be used to adaptively tune numerical weather prediction models via generalized cross validation (GCV) and related methods. In this paper a \u201ctoy\u201d four-dimensional data assimilation model is developed (actually one space and one time variable), consisting of an equivalent barotropic vorticity equation on a latitude circle, and used to demonstrate how this technique may be used to simultaneously tune weighting, smoothing, and physical parameters. Analyses both with the model as a strong constraint (corresponding to the usual 4D-Var approach) and as a weak constraint (corresponding theoretically to a fixed-interval Kalman smoother) are carried out. The conclusions are limited to the particular toy problem considered, but it can be seen how more elaborate experiments could be carried out, as well as how the method might be applied in practice. The authors have considered five adjustable parameters, two related to a distributed coeffici..."
            },
            "slug": "Adaptive-Tuning-of-Numerical-Weather-Prediction-of-Gong-Wahba",
            "title": {
                "fragments": [],
                "text": "Adaptive Tuning of Numerical Weather Prediction Models: Simultaneous Estimation of Weighting, Smoothing, and Physical Parameters"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1986103"
                        ],
                        "name": "R. Vanderbei",
                        "slug": "R.-Vanderbei",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Vanderbei",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Vanderbei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 121705174,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a62deb30b50004f1921772d0197b9b60b4736448",
            "isKey": false,
            "numCitedBy": 126,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "LOQO is a system for solving smooth constrained optimization problems. The problems can be linear or nonlinear, convex or nonconvex, constrained or unconstrained. The only real restriction is that the functions defining the problem be smooth (at the points evaluated by the algorithm). If the problem is convex, LOQO finds a globally optimal solution. Otherwise, it finds a locally optimal solution near to a given starting point. This manual describes 1. how to install LOQO on your hardware. 2. how to use AMPL together with LOQO to solve general optimization problems, 3. how to use the subroutine library to formulate and solve optimization problems, and 4. how to formulate and solve linear and quadratic programs in MPS format."
            },
            "slug": "LOQO-user's-manual-\u2014-version-3.10-Vanderbei",
            "title": {
                "fragments": [],
                "text": "LOQO user's manual \u2014 version 3.10"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This manual describes how to install LOQO on your hardware, how to use AMPL together withLOQO to solve general optimization problems, and how to formulate and solve linear and quadratic programs in MPS format."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2880906"
                        ],
                        "name": "V. Blanz",
                        "slug": "V.-Blanz",
                        "structuredName": {
                            "firstName": "Volker",
                            "lastName": "Blanz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Blanz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747836"
                        ],
                        "name": "H. B\u00fclthoff",
                        "slug": "H.-B\u00fclthoff",
                        "structuredName": {
                            "firstName": "Heinrich",
                            "lastName": "B\u00fclthoff",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. B\u00fclthoff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144517651"
                        ],
                        "name": "T. Vetter",
                        "slug": "T.-Vetter",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Vetter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Vetter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 855426,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a0f97e889f6fb4b71704f079407a5ef730ad95f",
            "isKey": false,
            "numCitedBy": 248,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Two view-based object recognition algorithms are compared: (1) a heuristic algorithm based on oriented filters, and (2) a support vector learning machine trained on low-resolution images of the objects. Classification performance is assessed using a high number of images generated by a computer graphics system under precisely controlled conditions. Training- and test-images show a set of 25 realistic three-dimensional models of chairs from viewing directions spread over the upper half of the viewing sphere. The percentage of correct identification of all 25 objects is measured."
            },
            "slug": "Comparison-of-View-Based-Object-Recognition-Using-Blanz-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "Comparison of View-Based Object Recognition Algorithms Using Realistic 3D Models"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "Two view-based object recognition algorithms are compared: a heuristic algorithm based on oriented filters, and a support vector learning machine trained on low-resolution images of the objects."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3291954"
                        ],
                        "name": "M. Golea",
                        "slug": "M.-Golea",
                        "structuredName": {
                            "firstName": "Mostefa",
                            "lastName": "Golea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Golea"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740222"
                        ],
                        "name": "Wee Sun Lee",
                        "slug": "Wee-Sun-Lee",
                        "structuredName": {
                            "firstName": "Wee",
                            "lastName": "Lee",
                            "middleNames": [
                                "Sun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wee Sun Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2586148"
                        ],
                        "name": "L. Mason",
                        "slug": "L.-Mason",
                        "structuredName": {
                            "firstName": "Llew",
                            "lastName": "Mason",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Mason"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 220
                            }
                        ],
                        "text": "Recent results on applying learn-ing theory to decision trees show that there is a tradeo between the structuralcomplexity of a tree, i.e. the depth and number of nodes, and the complexity of thedecisions that are used (Golea et al., 1998; Shawe-Taylor and Cristianini, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 21
                            }
                        ],
                        "text": "Support vector decision trees aredecision trees in which each decision is a support vector machine."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12088396,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fcc802dcc8b4a953d5c3880a3117449e23ee29ea",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent theoretical results for pattern classification with thresholded real-valued functions (such as support vector machines, sigmoid networks, and boosting) give bounds on misclassification probability that do not depend on the size of the classifier, and hence can be considerably smaller than the bounds that follow from the VC theory. In this paper, we show that these techniques can be more widely applied, by representing other boolean functions as two-layer neural networks (thresholded convex combinations of boolean functions). For example, we show that with high probability any decision tree of depth no more than d that is consistent with m training examples has misclassification probability no more than O((1/m(Neff VCdim(U) log2 m log d))1/2), where U is the class of node decision functions, and Neff \u2264 N can be thought of as the effective number of leaves (it becomes small as the distribution on the leaves induced by the training data gets far from uniform). This bound is qualitatively different from the VC bound and can be considerably smaller. \n \nWe use the same technique to give similar results for DNF formulae."
            },
            "slug": "Generalization-in-Decision-Trees-and-DNF:-Does-Size-Golea-Bartlett",
            "title": {
                "fragments": [],
                "text": "Generalization in Decision Trees and DNF: Does Size Matter?"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper shows that with high probability any decision tree of depth no more than d that is consistent with m training examples has misclassification probabilityNo more than O((1/m(Neff VCdim(U) log2 m log d))1/2), where U is the class of node decision functions, and Neff \u2264 N can be thought of as the effective number of leaves."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2633352"
                        ],
                        "name": "J. Kohlmorgen",
                        "slug": "J.-Kohlmorgen",
                        "structuredName": {
                            "firstName": "Jens",
                            "lastName": "Kohlmorgen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kohlmorgen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144902080"
                        ],
                        "name": "K. Pawelzik",
                        "slug": "K.-Pawelzik",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Pawelzik",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Pawelzik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16115099,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8ee3193983cdf7a9ca7a6edfb9aea191b15d878f",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a framework for the unsupervised segmentation of time series. It applies to non-stationary signals originating from di erent dynamical systems which alternate in time, a phenomenon which appears in many natural systems. In our approach, predictors compete for data points of a given time series. We combine competition and evolutionary inertia to a learning rule. Under this learning rule the system evolves such that the predictors, which nally survive, unambiguously identify the underlying processes. The segmentation achieved by this method is very precise and transients are included, a fact, which makes our approach promising for future applications. key words: neural networks, non-linear dynamics, chaos, time series analysis, prediction, competing neural networks"
            },
            "slug": "Analysis-of-switching-dynamics-with-competing-M\u00fcller-Kohlmorgen",
            "title": {
                "fragments": [],
                "text": "Analysis of switching dynamics with competing neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "The segmentation achieved by the unsupervised segmentation of time series is very precise and transients are included, a fact, which makes the approach promising for future applications."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207165665,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2599131a4bc2fa957338732a37c744cfe3e17b24",
            "isKey": false,
            "numCitedBy": 10833,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms."
            },
            "slug": "A-training-algorithm-for-optimal-margin-classifiers-Boser-Guyon",
            "title": {
                "fragments": [],
                "text": "A training algorithm for optimal margin classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented, applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '92"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 167309,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "20f6d89f13d8397b51f938f795e2666b4c0f33a9",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive the correspondence between regularization operators used in Regularization Networks and Hilbert Schmidt Kernels appearing in Support Vector Machines. More specifically, we prove that the Green's Functions associated with regularization operators are suitable Support Vector Kernels with equivalent regularization properties. As a by-product we show that a large number of Radial Basis Functions namely conditionally positive definite functions may be used as Support Vector kernels."
            },
            "slug": "From-Regularization-Operators-to-Support-Vector-Smola-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "From Regularization Operators to Support Vector Kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "It is proved that the Green's Functions associated with regularization operators are suitable Support Vector Kernels with equivalent regularization properties and a large number of Radial Basis Functions namely conditionally positive definite functions may be used as Support Vector kernels."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144902080"
                        ],
                        "name": "K. Pawelzik",
                        "slug": "K.-Pawelzik",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Pawelzik",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Pawelzik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2633352"
                        ],
                        "name": "J. Kohlmorgen",
                        "slug": "J.-Kohlmorgen",
                        "structuredName": {
                            "firstName": "Jens",
                            "lastName": "Kohlmorgen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kohlmorgen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18329778,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e926cb0288e5f1e0aec68022700b138e116fa331",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of predicting time series originating from mixtures of signals from independent dynamical systems is considered. We show that the problem of finding representations for the dynamics of such systems is hard if the mixing structure of the system is not taken into account. If, on the contrary, the sources can be unmixed in a preprocessing step the complexity of system identification may be drastically reduced. This is demonstrated using chaotic maps. It is shown that applications of methods for blind separation of sources can substantially improve both: prediction performance and prediction horizon."
            },
            "slug": "Prediction-of-Mixtures-Pawelzik-M\u00fcller",
            "title": {
                "fragments": [],
                "text": "Prediction of Mixtures"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that applications of methods for blind separation of sources can substantially improve both: prediction performance and prediction horizon."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683946"
                        ],
                        "name": "A. Ehrenfeucht",
                        "slug": "A.-Ehrenfeucht",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Ehrenfeucht",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ehrenfeucht"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056642528"
                        ],
                        "name": "M. Kearns",
                        "slug": "M.-Kearns",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kearns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kearns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741124"
                        ],
                        "name": "L. Valiant",
                        "slug": "L.-Valiant",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Valiant",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Valiant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1925579,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b83396caf4762c906530c9219a9e4dd0658232b0",
            "isKey": false,
            "numCitedBy": 499,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-general-lower-bound-on-the-number-of-examples-for-Ehrenfeucht-Haussler",
            "title": {
                "fragments": [],
                "text": "A general lower bound on the number of examples needed for learning"
            },
            "venue": {
                "fragments": [],
                "text": "COLT '88"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145039030"
                        ],
                        "name": "J. Platt",
                        "slug": "J.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Platt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 577580,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "53fcc056f79e04daf11eb798a7238e93699665aa",
            "isKey": false,
            "numCitedBy": 2853,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a new algorithm for training support vector machines: Sequential Minimal Optimization, or SMO. Training a support vector machine requires the solution of a very large quadratic programming (QP) optimization problem. SMO breaks this large QP problem into a series of smallest possible QP problems. These small QP problems are solved analytically, which avoids using a time-consuming numerical QP optimization as an inner loop. The amount of memory required for SMO is linear in the training set size, which allows SMO to handle very large training sets. Because matrix computation is avoided, SMO scales somewhere between linear and quadratic in the training set size for various test problems, while the standard chunking SVM algorithm scales somewhere between linear and cubic in the training set size. SMO\u2019s computation time is dominated by SVM evaluation, hence SMO is fastest for linear SVMs and sparse data sets. On realworld sparse data sets, SMO can be more than 1000 times faster than the chunking algorithm."
            },
            "slug": "Sequential-Minimal-Optimization-:-A-Fast-Algorithm-Platt",
            "title": {
                "fragments": [],
                "text": "Sequential Minimal Optimization : A Fast Algorithm for Training Support Vector Machines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144902080"
                        ],
                        "name": "K. Pawelzik",
                        "slug": "K.-Pawelzik",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Pawelzik",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Pawelzik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2633352"
                        ],
                        "name": "J. Kohlmorgen",
                        "slug": "J.-Kohlmorgen",
                        "structuredName": {
                            "firstName": "Jens",
                            "lastName": "Kohlmorgen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kohlmorgen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17485432,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "06db5354327cb66699353c55eacf2d3e13bc1b96",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for the unsupervised segmentation of data streams originating from different unknown sources that alternate in time. We use an architecture consisting of competing neural networks. Memory is included to resolve ambiguities of input-output relations. To obtain maximal specialization, the competition is adiabatically increased during training. Our method achieves almost perfect identification and segmentation in the case of switching chaotic dynamics where input manifolds overlap and input-output relations are ambiguous. Only a small dataset is needed for the training procedure. Applications to time series from complex systems demonstrate the potential relevance of our approach for time series analysis and short-term prediction."
            },
            "slug": "Annealed-Competition-of-Experts-for-a-Segmentation-Pawelzik-Kohlmorgen",
            "title": {
                "fragments": [],
                "text": "Annealed Competition of Experts for a Segmentation and Classification of Switching Dynamics"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "This work presents a method for the unsupervised segmentation of data streams originating from different unknown sources that alternate in time using an architecture consisting of competing neural networks to obtain maximal specialization."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 7035291,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ca97e1668e305fb719845f84a05a62dfb946a5d",
            "isKey": false,
            "numCitedBy": 578,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Very rarely are training data evenly distributed in the input space. Local learning algorithms attempt to locally adjust the capacity of the training system to the properties of the training set in each area of the input space. The family of local learning algorithms contains known methods, like the k-nearest neighbors method (kNN) or the radial basis function networks (RBF), as well as new algorithms. A single analysis models some aspects of these algorithms. In particular, it suggests that neither kNN or RBF, nor nonlocal classifiers, achieve the best compromise between locality and capacity. A careful control of these parameters in a simple local learning algorithm has provided a performance breakthrough for an optical character recognition problem. Both the error rate and the rejection performance have been significantly improved."
            },
            "slug": "Local-Learning-Algorithms-Bottou-Vapnik",
            "title": {
                "fragments": [],
                "text": "Local Learning Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A single analysis suggests that neither kNN or RBF, nor nonlocal classifiers, achieve the best compromise between locality and capacity."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781874"
                        ],
                        "name": "E. Osuna",
                        "slug": "E.-Osuna",
                        "structuredName": {
                            "firstName": "Edgar",
                            "lastName": "Osuna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Osuna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052797964"
                        ],
                        "name": "R. Freund",
                        "slug": "R.-Freund",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Freund",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5667586,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a61a3bf41fc770186a58fa34466af337e997ef6",
            "isKey": false,
            "numCitedBy": 1234,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the problem of training a support vector machine (SVM) on a very large database in the case in which the number of support vectors is also very large. Training a SVM is equivalent to solving a linearly constrained quadratic programming (QP) problem in a number of variables equal to the number of data points. This optimization problem is known to be challenging when the number of data points exceeds few thousands. In previous work done by us as well as by other researchers, the strategy used to solve the large scale QP problem takes advantage of the fact that the expected number of support vectors is small (<3,000). Therefore, the existing algorithms cannot deal with more than a few thousand support vectors. In this paper we present a decomposition algorithm that is guaranteed to solve the QP problem and that does not make assumptions on the expected number of support vectors. In order to present the feasibility of our approach we consider a foreign exchange rate time series database with 110,000 data points that generates 100,000 support vectors."
            },
            "slug": "An-improved-training-algorithm-for-support-vector-Osuna-Freund",
            "title": {
                "fragments": [],
                "text": "An improved training algorithm for support vector machines"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper presents a decomposition algorithm that is guaranteed to solve the QP problem and that does not make assumptions on the expected number of support vectors."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks for Signal Processing VII. Proceedings of the 1997 IEEE Signal Processing Society Workshop"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144902080"
                        ],
                        "name": "K. Pawelzik",
                        "slug": "K.-Pawelzik",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Pawelzik",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Pawelzik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2633352"
                        ],
                        "name": "J. Kohlmorgen",
                        "slug": "J.-Kohlmorgen",
                        "structuredName": {
                            "firstName": "Jens",
                            "lastName": "Kohlmorgen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kohlmorgen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17485432,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "06db5354327cb66699353c55eacf2d3e13bc1b96",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for the unsupervised segmentation of data streams originating from different unknown sources that alternate in time. We use an architecture consisting of competing neural networks. Memory is included to resolve ambiguities of input-output relations. To obtain maximal specialization, the competition is adiabatically increased during training. Our method achieves almost perfect identification and segmentation in the case of switching chaotic dynamics where input manifolds overlap and input-output relations are ambiguous. Only a small dataset is needed for the training procedure. Applications to time series from complex systems demonstrate the potential relevance of our approach for time series analysis and short-term prediction."
            },
            "slug": "Annealed-Competition-of-Experts-for-a-Segmentation-Pawelzik-Kohlmorgen",
            "title": {
                "fragments": [],
                "text": "Annealed Competition of Experts for a Segmentation and Classification of Switching Dynamics"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "This work presents a method for the unsupervised segmentation of data streams originating from different unknown sources that alternate in time using an architecture consisting of competing neural networks to obtain maximal specialization."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 7035291,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ca97e1668e305fb719845f84a05a62dfb946a5d",
            "isKey": false,
            "numCitedBy": 578,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Very rarely are training data evenly distributed in the input space. Local learning algorithms attempt to locally adjust the capacity of the training system to the properties of the training set in each area of the input space. The family of local learning algorithms contains known methods, like the k-nearest neighbors method (kNN) or the radial basis function networks (RBF), as well as new algorithms. A single analysis models some aspects of these algorithms. In particular, it suggests that neither kNN or RBF, nor nonlocal classifiers, achieve the best compromise between locality and capacity. A careful control of these parameters in a simple local learning algorithm has provided a performance breakthrough for an optical character recognition problem. Both the error rate and the rejection performance have been significantly improved."
            },
            "slug": "Local-Learning-Algorithms-Bottou-Vapnik",
            "title": {
                "fragments": [],
                "text": "Local Learning Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A single analysis suggests that neither kNN or RBF, nor nonlocal classifiers, achieve the best compromise between locality and capacity."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704341"
                        ],
                        "name": "E. Amaldi",
                        "slug": "E.-Amaldi",
                        "structuredName": {
                            "firstName": "Edoardo",
                            "lastName": "Amaldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Amaldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747198"
                        ],
                        "name": "V. Kann",
                        "slug": "V.-Kann",
                        "structuredName": {
                            "firstName": "Viggo",
                            "lastName": "Kann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Kann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026Vapnik, Volodya Vovk & Chris Watkins19 Combining Support Vector and Mathematical ProgrammingMethods for Classi cation 307Kristin P. Bennett20 Kernel Principal Component Analysis 327Bernhard Sch olkopf, Alex J. Smola & Klaus-Robert M uller\n1998/08/25 16:31\nviiReferences 353Index 373\n1998/08/25 16:31"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 26
                            }
                        ],
                        "text": "Both areNP-Hard problems (Amaldi and Kann, 1998, 1995), but approximate answers maybe found using nonconvex optimization techniques."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14385658,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d3ada827041115373dc79d2acd76b941c5b2fc0c",
            "isKey": false,
            "numCitedBy": 723,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-Approximability-of-Minimizing-Nonzero-or-in-Amaldi-Kann",
            "title": {
                "fragments": [],
                "text": "On the Approximability of Minimizing Nonzero Variables or Unsatisfied Relations in Linear Systems"
            },
            "venue": {
                "fragments": [],
                "text": "Theor. Comput. Sci."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144884649"
                        ],
                        "name": "C. Saunders",
                        "slug": "C.-Saunders",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Saunders",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Saunders"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70011052"
                        ],
                        "name": "Weston J Stitson Mo",
                        "slug": "Weston-J-Stitson-Mo",
                        "structuredName": {
                            "firstName": "Weston",
                            "lastName": "Stitson Mo",
                            "middleNames": [
                                "J"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weston J Stitson Mo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69902577"
                        ],
                        "name": "Sch\u00f6lkopf B Bottou L",
                        "slug": "Sch\u00f6lkopf-B-Bottou-L",
                        "structuredName": {
                            "firstName": "Sch\u00f6lkopf",
                            "lastName": "Bottou L",
                            "middleNames": [
                                "B"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sch\u00f6lkopf B Bottou L"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60691110,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d36f7543109a8c859d423ddb98bf6d2bd4e13d4d",
            "isKey": false,
            "numCitedBy": 168,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The Support Vector Machine (SVM) is a new type of learning machine. The SVM is a general architecture that can be applied to pattern recognition, regression estimation and other problems. The following researchers were involved in the development of the SVM: The Support Vector Machine (SVM) program allows a user to carry out pattern recognition and regression estimation, using support vector techniques on some given data. If you have any questions not answered by the documentation, you can e-mail us at:"
            },
            "slug": "Support-Vector-Machine-Reference-Manual-Saunders-Mo",
            "title": {
                "fragments": [],
                "text": "Support Vector Machine Reference Manual"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "The Support Vector Machine (SVM) program allows a user to carry out pattern recognition and regression estimation, using support vector techniques on some given data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052864185"
                        ],
                        "name": "P. Bradley",
                        "slug": "P.-Bradley",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Bradley",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bradley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747026"
                        ],
                        "name": "O. Mangasarian",
                        "slug": "O.-Mangasarian",
                        "structuredName": {
                            "firstName": "Olvi",
                            "lastName": "Mangasarian",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Mangasarian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 174
                            }
                        ],
                        "text": "\u2026constructingthe best linear discriminant using the minimum number of attributes; and misclas-si cation minimization: explicitly minimizing the number of points misclassi ed(Bradley and Mangasarian, 1998a; Bradley et al., 1995; Bredensteiner and Ben-nett, 1997; Bennett and Bredensteiner, 1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 256,
                                "start": 226
                            }
                        ],
                        "text": "The greater e ectiveness of lin-ear versus quadratic programming algorithms is de nitely true for general-purposesolvers but optimization methods adapted to SVM problem structure such as theones discussed in this book and in (Bradley and Mangasarian, 1998b) may helpalleviate this di erence."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 129
                            }
                        ],
                        "text": "RLP with 1-norm capacity control has been investigated in several papers (Ben-nett and Bredensteiner, 1998; Bredensteiner, 1997; Bradley and Mangasarian,1998a; Bennett et al., 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5885974,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23d9a5273bd9e08eb68cf3b097836d718e91d70c",
            "isKey": false,
            "numCitedBy": 1053,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Computational comparison is made between two feature selection approaches for nding a separating plane that discriminates between two point sets in an n-dimensional feature space that utilizes as few of the n features (dimensions) as possible. In the concave minimization approach [19, 5] a separating plane is generated by minimizing a weighted sum of distances of misclassi ed points to two parallel planes that bound the sets and which determine the separating plane midway between them. Furthermore, the number of dimensions of the space used to determine the plane is minimized. In the support vector machine approach [27, 7, 1, 10, 24, 28], in addition to minimizing the weighted sum of distances of misclassi ed points to the bounding planes, we also maximize the distance between the two bounding planes that generate the separating plane. Computational results show that feature suppression is an indirect consequence of the support vector machine approach when an appropriate norm is used. Numerical tests on 6 public data sets show that classi ers trained by the concave minimization approach and those trained by a support vector machine have comparable 10fold cross-validation correctness. However, in all data sets tested, the classi ers obtained by the concave minimization approach selected fewer problem features than those trained by a support vector machine."
            },
            "slug": "Feature-Selection-via-Concave-Minimization-and-Bradley-Mangasarian",
            "title": {
                "fragments": [],
                "text": "Feature Selection via Concave Minimization and Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Numerical tests on 6 public data sets show that classi ers trained by the concave minimization approach and those trained by a support vector machine have comparable 10fold cross-validation correctness."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60809283,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db869fa192a3222ae4f2d766674a378e47013b1b",
            "isKey": false,
            "numCitedBy": 3643,
            "numCiting": 92,
            "paperAbstract": {
                "fragments": [],
                "text": "Artificial \"neural networks\" are widely used as flexible models for classification and regression applications, but questions remain about how the power of these models can be safely exploited when training data is limited. This book demonstrates how Bayesian methods allow complex neural network models to be used without fear of the \"overfitting\" that can occur with traditional training methods. Insight into the nature of these complex Bayesian models is provided by a theoretical investigation of the priors over functions that underlie them. A practical implementation of Bayesian neural network learning using Markov chain Monte Carlo methods is also described, and software for it is freely available over the Internet. Presupposing only basic knowledge of probability and statistics, this book should be of interest to researchers in statistics, engineering, and artificial intelligence."
            },
            "slug": "Bayesian-Learning-for-Neural-Networks-Neal",
            "title": {
                "fragments": [],
                "text": "Bayesian Learning for Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Bayesian Learning for Neural Networks shows that Bayesian methods allow complex neural network models to be used without fear of the \"overfitting\" that can occur with traditional neural network learning methods."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740222"
                        ],
                        "name": "Wee Sun Lee",
                        "slug": "Wee-Sun-Lee",
                        "structuredName": {
                            "firstName": "Wee",
                            "lastName": "Lee",
                            "middleNames": [
                                "Sun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wee Sun Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143957317"
                        ],
                        "name": "R. C. Williamson",
                        "slug": "R.-C.-Williamson",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Williamson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Williamson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1298646,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "2a024c5dea98fe9d1ae2d3ed7ee6c7b157854e4d",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We show that if the closure of a function class under the metric induced by some probability distribution is not convex, then the sample complexity for agnostically learning with squared loss (using only hypotheses in )i s where is the probability of success and is the required accuracy. In comparison, if the class is convex and has finite pseudodimension, then the sample complexity is . If a nonconvex class has finite pseudodimension, then the sample complexity for agnostically learning the closure of the convex hull of ,i s . Hence, for agnostic learning, learning the convex hull provides better approximation capabilities with little sample complexity penalty."
            },
            "slug": "The-importance-of-convexity-in-learning-with-loss-Lee-Bartlett",
            "title": {
                "fragments": [],
                "text": "The importance of convexity in learning with squared loss"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "It is shown that if the closure of a function class under the metric induced by some probability distribution is not convex, then the sample complexity for agnostically learning with squared loss is lower than that for agnostic learning, so learning the convex hull provides better approximation capabilities with little sample complexity penalty."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '96"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32980796"
                        ],
                        "name": "Sreerama K. Murthy",
                        "slug": "Sreerama-K.-Murthy",
                        "structuredName": {
                            "firstName": "Sreerama",
                            "lastName": "Murthy",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sreerama K. Murthy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689332"
                        ],
                        "name": "S. Kasif",
                        "slug": "S.-Kasif",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Kasif",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kasif"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744109"
                        ],
                        "name": "S. Salzberg",
                        "slug": "S.-Salzberg",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Salzberg",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Salzberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 194
                            }
                        ],
                        "text": "SVDT performs top down induction of decision trees (TDIDT) like manyother decision tree algorithms including CHAID, CART, MSMT, C4.5, and OC1(Breiman et al., 1984; Bennett, 1992; Quinlan, 1993; Murthy et al., 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 13
                            }
                        ],
                        "text": "By combining statistical learning theory concepts andSVM ideas, such as kernels, with MPM, potentially many new SVM methods canbe derived."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11407469,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9fad1ee8b6ed596f72b81f61f01def620a4ee997",
            "isKey": false,
            "numCitedBy": 923,
            "numCiting": 108,
            "paperAbstract": {
                "fragments": [],
                "text": "This article describes a new system for induction of oblique decision trees. This system, OC1, combines deterministic hill-climbing with two forms of randomization to find a good oblique split (in the form of a hyperplane) at each node of a decision tree. Oblique decision tree methods are tuned especially for domains in which the attributes are numeric, although they can be adapted to symbolic or mixed symbolic/numeric attributes. We present extensive empirical studies, using both real and artificial data, that analyze OC1's ability to construct oblique trees that are smaller and more accurate than their axis-parallel counterparts. We also examine the benefits of randomization for the construction of oblique decision trees."
            },
            "slug": "A-System-for-Induction-of-Oblique-Decision-Trees-Murthy-Kasif",
            "title": {
                "fragments": [],
                "text": "A System for Induction of Oblique Decision Trees"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "This system, OC1, combines deterministic hill-climbing with two forms of randomization to find a good oblique split (in the form of a hyperplane) at each node of a decision tree."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Uniform convergence of frequencies of occurence of events to their probabilities"
            },
            "venue": {
                "fragments": [],
                "text": "Dokl. Akad. Nauk SSSR"
            },
            "year": 1968
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fast approximation of supportvector kernel expansions , and an interpretation of clustering as approximationin feature spaces"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Positive deenite functions on spheres. Duke Math"
            },
            "venue": {
                "fragments": [],
                "text": "J"
            },
            "year": 1942
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Another approach to polychotomous classiication. Technical re- 1998/08/25 16:31 358 References port"
            },
            "venue": {
                "fragments": [],
                "text": "Another approach to polychotomous classiication. Technical re- 1998/08/25 16:31 358 References port"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A new method for constructing artiicial neural networks: Interim technical report, ONR contract N00014-94-c-0186"
            },
            "venue": {
                "fragments": [],
                "text": "A new method for constructing artiicial neural networks: Interim technical report, ONR contract N00014-94-c-0186"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Priors for innnite networks"
            },
            "venue": {
                "fragments": [],
                "text": "Priors for innnite networks"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Incline Village, Nevada. Using the CPLEX Callable Library"
            },
            "venue": {
                "fragments": [],
                "text": "CPLEX Optimization Incorporated"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalization and the size of the weights: an experimental study"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Eighth Australian Conference on Neural Networks"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Reconstruction of underlying dynamics of an observed chaotic process"
            },
            "venue": {
                "fragments": [],
                "text": "Comm. Res. Lab"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Vilenkin. Special Functions and the Theory of Group Representations, volume 22 of Translations of Mathematical Monographs"
            },
            "venue": {
                "fragments": [],
                "text": "Vilenkin. Special Functions and the Theory of Group Representations, volume 22 of Translations of Mathematical Monographs"
            },
            "year": 1968
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Embedology. J. Stat. Phys"
            },
            "venue": {
                "fragments": [],
                "text": "Embedology. J. Stat. Phys"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Structural riskminimization over datadependent hierarchies"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Talagrand . The Glivenko { Cantelli problem , ten years later"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Theoretical Probability"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nonlinear perturbations of linear programs"
            },
            "venue": {
                "fragments": [],
                "text": "SIAM Journal on Control and Optimization"
            },
            "year": 1979
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Globally trained handwritten word recognizer using spatial representation, convolutional neural networks and hidden 1998/08/25 16:31 References markov models"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its Applications"
            },
            "venue": {
                "fragments": [],
                "text": "On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its Applications"
            },
            "year": 1971
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the uniform convergence of relative frequenciesof events to their probabilities"
            },
            "venue": {
                "fragments": [],
                "text": "Dokl . Akad . Nauk SSSR"
            },
            "year": 1968
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On absolute continuity of measures corresponding to homogeneous Gaussian elds. Theory of Probability and its Applications"
            },
            "venue": {
                "fragments": [],
                "text": "On absolute continuity of measures corresponding to homogeneous Gaussian elds. Theory of Probability and its Applications"
            },
            "year": 1973
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 142
                            }
                        ],
                        "text": "SVDT performs top down induction of decision trees (TDIDT) like manyother decision tree algorithms including CHAID, CART, MSMT, C4.5, and OC1(Breiman et al., 1984; Bennett, 1992; Quinlan, 1993; Murthy et al., 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 34
                            }
                        ],
                        "text": "Thus a great potential exists for interactionbetween the two approaches."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Classiication and Regression Trees"
            },
            "venue": {
                "fragments": [],
                "text": "Classiication and Regression Trees"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimation of the dimensionof a noisy attractor"
            },
            "venue": {
                "fragments": [],
                "text": "Physical Review E"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 102
                            }
                        ],
                        "text": "Inthe scoring process, potential customers are selected based on the model and theselected threshold (Thomas, 1996; Hughes, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Database marketing: Dual approach outdoes response modeling"
            },
            "venue": {
                "fragments": [],
                "text": "Database Marketing News"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 4
                            }
                        ],
                        "text": "See(Bredensteiner and Bennett, 1998; Bredensteiner, 1997) for full details on theproblem formulation and results."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 75
                            }
                        ],
                        "text": "These results are drawn primarily from existing work (Bennett et al.,1998; Bredensteiner and Bennett, 1998; Bennett and Demiriz, 1998)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 140
                            }
                        ],
                        "text": "\u2026Vovk, Chris Watkins & Jason Weston18 Support Vector Density Estimation 293Jason Weston, Alex Gammerman, Mark O. Stitson,Vladimir Vapnik, Volodya Vovk & Chris Watkins19 Combining Support Vector and Mathematical ProgrammingMethods for Classi cation 307Kristin P. Bennett20 Kernel Principal\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multicategory classiication by support vector machines"
            },
            "venue": {
                "fragments": [],
                "text": "Computational Optimization and Applications"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Computation with innnite networks"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems 9"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hedonic prices and the demand for clean air Original source of the Boston Housing data, actually from ftp://ftp.ics.uci.com/pub/machine-learning-databases/housing"
            },
            "venue": {
                "fragments": [],
                "text": "In J. Environ. Economics & Management"
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "WI"
            },
            "venue": {
                "fragments": [],
                "text": "WI"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "E \u000e cient pattern recognition using a newtransformation distance"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Computational Learning Theory, volume 30 of Cambridge Tracts in Theoretical Computer Science Theory of reproducing kernels"
            },
            "venue": {
                "fragments": [],
                "text": "Transactions of the American Mathematical Society"
            },
            "year": 1950
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Physics of generalization"
            },
            "venue": {
                "fragments": [],
                "text": "Physics of Neural Networks III"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fuzzy topographic kernel clustering"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 5th GI Workshop Fuzzy Neuro Systems '98"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Support Vector Learning. R. Oldenbourg Verlag"
            },
            "venue": {
                "fragments": [],
                "text": "Support Vector Learning. R. Oldenbourg Verlag"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The rst census optical character recognition system conference"
            },
            "venue": {
                "fragments": [],
                "text": "National Institute of Standards and Technology (NIST)"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Detecting strange attractors in uid turbulence"
            },
            "venue": {
                "fragments": [],
                "text": "Dynamical Systems and Turbulence"
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Introduction to Partial Diierential Equations with Applications"
            },
            "venue": {
                "fragments": [],
                "text": "Introduction to Partial Diierential Equations with Applications"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pattern classiication techniques based on function 1998/08/25 16:31 References 361 approximation"
            },
            "venue": {
                "fragments": [],
                "text": "Handbook on Optical Character Recognition and Document Analysis"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pattern classiication in neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Support vector machines: Training and applications. AI Memo 1602"
            },
            "venue": {
                "fragments": [],
                "text": "Support vector machines: Training and applications. AI Memo 1602"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Uniform convergence of frequencies of occurenceof events to their probabilities"
            },
            "venue": {
                "fragments": [],
                "text": "Automationand Remote Control"
            },
            "year": 1964
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Asymptotic behaviour of eigenvalues of certain integral operators. Archive for Rational Mechanics and Analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Asymptotic behaviour of eigenvalues of certain integral operators. Archive for Rational Mechanics and Analysis"
            },
            "year": 1964
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Techniques in Partial Di erential Equations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Simple architectures on fast machines: practical issues in nonlinear time series prediction"
            },
            "venue": {
                "fragments": [],
                "text": "Time Series Prediction: Forecasting the Future and Understanding the Past. Santa Fe Institute"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimal unsupervised learning in a single-layer linear feedforward network"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Database marketing : Dual approach outdoes response modeling"
            },
            "venue": {
                "fragments": [],
                "text": "Database Marketing News"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Geometry from a 1998/08/25 16:31 References time series"
            },
            "venue": {
                "fragments": [],
                "text": "Phys. Rev. Lett"
            },
            "year": 1980
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Structure of statistical learning theory Computational and Probabalistic Reasoning, chapter 1"
            },
            "venue": {
                "fragments": [],
                "text": "Vapnik and A. Chervonenkis. A note on one class of perceptrons. Automation and Remote Control"
            },
            "year": 1964
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "While MSM was successfully used in an initial automatedbreast cancer diagnosis system at the University of Wisconsin-Madison (Wolberg\n1998/08/25 16:31"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Building projectable classiiers for arbitrary complexity"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 12th International Conference on Pattern Recognition"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Priors, stabilizers and basis functions: From regularization to radial, tensor and additive splines. A.I. Memo No"
            },
            "venue": {
                "fragments": [],
                "text": "Priors, stabilizers and basis functions: From regularization to radial, tensor and additive splines. A.I. Memo No"
            },
            "year": 1430
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A framework forstructural risk minimization"
            },
            "venue": {
                "fragments": [],
                "text": "Physical Review A"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 108
                            }
                        ],
                        "text": "These results are drawn primarily from existing work (Bennett et al.,1998; Bredensteiner and Bennett, 1998; Bennett and Demiriz, 1998)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semi-supervised support vector machines. Unpublished manuscript based on talk given at Machines That Learn Conference"
            },
            "venue": {
                "fragments": [],
                "text": "Semi-supervised support vector machines. Unpublished manuscript based on talk given at Machines That Learn Conference"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "While MSM was successfully used in an initial automatedbreast cancer diagnosis system at the University of Wisconsin-Madison (Wolberg\n1998/08/25 16:31"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 116
                            }
                        ],
                        "text": "Inthe scoring process, potential customers are selected based on the model and theselected threshold (Thomas, 1996; Hughes, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Complete Database Marketer"
            },
            "venue": {
                "fragments": [],
                "text": "The Complete Database Marketer"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Principal Component Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Principal Component Neural Networks"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Deterministic nonperiodic ow"
            },
            "venue": {
                "fragments": [],
                "text": "J. Atmos. Sci"
            },
            "year": 1963
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Some results on Tchebycheean spline functions"
            },
            "venue": {
                "fragments": [],
                "text": "J. Math. Anal. Applic"
            },
            "year": 1971
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Some results on Tchebycheean spline functions"
            },
            "venue": {
                "fragments": [],
                "text": "J. Math. Anal. Applic"
            },
            "year": 1971
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Speaker identiication via support vector classiiers"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. ICASSP '96"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Chaotic time series: Part ii. system identiication and prediction. Modeling, Identiication and Control"
            },
            "venue": {
                "fragments": [],
                "text": "Chaotic time series: Part ii. system identiication and prediction. Modeling, Identiication and Control"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On lines and planes of closest t to points in space"
            },
            "venue": {
                "fragments": [],
                "text": "Philosophical Magazine"
            },
            "year": 1901
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Structure of statistical learning theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Comparison of classiier methods: a case study in handwritten digit recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 12th International Conference on Pattern Recognition and Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Eigenvalue Distribution of Compact Operators Birkhh auser, Basel, 1986. U. Kreeel. The impact of the learning{set size in handwritten{digit recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Artiicial Neural Networks | ICANN'91 North{Holland. U. Kreeel. Polynomial classiiers and support vector machines. In W. Gerstner et al., editor, Artiicial Neural Networks | ICANN'97"
            },
            "year": 1327
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Necessary and suucient conditions for the uniform convergence of means to their expectations. Theory of Probability and its Applications"
            },
            "venue": {
                "fragments": [],
                "text": "Necessary and suucient conditions for the uniform convergence of means to their expectations. Theory of Probability and its Applications"
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Robust statistics: a review"
            },
            "venue": {
                "fragments": [],
                "text": "Ann. Statist"
            },
            "year": 1972
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Techniques in Partial Diierential Equations"
            },
            "venue": {
                "fragments": [],
                "text": "Techniques in Partial Diierential Equations"
            },
            "year": 1971
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 18,
            "methodology": 23,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 260,
        "totalPages": 26
    },
    "page_url": "https://www.semanticscholar.org/paper/Advances-in-kernel-methods:-support-vector-learning-Sch\u00f6lkopf-Burges/9c4da62e9e89e65ac78ee271e424e8b498053e8c?sort=total-citations"
}