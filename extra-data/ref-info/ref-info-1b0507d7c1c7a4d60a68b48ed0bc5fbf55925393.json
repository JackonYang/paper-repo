{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31649548"
                        ],
                        "name": "Baokang Wang",
                        "slug": "Baokang-Wang",
                        "structuredName": {
                            "firstName": "Baokang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Baokang Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107903806"
                        ],
                        "name": "Changsong Liu",
                        "slug": "Changsong-Liu",
                        "structuredName": {
                            "firstName": "Changsong",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Changsong Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145507765"
                        ],
                        "name": "X. Ding",
                        "slug": "X.-Ding",
                        "structuredName": {
                            "firstName": "Xiaoqing",
                            "lastName": "Ding",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Ding"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62683938,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "594a582989a701286097aad515412156c481a0c5",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Nowadays, video has gradually become the mainstream of dissemination media for its rich information capacity and intelligibility, and texts in videos often carry significant semantic information, thus making great contribution to video content understanding and construction of content-based video retrieval system. Text-based video analyses usually consist of text detection, localization, tracking, segmentation and recognition. There has been a large amount of research done on video text detection and tracking, but most solutions focus on text content processing in static frames, few making full use of redundancy between video frames. In this paper, a unified framework for text detection, localization and tracking in video frames is proposed. We select edge and corner distribution of text blocks as text features, localizing and tracking are performed. By making good use of redundancy between frames, location relations and motion characteristics are determined, thus effectively reduce false-alarm and raise correct rate in localizing. Tracking schemes are proposed for static and rolling texts respectively. Through multi-frame integration, text quality is promoted, so is correct rate of OCR. Experiments demonstrate the reduction of false-alarm and the increase of correct rate of localization and recognition."
            },
            "slug": "A-research-on-Video-text-tracking-and-recognition-Wang-Liu",
            "title": {
                "fragments": [],
                "text": "A research on Video text tracking and recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A unified framework for text detection, localization and tracking in video frames is proposed and edge and corner distribution of text blocks are selected as text features, localizing and tracking are performed to effectively reduce false-alarm and raise correct rate in localizing."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2621738"
                        ],
                        "name": "Xuejian Rong",
                        "slug": "Xuejian-Rong",
                        "structuredName": {
                            "firstName": "Xuejian",
                            "lastName": "Rong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuejian Rong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40263913"
                        ],
                        "name": "Chucai Yi",
                        "slug": "Chucai-Yi",
                        "structuredName": {
                            "firstName": "Chucai",
                            "lastName": "Yi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chucai Yi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38101706"
                        ],
                        "name": "Xiaodong Yang",
                        "slug": "Xiaodong-Yang",
                        "structuredName": {
                            "firstName": "Xiaodong",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodong Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35484757"
                        ],
                        "name": "Yingli Tian",
                        "slug": "Yingli-Tian",
                        "structuredName": {
                            "firstName": "Yingli",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yingli Tian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6652095,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5df427398f72c10f83578887a20378a53e2f6df0",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Text signage as visual indicators in natural scene plays an important role in navigation and notification in our daily life. Most previous methods of scene text extraction are developed from a single scene image. In this paper, we propose a multi-frame based scene text recognition method by tracking text regions in a video captured by a moving camera. The main contributions of this paper are as follows. First, we present a framework of scene text recognition in multiple frames based on feature representation of scene text character (STC) for character prediction and conditional random field (CRF) model for word configuration. Second, a feature representation of STC is employed from dense sampled SIFT descriptors and Fisher Vector. Third, we collect a dataset for text information extraction from natural scene videos. Our proposed multi-frame scene text recognition is more compatible with image/video-based mobile applications. The experimental results demonstrate that STC prediction and word configuration in multiple frames based on text tracking significantly improves the performance of scene text recognition."
            },
            "slug": "Scene-text-recognition-in-multiple-frames-based-on-Rong-Yi",
            "title": {
                "fragments": [],
                "text": "Scene text recognition in multiple frames based on text tracking"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper proposes a multi-frame based scene text recognition method by tracking text regions in a video captured by a moving camera that is more compatible with image/video-based mobile applications."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE International Conference on Multimedia and Expo (ICME)"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127860"
                        ],
                        "name": "Yinan Na",
                        "slug": "Yinan-Na",
                        "structuredName": {
                            "firstName": "Yinan",
                            "lastName": "Na",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yinan Na"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145562240"
                        ],
                        "name": "Di Wen",
                        "slug": "Di-Wen",
                        "structuredName": {
                            "firstName": "Di",
                            "lastName": "Wen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Di Wen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 23468781,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b2509153ada60f99e6a8de429ba0d164c981f10e",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Video text provides important clues for semantic-based video analysis, indexing and retrieval. And text tracking is performed to locate specific text information across video frames and enhance text segmentation and recognition over time. This paper presents a multilingual video text tracking algorithm based on the extraction and tracking of Scale Invariant Feature Transform (SIFT) features description through video frames. SIFT features are extracted from video frames to correspond the region of interests across frames. Meanwhile, a global matching method using geometric constraint is proposed to decrease false matches, which effectively improves the accuracy and stability of text tracking results. Based on the correct matches, the motion of text is estimated in adjacent frames and a match score of text is calculated to determine Text Change Boundary (TCB). Experimental results on a large number of video frames show that the proposed text tracking algorithm is robust to different text forms, including multilingual captions, credits, scene texts with shift, rotation and scale change, under complex backgrounds and light changing."
            },
            "slug": "An-Effective-Video-Text-Tracking-Algorithm-Based-on-Na-Wen",
            "title": {
                "fragments": [],
                "text": "An Effective Video Text Tracking Algorithm Based on SIFT Feature and Geometric Constraint"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Experimental results show that the proposed text tracking algorithm is robust to different text forms, including multilingual captions, credits, scene texts with shift, rotation and scale change, under complex backgrounds and light changing."
            },
            "venue": {
                "fragments": [],
                "text": "PCM"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785083"
                        ],
                        "name": "Michael R. Lyu",
                        "slug": "Michael-R.-Lyu",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lyu",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael R. Lyu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3332642"
                        ],
                        "name": "Jiqiang Song",
                        "slug": "Jiqiang-Song",
                        "structuredName": {
                            "firstName": "Jiqiang",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiqiang Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052052370"
                        ],
                        "name": "Min Cai",
                        "slug": "Min-Cai",
                        "structuredName": {
                            "firstName": "Min",
                            "lastName": "Cai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Min Cai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18648576,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e14cf92ecb1589d21324d934b2009451e602d1be",
            "isKey": false,
            "numCitedBy": 371,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Text in video is a very compact and accurate clue for video indexing and summarization. Most video text detection and extraction methods hold assumptions on text color, background contrast, and font style. Moreover, few methods can handle multilingual text well since different languages may have quite different appearances. This paper performs a detailed analysis of multilingual text characteristics, including English and Chinese. Based on the analysis, we propose a comprehensive, efficient video text detection, localization, and extraction method, which emphasizes the multilingual capability over the whole processing. The proposed method is also robust to various background complexities and text appearances. The text detection is carried out by edge detection, local thresholding, and hysteresis edge recovery. The coarse-to-fine localization scheme is then performed to identify text regions accurately. The text extraction consists of adaptive thresholding, dam point labeling, and inward filling. Experimental results on a large number of video images and comparisons with other methods are reported in detail."
            },
            "slug": "A-comprehensive-method-for-multilingual-video-text-Lyu-Song",
            "title": {
                "fragments": [],
                "text": "A comprehensive method for multilingual video text detection, localization, and extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A comprehensive, efficient video text detection, localization, and extraction method, which emphasizes the multilingual capability over the whole processing, and is also robust to various background complexities and text appearances."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Circuits and Systems for Video Technology"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115196978"
                        ],
                        "name": "Huiping Li",
                        "slug": "Huiping-Li",
                        "structuredName": {
                            "firstName": "Huiping",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huiping Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3272081"
                        ],
                        "name": "O. Kia",
                        "slug": "O.-Kia",
                        "structuredName": {
                            "firstName": "Omid",
                            "lastName": "Kia",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Kia"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 94
                            }
                        ],
                        "text": "The major researchers concerned with video text tracking mainly use template matching methods [12], [l3], [14], [15], [16] to track graphic text (video captions)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[12] proposed a SSD (Sum of Square Differences) based correlation module to track the detected text between consecutive frames."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 94
                            }
                        ],
                        "text": "To improve the efficiency, text detection was only implemented on some key frames in video in [12], [23]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15485643,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f8f5c282dc11937d29183b955dc3e4fbb677571b",
            "isKey": false,
            "numCitedBy": 652,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "Text that appears in a scene or is graphically added to video can provide an important supplemental source of index information as well as clues for decoding the video's structure and for classification. In this work, we present algorithms for detecting and tracking text in digital video. Our system implements a scale-space feature extractor that feeds an artificial neural processor to detect text blocks. Our text tracking scheme consists of two modules: a sum of squared difference (SSD)-based module to find the initial position and a contour-based module to refine the position. Experiments conducted with a variety of video sources show that our scheme can detect and track text robustly."
            },
            "slug": "Automatic-text-detection-and-tracking-in-digital-Li-Doermann",
            "title": {
                "fragments": [],
                "text": "Automatic text detection and tracking in digital video"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work presents algorithms for detecting and tracking text in digital video that implements a scale-space feature extractor that feeds an artificial neural processor to detect text blocks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152179974"
                        ],
                        "name": "Jianming Hu",
                        "slug": "Jianming-Hu",
                        "structuredName": {
                            "firstName": "Jianming",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianming Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39903487"
                        ],
                        "name": "Jie Xi",
                        "slug": "Jie-Xi",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Xi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Xi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678966"
                        ],
                        "name": "Lide Wu",
                        "slug": "Lide-Wu",
                        "structuredName": {
                            "firstName": "Lide",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lide Wu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 31
                            }
                        ],
                        "text": "Conventional methods [7], [8], [9], [10] for video text detection problem mainly focus on detecting text in each individual frame or some key frames."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 36880197,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e2d72f5a3c55c6d5e9ad2a2277bbfcfef2111285",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Textual information in a video is very useful for video indexing and retrieving. Detecting text blocks in video frames is the first important procedure for extracting the textual information. Automatic text location is a very difficult problem due to the large variety of character styles and the complex backgrounds. In this paper, we describe the various steps of the proposed text detection algorithm. First, the gray scale edges are detected and smoothed horizontally. Second, the edge image is binarized, and run length analysis is applied to find candidate text blocks. Finally, each detected block is verified by an improved logical level technique (ILLT). Experiments show this method is not sensitive to color/texture changes of the characters, and can be used to detect text lines in news videos effectively."
            },
            "slug": "Automatic-Detection-and-Verification-of-Text-in-Hu-Xi",
            "title": {
                "fragments": [],
                "text": "Automatic Detection and Verification of Text Regions in News Video Frames"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Experiments show the proposed text detection algorithm is not sensitive to color/texture changes of the characters, and can be used to detect text lines in news videos effectively."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Pattern Recognit. Artif. Intell."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744575"
                        ],
                        "name": "P. Shivakumara",
                        "slug": "P.-Shivakumara",
                        "structuredName": {
                            "firstName": "Palaiahnakote",
                            "lastName": "Shivakumara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Shivakumara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39221003"
                        ],
                        "name": "Rushi Padhuman Sreedhar",
                        "slug": "Rushi-Padhuman-Sreedhar",
                        "structuredName": {
                            "firstName": "Rushi",
                            "lastName": "Sreedhar",
                            "middleNames": [
                                "Padhuman"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rushi Padhuman Sreedhar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715066"
                        ],
                        "name": "T. Phan",
                        "slug": "T.-Phan",
                        "structuredName": {
                            "firstName": "Trung",
                            "lastName": "Phan",
                            "middleNames": [
                                "Quy"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Phan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771189"
                        ],
                        "name": "Shijian Lu",
                        "slug": "Shijian-Lu",
                        "structuredName": {
                            "firstName": "Shijian",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shijian Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679749"
                        ],
                        "name": "C. Tan",
                        "slug": "C.-Tan",
                        "structuredName": {
                            "firstName": "Chew",
                            "lastName": "Tan",
                            "middleNames": [
                                "Lim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "Conventional methods [7], [8], [9], [10] for video text detection problem mainly focus on detecting text in each individual frame or some key frames."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7444511,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2143c853913968945037eaedd420d8db7a4022e7",
            "isKey": false,
            "numCitedBy": 107,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Multioriented text detection in video frames is not as easy as detection of captions or graphics or overlaid texts, which usually appears in the horizontal direction and has high contrast compared to its background. Multioriented text generally refers to scene text that makes text detection more challenging and interesting due to unfavorable characteristics of scene text. Therefore, conventional text detection methods may not give good results for multioriented scene text detection. Hence, in this paper, we present a new enhancement method that includes the product of Laplacian and Sobel operations to enhance text pixels in videos. To classify true text pixels, we propose a Bayesian classifier without assuming a priori probability about the input frame but estimating it based on three probable matrices. Three different ways of clustering are performed on the output of the enhancement method to obtain the three probable matrices. Text candidates are obtained by intersecting the output of the Bayesian classifier with the Canny edge map of the input frame. A boundary growing method is introduced to traverse the multioriented scene text lines using text candidates. The boundary growing method works based on the concept of nearest neighbors. The robustness of the method has been tested on a variety of datasets that include our own created data (nonhorizontal and horizontal text data) and two publicly available data, namely, video frames of Hua and complex scene text data of ICDAR 2003 competition (camera images). Experimental results show that the performance of the proposed method is encouraging compared with results of existing methods in terms of recall, precision, F-measures, and computational times."
            },
            "slug": "Multioriented-Video-Scene-Text-Detection-Through-Shivakumara-Sreedhar",
            "title": {
                "fragments": [],
                "text": "Multioriented Video Scene Text Detection Through Bayesian Classification and Boundary Growing"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new enhancement method that includes the product of Laplacian and Sobel operations to enhance text pixels in videos and proposes a Bayesian classifier without assuming a priori probability about the input frame but estimating it based on three probable matrices."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Circuits and Systems for Video Technology"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796455"
                        ],
                        "name": "R. Minetto",
                        "slug": "R.-Minetto",
                        "structuredName": {
                            "firstName": "Rodrigo",
                            "lastName": "Minetto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Minetto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728523"
                        ],
                        "name": "Nicolas Thome",
                        "slug": "Nicolas-Thome",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Thome",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Thome"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51021910"
                        ],
                        "name": "M. Cord",
                        "slug": "M.-Cord",
                        "structuredName": {
                            "firstName": "Matthieu",
                            "lastName": "Cord",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Cord"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756125"
                        ],
                        "name": "N. J. Leite",
                        "slug": "N.-J.-Leite",
                        "structuredName": {
                            "firstName": "Neucimar",
                            "lastName": "Leite",
                            "middleNames": [
                                "Jer\u00f4nimo"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. J. Leite"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719901"
                        ],
                        "name": "J. Stolfi",
                        "slug": "J.-Stolfi",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Stolfi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Stolfi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17419543,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9eb30d9450f09963f3a2a07c4692657e98c77f5",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we introduced SnooperTrack, an algorithm for the automatic detection and tracking of text objects \u2014 such as store names, traffic signs, license plates, and advertisements \u2014 in videos of outdoor scenes. The purpose is to improve the performances of text detection process in still images by taking advantage of the temporal coherence in videos. We first propose an efficient tracking algorithm using particle filtering framework with original region descriptors. The second contribution is our strategy to merge tracked regions and new detections. We also propose an improved version of our previously published text detection algorithm in still images. Tests indicate that SnooperTrack is fast, robust, enable false positive suppression, and achieved great performances in complex videos of outdoor scenes."
            },
            "slug": "Snoopertrack:-Text-detection-and-tracking-for-Minetto-Thome",
            "title": {
                "fragments": [],
                "text": "Snoopertrack: Text detection and tracking for outdoor videos"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "An efficient tracking algorithm using particle filtering framework with original region descriptors and a strategy to merge tracked regions and new detections to improve the performances of text detection process in still images by taking advantage of the temporal coherence in videos."
            },
            "venue": {
                "fragments": [],
                "text": "2011 18th IEEE International Conference on Image Processing"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109645173"
                        ],
                        "name": "Zuyi Li",
                        "slug": "Zuyi-Li",
                        "structuredName": {
                            "firstName": "Zuyi",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zuyi Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2158130954"
                        ],
                        "name": "G. Liu",
                        "slug": "G.-Liu",
                        "structuredName": {
                            "firstName": "Guo",
                            "lastName": "Liu",
                            "middleNames": [
                                "Ping"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067734216"
                        ],
                        "name": "Xiangchen Qian",
                        "slug": "Xiangchen-Qian",
                        "structuredName": {
                            "firstName": "Xiangchen",
                            "lastName": "Qian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiangchen Qian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068109890"
                        ],
                        "name": "Di Guo",
                        "slug": "Di-Guo",
                        "structuredName": {
                            "firstName": "Di",
                            "lastName": "Guo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Di Guo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47067776"
                        ],
                        "name": "Hongbo Jiang",
                        "slug": "Hongbo-Jiang",
                        "structuredName": {
                            "firstName": "Hongbo",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hongbo Jiang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58065643,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4cedb1297f4960c7b73245b8dfbc20f40101baa",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Text information contains important clues for video analysis, indexing and retrieval. Effective and efficient text extraction has been a challenging and significant topic. Focusing on this issue, this study proposes a video text extraction scheme using key text points (KTPs). A KTP is defined as the point that has strong textural structure in multi-directions simultaneously. First, a KTP can be acquired by the three high-frequency subbands obtained from the wavelet transform. An anti-texture-direction-projection method is then proposed to improve the accuracy of text localisation and verification. Second, the difference at the KTPs between neighbouring frames is calculated as the similarity measure in text tracking, which can reduce the influence of dramatic background variation. Finally, the total number of the KTPs in each connected component is calculated to remove the background interference and to extract texts for text segmentation. Experimental results show that the proposed text detection and localisation algorithm is robust to the font size, style, colour and alignment of texts. Text tracking significantly improves the efficiency of text detection, and the similarity measure based on KTPs decreases the influence of scene changes and motions. The proposed text segmentation has a promising performance when processing video scenes with complex backgrounds and low contrast between texts and backgrounds."
            },
            "slug": "Effective-and-efficient-video-text-extraction-using-Li-Liu",
            "title": {
                "fragments": [],
                "text": "Effective and efficient video text extraction using key text points"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Experimental results show that the proposed text detection and localisation algorithm is robust to the font size, style, colour and alignment of texts and decreases the influence of scene changes and motions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682664"
                        ],
                        "name": "Xu-Cheng Yin",
                        "slug": "Xu-Cheng-Yin",
                        "structuredName": {
                            "firstName": "Xu-Cheng",
                            "lastName": "Yin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xu-Cheng Yin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8135633"
                        ],
                        "name": "Xuwang Yin",
                        "slug": "Xuwang-Yin",
                        "structuredName": {
                            "firstName": "Xuwang",
                            "lastName": "Yin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuwang Yin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5380819"
                        ],
                        "name": "Kaizhu Huang",
                        "slug": "Kaizhu-Huang",
                        "structuredName": {
                            "firstName": "Kaizhu",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaizhu Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143680014"
                        ],
                        "name": "Hongwei Hao",
                        "slug": "Hongwei-Hao",
                        "structuredName": {
                            "firstName": "Hongwei",
                            "lastName": "Hao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hongwei Hao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 65
                            }
                        ],
                        "text": "In this approach, a state-of-the-art scene text detection module [1] is first used to detect text in each video frame."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 42
                            }
                        ],
                        "text": "We first use the text detection method in [1] to detect text words in each video frame, and then focus on tracking to improve the accuracy of detection."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 89
                            }
                        ],
                        "text": "The text detection procedure used in our method is based on the USTB _ TexStar algorithm [1], [24]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 18
                            }
                        ],
                        "text": "A text classifier [1] is utilized to determine whether there is a word in predicted text block."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 42
                            }
                        ],
                        "text": "Compared to the text detection results in [1], the average performance of text tracking has an increase of f-score by 14%."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 29
                            }
                        ],
                        "text": "'s method [17] and method in [1] respectively."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 54522713,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff8e46522ef1a0c5dffd72a4f6faf4cdf57b8061",
            "isKey": false,
            "numCitedBy": 255,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Text detection in natural scene images is an important prerequisite for many content-based image analysis tasks. In this paper, we propose an accurate and robust method for detecting texts in natural scene images. A fast and effective pruning algorithm is designed to extract Maximally Stable Extremal Regions (MSERs) as character candidates using the strategy of minimizing regularized variations. Character candidates are grouped into text candidates by the single-link clustering algorithm, where distance weights and clustering threshold are learned automatically by a novel self-training distance metric learning algorithm. The posterior probabilities of text candidates corresponding to non-text are estimated with a character classifier; text candidates with high non-text probabilities are eliminated and texts are identified with a text classifier. The proposed system is evaluated on the ICDAR 2011 Robust Reading Competition database; the f-measure is over 76%, much better than the state-of-the-art performance of 71%. Experiments on multilingual, street view, multi-orientation and even born-digital databases also demonstrate the effectiveness of the proposed method."
            },
            "slug": "Robust-Text-Detection-in-Natural-Scene-Images.-Yin-Yin",
            "title": {
                "fragments": [],
                "text": "Robust Text Detection in Natural Scene Images."
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "An accurate and robust method for detecting texts in natural scene images using a fast and effective pruning algorithm to extract Maximally Stable Extremal Regions (MSERs) as character candidates using the strategy of minimizing regularized variations is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE transactions on pattern analysis and machine intelligence"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739319"
                        ],
                        "name": "R. Lienhart",
                        "slug": "R.-Lienhart",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Lienhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lienhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32544716"
                        ],
                        "name": "Axel Wernicke",
                        "slug": "Axel-Wernicke",
                        "structuredName": {
                            "firstName": "Axel",
                            "lastName": "Wernicke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Axel Wernicke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 143774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8387d4998f810cd2b60bd81545cb993087bc8788",
            "isKey": false,
            "numCitedBy": 467,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Many images, especially those used for page design on Web pages, as well as videos contain visible text. If these text occurrences could be detected, segmented, and recognized automatically, they would be a valuable source of high-level semantics for indexing and retrieval. We propose a novel method for localizing and segmenting text in complex images and videos. Text lines are identified by using a complex-valued multilayer feed-forward network trained to detect text at a fixed scale and position. The network's output at all scales and positions is integrated into a single text-saliency map, serving as a starting point for candidate text lines. In the case of video, these candidate text lines are refined by exploiting the temporal redundancy of text in video. Localized text lines are then scaled to a fixed height of 100 pixels and segmented into a binary image with black characters on white background. For videos, temporal redundancy is exploited to improve segmentation performance. Input images and videos can be of any size due to a true multiresolution approach. Moreover, the system is not only able to locate and segment text occurrences into large binary images, but is also able to track each text line with sub-pixel accuracy over the entire occurrence in a video, so that one text bitmap is created for all instances of that text line. Therefore, our text segmentation results can also be used for object-based video encoding such as that enabled by MPEG-4."
            },
            "slug": "Localizing-and-segmenting-text-in-images-and-videos-Lienhart-Wernicke",
            "title": {
                "fragments": [],
                "text": "Localizing and segmenting text in images and videos"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work proposes a novel method for localizing and segmenting text in complex images and videos that is not only able to locate and segment text occurrences into large binary images, but is also able to track each text line with sub-pixel accuracy over the entire occurrence in a video."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Circuits Syst. Video Technol."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2730090"
                        ],
                        "name": "L. G. I. Bigorda",
                        "slug": "L.-G.-I.-Bigorda",
                        "structuredName": {
                            "firstName": "Llu\u00eds",
                            "lastName": "Bigorda",
                            "middleNames": [
                                "G\u00f3mez",
                                "i"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. G. I. Bigorda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694974"
                        ],
                        "name": "Dimosthenis Karatzas",
                        "slug": "Dimosthenis-Karatzas",
                        "structuredName": {
                            "firstName": "Dimosthenis",
                            "lastName": "Karatzas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dimosthenis Karatzas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17419551,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "291be6e3027575287c24f4363e4bf7a8b415d4c1",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a hybrid algorithm for detection and tracking of text in natural scenes that goes beyond the full-detection approaches in terms of time performance optimization. A state-of-the-art scene text detection module based on Maximally Stable Extremal Regions (MSER) is used to detect text asynchronously, while on a separate thread detected text objects are tracked by MSER propagation. The cooperation of these two modules yields real time video processing at high frame rates even on low-resource devices."
            },
            "slug": "MSER-Based-Real-Time-Text-Detection-and-Tracking-Bigorda-Karatzas",
            "title": {
                "fragments": [],
                "text": "MSER-Based Real-Time Text Detection and Tracking"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A hybrid algorithm for detection and tracking of text in natural scenes that goes beyond the full-detection approaches in terms of time performance optimization and yields real time video processing at high frame rates even on low-resource devices."
            },
            "venue": {
                "fragments": [],
                "text": "2014 22nd International Conference on Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682664"
                        ],
                        "name": "Xu-Cheng Yin",
                        "slug": "Xu-Cheng-Yin",
                        "structuredName": {
                            "firstName": "Xu-Cheng",
                            "lastName": "Yin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xu-Cheng Yin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8135633"
                        ],
                        "name": "Xuwang Yin",
                        "slug": "Xuwang-Yin",
                        "structuredName": {
                            "firstName": "Xuwang",
                            "lastName": "Yin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuwang Yin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5380819"
                        ],
                        "name": "Kaizhu Huang",
                        "slug": "Kaizhu-Huang",
                        "structuredName": {
                            "firstName": "Kaizhu",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaizhu Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143680014"
                        ],
                        "name": "Hongwei Hao",
                        "slug": "Hongwei-Hao",
                        "structuredName": {
                            "firstName": "Hongwei",
                            "lastName": "Hao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hongwei Hao"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10088843,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8db6d30b0fd00d2d608ee3bd012fe3660c25936",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose and implement a robust text detection system, which is a prominent step-in for text retrieval in natural scene images or videos. Our system includes several key components: (1) A fast and effective pruning algorithm is designed to extract Maximally Stable Extremal Regions as character candidates using the strategy of minimizing regularized variations. (2) Character candidates are grouped into text candidates by the single-link clustering algorithm, where distance weights and threshold of clustering are learned automatically by a novel self-training distance metric learning algorithm. (3) The posterior probabilities of text candidates corresponding to non-text are estimated with an character classifier; text candidates with high probabilities are then eliminated and finally texts are identified with a text classifier. The proposed system is evaluated on the ICDAR 2011 Robust Reading Competition dataset and a publicly available multilingual dataset; the f measures are over 76% and 74% which are significantly better than the state-of-the-art performances of 71% and 65%, respectively."
            },
            "slug": "Accurate-and-robust-text-detection:-a-step-in-for-Yin-Yin",
            "title": {
                "fragments": [],
                "text": "Accurate and robust text detection: a step-in for text retrieval in natural scene images"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "A robust text detection system which is a prominent step-in for text retrieval in natural scene images or videos and is evaluated on the ICDAR 2011 Robust Reading Competition dataset and a publicly available multilingual dataset."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2257624"
                        ],
                        "name": "Hideaki Goto",
                        "slug": "Hideaki-Goto",
                        "structuredName": {
                            "firstName": "Hideaki",
                            "lastName": "Goto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hideaki Goto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110306087"
                        ],
                        "name": "Makoto Tanaka",
                        "slug": "Makoto-Tanaka",
                        "structuredName": {
                            "firstName": "Makoto",
                            "lastName": "Tanaka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Makoto Tanaka"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 194
                            }
                        ],
                        "text": "Text detection and tracking technology in video have been receiving increasingly attention due to their wide used in content-based video analysis and retrieval [2], wearable camera systems [3], [4] and Augmented Reality translators [5], [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 59
                            }
                        ],
                        "text": "Although few methods are proposed to track scene text [3], [4], [17], [18], [19], there is no effective and robust algorithm to solve the problem of text tracking in scene videos."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 21
                            }
                        ],
                        "text": "Goto and Tanaka [3], [4] extracted text string using the revised DCT-based method, and the text regions were then grouped into image chains by a text tracking method based on particle filter."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 8
                            }
                        ],
                        "text": "In [3], [4], [15], the length of text trajectory must be more than the threshold values, otherwise it would be regarded as noise and discarded."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10379396,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2272ce3c452ddafa9a92b276bf37b48d233c5a1d",
            "isKey": true,
            "numCitedBy": 45,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Disability of visual text reading has a huge impact on the quality of life for visually disabled people.One of the most anticipated devices is a wearable camera capable of finding text regions in natural scenes and translating the text into another representation such as speech or braille.In order to develop such a device,text tracking in video sequences is required as well as text detection.The device needs to group homogeneous text regions to avoid multiple and redundant speech syntheses or braille conversions.An automatic text image selection is also required for better character recognition and timely text message presentation.We have developed a prototype system equipped with a head-mounted video camera.Particle filter is employed for fast and robust text tracking.We have tested the performance of our system using 1,730 video frames of hall ways with 27 signboards.The number of text candidate regions is reduced to 1.47%."
            },
            "slug": "Text-Tracking-Wearable-Camera-System-for-the-Blind-Goto-Tanaka",
            "title": {
                "fragments": [],
                "text": "Text-Tracking Wearable Camera System for the Blind"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A prototype system equipped with a head-mounted video camera and particle filter is employed for fast and robust text tracking and an automatic text image selection is also required for better character recognition and timely text message presentation."
            },
            "venue": {
                "fragments": [],
                "text": "2009 10th International Conference on Document Analysis and Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110306087"
                        ],
                        "name": "Makoto Tanaka",
                        "slug": "Makoto-Tanaka",
                        "structuredName": {
                            "firstName": "Makoto",
                            "lastName": "Tanaka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Makoto Tanaka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2257624"
                        ],
                        "name": "Hideaki Goto",
                        "slug": "Hideaki-Goto",
                        "structuredName": {
                            "firstName": "Hideaki",
                            "lastName": "Goto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hideaki Goto"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 189
                            }
                        ],
                        "text": "Text detection and tracking technology in video have been receiving increasingly attention due to their wide used in content-based video analysis and retrieval [2], wearable camera systems [3], [4] and Augmented Reality translators [5], [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 54
                            }
                        ],
                        "text": "Although few methods are proposed to track scene text [3], [4], [17], [18], [19], there is no effective and robust algorithm to solve the problem of text tracking in scene videos."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 16
                            }
                        ],
                        "text": "Goto and Tanaka [3], [4] extracted text string using the revised DCT-based method, and the text regions were then grouped into image chains by a text tracking method based on particle filter."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "In [3], [4], [15], the length of text trajectory must be more than the threshold values, otherwise it would be regarded as noise and discarded."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12895734,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b11f8d30175707487f4a9f78ba92a7387dbcfde3",
            "isKey": true,
            "numCitedBy": 37,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Disability of visual text reading has a huge impact on the quality of life for visually disabled people. One of the most anticipated devices is a wearable camera capable of finding text regions in natural scenes and translating the text into another representation such as sound or braille. In order to develop such a device, text tracking in video sequences is required as well as text detection. We need to group homogeneous text regions to avoid multiple and redundant speech syntheses or braille conversions. We have developed a prototype system equipped with a head-mounted video camera. Text regions are extracted from the video frames using a revised DCT feature. Particle filtering is employed for fast and robust text tracking. We have tested the performance of our system using 1,000 video frames of a hall way with eight signboards. The number of text candidate images is reduced to 0.98%."
            },
            "slug": "Text-tracking-wearable-camera-system-for-people-Tanaka-Goto",
            "title": {
                "fragments": [],
                "text": "Text-tracking wearable camera system for visually-impaired people"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A prototype system equipped with a head-mounted video camera capable of finding text regions in natural scenes and translating the text into another representation such as sound or braille, and a revised DCT feature is developed."
            },
            "venue": {
                "fragments": [],
                "text": "2008 19th International Conference on Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143863244"
                        ],
                        "name": "Xiansheng Hua",
                        "slug": "Xiansheng-Hua",
                        "structuredName": {
                            "firstName": "Xiansheng",
                            "lastName": "Hua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiansheng Hua"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46772671"
                        ],
                        "name": "Xiangrong Chen",
                        "slug": "Xiangrong-Chen",
                        "structuredName": {
                            "firstName": "Xiangrong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiangrong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120640002"
                        ],
                        "name": "Wenyin Liu",
                        "slug": "Wenyin-Liu",
                        "structuredName": {
                            "firstName": "Wenyin",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenyin Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108698841"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 26
                            }
                        ],
                        "text": "Conventional methods [7], [8], [9], [10] for video text detection problem mainly focus on detecting text in each individual frame or some key frames."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13999848,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "24591ec88e706697bffa18f36728f192e0d797b6",
            "isKey": false,
            "numCitedBy": 111,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "A new automatic text location approach for videos is proposed. First of all, the corner points of the selected video frames are detected. After deleting some isolate corners, we merge the remaining corners to form candidate text regions. The regions are then decomposed vertically and horizontally using edge maps of the video frames to get candidate text lines. Finally, a text box verification step based on the feature derived from edge maps is taken to significantly reduce false alarms. Experimental results show that the new text location scheme proposed in this paper is accurate."
            },
            "slug": "Automatic-location-of-text-in-video-frames-Hua-Chen",
            "title": {
                "fragments": [],
                "text": "Automatic location of text in video frames"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Experimental results show that the new text location scheme proposed in this paper is accurate and can be used to significantly reduce false alarms."
            },
            "venue": {
                "fragments": [],
                "text": "MULTIMEDIA '01"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37649047"
                        ],
                        "name": "Marc Petter",
                        "slug": "Marc-Petter",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Petter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc Petter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2245862"
                        ],
                        "name": "Victor Fragoso",
                        "slug": "Victor-Fragoso",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Fragoso",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Victor Fragoso"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144097660"
                        ],
                        "name": "M. Turk",
                        "slug": "M.-Turk",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Turk",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Turk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46725291"
                        ],
                        "name": "C. Baur",
                        "slug": "C.-Baur",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Baur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Baur"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 237
                            }
                        ],
                        "text": "Text detection and tracking technology in video have been receiving increasingly attention due to their wide used in content-based video analysis and retrieval [2], wearable camera systems [3], [4] and Augmented Reality translators [5], [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15533914,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc8ed5d3f3734df74768adf47550e4098d942da3",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a fast automatic text detection algorithm devised for a mobile augmented reality (AR) translation system on a mobile phone. In this application, scene text must be detected, recognized, and translated into a desired language, and then the translation is displayed overlaid properly on the real-world scene. In order to offer a fast automatic text detector, we focused our initial search to find a single letter. Detecting one letter provides useful information that is processed with efficient rules to quickly find the reminder of a word. This approach allows for detecting all the contiguous text regions in an image quickly. We also present a method that exploits the redundancy of the information contained in the video stream to remove false alarms. Our experimental results quantify the accuracy and efficiency of the algorithm and show the strengths and weaknesses of the method as well as its speed (about 160 ms on a recent generation smartphone, not optimized). The algorithm is well suited for real-time, real-world applications."
            },
            "slug": "Automatic-text-detection-for-mobile-augmented-Petter-Fragoso",
            "title": {
                "fragments": [],
                "text": "Automatic text detection for mobile augmented reality translation"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A fast automatic text detection algorithm devised for a mobile augmented reality (AR) translation system on a mobile phone and a method that exploits the redundancy of the information contained in the video stream to remove false alarms is presented."
            },
            "venue": {
                "fragments": [],
                "text": "2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2263912"
                        ],
                        "name": "Jingchao Zhou",
                        "slug": "Jingchao-Zhou",
                        "structuredName": {
                            "firstName": "Jingchao",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jingchao Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112280400"
                        ],
                        "name": "Lei Xu",
                        "slug": "Lei-Xu",
                        "structuredName": {
                            "firstName": "Lei",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lei Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2658590"
                        ],
                        "name": "Baihua Xiao",
                        "slug": "Baihua-Xiao",
                        "structuredName": {
                            "firstName": "Baihua",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Baihua Xiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145841729"
                        ],
                        "name": "Ruwei Dai",
                        "slug": "Ruwei-Dai",
                        "structuredName": {
                            "firstName": "Ruwei",
                            "lastName": "Dai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ruwei Dai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053740005"
                        ],
                        "name": "Si si",
                        "slug": "Si-si",
                        "structuredName": {
                            "firstName": "Si",
                            "lastName": "si",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Si si"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 11104278,
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "id": "f1407d98cf7d73542f743a08755eaeb28832bdae",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a novel system to extract caption text in video. Firstly, text regions are detected primarily with emphasis on the recall rate. Then a multiple stage verification scheme is adopted to discard false alarms and boost the precision rate. Secondly, a text polarity estimation algorithm is provided. Based on it, multiple frame enhancement is conducted to strengthen the contrast between text and its background. Finally, a connected component filtering method is proposed to generate clear segmentation results and improve recognition performance. Experimental results confirm that the proposed system is robust and efficient."
            },
            "slug": "A-robust-system-for-text-extraction-in-video-Zhou-Xu",
            "title": {
                "fragments": [],
                "text": "A robust system for text extraction in video"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A multiple stage verification scheme is adopted to discard false alarms and boost the precision rate, and a text polarity estimation algorithm is provided to strengthen the contrast between text and its background."
            },
            "venue": {
                "fragments": [],
                "text": "2007 International Conference on Machine Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739319"
                        ],
                        "name": "R. Lienhart",
                        "slug": "R.-Lienhart",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Lienhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lienhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750165"
                        ],
                        "name": "W. Effelsberg",
                        "slug": "W.-Effelsberg",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Effelsberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Effelsberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 160
                            }
                        ],
                        "text": "Text detection and tracking technology in video have been receiving increasingly attention due to their wide used in content-based video analysis and retrieval [2], wearable camera systems [3], [4] and Augmented Reality translators [5], [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1306072,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "53ecdc232d2e640f890c831944761fe5604af033",
            "isKey": false,
            "numCitedBy": 201,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract. Efficient indexing and retrieval of digital video is an important function of video databases. One powerful index for retrieval is the text appearing in them. It enables content-based browsing. We present our new methods for automatic segmentation of text in digital videos. The algorithms we propose make use of typical characteristics of text in videos in order to enable and enhance segmentation performance. The unique features of our approach are the tracking of characters and words over their complete duration of occurrence in a video and the integration of the multiple bitmaps of a character over time into a single bitmap. The output of the text segmentation step is then directly passed to a standard OCR software package in order to translate the segmented text into ASCII. Also, a straightforward indexing and retrieval scheme is introduced. It is used in the experiments to demonstrate that the proposed text segmentation algorithms together with existing text recognition algorithms are suitable for indexing and retrieval of relevant video sequences in and from a video database. Our experimental results are very encouraging and suggest that these algorithms can be used in video retrieval applications as well as to recognize higher level semantics in videos."
            },
            "slug": "Automatic-text-segmentation-and-text-recognition-Lienhart-Effelsberg",
            "title": {
                "fragments": [],
                "text": "Automatic text segmentation and text recognition for video indexing"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The proposed text segmentation algorithms together with existing text recognition algorithms are suitable for indexing and retrieval of relevant video sequences in and from a video database."
            },
            "venue": {
                "fragments": [],
                "text": "Multimedia Systems"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32544716"
                        ],
                        "name": "Axel Wernicke",
                        "slug": "Axel-Wernicke",
                        "structuredName": {
                            "firstName": "Axel",
                            "lastName": "Wernicke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Axel Wernicke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739319"
                        ],
                        "name": "R. Lienhart",
                        "slug": "R.-Lienhart",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Lienhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lienhart"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 188
                            }
                        ],
                        "text": "On the other hand, a key characteristic of video text is temporal redundancy due to the fact that each video text often stays on the screen for 2 seconds at least, in order to be readable [11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13609029,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "641483b9dcaa2bac13f95a3f2f6738140c170184",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "A new and robust multi-resolution approach of localizing and segmenting text in videos is proposed. The approach has been tested extensively on a large variety of video frame sizes such 352/spl times/240 up to 1920/spl times/1280 and a large representative set of video sequences such as home videos, newscasts, title sequences and commercials. 95% of the text bounding boxes in videos were localized correctly. 80% of all characters were segmented correctly, while 7.8% characters were damaged. 90% of the correctly segmented characters were recognized correctly by standard OCR software."
            },
            "slug": "On-the-segmentation-of-text-in-videos-Wernicke-Lienhart",
            "title": {
                "fragments": [],
                "text": "On the segmentation of text in videos"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A new and robust multi-resolution approach of localizing and segmenting text in videos is proposed that has been tested extensively on a large variety of video frame sizes and a large representative set of video sequences."
            },
            "venue": {
                "fragments": [],
                "text": "2000 IEEE International Conference on Multimedia and Expo. ICME2000. Proceedings. Latest Advances in the Fast Changing World of Multimedia (Cat. No.00TH8532)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2673919"
                        ],
                        "name": "V. Mariano",
                        "slug": "V.-Mariano",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Mariano",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Mariano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110392"
                        ],
                        "name": "R. Kasturi",
                        "slug": "R.-Kasturi",
                        "structuredName": {
                            "firstName": "Rangachar",
                            "lastName": "Kasturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kasturi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 21
                            }
                        ],
                        "text": "Conventional methods [7], [8], [9], [10] for video text detection problem mainly focus on detecting text in each individual frame or some key frames."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 195348929,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "1d4a5e70874d828bbd6c16300511fa2744f0b2fd",
            "isKey": false,
            "numCitedBy": 107,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper a method is proposed for locating horizontal, uniform-colored text in video frames. It was observed that when a row of pixels across such a text region is clustered in perceptually uniform L*a*b* color space, the pixels of one of these clusters would belong to the text strokes. These pixels would appeal as a line of short streaks on the row since a typical text region has many vertical and diagonal strokes. The proposed method examines every third row of the the image and checks whether this row passes through a horizontal text region. For a given row R, the pixels of R are hierarchically clustered in L*a*b* space and each cluster is tested whether similar-colored pixels in R's vicinity are possibly part of a text region. Candidate text blocks are marked by heuristics using information about the cluster's line of shell streaks. The detected text blocks are fused with the text regions. The method was tested on key frames of several video sequences and was able to locate a wide variety of text."
            },
            "slug": "Locating-uniform-colored-text-in-video-frames-Mariano-Kasturi",
            "title": {
                "fragments": [],
                "text": "Locating uniform-colored text in video frames"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The proposed method examines every third row of the the image and checks whether this row passes through a horizontal text region, and detects text blocks marked by heuristics using information about the cluster's line of shell streaks."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 15th International Conference on Pattern Recognition. ICPR-2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3198263"
                        ],
                        "name": "Kaihua Zhang",
                        "slug": "Kaihua-Zhang",
                        "structuredName": {
                            "firstName": "Kaihua",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaihua Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36685537"
                        ],
                        "name": "Lei Zhang",
                        "slug": "Lei-Zhang",
                        "structuredName": {
                            "firstName": "Lei",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lei Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715634"
                        ],
                        "name": "Ming-Hsuan Yang",
                        "slug": "Ming-Hsuan-Yang",
                        "structuredName": {
                            "firstName": "Ming-Hsuan",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Hsuan Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109588580"
                        ],
                        "name": "David Zhang",
                        "slug": "David-Zhang",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7805658,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5d22a122272e7cc728e6d344a0b8a070c24c2999",
            "isKey": false,
            "numCitedBy": 142,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a simple yet fast and robust algorithm which exploits the spatio-temporal context for visual tracking. Our approach formulates the spatio-temporal relationships between the object of interest and its local context based on a Bayesian framework, which models the statistical correlation between the low-level features (i.e., image intensity and position) from the target and its surrounding regions. The tracking problem is posed by computing a confidence map, and obtaining the best target location by maximizing an object location likelihood function. The Fast Fourier Transform is adopted for fast learning and detection in this work. Implemented in MATLAB without code optimization, the proposed tracker runs at 350 frames per second on an i7 machine. Extensive experimental results show that the proposed algorithm performs favorably against state-of-the-art methods in terms of efficiency, accuracy and robustness."
            },
            "slug": "Fast-Tracking-via-Spatio-Temporal-Context-Learning-Zhang-Zhang",
            "title": {
                "fragments": [],
                "text": "Fast Tracking via Spatio-Temporal Context Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This approach formulates the spatio-temporal relationships between the object of interest and its local context based on a Bayesian framework, which models the statistical correlation between the low-level features from the target and its surrounding regions."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061881049"
                        ],
                        "name": "Wang Zhen",
                        "slug": "Wang-Zhen",
                        "structuredName": {
                            "firstName": "Wang",
                            "lastName": "Zhen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wang Zhen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680098"
                        ],
                        "name": "W. Zhiqiang",
                        "slug": "W.-Zhiqiang",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Zhiqiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Zhiqiang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 40662630,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fd10fa5e0f211c1447d0b81f338ed0fafa065a22",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present efficient schemes to utilize multiple frames that contain the same text to get every clear word from these frames. Firstly, we use multiple frame traction to reduce text detection false alarms. And then detect and joint every clear text block from those frames to form a clearer integration frame. Later we uses Otsu threshold method to generate a binary text image. Finally, the binarized frames are sent to OCR engine for recognition. The experimental results show our method can improve the performance of text recognition in video significantly."
            },
            "slug": "An-Efficient-Video-Text-Recognition-System-Zhen-Zhiqiang",
            "title": {
                "fragments": [],
                "text": "An Efficient Video Text Recognition System"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "Efficient schemes to utilize multiple frames that contain the same text to get every clear word from these frames are presented and can improve the performance of text recognition in video significantly."
            },
            "venue": {
                "fragments": [],
                "text": "2010 Second International Conference on Intelligent Human-Machine Systems and Cybernetics"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2245862"
                        ],
                        "name": "Victor Fragoso",
                        "slug": "Victor-Fragoso",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Fragoso",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Victor Fragoso"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1940938"
                        ],
                        "name": "Steffen Gauglitz",
                        "slug": "Steffen-Gauglitz",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Gauglitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steffen Gauglitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34747749"
                        ],
                        "name": "S. Zamora",
                        "slug": "S.-Zamora",
                        "structuredName": {
                            "firstName": "Shane",
                            "lastName": "Zamora",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Zamora"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2425559"
                        ],
                        "name": "Jim Kleban",
                        "slug": "Jim-Kleban",
                        "structuredName": {
                            "firstName": "Jim",
                            "lastName": "Kleban",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jim Kleban"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144097660"
                        ],
                        "name": "M. Turk",
                        "slug": "M.-Turk",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Turk",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Turk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 232
                            }
                        ],
                        "text": "Text detection and tracking technology in video have been receiving increasingly attention due to their wide used in content-based video analysis and retrieval [2], wearable camera systems [3], [4] and Augmented Reality translators [5], [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15483110,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68c849ace1f447364962da859e6addc1168ae972",
            "isKey": false,
            "numCitedBy": 91,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a mobile augmented reality (AR) translation system, using a smartphone's camera and touchscreen, that requires the user to simply tap on the word of interest once in order to produce a translation, presented as an AR overlay. The translation seamlessly replaces the original text in the live camera stream, matching background and foreground colors estimated from the source images. For this purpose, we developed an efficient algorithm for accurately detecting the location and orientation of the text in a live camera stream that is robust to perspective distortion, and we combine it with OCR and a text-to-text translation engine. Our experimental results, using the ICDAR 2003 dataset and our own set of video sequences, quantify the accuracy of our detection and analyze the sources of failure among the system's components. With the OCR and translation running in a background thread, the system runs at 26 fps on a current generation smartphone (Nokia N900) and offers a particularly easy-to-use and simple method for translation, especially in situations in which typing or correct pronunciation (for systems with speech input) is cumbersome or impossible."
            },
            "slug": "TranslatAR:-A-mobile-augmented-reality-translator-Fragoso-Gauglitz",
            "title": {
                "fragments": [],
                "text": "TranslatAR: A mobile augmented reality translator"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A mobile augmented reality (AR) translation system that requires the user to simply tap on the word of interest once in order to produce a translation, presented as an AR overlay, and offers a particularly easy-to-use and simple method for translation."
            },
            "venue": {
                "fragments": [],
                "text": "2011 IEEE Workshop on Applications of Computer Vision (WACV)"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48246959"
                        ],
                        "name": "Naiyan Wang",
                        "slug": "Naiyan-Wang",
                        "structuredName": {
                            "firstName": "Naiyan",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naiyan Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788070"
                        ],
                        "name": "Jianping Shi",
                        "slug": "Jianping-Shi",
                        "structuredName": {
                            "firstName": "Jianping",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianping Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739816"
                        ],
                        "name": "D. Yeung",
                        "slug": "D.-Yeung",
                        "structuredName": {
                            "firstName": "Dit-Yan",
                            "lastName": "Yeung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Yeung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729056"
                        ],
                        "name": "Jiaya Jia",
                        "slug": "Jiaya-Jia",
                        "structuredName": {
                            "firstName": "Jiaya",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiaya Jia"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 106
                            }
                        ],
                        "text": "The major researchers concerned with video text tracking mainly use template matching methods [12], [l3], [14], [15], [16] to track graphic text (video captions)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "Lienhart and Wernickes [14] proposed a projection profile based text tracking method in which the matching measure was based on the vertical and horizontal projection profile."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15385433,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bf97833a354d7faa3793599ac5c1ec1eb3eb7e07",
            "isKey": false,
            "numCitedBy": 313,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Several benchmark datasets for visual tracking research have been created in recent years. Despite their usefulness, whether they are sufficient for understanding and diagnosing the strengths and weaknesses of different trackers remains questionable. To address this issue, we propose a framework by breaking a tracker down into five constituent parts, namely, motion model, feature extractor, observation model, model updater, and ensemble post-processor. We then conduct ablative experiments on each component to study how it affects the overall result. Surprisingly, our findings are discrepant with some common beliefs in the visual tracking research community. We find that the feature extractor plays the most important role in a tracker. On the other hand, although the observation model is the focus of many studies, we find that it often brings no significant improvement. Moreover, the motion model and model updater contain many details that could affect the result. Also, the ensemble post-processor can improve the result substantially when the constituent trackers have high diversity. Based on our findings, we put together some very elementary building blocks to give a basic tracker which is competitive in performance to the state-of-the-art trackers. We believe our framework can provide a solid baseline when conducting controlled experiments for visual tracking research."
            },
            "slug": "Understanding-and-Diagnosing-Visual-Tracking-Wang-Shi",
            "title": {
                "fragments": [],
                "text": "Understanding and Diagnosing Visual Tracking Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A framework by breaking a tracker down into five constituent parts, namely, motion model, feature extractor, observation model, model updater, and ensemble post-processor is proposed, which can provide a solid baseline when conducting controlled experiments for visual tracking research."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145815031"
                        ],
                        "name": "S. Lucas",
                        "slug": "S.-Lucas",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Lucas",
                            "middleNames": [
                                "M.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lucas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1842569,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bbf50fe5622253f401e892ed943a18033e18b7b9",
            "isKey": false,
            "numCitedBy": 318,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the results of the ICDAR 2005 competition for locating text in camera captured scenes. For this we used the same data as the ICDAR 2003 competition, which has been kept private until now. This allows a direct comparison with the 2003 entries. The main result is that the leading 2005 entry has improved significantly on the leading 2003 entry, with an increase in average f-score from 0.5 to 0.62, where the f-score is the same adapted information retrieval measure used for the 2003 competition. The paper also discusses the Web-based deployment and evaluation of text locating systems, and one of the leading entries has now been deployed in this way. This mode of usage could lead to more complete and more immediate knowledge of the strengths and weaknesses of each newly developed system."
            },
            "slug": "ICDAR-2005-text-locating-competition-results-Lucas",
            "title": {
                "fragments": [],
                "text": "ICDAR 2005 text locating competition results"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The main result is that the leading 2005 entry has improved significantly on the leading 2003 entry, with an increase in average f- score from 0.5 to 0.62, where the f-score is the same adapted information retrieval measure used for the 2003 competition."
            },
            "venue": {
                "fragments": [],
                "text": "Eighth International Conference on Document Analysis and Recognition (ICDAR'05)"
            },
            "year": 2005
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 8,
            "methodology": 9
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 26,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Multi-strategy-tracking-based-text-detection-in-Zuo-Tian/1b0507d7c1c7a4d60a68b48ed0bc5fbf55925393?sort=total-citations"
}