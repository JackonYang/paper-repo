{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1402440928"
                        ],
                        "name": "Azka Gilani",
                        "slug": "Azka-Gilani",
                        "structuredName": {
                            "firstName": "Azka",
                            "lastName": "Gilani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Azka Gilani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39404123"
                        ],
                        "name": "S. Qasim",
                        "slug": "S.-Qasim",
                        "structuredName": {
                            "firstName": "Shah",
                            "lastName": "Qasim",
                            "middleNames": [
                                "Rukh"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Qasim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49012494"
                        ],
                        "name": "M. I. Malik",
                        "slug": "M.-I.-Malik",
                        "structuredName": {
                            "firstName": "Muhammad",
                            "lastName": "Malik",
                            "middleNames": [
                                "Imran"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. I. Malik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688013"
                        ],
                        "name": "F. Shafait",
                        "slug": "F.-Shafait",
                        "structuredName": {
                            "firstName": "Faisal",
                            "lastName": "Shafait",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Shafait"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[15] also used Faster R-CNN for table detection."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[15], which constitute the state-ofthe-art in table detection."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[15] With Color Coding only Our Approach"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 87
                            }
                        ],
                        "text": "Table II provides A comparison of the proposed system with the approaches presented in [15] and [14], trained on our proposed dataset."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[15] used UNLV dataset for training, validation and testing of the system."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206777650,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9d6931dd0ba9e492b0ceab00268ca4be62ef663a",
            "isKey": false,
            "numCitedBy": 91,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Table detection is a crucial step in many document analysis applications as tables are used for presenting essential information to the reader in a structured manner. It is a hard problem due to varying layouts and encodings of the tables. Researchers have proposed numerous techniques for table detection based on layout analysis of documents. Most of these techniques fail to generalize because they rely on hand engineered features which are not robust to layout variations. In this paper, we have presented a deep learning based method for table detection. In the proposed method, document images are first pre-processed. These images are then fed to a Region Proposal Network followed by a fully connected neural network for table detection. The proposed method works with high precision on document images with varying layouts that include documents, research papers, and magazines. We have done our evaluations on publicly available UNLV dataset where it beats Tesseract's state of the art table detection system by a significant margin."
            },
            "slug": "Table-Detection-Using-Deep-Learning-Gilani-Qasim",
            "title": {
                "fragments": [],
                "text": "Table Detection Using Deep Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The proposed method works with high precision on document images with varying layouts that include documents, research papers, and magazines and beats Tesseract's state of the art table detection system by a significant margin."
            },
            "venue": {
                "fragments": [],
                "text": "2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2656315"
                        ],
                        "name": "Leipeng Hao",
                        "slug": "Leipeng-Hao",
                        "structuredName": {
                            "firstName": "Leipeng",
                            "lastName": "Hao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Leipeng Hao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777642"
                        ],
                        "name": "Liangcai Gao",
                        "slug": "Liangcai-Gao",
                        "structuredName": {
                            "firstName": "Liangcai",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liangcai Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3412797"
                        ],
                        "name": "Xiaohan Yi",
                        "slug": "Xiaohan-Yi",
                        "structuredName": {
                            "firstName": "Xiaohan",
                            "lastName": "Yi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaohan Yi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143830636"
                        ],
                        "name": "Zhi Tang",
                        "slug": "Zhi-Tang",
                        "structuredName": {
                            "firstName": "Zhi",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhi Tang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[11] back in 2016."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2870724,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "06bf934004b6f93711298f905b1e447683a8d0b9",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Because of the better performance of deep learning on many computer vision tasks, researchers in the area of document analysis and recognition begin to adopt this technique into their work. In this paper, we propose a novel method for table detection in PDF documents based on convolutional neutral networks, one of the most popular deep learning models. In the proposed method, some table-like areas are selected first by some loose rules, and then the convolutional networks are built and refined to determine whether the selected areas are tables or not. Besides, the visual features of table areas are directly extracted and utilized through the convolutional networks, while the non-visual information (e.g. characters, rendering instructions) contained in original PDF documents is also taken into consideration to help achieve better recognition results. The primary experimental results show that the approach is effective in table detection."
            },
            "slug": "A-Table-Detection-Method-for-PDF-Documents-Based-on-Hao-Gao",
            "title": {
                "fragments": [],
                "text": "A Table Detection Method for PDF Documents Based on Convolutional Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel method for table detection in PDF documents based on convolutional neutral networks, one of the most popular deep learning models, which shows that the approach is effective in table detection."
            },
            "venue": {
                "fragments": [],
                "text": "2016 12th IAPR Workshop on Document Analysis Systems (DAS)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688013"
                        ],
                        "name": "F. Shafait",
                        "slug": "F.-Shafait",
                        "structuredName": {
                            "firstName": "Faisal",
                            "lastName": "Shafait",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Shafait"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108985360"
                        ],
                        "name": "Raymond W. Smith",
                        "slug": "Raymond-W.-Smith",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Smith",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raymond W. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 43
                            }
                        ],
                        "text": "Details about this dataset can be found in [17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 84
                            }
                        ],
                        "text": "Since we are focusing only on table detection, the evaluation measures described in [17] have been employed as these performance measures provide a more detailed and"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 159
                            }
                        ],
                        "text": "One important point should be kept in mind while annotating the table regions that table caption and table footnote is not considered as the part of the table [17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "As described in [17], consider Gi represents the ground truth box and Dj represents the bounding box detected by our proposed system."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2534837,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2dea52f04a90dc993d2aa30e75b5a5a535e7ca70",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Detecting tables in document images is important since not only do tables contain important information, but also most of the layout analysis methods fail in the presence of tables in the document image. Existing approaches for table detection mainly focus on detecting tables in single columns of text and do not work reliably on documents with varying layouts. This paper presents a practical algorithm for table detection that works with a high accuracy on documents with varying layouts (company reports, newspaper articles, magazine pages, ...). An open source implementation of the algorithm is provided as part of the Tesseract OCR engine. Evaluation of the algorithm on document images from publicly available UNLV dataset shows competitive performance in comparison to the table detection module of a commercial OCR system."
            },
            "slug": "Table-detection-in-heterogeneous-documents-Shafait-Smith",
            "title": {
                "fragments": [],
                "text": "Table detection in heterogeneous documents"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Evaluation of the algorithm on document images from publicly available UNLV dataset shows competitive performance in comparison to the table detection module of a commercial OCR system."
            },
            "venue": {
                "fragments": [],
                "text": "DAS '10"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2072578366"
                        ],
                        "name": "T. A. Tran",
                        "slug": "T.-A.-Tran",
                        "structuredName": {
                            "firstName": "Tuan",
                            "lastName": "Tran",
                            "middleNames": [
                                "Anh"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. A. Tran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38911842"
                        ],
                        "name": "H. Tran",
                        "slug": "H.-Tran",
                        "structuredName": {
                            "firstName": "Hong",
                            "lastName": "Tran",
                            "middleNames": [
                                "Tai"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Tran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31725329"
                        ],
                        "name": "In Seop Na",
                        "slug": "In-Seop-Na",
                        "structuredName": {
                            "firstName": "In",
                            "lastName": "Na",
                            "middleNames": [
                                "Seop"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "In Seop Na"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144096223"
                        ],
                        "name": "Gueesang Lee",
                        "slug": "Gueesang-Lee",
                        "structuredName": {
                            "firstName": "Gueesang",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gueesang Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "97598888"
                        ],
                        "name": "Hyung-Jeong Yang",
                        "slug": "Hyung-Jeong-Yang",
                        "structuredName": {
                            "firstName": "Hyung-Jeong",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hyung-Jeong Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2355626"
                        ],
                        "name": "Soohyung Kim",
                        "slug": "Soohyung-Kim",
                        "structuredName": {
                            "firstName": "Soohyung",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Soohyung Kim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 55
                            }
                        ],
                        "text": "detection of tabular regions in UNLV dataset difficult [25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 41922795,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "003773d38c19d29e371a2554a2bd5f0ce625e822",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-mixture-model-using-Random-Rotation-Bounding-Box-Tran-Tran",
            "title": {
                "fragments": [],
                "text": "A mixture model using Random Rotation Bounding Box to detect table region in document image"
            },
            "venue": {
                "fragments": [],
                "text": "J. Vis. Commun. Image Represent."
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057440295"
                        ],
                        "name": "Sebastian Schreiber",
                        "slug": "Sebastian-Schreiber",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Schreiber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Schreiber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2582412"
                        ],
                        "name": "S. Agne",
                        "slug": "S.-Agne",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Agne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Agne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144651288"
                        ],
                        "name": "I. Wolf",
                        "slug": "I.-Wolf",
                        "structuredName": {
                            "firstName": "Ivo",
                            "lastName": "Wolf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Wolf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145279674"
                        ],
                        "name": "A. Dengel",
                        "slug": "A.-Dengel",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Dengel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dengel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734717217"
                        ],
                        "name": "Sheraz Ahmed",
                        "slug": "Sheraz-Ahmed",
                        "structuredName": {
                            "firstName": "Sheraz",
                            "lastName": "Ahmed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sheraz Ahmed"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 96
                            }
                        ],
                        "text": "Table II provides A comparison of the proposed system with the approaches presented in [15] and [14], trained on our proposed dataset."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[14] used ICDAR 2013 table competition dataset for testing."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[14] focuses on table detection and structure analysis as well."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10191334,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f8bead3ae810cd3f7427d3004e45b4158da9b744",
            "isKey": false,
            "numCitedBy": 158,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a novel end-to-end system for table understanding in document images called DeepDeSRT. In particular, the contribution of DeepDeSRT is two-fold. First, it presents a deep learning-based solution for table detection in document images. Secondly, it proposes a novel deep learning-based approach for table structure recognition, i.e. identifying rows, columns, and cell positions in the detected tables. In contrast to existing rule-based methods, which rely on heuristics or additional PDF metadata (like, for example, print instructions, character bounding boxes, or line segments), the presented system is data-driven and does not need any heuristics or metadata to detect as well as to recognize tabular structures in document images. Furthermore, in contrast to most existing table detection and structure recognition methods, which are applicable only to PDFs, DeepDeSRT processes document images, which makes it equally suitable for born-digital PDFs (as they can automatically be converted into images) as well as even harder problems, e.g. scanned documents. To gauge the performance of DeepDeSRT, the system is evaluated on the publicly available ICDAR 2013 table competition dataset containing 67 documents with 238 pages overall. Evaluation results reveal that DeepDeSRT outperforms state-of-the-art methods for table detection and structure recognition and achieves F1-measures of 96.77% and 91.44% for table detection and structure recognition, respectively. Additionally, DeepDeSRT is evaluated on a closed dataset from a real use case of a major European aviation company comprising documents which are highly unlike those in ICDAR 2013. Tested on a randomly selected sample from this dataset, DeepDeSRT achieves high detection accuracy for tables which demonstrates the sound generalization capabilities of our system."
            },
            "slug": "DeepDeSRT:-Deep-Learning-for-Detection-and-of-in-Schreiber-Agne",
            "title": {
                "fragments": [],
                "text": "DeepDeSRT: Deep Learning for Detection and Structure Recognition of Tables in Document Images"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "In contrast to most existing table detection and structure recognition methods, which are applicable only to PDFs, DeepDeSRT processes document images, which makes it equally suitable for born-digital PDFs as well as even harder problems, e.g. scanned documents."
            },
            "venue": {
                "fragments": [],
                "text": "2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7232446"
                        ],
                        "name": "B. Gatos",
                        "slug": "B.-Gatos",
                        "structuredName": {
                            "firstName": "Basilios",
                            "lastName": "Gatos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Gatos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3148833"
                        ],
                        "name": "Dimitrios Danatsas",
                        "slug": "Dimitrios-Danatsas",
                        "structuredName": {
                            "firstName": "Dimitrios",
                            "lastName": "Danatsas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dimitrios Danatsas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748249"
                        ],
                        "name": "I. Pratikakis",
                        "slug": "I.-Pratikakis",
                        "structuredName": {
                            "firstName": "Ioannis",
                            "lastName": "Pratikakis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Pratikakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2397702"
                        ],
                        "name": "S. Perantonis",
                        "slug": "S.-Perantonis",
                        "structuredName": {
                            "firstName": "Stavros",
                            "lastName": "Perantonis",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Perantonis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[8] presented an approach which after preprocessing, makes some estimation about presence of vertical and horizontal lines and improves these estimations by removing text areas."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11485240,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "800ce7dc6b97a48b1e721f03269eb4a59adacabe",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a novel technique for automatic table detection in document images. Lines and tables are among the most frequent graphic, non-textual entities in documents and their detection is directly related to the OCR performance as well as to the document layout description. We propose a workflow for table detection that comprises three distinct steps: (i) image pre-processing; (ii) horizontal and vertical line detection and (iii) table detection. The efficiency of the proposed method is demonstrated by using a performance evaluation scheme which considers a great variety of documents such as forms, newspapers/magazines, scientific journals, tickets/bank cheques, certificates and handwritten documents."
            },
            "slug": "Automatic-Table-Detection-in-Document-Images-Gatos-Danatsas",
            "title": {
                "fragments": [],
                "text": "Automatic Table Detection in Document Images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The efficiency of the proposed method is demonstrated by using a performance evaluation scheme which considers a great variety of documents such as forms, newspapers/magazines, scientific journals, tickets/bank cheques, certificates and handwritten documents."
            },
            "venue": {
                "fragments": [],
                "text": "ICAPR"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1885312"
                        ],
                        "name": "Sheikh Faisal Rashid",
                        "slug": "Sheikh-Faisal-Rashid",
                        "structuredName": {
                            "firstName": "Sheikh",
                            "lastName": "Rashid",
                            "middleNames": [
                                "Faisal"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sheikh Faisal Rashid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "81314270"
                        ],
                        "name": "A. Akmal",
                        "slug": "A.-Akmal",
                        "structuredName": {
                            "firstName": "Ab.",
                            "lastName": "Akmal",
                            "middleNames": [
                                "Razak",
                                "Nurul",
                                "Saidatul"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Akmal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739137808"
                        ],
                        "name": "M. Adnan",
                        "slug": "M.-Adnan",
                        "structuredName": {
                            "firstName": "Muhammad",
                            "lastName": "Adnan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Adnan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33354035"
                        ],
                        "name": "Ali Adnan Aslam",
                        "slug": "Ali-Adnan-Aslam",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Aslam",
                            "middleNames": [
                                "Adnan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ali Adnan Aslam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145279674"
                        ],
                        "name": "A. Dengel",
                        "slug": "A.-Dengel",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Dengel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dengel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[12] presented an approach for table spotting and recognition based on machine learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4775097,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4be161e3c9731f0c431ce5d890441c867b872938",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Tables are an easy way to represent information in a structural form. Table recognition is important for the extraction of such information from document images. Usually, modern OCR systems provide textual information coming from tables without recognizing actual table structure. However, recognition of table structure is important to get the contextual meaning of the contents. Table structure recognition in heterogeneous documents is challenging due to a variety of table layouts. It becomes harder where no physical rulings are present in a table. This work proposes a novel learning based methodology for the recognition of table contents in heterogeneous document images. Textual contents of documents are classified as table or non-table elements using a pre-trained neural network model. The output of the neural network is further enhanced by applying a contextual post processing on each element to correct the classifications errors if any. The system is trained using a subset of UNLV and UW3 document images and depicted more than 97% accuracy on a test set in detection of table and non-table elements."
            },
            "slug": "Table-Recognition-in-Heterogeneous-Documents-Using-Rashid-Akmal",
            "title": {
                "fragments": [],
                "text": "Table Recognition in Heterogeneous Documents Using Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work proposes a novel learning based methodology for the recognition of table contents in heterogeneous document images and depicts more than 97% accuracy on a test set in detection of table and non-table elements."
            },
            "venue": {
                "fragments": [],
                "text": "2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2595112"
                        ],
                        "name": "Sekhar Mandal",
                        "slug": "Sekhar-Mandal",
                        "structuredName": {
                            "firstName": "Sekhar",
                            "lastName": "Mandal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sekhar Mandal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105636642"
                        ],
                        "name": "S. Chowdhury",
                        "slug": "S.-Chowdhury",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Chowdhury",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chowdhury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152167018"
                        ],
                        "name": "Amit Kumar Das",
                        "slug": "Amit-Kumar-Das",
                        "structuredName": {
                            "firstName": "Amit Kumar",
                            "lastName": "Das",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amit Kumar Das"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784810"
                        ],
                        "name": "B. Chanda",
                        "slug": "B.-Chanda",
                        "structuredName": {
                            "firstName": "Bhabatosh",
                            "lastName": "Chanda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Chanda"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[9] is based on the assumption that table columns usually have a large inter-word gap as compared to the rest of the text lines."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 27098537,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "86a11c997be11cc9ea43ae018d7970c9257be1eb",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "The requirement of detection and identification of tables from document images is crucial to any document image analysis and digital library system. In this paper we report a very simple but extremely powerful approach to detect tables present in document pages. The algorithm relies on the observation that the tables have distinct columns which implies that gaps between the fields are substantially larger than the gaps between the words in text lines. This deceptively simple observation has led to the design of a simple but powerful table detection system with low computation cost. Moreover, mathematical foundation of the approach is also established including formation of a regular expression for ease of implementation."
            },
            "slug": "A-simple-and-effective-table-detection-system-from-Mandal-Chowdhury",
            "title": {
                "fragments": [],
                "text": "A simple and effective table detection system from document images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A very simple but extremely powerful approach to detect tables present in document pages based on the observation that the tables have distinct columns which implies that gaps between the fields are substantially larger than the gap between the words in text lines."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Document Analysis and Recognition (IJDAR)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47756656"
                        ],
                        "name": "Jianying Hu",
                        "slug": "Jianying-Hu",
                        "structuredName": {
                            "firstName": "Jianying",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianying Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32225756"
                        ],
                        "name": "R. Kashi",
                        "slug": "R.-Kashi",
                        "structuredName": {
                            "firstName": "Ramanujan",
                            "lastName": "Kashi",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kashi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1828940"
                        ],
                        "name": "D. Lopresti",
                        "slug": "D.-Lopresti",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Lopresti",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lopresti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2859740"
                        ],
                        "name": "G. Wilfong",
                        "slug": "G.-Wilfong",
                        "structuredName": {
                            "firstName": "Gordon",
                            "lastName": "Wilfong",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wilfong"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 37293831,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "edbd577d793a083de4f337acac992ec7837609e0",
            "isKey": false,
            "numCitedBy": 90,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "An important step towards the goal of table understanding is a method for reliable table detection. This paper describes a general solution for detecting tables based on computing an optimal partitioning of a document into some number of tables. A dynamic programming algorithm is given to solve the resulting optimization problem. This high-level framework is independent of any particular table quality measure and independent of the document medium. Moreover, it does not rely on the presence of ruling lines or other table delimiters. We also present table quality measures based on white space correlation and vertical connected component analysis. These measures can be applied equally well to ASCII text and scanned images. We report on some preliminary experiments using this method to detect tables in both ASCII text and scanned images, yielding promising results. We present detailed evaluation of these results using three different criteria which by themselves pose interesting research questions."
            },
            "slug": "Medium-independent-table-detection-Hu-Kashi",
            "title": {
                "fragments": [],
                "text": "Medium-independent table detection"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A general solution for detecting tables based on computing an optimal partitioning of a document into some number of tables is described and a dynamic programming algorithm is given to solve the resulting optimization problem."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3493665"
                        ],
                        "name": "Dafang He",
                        "slug": "Dafang-He",
                        "structuredName": {
                            "firstName": "Dafang",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dafang He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145823372"
                        ],
                        "name": "Scott D. Cohen",
                        "slug": "Scott-D.-Cohen",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Cohen",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott D. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31844147"
                        ],
                        "name": "Brian L. Price",
                        "slug": "Brian-L.-Price",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Price",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian L. Price"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1852261"
                        ],
                        "name": "Daniel Kifer",
                        "slug": "Daniel-Kifer",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Kifer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Kifer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157784"
                        ],
                        "name": "C. Lee Giles",
                        "slug": "C.-Lee-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee Giles"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[13] for semantic page segmentation and table detection."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29473897,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b3cf063c3e25b2ab30e44ba49920b811d40f7702",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Page segmentation and table detection play an important role in understanding the structure of documents. We present a page segmentation algorithm that incorporates state-of-the-art deep learning methods for segmenting three types of document elements: text blocks, tables, and figures. We propose a multi-scale, multi-task fully convolutional neural network (FCN) for the tasks of semantic page segmentation and element contour detection. The semantic segmentation network accurately predicts the probability at each pixel of the three element classes. The contour detection network accurately predicts instance level \"edges\" around each element occurrence. We propose a conditional random field (CRF) that uses features output from the semantic segmentation and contour networks to improve upon the semantic segmentation network output. Given the semantic segmentation output, we also extract individual table instances from the page using some heuristic rules and a verification network to remove false positives. We show that although we only consider a page image as input, we produce comparable results with other methods that relies on PDF file information and heuristics and hand crafted features tailored to specific types of documents. Our approach learns the representative features for page segmentation from real and synthetic training data. %, and produces good results on real documents. The learning-based property makes it a more general method than existing methods in terms of document types and element appearances. For example, our method reliably detects sparsely lined tables which are hard for rule-based or heuristic methods."
            },
            "slug": "Multi-Scale-Multi-Task-FCN-for-Semantic-Page-and-He-Cohen",
            "title": {
                "fragments": [],
                "text": "Multi-Scale Multi-Task FCN for Semantic Page Segmentation and Table Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This work presents a page segmentation algorithm that incorporates state-of-the-art deep learning methods for segmenting three types of document elements: text blocks, tables, and figures and proposes a conditional random field (CRF) that uses features output from the semantic segmentsation and contour networks to improve upon the semantic segmentation network output."
            },
            "venue": {
                "fragments": [],
                "text": "2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115625054"
                        ],
                        "name": "Yalin Wang",
                        "slug": "Yalin-Wang",
                        "structuredName": {
                            "firstName": "Yalin",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yalin Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710238"
                        ],
                        "name": "R. Haralick",
                        "slug": "R.-Haralick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Haralick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haralick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744200"
                        ],
                        "name": "I. T. Phillips",
                        "slug": "I.-T.-Phillips",
                        "structuredName": {
                            "firstName": "Ihsin",
                            "lastName": "Phillips",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. T. Phillips"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[7] proposed a system for table identification and decomposition based on statistical learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 628973,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd7e243fb7041c582a479116a11126904f2be010",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We first describe an automatic table ground truth generation system which can efficiently generate a large amount of accurate table ground truth suitable for the development of table detection algorithms. Then a novel background analysis-based, coarse-to-fine table identification algorithm and an X-Y cut table decomposition algorithm are described. We discuss an experimental protocol to evaluate the table detection algorithms. For a total of 1,125 document pages having 518 table entities and a total of 10,941 cell entities, our table detection algorithm takes line, word segmentation results as input and obtains around 90% cell correct detection rates."
            },
            "slug": "Automatic-table-ground-truth-generation-and-a-table-Wang-Haralick",
            "title": {
                "fragments": [],
                "text": "Automatic table ground truth generation and a background-analysis-based table structure extraction method"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "An automatic table groundtruth generation system which can efficiently generate a large amount of accurate table ground truth suitable for the development of table detection algorithms and an X-Y cut table decomposition algorithm are described."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Sixth International Conference on Document Analysis and Recognition"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34838386"
                        ],
                        "name": "K. Simonyan",
                        "slug": "K.-Simonyan",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Simonyan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Simonyan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 61
                            }
                        ],
                        "text": "The original Faster R-CNN used Simonyan and Zisserman model (VGG-16) [19] and Zeiler and Fergus\nmodel (ZF) [20] pretrained on ImageNet dataset [21] as feature extractor."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 29
                            }
                        ],
                        "text": "and Zisserman model (VGG-16) [19] and Zeiler and Fergus model (ZF) [20] pretrained on ImageNet dataset [21] as feature extractor."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14124313,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb42cf88027de515750f230b23b1a057dc782108",
            "isKey": false,
            "numCitedBy": 62220,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision."
            },
            "slug": "Very-Deep-Convolutional-Networks-for-Large-Scale-Simonyan-Zisserman",
            "title": {
                "fragments": [],
                "text": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This work investigates the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting using an architecture with very small convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2032184078"
                        ],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10328909,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "424561d8585ff8ebce7d5d07de8dbf7aae5e7270",
            "isKey": false,
            "numCitedBy": 32561,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available"
            },
            "slug": "Faster-R-CNN:-Towards-Real-Time-Object-Detection-Ren-He",
            "title": {
                "fragments": [],
                "text": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work introduces a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals and further merge RPN and Fast R-CNN into a single network by sharing their convolutionAL features."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3320897"
                        ],
                        "name": "T. Kieninger",
                        "slug": "T.-Kieninger",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Kieninger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kieninger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145279674"
                        ],
                        "name": "A. Dengel",
                        "slug": "A.-Dengel",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Dengel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dengel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "[3]\u2013[5] are among the pioneers who laid the groundwork for table spotting and structure recognition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206776168,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "d25a61cc0cd816eba75864912fe2f6f44be3cecf",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper summarizes the core idea of the T-Recs table recognition system, an integrated system covering block-segmentation, table location and a model-free structural analysis of tables. T-Recs works on the output of commercial OCR systems that provide the word bounding box geometry together with the text itself (e.g. Xerox ScanWorX). While T-Recs performs well on a number of document categories, business letters still remained a challenging domain because the T-Recs location heuristics are mislead by their header or footer resulting in a low recognition precision. Business letters such as invoices are a very interesting domain for industrial applications due to the large amount of documents to be analyzed and the importance of the data carried within their tables. Hence, we developed a more restrictive approach which is implemented in the T-Recs++ prototype. This paper describes the ideas of the T-Recs++ location and also proposes a quality evaluation measure that reflects the bottom-up strategy of either T-Recs or T-Recs++. Finally, some results comparing both systems on a collection of business letters are given."
            },
            "slug": "Applying-the-T-Recs-table-recognition-system-to-the-Kieninger-Dengel",
            "title": {
                "fragments": [],
                "text": "Applying the T-Recs table recognition system to the business letter domain"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "This paper summarizes the core idea of the T-Recs table recognition system, an integrated system covering block-segmentation, table location and a model-free structural analysis of tables, and proposes a quality evaluation measure that reflects the bottom-up strategy of either T-recs or T- Recs++."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Sixth International Conference on Document Analysis and Recognition"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48799969"
                        ],
                        "name": "Matthew D. Zeiler",
                        "slug": "Matthew-D.-Zeiler",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Zeiler",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew D. Zeiler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "and Zisserman model (VGG-16) [19] and Zeiler and Fergus model (ZF) [20] pretrained on ImageNet dataset [21] as feature extractor."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3960646,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1a2a770d23b4a171fa81de62a78a3deb0588f238",
            "isKey": false,
            "numCitedBy": 11811,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets."
            },
            "slug": "Visualizing-and-Understanding-Convolutional-Zeiler-Fergus",
            "title": {
                "fragments": [],
                "text": "Visualizing and Understanding Convolutional Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel visualization technique is introduced that gives insight into the function of intermediate feature layers and the operation of the classifier in large Convolutional Network models, used in a diagnostic role to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2136435893"
                        ],
                        "name": "Jonathan Huang",
                        "slug": "Jonathan-Huang",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1382126732"
                        ],
                        "name": "V. Rathod",
                        "slug": "V.-Rathod",
                        "structuredName": {
                            "firstName": "Vivek",
                            "lastName": "Rathod",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Rathod"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1491624845"
                        ],
                        "name": "Chen Sun",
                        "slug": "Chen-Sun",
                        "structuredName": {
                            "firstName": "Chen",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chen Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2717876"
                        ],
                        "name": "Menglong Zhu",
                        "slug": "Menglong-Zhu",
                        "structuredName": {
                            "firstName": "Menglong",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Menglong Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34786378"
                        ],
                        "name": "A. Balan",
                        "slug": "A.-Balan",
                        "structuredName": {
                            "firstName": "Anoop",
                            "lastName": "Balan",
                            "middleNames": [
                                "Korattikara"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Balan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50706340"
                        ],
                        "name": "A. Fathi",
                        "slug": "A.-Fathi",
                        "structuredName": {
                            "firstName": "Alireza",
                            "lastName": "Fathi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Fathi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33091759"
                        ],
                        "name": "Ian S. Fischer",
                        "slug": "Ian-S.-Fischer",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Fischer",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian S. Fischer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3282833"
                        ],
                        "name": "Z. Wojna",
                        "slug": "Z.-Wojna",
                        "structuredName": {
                            "firstName": "Zbigniew",
                            "lastName": "Wojna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Wojna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157997231"
                        ],
                        "name": "Yang Song",
                        "slug": "Yang-Song",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yang Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687120"
                        ],
                        "name": "S. Guadarrama",
                        "slug": "S.-Guadarrama",
                        "structuredName": {
                            "firstName": "Sergio",
                            "lastName": "Guadarrama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Guadarrama"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702318"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "We have used Tensorflow Object Detection API [22] as it supports many different base models trained on various datasets for Faster R-CNN."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206595627,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a312a573ef81793d56401e932ef6c9498791a3d1",
            "isKey": false,
            "numCitedBy": 1999,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of this paper is to serve as a guide for selecting a detection architecture that achieves the right speed/memory/accuracy balance for a given application and platform. To this end, we investigate various ways to trade accuracy for speed and memory usage in modern convolutional object detection systems. A number of successful systems have been proposed in recent years, but apples-toapples comparisons are difficult due to different base feature extractors (e.g., VGG, Residual Networks), different default image resolutions, as well as different hardware and software platforms. We present a unified implementation of the Faster R-CNN [30], R-FCN [6] and SSD [25] systems, which we view as meta-architectures and trace out the speed/accuracy trade-off curve created by using alternative feature extractors and varying other critical parameters such as image size within each of these meta-architectures. On one extreme end of this spectrum where speed and memory are critical, we present a detector that achieves real time speeds and can be deployed on a mobile device. On the opposite end in which accuracy is critical, we present a detector that achieves state-of-the-art performance measured on the COCO detection task."
            },
            "slug": "Speed/Accuracy-Trade-Offs-for-Modern-Convolutional-Huang-Rathod",
            "title": {
                "fragments": [],
                "text": "Speed/Accuracy Trade-Offs for Modern Convolutional Object Detectors"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A unified implementation of the Faster R-CNN, R-FCN and SSD systems is presented and the speed/accuracy trade-off curve created by using alternative feature extractors and varying other critical parameters such as image size within each of these meta-architectures is traced out."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771551"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "For fine tuning, we used Resnet101 [23] trained on"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 25
                            }
                        ],
                        "text": "For fine tuning, we used Resnet101 [23] trained on KITTI dataset [24]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 206594692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "isKey": false,
            "numCitedBy": 95318,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."
            },
            "slug": "Deep-Residual-Learning-for-Image-Recognition-He-Zhang",
            "title": {
                "fragments": [],
                "text": "Deep Residual Learning for Image Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This work presents a residual learning framework to ease the training of networks that are substantially deeper than those used previously, and provides comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 11
                            }
                        ],
                        "text": "Fast R-CNN [18], predecessor of Faster R-CNN, uses selective search for region proposals generation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206770307,
            "fieldsOfStudy": [
                "Computer Science",
                "Environmental Science"
            ],
            "id": "7ffdbc358b63378f07311e883dddacc9faeeaf4b",
            "isKey": false,
            "numCitedBy": 14072,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn."
            },
            "slug": "Fast-R-CNN-Girshick",
            "title": {
                "fragments": [],
                "text": "Fast R-CNN"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection that builds on previous work to efficiently classify object proposals using deep convolutional networks."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3200914"
                        ],
                        "name": "Asif Shahab",
                        "slug": "Asif-Shahab",
                        "structuredName": {
                            "firstName": "Asif",
                            "lastName": "Shahab",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Asif Shahab"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688013"
                        ],
                        "name": "F. Shafait",
                        "slug": "F.-Shafait",
                        "structuredName": {
                            "firstName": "Faisal",
                            "lastName": "Shafait",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Shafait"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3320897"
                        ],
                        "name": "T. Kieninger",
                        "slug": "T.-Kieninger",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Kieninger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kieninger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145279674"
                        ],
                        "name": "A. Dengel",
                        "slug": "A.-Dengel",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Dengel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dengel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "Publicly accessible UNLV dataset [2] is used for evaluation of our approach and it gave better results as compared to previous best in-class techniques."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 16140787,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bff7592f28354b3be632f4bf678b622f765a2b8d",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Table spotting and structural analysis are just a small fraction of tasks relevant when speaking of table analysis. Today, quite a large number of different approaches facing these tasks have been described in literature or are available as part of commercial OCR systems that claim to deal with tables on the scanned documents and to treat them accordingly.\n However, the problem of detecting tables is not yet solved at all. Different approaches have different strengths and weak points. Some fail in certain situations or layouts where others perform better. How shall one know, which approach or system is the best for his specific job? The answer to this question raises the demand for an objective comparison of different approaches which address the same task of spotting tables and recognizing their structure.\n This paper describes our approach towards establishing a complete and publicly available, hence open environment for the benchmarking of table spotting and structural analysis. We provide free access to the ground truthing tool and evaluation mechanism described in this paper, describe the ideas behind and we also provide ground truth for the 547 documents of the UNLV and UW-3 datasets that contain tables.\n In addition, we applied the quality measures to the results that were generated by the T-Recs system which we developed some years ago and which we started to further advance since a few months."
            },
            "slug": "An-open-approach-towards-the-benchmarking-of-table-Shahab-Shafait",
            "title": {
                "fragments": [],
                "text": "An open approach towards the benchmarking of table structure recognition systems"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The approach towards establishing a complete and publicly available, hence open environment for the benchmarking of table spotting and structural analysis is described and free access to the ground truthing tool and evaluation mechanism described is provided."
            },
            "venue": {
                "fragments": [],
                "text": "DAS '10"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3320897"
                        ],
                        "name": "T. Kieninger",
                        "slug": "T.-Kieninger",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Kieninger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kieninger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145279674"
                        ],
                        "name": "A. Dengel",
                        "slug": "A.-Dengel",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Dengel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dengel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58220542,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d1fdbf577a1b8d6e78556a509326bbf304af5780",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Document organization may be described in different ways. The physical presentation on one hand by means of a nested layout structure representing a \u201cpart-of\u201d-relationship, e.g. of text blocks, text line segments, words, and characters. The composition of meaningful entities, such as title, author, address, or abstract, on the other, in terms of a logical structure. Both views to the contents of a document are complementary. They relate to each other as being explicitely given by publication guidelines where the position and dimensions of logical objects are precisely described. Moreover such \u201cpublication guidelines\u201d more or less hold for various types of documents. Although they are intuitively understandable for human beings, they are hard to formalize explicitly because of the freedom originators and authors of documents have in order to incorporate their own creativity. However, humans use these intrinsic layout features to get first hints towards the logical meaning of document information. This is even obvious when considering the very difficult document examples in Figure 1 where no text is given but it is nevertheless possible to generate hypothesis for logical meaning."
            },
            "slug": "Table-Recognition-and-Labeling-Using-Intrinsic-Kieninger-Dengel",
            "title": {
                "fragments": [],
                "text": "Table Recognition and Labeling Using Intrinsic Layout Features"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Although intrinsic layout features of documents are intuitively understandable for human beings, they are hard to formalize explicitly because of the freedom originators and authors of documents have in order to incorporate their own creativity."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153302678"
                        ],
                        "name": "Jia Deng",
                        "slug": "Jia-Deng",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144847596"
                        ],
                        "name": "Wei Dong",
                        "slug": "Wei-Dong",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Dong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Dong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2040091191"
                        ],
                        "name": "Li-Jia Li",
                        "slug": "Li-Jia-Li",
                        "structuredName": {
                            "firstName": "Li-Jia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li-Jia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "94451829"
                        ],
                        "name": "K. Li",
                        "slug": "K.-Li",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 103
                            }
                        ],
                        "text": "and Zisserman model (VGG-16) [19] and Zeiler and Fergus model (ZF) [20] pretrained on ImageNet dataset [21] as feature extractor."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 57246310,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b47265245e8db53a553049dcb27ed3e495fd625",
            "isKey": false,
            "numCitedBy": 27403,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called \u201cImageNet\u201d, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond."
            },
            "slug": "ImageNet:-A-large-scale-hierarchical-image-database-Deng-Dong",
            "title": {
                "fragments": [],
                "text": "ImageNet: A large-scale hierarchical image database"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new database called \u201cImageNet\u201d is introduced, a large-scale ontology of images built upon the backbone of the WordNet structure, much larger in scale and diversity and much more accurate than the current image datasets."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47237027"
                        ],
                        "name": "Andreas Geiger",
                        "slug": "Andreas-Geiger",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Geiger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Geiger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37108776"
                        ],
                        "name": "Philip Lenz",
                        "slug": "Philip-Lenz",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Lenz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip Lenz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760556"
                        ],
                        "name": "C. Stiller",
                        "slug": "C.-Stiller",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Stiller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Stiller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2422559"
                        ],
                        "name": "R. Urtasun",
                        "slug": "R.-Urtasun",
                        "structuredName": {
                            "firstName": "Raquel",
                            "lastName": "Urtasun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Urtasun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "KITTI dataset [24]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9455111,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79b949d9b35c3f51dd20fb5c746cc81fc87147eb",
            "isKey": false,
            "numCitedBy": 4620,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research. In total, we recorded 6 hours of traffic scenarios at 10\u2013100 Hz using a variety of sensor modalities such as high-resolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and a high-precision GPS/IMU inertial navigation system. The scenarios are diverse, capturing real-world traffic situations, and range from freeways over rural areas to inner-city scenes with many static and dynamic objects. Our data is calibrated, synchronized and timestamped, and we provide the rectified and raw image sequences. Our dataset also contains object labels in the form of 3D tracklets, and we provide online benchmarks for stereo, optical flow, object detection and other tasks. This paper describes our recording platform, the data format and the utilities that we provide."
            },
            "slug": "Vision-meets-robotics:-The-KITTI-dataset-Geiger-Lenz",
            "title": {
                "fragments": [],
                "text": "Vision meets robotics: The KITTI dataset"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research, using a variety of sensor modalities such as high-resolution color and grayscale stereo cameras and a high-precision GPS/IMU inertial navigation system."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Robotics Res."
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49915337"
                        ],
                        "name": "A. C. E. Silva",
                        "slug": "A.-C.-E.-Silva",
                        "structuredName": {
                            "firstName": "Ana",
                            "lastName": "Silva",
                            "middleNames": [
                                "Costa",
                                "e"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. C. E. Silva"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 51
                            }
                        ],
                        "text": "An early data driven approach developed by A. C. e Silva [10] back in 2009 focuses on table detection using Hidden Markov Models (HMM)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 8
                            }
                        ],
                        "text": "e Silva [10] back in 2009 focuses on table detection using Hidden Markov Models (HMM)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7260147,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "166e2f3ff52620ffc975acca6281915995498605",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Hidden Markov Models (HMM) are probabilistic graphical models for interdependent classification. In this paper we experiment with different ways of combining the components of an HMM for document analysis applications, in particular for finding tables in text. We show: a) how to integrate different document structure finders into the HMM; b) that transition probabilities should vary along the chain to embed general knowledge axioms of our field, c) some emission energies can be selectively ignored, and d) emission and transition probabilities can be weighed differently. We conclude these changes increase the expressiveness and usability of HMMs in our field."
            },
            "slug": "Learning-Rich-Hidden-Markov-Models-in-Document-Silva",
            "title": {
                "fragments": [],
                "text": "Learning Rich Hidden Markov Models in Document Analysis: Table Location"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This paper experiments with different ways of combining the components of an HMM for document analysis applications, in particular for finding tables in text, and shows how to integrate different document structure finders into the HMM."
            },
            "venue": {
                "fragments": [],
                "text": "2009 10th International Conference on Document Analysis and Recognition"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[6] developed a table detection system based on dynamic programming."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mediumindependent table detection"
            },
            "venue": {
                "fragments": [],
                "text": "Document Recognition and Retrieval VII, vol. 3967, pp. 291\u2013303, International Society for Optics and Photonics, 1999."
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[3]\u2013[5] are among the pioneers who laid the groundwork for table spotting and structure recognition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A paper-to-html table converting system"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of document analysis systems (DAS), vol. 98, 1998."
            },
            "year": 1998
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 11,
            "methodology": 11,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 25,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Table-Detection-in-Document-Images-using-Foreground-Arif-Shafait/2958f4aa3a7062e361941596008893121722df60?sort=total-citations"
}