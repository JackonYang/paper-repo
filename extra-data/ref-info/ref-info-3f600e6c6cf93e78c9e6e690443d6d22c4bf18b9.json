{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2995060"
                        ],
                        "name": "D. Swets",
                        "slug": "D.-Swets",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Swets",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Swets"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145926447"
                        ],
                        "name": "J. Weng",
                        "slug": "J.-Weng",
                        "structuredName": {
                            "firstName": "Juyang",
                            "lastName": "Weng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weng"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "As = kVk+ k ~ Vk 2X\u0300i=1 m Xj=1 i jk(xi; zj); (27) the gradient of with respect to the j and the zj is readily expressed in terms of the kernel function, thus can be minimized by standard gradient methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Swets and Weng (1996)show a solution to this problem by using PCA as apreprocessing step only."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10952196,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5e28e81e757009d2f76b8674e0da431f5845884a",
            "isKey": false,
            "numCitedBy": 1773,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the automatic selection of features from an image training set using the theories of multidimensional discriminant analysis and the associated optimal linear projection. We demonstrate the effectiveness of these most discriminating features for view-based class retrieval from a large database of widely varying real-world objects presented as \"well-framed\" views, and compare it with that of the principal component analysis."
            },
            "slug": "Using-Discriminant-Eigenfeatures-for-Image-Swets-Weng",
            "title": {
                "fragments": [],
                "text": "Using Discriminant Eigenfeatures for Image Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "This paper describes the automatic selection of features from an image training set using the theories of multidimensional discriminant analysis and the associated optimal linear projection, and demonstrates the effectiveness of these most discriminating features for view-based class retrieval from a large database of widely varying real-world objects."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110576212"
                        ],
                        "name": "D. Wilson",
                        "slug": "D.-Wilson",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Wilson",
                            "middleNames": [
                                "J.",
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Wilson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751846"
                        ],
                        "name": "G. Irwin",
                        "slug": "G.-Irwin",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Irwin",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Irwin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752416"
                        ],
                        "name": "G. Lightbody",
                        "slug": "G.-Lightbody",
                        "structuredName": {
                            "firstName": "Gordon",
                            "lastName": "Lightbody",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lightbody"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 2109380,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1cadd70479463bcccef3ab118a33e241ec9a5b60",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a novel means for creating a nonlinear extension of principal component analysis (PCA) using radial basis function (RBF) networks. This algorithm comprises two distinct stages: projection and self-consistency. The projection stage contains a single network, trained to project data from a high- to a low-dimensional space. Training requires solution of a generalized eigenvector equation. The second stage, trained using a novel hybrid nonlinear optimization algorithm, then performs the inverse transformation. Issues relating to the practical implementation of the procedure are discussed, and the algorithm is demonstrated on a nonlinear test problem. An example of the application of the algorithm to data from a benchmark simulation of an industrial overheads condenser and reflux drum rig is also included. This shows the usefulness of the procedure in detecting and isolating both sensor and process faults. Pointers for future research in this area are also given."
            },
            "slug": "RBF-principal-manifolds-for-process-monitoring-Wilson-Irwin",
            "title": {
                "fragments": [],
                "text": "RBF principal manifolds for process monitoring"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This paper describes a novel means for creating a nonlinear extension of principal component analysis (PCA) using radial basis function (RBF) networks, which comprises two distinct stages: projection and self-consistency."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705272"
                        ],
                        "name": "K. Diamantaras",
                        "slug": "K.-Diamantaras",
                        "structuredName": {
                            "firstName": "Konstantinos",
                            "lastName": "Diamantaras",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Diamantaras"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144410963"
                        ],
                        "name": "S. Kung",
                        "slug": "S.-Kung",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Kung",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kung"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 115
                            }
                        ],
                        "text": "Rather than giving a full review of this eldhere, we brie y describe just three approaches,and refer the reader to Diamantaras & Kung(1996) for more details."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 270
                            }
                        ],
                        "text": "\u2026inrepresenting the observations by the rst qprincipal components is minimal the principal components are uncorrelated the representation entropy is minimized the rst q principal components have maxi-mal mutual information with respect to theinputsFor more details, see Diamantaras & Kung (1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 222
                            }
                        ],
                        "text": "If we train itto reproduce the input values as outputs (i.e. useit in an autoassociative mode), then the hiddenunit activations form a lower{dimensional repre-sentation of the data, closely related to PCA (seefor instance Diamantaras & Kung, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 53883702,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f1a9350fd8141bcda3068aec33aef385d5c02eb",
            "isKey": false,
            "numCitedBy": 481,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A Review of Linear Algebra. Principal Component Analysis. PCA Neural Networks. Channel Noise and Hidden Units. Heteroassociative Models. Signal Enhancement Against Noise. VLSI Implementation. Appendices. Bibliography. Index."
            },
            "slug": "Principal-Component-Neural-Networks:-Theory-and-Diamantaras-Kung",
            "title": {
                "fragments": [],
                "text": "Principal Component Neural Networks: Theory and Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A review of Linear Algebra, Principal Component Analysis, and VLSI Implementation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144097660"
                        ],
                        "name": "M. Turk",
                        "slug": "M.-Turk",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Turk",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Turk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 54
                            }
                        ],
                        "text": "PCA has been successfully used for face recogni-tion (Turk & Pentland, 1991) and face representa-tion (Vetter & Poggio, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 148
                            }
                        ],
                        "text": "This is due to the fact that for k(x;y) = (x y), the Support Vector decision function (Boser, Guyon, & Vapnik, 1992) f(x) = sgn(X\u0300i=1 ik(x;xi) + b) (28) can be expressed with a single weight vector w = P\u00ec=1 ixi asf(x) = sgn((x w) + b): (29) Thus the nal stage of classi cation can be done extremely fast; the speed of the principal component extraction phase, on the other hand, and thus the accuracy{speed tradeo of the whole classi er, can be controlled by the number of components which we extract, or by the above reduced set parameter m."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 26127529,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6f1dfcc44277d4cfd8507284d994c9283dc3a2f",
            "isKey": false,
            "numCitedBy": 14954,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We have developed a near-real-time computer system that can locate and track a subject's head, and then recognize the person by comparing characteristics of the face to those of known individuals. The computational approach taken in this system is motivated by both physiology and information theory, as well as by the practical requirements of near-real-time performance and accuracy. Our approach treats the face recognition problem as an intrinsically two-dimensional (2-D) recognition problem rather than requiring recovery of three-dimensional geometry, taking advantage of the fact that faces are normally upright and thus may be described by a small set of 2-D characteristic views. The system functions by projecting face images onto a feature space that spans the significant variations among known face images. The significant features are known as \"eigenfaces,\" because they are the eigenvectors (principal components) of the set of faces; they do not necessarily correspond to features such as eyes, ears, and noses. The projection operation characterizes an individual face by a weighted sum of the eigenface features, and so to recognize a particular face it is necessary only to compare these weights to those of known individuals. Some particular advantages of our approach are that it provides for the ability to learn and later recognize new faces in an unsupervised manner, and that it is easy to implement using a neural network architecture."
            },
            "slug": "Eigenfaces-for-Recognition-Turk-Pentland",
            "title": {
                "fragments": [],
                "text": "Eigenfaces for Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A near-real-time computer system that can locate and track a subject's head, and then recognize the person by comparing characteristics of the face to those of known individuals, and that is easy to implement using a neural network architecture."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Cognitive Neuroscience"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11382731,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8314dda1ec43ce57ff877f8f02ed89acb68ca035",
            "isKey": false,
            "numCitedBy": 581,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Memory-based classification algorithms such as radial basis functions or K-nearest neighbors typically rely on simple distances (Euclidean, dot product...), which are not particularly meaningful on pattern vectors. More complex, better suited distance measures are often expensive and rather ad-hoc (elastic matching, deformable templates). We propose a new distance measure which (a) can be made locally invariant to any set of transformations of the input and (b) can be computed efficiently. We tested the method on large handwritten character databases provided by the Post Office and the NIST. Using invariances with respect to translation, rotation, scaling, shearing and line thickness, the method consistently outperformed all other systems tested on the same databases."
            },
            "slug": "Efficient-Pattern-Recognition-Using-a-New-Distance-Simard-LeCun",
            "title": {
                "fragments": [],
                "text": "Efficient Pattern Recognition Using a New Transformation Distance"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new distance measure which can be made locally invariant to any set of transformations of the input and can be computed efficiently is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2459012"
                        ],
                        "name": "S. Mika",
                        "slug": "S.-Mika",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Mika",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mika"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3125657"
                        ],
                        "name": "Phil Knirsch",
                        "slug": "Phil-Knirsch",
                        "structuredName": {
                            "firstName": "Phil",
                            "lastName": "Knirsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Phil Knirsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152597562"
                        ],
                        "name": "Gunnar R\u00e4tsch",
                        "slug": "Gunnar-R\u00e4tsch",
                        "structuredName": {
                            "firstName": "Gunnar",
                            "lastName": "R\u00e4tsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gunnar R\u00e4tsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14669541,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b6a3e0028d99439ce2741d0e147b6e9a34bc4267",
            "isKey": false,
            "numCitedBy": 1231,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper collects some ideas targeted at advancing our understanding of the feature spaces associated with support vector (SV) kernel functions. We first discuss the geometry of feature space. In particular, we review what is known about the shape of the image of input space under the feature space map, and how this influences the capacity of SV methods. Following this, we describe how the metric governing the intrinsic geometry of the mapped surface can be computed in terms of the kernel, using the example of the class of inhomogeneous polynomial kernels, which are often used in SV pattern recognition. We then discuss the connection between feature space and input space by dealing with the question of how one can, given some vector in feature space, find a preimage (exact or approximate) in input space. We describe algorithms to tackle this issue, and show their utility in two applications of kernel methods. First, we use it to reduce the computational complexity of SV decision functions; second, we combine it with the Kernel PCA algorithm, thereby constructing a nonlinear statistical denoising technique which is shown to perform well on real-world data."
            },
            "slug": "Input-space-versus-feature-space-in-kernel-based-Sch\u00f6lkopf-Mika",
            "title": {
                "fragments": [],
                "text": "Input space versus feature space in kernel-based methods"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The geometry of feature space is reviewed, and the connection between feature space and input space is discussed by dealing with the question of how one can, given some vector in feature space, find a preimage in input space."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207165665,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2599131a4bc2fa957338732a37c744cfe3e17b24",
            "isKey": false,
            "numCitedBy": 10835,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms."
            },
            "slug": "A-training-algorithm-for-optimal-margin-classifiers-Boser-Guyon",
            "title": {
                "fragments": [],
                "text": "A training algorithm for optimal margin classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented, applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '92"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053520352"
                        ],
                        "name": "M. Kirby",
                        "slug": "M.-Kirby",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kirby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kirby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49555086"
                        ],
                        "name": "L. Sirovich",
                        "slug": "L.-Sirovich",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Sirovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Sirovich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 570648,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66d75a5fe9e1b6511c5135d68e9ce8c0da5a7374",
            "isKey": false,
            "numCitedBy": 2852,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of natural symmetries (mirror images) in a well-defined family of patterns (human faces) is discussed within the framework of the Karhunen-Loeve expansion. This results in an extension of the data and imposes even and odd symmetry on the eigenfunctions of the covariance matrix, without increasing the complexity of the calculation. The resulting approximation of faces projected from outside of the data set onto this optimal basis is improved on average. >"
            },
            "slug": "Application-of-the-Karhunen-Loeve-Procedure-for-the-Kirby-Sirovich",
            "title": {
                "fragments": [],
                "text": "Application of the Karhunen-Loeve Procedure for the Characterization of Human Faces"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "The use of natural symmetries (mirror images) in a well-defined family of patterns (human faces) is discussed within the framework of the Karhunen-Loeve expansion, which results in an extension of the data and imposes even and odd symmetry on the eigenfunctions of the covariance matrix."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 224
                            }
                        ],
                        "text": "For us, this has two useful consequences: rst, we can consider the equivalent equation ( (xk) V) = ( (xk) CV) for all k = 1; : : : ;M; (7) and second, there exist coe cients i (i = 1; : : : ;M ) such that V = M Xi=1 i (xi): (8) Combining (7) and (8), we get M Xi=1 i( (xk) (xi)) = 1 M M Xi=1 i( (xk) M Xj=1 (xj))( (xj) (xi)) for all k = 1; : : : ;M: (9) De ning an M M matrix K by Kij := ( (xi) (xj)); (10) this reads M K = K2 ; (11) where denotes the column vector with entries 1; : : : ; M ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 20
                            }
                        ],
                        "text": "Using the expansion (8) Vjk = M X m=1 jk m (xm); we get (xi);Vjk =Xm Kim jk m thus Pj1:::jp = 1 M M Xi=1 p Y k=1 M X m=1Kim jk m (35) 10This requires the existence of these moments, which need not always be the case, as they are the Taylor expansion of the characteristic function of the probability density function in feature space."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 104
                            }
                        ],
                        "text": "However, in that case, wecan speed up the extraction by a method analogu-ous to a technique proposed by Burges (1996) inthe context of Support Vector machines."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "(8)) by another vector ~ V = m Xj=1 j (zj); (25) where m < ` is chosen a priori according to the desired speed{up, and zj 2 RN ; j = 1; ;m."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 173
                            }
                        ],
                        "text": "In the case k(x;y) = (x y)2 it is possi-ble to get an exact expansion with at most N vec-tors zj (N being the dimension of the input space)by solving an Eigenvalue problem (Burges, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 42
                            }
                        ],
                        "text": "Even though this gen-eral fact was known (Burges, 1996), the machinelearning community has made little use of it, theexception being Support Vector machines (Vap-nik, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 55
                            }
                        ],
                        "text": "(Vk Vk) = 1 for all k = p; : : : ;M: (15) By virtue of (8) and (12), this translates into a normalization condition for p; : : : ; M : 1 = M X i;j=1 ki kj ( (xi) (xj)) = M X i;j=1 ki kjKij = ( k K k) = k( k k) (16) For the purpose of principal component extraction, we need to compute projections on the Eigenvectors Vk in F (k = p; : : : ;M )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52810328,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1061ff8a216a8d00f5f189d7ea593c6f0703b771",
            "isKey": true,
            "numCitedBy": 511,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A Support Vector Machine SVM is a uni versal learning machine whose decision sur face is parameterized by a set of support vec tors and by a set of corresponding weights An SVM is also characterized by a kernel function Choice of the kernel determines whether the resulting SVM is a polynomial classi er a two layer neural network a ra dial basis function machine or some other learning machine SVMs are currently considerably slower in test phase than other approaches with sim ilar generalization performance To address this we present a general method to signif icantly decrease the complexity of the deci sion rule obtained using an SVM The pro posed method computes an approximation to the decision rule in terms of a reduced set of vectors These reduced set vectors are not support vectors and can in some cases be computed analytically We give ex perimental results for three pattern recogni tion problems The results show that the method can decrease the computational com plexity of the decision rule by a factor of ten with no loss in generalization perfor mance making the SVM test speed com petitive with that of other methods Fur ther the method allows the generalization performance complexity trade o to be di rectly controlled The proposed method is not speci c to pattern recognition and can be applied to any problem where the Sup port Vector algorithm is used for example regression INTRODUCTION SUPPORT VECTOR MACHINES Consider a two class classi er for which the decision rule takes the form"
            },
            "slug": "Simplified-Support-Vector-Decision-Rules-Burges",
            "title": {
                "fragments": [],
                "text": "Simplified Support Vector Decision Rules"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The results show that the method can decrease the computational complexity of the decision rule by a factor of ten with no loss in generalization perfor mance making the SVM test speed com petitive with that of other methods."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 87
                            }
                        ],
                        "text": "We can, however, go onestep further and construct tools for nonlinear ICAand nonlinear Projection Pursuit."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 145
                            }
                        ],
                        "text": "\u2026out some possibilities for future work.7.1 Projection Pursuit, IndependentComponents, and Higher OrderMomentsIn order to do Projection Pursuit (Friedman, 1987)and Independent component Analysis (ICA) (Jut-ten & Herault, 1991, Bell & Sejnowski, 1995), weare, loosely speaking, looking for\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 207
                            }
                        ],
                        "text": "Note that K is positive semide nite, which can be seen by noticing that it equals ( (x1); : : : ; (xM))> ( (x1); : : : ; (xM )); (13) which implies that for all X 2 F , (X KX) = k( (x1); : : : ; (xM))Xk2 0: (14) Consequently, K's Eigenvalues will be nonnegative, and will exactly give the solutions M of Eq."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 96
                            }
                        ],
                        "text": "Instead, we shall just give a few examples,and point out some possibilities for future work.7.1 Projection Pursuit, IndependentComponents, and Higher OrderMomentsIn order to do Projection Pursuit (Friedman, 1987)and Independent component Analysis (ICA) (Jut-ten & Herault, 1991, Bell & Sejnowski, 1995), weare, loosely speaking, looking for directions in thedataset such that the projections onto these aremaximally \\non{Gaussian\"."
                    },
                    "intents": []
                }
            ],
            "corpusId": 120727315,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cce218b91cf634413ef9a71f702bd37b1a9ad2a6",
            "isKey": true,
            "numCitedBy": 590,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract A new projection pursuit algorithm for exploring multivariate data is presented that has both statistical and computational advantages over previous methods. A number of practical issues concerning its application are addressed. A connection to multivariate density estimation is established, and its properties are investigated through simulation studies and application to real data. The goal of exploratory projection pursuit is to use the data to find low- (one-, two-, or three-) dimensional projections that provide the most revealing views of the full-dimensional data. With these views the human gift for pattern recognition can be applied to help discover effects that may not have been anticipated in advance. Since linear effects are directly captured by the covariance structure of the variable pairs (which are straightforward to estimate) the emphasis here is on the discovery of nonlinear effects such as clustering or other general nonlinear associations among the variables. Although arbitrary ..."
            },
            "slug": "Exploratory-Projection-Pursuit-Friedman",
            "title": {
                "fragments": [],
                "text": "Exploratory Projection Pursuit"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A new projection pursuit algorithm for exploring multivariate data is presented that has both statistical and computational advantages over previous methods and the emphasis here is on the discovery of nonlinear effects such as clustering or other general nonlinear associations among the variables."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784682"
                        ],
                        "name": "T. Hastie",
                        "slug": "T.-Hastie",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Hastie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hastie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761784"
                        ],
                        "name": "R. Tibshirani",
                        "slug": "R.-Tibshirani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tibshirani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tibshirani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1963591"
                        ],
                        "name": "A. Buja",
                        "slug": "A.-Buja",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Buja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Buja"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "As in (16), the solutions ~ k are normalized by normalizing the corresponding vectors ~ Vk in F , which translates into ~ k(~ k ~ k) = 1: (50) For feature extraction, we compute projections of centered {images of test patterns t onto the Eigenvectors of the covariance matrix of the centered points, ( ~ Vk (t)) = M Xi=1 ~ ki (~ (xk) ~ (t)) = M Xi=1 ~ ki ~ K(xk; t): (51) Consider a set of test points t1; : : : ; tL, and de ne two L M test dot product matrices by Ktest ij = ( (ti) (xj)) (52) and ~ Ktest ij = (( (ti) 1 M M X m=1 (xm)) ( (xj) 1 M M X n=1 (xn))): (53) Similar to (49), we can then again express ~ Ktest in terms of Ktest, and arrive at ~ Ktest = Ktest 10MK Ktest1M+10MK1M ; (54) where 10M is the L M matrix with all entries equal to 1=M ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 210
                            }
                        ],
                        "text": "(Vk Vk) = 1 for all k = p; : : : ;M: (15) By virtue of (8) and (12), this translates into a normalization condition for p; : : : ; M : 1 = M X i;j=1 ki kj ( (xi) (xj)) = M X i;j=1 ki kjKij = ( k K k) = k( k k) (16) For the purpose of principal component extraction, we need to compute projections on the Eigenvectors Vk in F (k = p; : : : ;M )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122018810,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "03fea2fbb8e780fdebcce89bb19c6f4b87f73347",
            "isKey": false,
            "numCitedBy": 668,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Fisher's linear discriminant analysis is a valuable tool for multigroup classification. With a large number of predictors, one can find a reduced number of discriminant coordinate functions that are \u201coptimal\u201d for separating the groups. With two such functions, one can produce a classification map that partitions the reduced space into regions that are identified with group membership, and the decision boundaries are linear. This article is about richer nonlinear classification schemes. Linear discriminant analysis is equivalent to multiresponse linear regression using optimal scorings to represent the groups. In this paper, we obtain nonparametric versions of discriminant analysis by replacing linear regression by any nonparametric regression method. In this way, any multiresponse regression technique (such as MARS or neural networks) can be postprocessed to improve its classification performance."
            },
            "slug": "Flexible-Discriminant-Analysis-by-Optimal-Scoring-Hastie-Tibshirani",
            "title": {
                "fragments": [],
                "text": "Flexible Discriminant Analysis by Optimal Scoring"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Nonparametric versions of discriminant analysis are obtained by replacing linear regression by any nonparametric regression method so that any multiresponse regression technique can be postprocessed to improve its classification performance."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111170937"
                        ],
                        "name": "Martin Brown",
                        "slug": "Martin-Brown",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Brown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3299838"
                        ],
                        "name": "H. Lewis",
                        "slug": "H.-Lewis",
                        "structuredName": {
                            "firstName": "Hugh",
                            "lastName": "Lewis",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Lewis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2317393"
                        ],
                        "name": "S. Gunn",
                        "slug": "S.-Gunn",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Gunn",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gunn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16520624,
            "fieldsOfStudy": [
                "Environmental Science",
                "Computer Science"
            ],
            "id": "94a1092bd202dda5f014cfa0e5afce959bb0d69f",
            "isKey": false,
            "numCitedBy": 209,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "Mixture modeling is becoming an increasingly important tool in the remote sensing community as researchers attempt to resolve subpixel, area information. This paper compares a well-established technique, linear spectral mixture models (LSMM), with a much newer idea based on data selection, support vector machines (SVM). It is shown that the constrained least squares LSMM is equivalent to the linear SVM, which relies on proving that the LSMM algorithm possesses the \"maximum margin\" property. This in turn shows that the LSMM algorithm can be derived from the same optimality conditions as the linear SVM, which provides important insights about the role of the bias term and rank deficiency in the pure pixel matrix within the LSMM algorithm. It also highlights one of the main advantages for using the linear SVM algorithm in that it performs automatic \"pure pixel\" selection from a much larger database. In addition, extensions to the basic SVM algorithm allow the technique to be applied to data sets that exhibit spectral confusion (overlapping sets of pure pixels) and to data sets that have nonlinear mixture regions. Several illustrative examples, based on an area-labeled Landsat dataset, are used to demonstrate the potential of this approach."
            },
            "slug": "Linear-spectral-mixture-models-and-support-vector-Brown-Lewis",
            "title": {
                "fragments": [],
                "text": "Linear spectral mixture models and support vector machines for remote sensing"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that the constrained least squares LSMM is equivalent to the linear SVM, which relies on proving that the LSMM algorithm possesses the \"maximum margin\" property, which provides important insights about the role of the bias term and rank deficiency in the pure pixel matrix within the LS MM algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Geosci. Remote. Sens."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16577977,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "3e00dd12caea7c4dab1633a35d1da3cb2e76b420",
            "isKey": false,
            "numCitedBy": 2357,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "A simple linear neuron model with constrained Hebbian-type synaptic modification is analyzed and a new class of unconstrained learning rules is derived. It is shown that the model neuron tends to extract the principal component from a stationary input vector sequence."
            },
            "slug": "Simplified-neuron-model-as-a-principal-component-Oja",
            "title": {
                "fragments": [],
                "text": "Simplified neuron model as a principal component analyzer"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A simple linear neuron model with constrained Hebbian-type synaptic modification is analyzed and a new class of unconstrained learning rules is derived."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of mathematical biology"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 44
                            }
                        ],
                        "text": "(8)) by another vector ~ V = m Xj=1 j (zj); (25) where m < ` is chosen a priori according to the desired speed{up, and zj 2 RN ; j = 1; ;m."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9756494,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "55590f229e23a8e67af7d6d36f7456a595c251d1",
            "isKey": false,
            "numCitedBy": 318,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Developed only recently, support vector learning machines achieve high generalization ability by minimizing a bound on the expected test error; however, so far there existed no way of adding knowledge about invariances of a classification problem at hand. We present a method of incorporating prior knowledge about transformation invariances by applying transformations to support vectors, the training examples most critical for determining the classification boundary."
            },
            "slug": "Incorporating-Invariances-in-Support-Vector-Sch\u00f6lkopf-Burges",
            "title": {
                "fragments": [],
                "text": "Incorporating Invariances in Support Vector Learning Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This work presents a method of incorporating prior knowledge about transformation invariances by applying transformations to support vectors, the training examples most critical for determining the classification boundary."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2428034"
                        ],
                        "name": "C. Bregler",
                        "slug": "C.-Bregler",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Bregler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bregler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808760"
                        ],
                        "name": "S. Omohundro",
                        "slug": "S.-Omohundro",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Omohundro",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Omohundro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8783809,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56efe7bf4bd52a6369d9ebbe55033e81e716f7d0",
            "isKey": false,
            "numCitedBy": 132,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Most connectionist research has focused on learning mappings from one space to another (eg. classification and regression). This paper introduces the more general task of learning constraint surfaces. It describes a simple but powerful architecture for learning and manipulating nonlinear surfaces from data. We demonstrate the technique on low dimensional synthetic surfaces and compare it to nearest neighbor approaches. We then show its utility in learning the space of lip images in a system for improving speech recognition by lip reading. This learned surface is used to improve the visual tracking performance during recognition."
            },
            "slug": "Surface-Learning-with-Applications-to-Lipreading-Bregler-Omohundro",
            "title": {
                "fragments": [],
                "text": "Surface Learning with Applications to Lipreading"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper describes a simple but powerful architecture for learning and manipulating nonlinear surfaces from data and shows its utility in learning the space of lip images in a system for improving speech recognition by lip reading."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747298"
                        ],
                        "name": "R. Duin",
                        "slug": "R.-Duin",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Duin",
                            "middleNames": [
                                "P.",
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4723637"
                        ],
                        "name": "J. Mao",
                        "slug": "J.-Mao",
                        "structuredName": {
                            "firstName": "Jianchang",
                            "lastName": "Mao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Mao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 192934,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3626f388371b678b2f02f6eefc44fa5abc53ceb3",
            "isKey": false,
            "numCitedBy": 6533,
            "numCiting": 473,
            "paperAbstract": {
                "fragments": [],
                "text": "The primary goal of pattern recognition is supervised or unsupervised classification. Among the various frameworks in which pattern recognition has been traditionally formulated, the statistical approach has been most intensively studied and used in practice. More recently, neural network techniques and methods imported from statistical learning theory have been receiving increasing attention. The design of a recognition system requires careful attention to the following issues: definition of pattern classes, sensing environment, pattern representation, feature extraction and selection, cluster analysis, classifier design and learning, selection of training and test samples, and performance evaluation. In spite of almost 50 years of research and development in this field, the general problem of recognizing complex patterns with arbitrary orientation, location, and scale remains unsolved. New and emerging applications, such as data mining, web searching, retrieval of multimedia data, face recognition, and cursive handwriting recognition, require robust and efficient pattern recognition techniques. The objective of this review paper is to summarize and compare some of the well-known methods used in various stages of a pattern recognition system and identify research topics and applications which are at the forefront of this exciting and challenging field."
            },
            "slug": "Statistical-Pattern-Recognition:-A-Review-Jain-Duin",
            "title": {
                "fragments": [],
                "text": "Statistical Pattern Recognition: A Review"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The objective of this review paper is to summarize and compare some of the well-known methods used in various stages of a pattern recognition system and identify research topics and applications which are at the forefront of this exciting and challenging field."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696508"
                        ],
                        "name": "C. Jutten",
                        "slug": "C.-Jutten",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Jutten",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jutten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798563"
                        ],
                        "name": "J. H\u00e9rault",
                        "slug": "J.-H\u00e9rault",
                        "structuredName": {
                            "firstName": "Jeanny",
                            "lastName": "H\u00e9rault",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H\u00e9rault"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "The application of (18) to our problem is straightforward: we simply substitute an a priori chosen kernel function k(x;y) for all occurances of ( (x) (y))."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 162
                            }
                        ],
                        "text": "3 Computing Dot Products in Feature Space In order to compute dot products of the form ( (x) (y)), we use kernel representations of the form k(x;y) = ( (x) (y)); (18) which allow us to compute the value of the dot product in F without having to carry out the map ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 33162734,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e73081ed096c62c073b3faa1b3b80aab89998c5",
            "isKey": false,
            "numCitedBy": 2689,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Blind-separation-of-sources,-part-I:-An-adaptive-on-Jutten-H\u00e9rault",
            "title": {
                "fragments": [],
                "text": "Blind separation of sources, part I: An adaptive algorithm based on neuromimetic architecture"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 62
                            }
                        ],
                        "text": "Using the covariance matrix in F , C = 1 M M Xj=1 (xj) (xj)>; (5) 1More precisely, the covariance matrix is de ned as the expectation of xx>; for convenience, we shall use the same term to refer to the maximum likelihood estimate (1) of the covariance matrix from a nite sample."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 269
                            }
                        ],
                        "text": "In input space, locality consists of basing our com-ponent extraction for a point x on other points inan appropriately chosen neighbourhood of x. Thisadditional degree of freedom can greatly improvestatistical estimates which are computed from alimited amount of data (Bottou & Vapnik, 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7035291,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ca97e1668e305fb719845f84a05a62dfb946a5d",
            "isKey": false,
            "numCitedBy": 578,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Very rarely are training data evenly distributed in the input space. Local learning algorithms attempt to locally adjust the capacity of the training system to the properties of the training set in each area of the input space. The family of local learning algorithms contains known methods, like the k-nearest neighbors method (kNN) or the radial basis function networks (RBF), as well as new algorithms. A single analysis models some aspects of these algorithms. In particular, it suggests that neither kNN or RBF, nor nonlocal classifiers, achieve the best compromise between locality and capacity. A careful control of these parameters in a simple local learning algorithm has provided a performance breakthrough for an optical character recognition problem. Both the error rate and the rejection performance have been significantly improved."
            },
            "slug": "Local-Learning-Algorithms-Bottou-Vapnik",
            "title": {
                "fragments": [],
                "text": "Local Learning Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A single analysis suggests that neither kNN or RBF, nor nonlocal classifiers, achieve the best compromise between locality and capacity."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40574103"
                        ],
                        "name": "S. Klinke",
                        "slug": "S.-Klinke",
                        "structuredName": {
                            "firstName": "Sigbert",
                            "lastName": "Klinke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Klinke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2549124"
                        ],
                        "name": "J. Polzehl",
                        "slug": "J.-Polzehl",
                        "structuredName": {
                            "firstName": "J\u00f6rg",
                            "lastName": "Polzehl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Polzehl"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 87
                            }
                        ],
                        "text": "We can, however, go onestep further and construct tools for nonlinear ICAand nonlinear Projection Pursuit."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 145
                            }
                        ],
                        "text": "\u2026out some possibilities for future work.7.1 Projection Pursuit, IndependentComponents, and Higher OrderMomentsIn order to do Projection Pursuit (Friedman, 1987)and Independent component Analysis (ICA) (Jut-ten & Herault, 1991, Bell & Sejnowski, 1995), weare, loosely speaking, looking for\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 96
                            }
                        ],
                        "text": "Instead, we shall just give a few examples,and point out some possibilities for future work.7.1 Projection Pursuit, IndependentComponents, and Higher OrderMomentsIn order to do Projection Pursuit (Friedman, 1987)and Independent component Analysis (ICA) (Jut-ten & Herault, 1991, Bell & Sejnowski, 1995), weare, loosely speaking, looking for directions in thedataset such that the projections onto these aremaximally \\non{Gaussian\"."
                    },
                    "intents": []
                }
            ],
            "corpusId": 60628897,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43d7cce568c4d6ec9cc2d95fe54dd5fa8f51e936",
            "isKey": true,
            "numCitedBy": 338,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "\u201cProjection Pursuit\u201d (PP) stands for a class of exploratory projection techniques. This class contains methods designed for analyzing high dimensional data using low-dimensional projections. The main idea is to describe \u201cinteresting\u201d projections by maximizing an objective function or projection pursuit index."
            },
            "slug": "Exploratory-Projection-Pursuit-Klinke-Polzehl",
            "title": {
                "fragments": [],
                "text": "Exploratory Projection Pursuit"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "\u201cProjection Pursuit\u201d (PP) stands for a class of exploratory projection techniques that contains methods designed for analyzing high dimensional data using low-dimensional projections."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187218"
                        ],
                        "name": "A. J. Bell",
                        "slug": "A.-J.-Bell",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Bell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 235
                            }
                        ],
                        "text": "2 PCA in Feature Spaces Given a set of M centered observations xk, k = 1; : : : ;M , xk 2 RN ,PMk=1 xk = 0, PCA diagonalizes the covariance matrix1 C = 1 M M Xj=1 xjx>j : (1) To do this, one has to solve the Eigenvalue equation v = Cv (2) for Eigenvalues 0 and v 2 RNnf0g."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 198
                            }
                        ],
                        "text": "Compared to other techniques for nonlinear feature extraction, kernel PCA has the advantages that (1) it does not require nonlinear optimization, but just the solution of an Eigenvalue problem, and (2) by the possibility to use di erent kernels, it comprises a fairly general class of nonlinearities that can be used."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 141
                            }
                        ],
                        "text": "\u2026and Higher OrderMomentsIn order to do Projection Pursuit (Friedman, 1987)and Independent component Analysis (ICA) (Jut-ten & Herault, 1991, Bell & Sejnowski, 1995), weare, loosely speaking, looking for directions in thedataset such that the projections onto these aremaximally\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 86
                            }
                        ],
                        "text": "As Cv = 1 M PMj=1(xj v)xj, all solutions v must lie in the span of x1 : : :xM , hence (2) is equivalent to (xk v) = (xk Cv) for all k = 1; : : : ;M: (3) The remainder of this section is devoted to a straightforward translation to a nonlinear scenario, in order to prepare the ground for the method proposed in the present paper."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1701422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d7d0e8c4791700defd4b0df82a26b50055346e0",
            "isKey": true,
            "numCitedBy": 8754,
            "numCiting": 121,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive a new self-organizing learning algorithm that maximizes the information transferred in a network of nonlinear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximization has extra properties not found in the linear case (Linsker 1989). The nonlinearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalization of principal components analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to 10 speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal). Finally, we derive dependencies of information transfer on time delays. We suggest that information maximization provides a unifying framework for problems in \"blind\" signal processing."
            },
            "slug": "An-Information-Maximization-Approach-to-Blind-and-Bell-Sejnowski",
            "title": {
                "fragments": [],
                "text": "An Information-Maximization Approach to Blind Separation and Blind Deconvolution"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is suggested that information maximization provides a unifying framework for problems in \"blind\" signal processing and dependencies of information transfer on time delays are derived."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2941532"
                        ],
                        "name": "E. Bagarinao",
                        "slug": "E.-Bagarinao",
                        "structuredName": {
                            "firstName": "Epifanio",
                            "lastName": "Bagarinao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Bagarinao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702940"
                        ],
                        "name": "K. Pakdaman",
                        "slug": "K.-Pakdaman",
                        "structuredName": {
                            "firstName": "Khashayar",
                            "lastName": "Pakdaman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Pakdaman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685177"
                        ],
                        "name": "T. Nomura",
                        "slug": "T.-Nomura",
                        "structuredName": {
                            "firstName": "Taishin",
                            "lastName": "Nomura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Nomura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107826793"
                        ],
                        "name": "S. Sato",
                        "slug": "S.-Sato",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Sato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sato"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 23152985,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "cdf6aef34cc3f0e779d44811ff9b2cb5eae87398",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a formalism for the reconstruction of bifurcation diagrams from noisy time series. The method consists in finding a parametrized predictor function whose bifurcation structure is similar to that of the given system. The reconstruction algorithm is composed of two stages: model selection and bifurcation parameter identification. In the first stage, an appropriate model that best represents all the given time series is selected. A nonlinear autoregressive model with polynomial terms is employed in this study. The identification of the bifurcation parameters from among the many model parameters is done in the second stage. The algorithm works well even for a limited number of time series."
            },
            "slug": "Reconstructing-bifurcation-diagrams-from-noisy-time-Bagarinao-Pakdaman",
            "title": {
                "fragments": [],
                "text": "Reconstructing bifurcation diagrams from noisy time series using nonlinear autoregressive models."
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A nonlinear autoregressive model with polynomial terms is employed in this study, and it is shown that this model works well even for a limited number of time series."
            },
            "venue": {
                "fragments": [],
                "text": "Physical review. E, Statistical physics, plasmas, fluids, and related interdisciplinary topics"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "Combining (24) with the Support Vector decision function (Vapnik, 1995), we thus get machines of the type f(x) = sgn X\u0300i=1 iK2(~g(xi); ~g(x)) + b! (30) with~g(x)j = Vj ; (x) = M Xk=1 jkK1(xk;x): (31) Here, the expansion coe cients i are computed by a standard Support Vector Machine."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 49
                            }
                        ],
                        "text": "(17)), (kPC)n(x) = (Vn (x)) = M Xi=1 ni k(xi;x): (24) If we use a kernel as described in Sec."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 100
                            }
                        ],
                        "text": "7 Multi{Layer Support Vector machines By rst extracting nonlinear principal components according to (24), and then training a Support Vector machine (Vapnik, 1995), we can construct Support Vector type machines with additional layers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 94
                            }
                        ],
                        "text": "There, we have to evaluate the kernel function ` times for each extracted principal component (24), rather than just evaluating one dot product as for a linear PCA."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6636078,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7ec8029e5855b6efbac161488a2e68f83298091c",
            "isKey": false,
            "numCitedBy": 650,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We report a novel possibility for extracting a small subset of a data base which contains all the information necessary to solve a given classification task: using the Support Vector Algorithm to train three different types of handwritten digit classifiers, we observed that these types of classifiers construct their decision surface from strongly overlapping small (\u2248 4%) subsets of the data base. This finding opens up the possibility of compressing data bases significantly by disposing of the data which is not important for the solution of a given task. \n \nIn addition, we show that the theory allows us to predict the classifier that will have the best generalization ability, based solely on performance on the training set and characteristics of the learning machines. This finding is important for cases where the amount of available data is limited."
            },
            "slug": "Extracting-Support-Data-for-a-Given-Task-Sch\u00f6lkopf-Burges",
            "title": {
                "fragments": [],
                "text": "Extracting Support Data for a Given Task"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is observed that three different types of handwritten digit classifiers construct their decision surface from strongly overlapping small subsets of the data base, which opens up the possibility of compressing data bases significantly by disposing of theData which is not important for the solution of a given task."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37274089"
                        ],
                        "name": "D. Henderson",
                        "slug": "D.-Henderson",
                        "structuredName": {
                            "firstName": "Donnie",
                            "lastName": "Henderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Henderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2799635"
                        ],
                        "name": "R. Howard",
                        "slug": "R.-Howard",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Howard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Howard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34859193"
                        ],
                        "name": "W. Hubbard",
                        "slug": "W.-Hubbard",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "Hubbard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Hubbard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041866"
                        ],
                        "name": "L. Jackel",
                        "slug": "L.-Jackel",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Jackel",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jackel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 46
                            }
                        ],
                        "text": "(10)) Kij = (k(xi;xj))ij: (23) Next, we solve (12) by diagonalizing K, and normalize the Eigenvector expansion coe cients n by requiring Eq."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 88
                            }
                        ],
                        "text": "As K is symmetric, it has a set of Eigenvectors which spans the whole space, thus M = K (12) gives us all solutions of Eq."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 63
                            }
                        ],
                        "text": "(Vk Vk) = 1 for all k = p; : : : ;M: (15) By virtue of (8) and (12), this translates into a normalization condition for p; : : : ; M : 1 = M X i;j=1 ki kj ( (xi) (xj)) = M X i;j=1 ki kjKij = ( k K k) = k( k k) (16) For the purpose of principal component extraction, we need to compute projections on the Eigenvectors Vk in F (k = p; : : : ;M )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 41312633,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8e8f3c8d4418c8d62e306538c9c1292635e9d27",
            "isKey": false,
            "numCitedBy": 7828,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification."
            },
            "slug": "Backpropagation-Applied-to-Handwritten-Zip-Code-LeCun-Boser",
            "title": {
                "fragments": [],
                "text": "Backpropagation Applied to Handwritten Zip Code Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This paper demonstrates how constraints from the task domain can be integrated into a backpropagation network through the architecture of the network, successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2880906"
                        ],
                        "name": "V. Blanz",
                        "slug": "V.-Blanz",
                        "structuredName": {
                            "firstName": "Volker",
                            "lastName": "Blanz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Blanz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747836"
                        ],
                        "name": "H. B\u00fclthoff",
                        "slug": "H.-B\u00fclthoff",
                        "structuredName": {
                            "firstName": "Heinrich",
                            "lastName": "B\u00fclthoff",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. B\u00fclthoff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144517651"
                        ],
                        "name": "T. Vetter",
                        "slug": "T.-Vetter",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Vetter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Vetter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 130
                            }
                        ],
                        "text": "2).6.2 Object RecognitionIn this set of experiments, we used the MPIdatabase of images of realistically rendered 3{Dchair models (Blanz et al., 1996).6 It containsdownsampled (16 16) snapshots of 25 chairs,taken from the upper half of the viewing sphere;for an example image, see Fig."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 62
                            }
                        ],
                        "text": "9\nSupport Vector machines, which on this data setachieved 8% (Blanz et al. 1996).6.3 Character RecognitionTo validate the above results on a widely usedpattern recognition benchmark database, we re-peated the same experiments on the US postalservice (USPS) database of handwritten digitscollected\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 855426,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a0f97e889f6fb4b71704f079407a5ef730ad95f",
            "isKey": false,
            "numCitedBy": 248,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Two view-based object recognition algorithms are compared: (1) a heuristic algorithm based on oriented filters, and (2) a support vector learning machine trained on low-resolution images of the objects. Classification performance is assessed using a high number of images generated by a computer graphics system under precisely controlled conditions. Training- and test-images show a set of 25 realistic three-dimensional models of chairs from viewing directions spread over the upper half of the viewing sphere. The percentage of correct identification of all 25 objects is measured."
            },
            "slug": "Comparison-of-View-Based-Object-Recognition-Using-Blanz-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "Comparison of View-Based Object Recognition Algorithms Using Realistic 3D Models"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "Two view-based object recognition algorithms are compared: a heuristic algorithm based on oriented filters, and a support vector learning machine trained on low-resolution images of the objects."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6294728,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4609f6bdc3beab00c9beceaa12dd8101fefe6f1c",
            "isKey": false,
            "numCitedBy": 4841,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical learning theory was introduced in the late 1960's. Until the 1990's it was a purely theoretical analysis of the problem of function estimation from a given collection of data. In the middle of the 1990's new types of learning algorithms (called support vector machines) based on the developed theory were proposed. This made statistical learning theory not only a tool for the theoretical analysis but also a tool for creating practical algorithms for estimating multidimensional functions. This article presents a very general overview of statistical learning theory including both theoretical and algorithmic aspects of the theory. The goal of this overview is to demonstrate how the abstract learning theory established conditions for generalization which are more general than those discussed in classical statistical paradigms and how the understanding of these conditions inspired new algorithmic approaches to function estimation problems. A more detailed overview of the theory (without proofs) can be found in Vapnik (1995). In Vapnik (1998) one can find detailed description of the theory (including proofs)."
            },
            "slug": "An-overview-of-statistical-learning-theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "An overview of statistical learning theory"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "How the abstract learning theory established conditions for generalization which are more general than those discussed in classical statistical paradigms are demonstrated and how the understanding of these conditions inspired new algorithmic approaches to function estimation problems are demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144517651"
                        ],
                        "name": "T. Vetter",
                        "slug": "T.-Vetter",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Vetter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Vetter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 195
                            }
                        ],
                        "text": "Combining (24) with the Support Vector decision function (Vapnik, 1995), we thus get machines of the type f(x) = sgn X\u0300i=1 iK2(~g(xi); ~g(x)) + b! (30) with~g(x)j = Vj ; (x) = M Xk=1 jkK1(xk;x): (31) Here, the expansion coe cients i are computed by a standard Support Vector Machine."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 26288947,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "688da1cdc435eadaed83a564ae2c30de4f216e59",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The need to generate new views of a 3D object from a single real image arises in several fields, including graphics and object recognition. While the traditional approach relies on the use of 3D models, we exploit 2D image transformations that are specific to the relevant object class and learnable from example views of other \u201cprototypical\u201d objects of the same class."
            },
            "slug": "Image-Synthesis-from-a-Single-Example-Image-Vetter-Poggio",
            "title": {
                "fragments": [],
                "text": "Image Synthesis from a Single Example Image"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This work exploits 2D image transformations that are specific to the relevant object class and learnable from example views of other \u201cprototypical\u201d objects of the same class."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2522238"
                        ],
                        "name": "A. Ziehe",
                        "slug": "A.-Ziehe",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Ziehe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ziehe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145218329"
                        ],
                        "name": "G. Nolte",
                        "slug": "G.-Nolte",
                        "structuredName": {
                            "firstName": "Guido",
                            "lastName": "Nolte",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Nolte"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144100806"
                        ],
                        "name": "B. Mackert",
                        "slug": "B.-Mackert",
                        "structuredName": {
                            "firstName": "Bruno-Marcel",
                            "lastName": "Mackert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Mackert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3073253"
                        ],
                        "name": "G. Curio",
                        "slug": "G.-Curio",
                        "structuredName": {
                            "firstName": "Gabriel",
                            "lastName": "Curio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Curio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 286706,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "66c14d8a53dbebb7b56abc6b4669dcb1df01a78d",
            "isKey": false,
            "numCitedBy": 143,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Artifacts in magnetoneurography data due to endogenous biological noise sources, like the cardiac signal, can be four orders of magnitude higher than the signal of interest. Therefore, it is important to establish effective artifact reduction methods. We propose a blind source separation algorithm using only second-order temporal correlations for cleaning biomagnetic measurements of evoked responses in the peripheral nervous system. The algorithm showed its efficiency by eliminating disturbances originating from biological and technical noise sources and successfully extracting the signal of interest. This yields a significant improvement of the neuro-magnetic source analysis."
            },
            "slug": "Artifact-reduction-in-magnetoneurography-based-on-Ziehe-M\u00fcller",
            "title": {
                "fragments": [],
                "text": "Artifact reduction in magnetoneurography based on time-delayed second-order correlations"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A blind source separation algorithm using only second-order temporal correlations for cleaning biomagnetic measurements of evoked responses in the peripheral nervous system is proposed and shows its efficiency by eliminating disturbances originating from biological and technical noise sources and successfully extracting the signal of interest."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Biomed. Eng."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 184
                            }
                        ],
                        "text": "Clearly, the last point has yetto be evaluated in practise, however, for the Sup-port Vector machine, the utility of di erent kernelshas already been established (Sch olkopf, Burges,& Vapnik, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 138
                            }
                        ],
                        "text": "The general question which function k corre-sponds to a dot product in some space F hasbeen discussed by Boser, Guyon, & Vapnik (1992)and Vapnik (1995): Mercer's theorem of functionalanalysis states that if k is a continuous kernel ofa positive integral operator, we can construct amapping into a\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 170
                            }
                        ],
                        "text": "In addition, theyall construct their decision functions from an al-most identical subset of a small number of trainingpatterns, the Support Vectors (Sch olkopf, Burges,& Vapnik, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 143
                            }
                        ],
                        "text": "The number of components extracted thendetermines the size of of the rst hidden layer.Combining (24) with the Support Vector decisionfunction (Vapnik, 1995), we thus get machines ofthe typef(x) = sgn X\u0300i=1 iK2(~g(xi); ~g(x)) + b!"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 150
                            }
                        ],
                        "text": "\u2026convolutional 5{layerneural networks (5.0% were reported by LeCunet al., 1989) and nonlinear Support Vector classi- ers (4.0%, Sch olkopf, Burges, & Vapnik, 1995);it is far superior to linear classi ers operating di-rectly on the image data (a linear Support Vec-tor machine achieves 8.9%; Sch\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 207
                            }
                        ],
                        "text": "\u2026the approximate pre{images in RN of theEigenvectors in F .4.7 Multi{Layer Support VectormachinesBy rst extracting nonlinear principal componentsaccording to (24), and then training a SupportVector machine (Vapnik, 1995), we can constructSupport Vector type machines with additional lay-ers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 236
                            }
                        ],
                        "text": "This is due to the fact that for k(x;y) = (x y), the Support Vector decision function (Boser, Guyon, & Vapnik, 1992) f(x) = sgn(X\u0300i=1 ik(x;xi) + b) (28) can be expressed with a single weight vector w = P\u00ec=1 ixi asf(x) = sgn((x w) + b): (29) Thus the nal stage of classi cation can be done extremely fast; the speed of the principal component extraction phase, on the other hand, and thus the accuracy{speed tradeo of the whole classi er, can be controlled by the number of components which we extract, or by the above reduced set parameter m."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 123
                            }
                        ],
                        "text": "To assess the utility of the components, wetrained a linear Support Vector classi er (Vapnik& Chervonenkis, 1979; Cortes & Vapnik, 1995) onthe classi cation task.7 Table 1 summarizes our ndings: in all cases, nonlinear components as ex-tracted by polynomial kernels (cf. Eq."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 296,
                                "start": 284
                            }
                        ],
                        "text": "\u2026networks (5.0% were reported by LeCunet al., 1989) and nonlinear Support Vector classi- ers (4.0%, Sch olkopf, Burges, & Vapnik, 1995);it is far superior to linear classi ers operating di-rectly on the image data (a linear Support Vec-tor machine achieves 8.9%; Sch olkopf, Burges, &Vapnik, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 14
                            }
                        ],
                        "text": "For instance (Vapnik, 1995), ifx = (x1; x2), then C2(x) = (x21; x22; x1x2; x2x1),or, yielding the same value of the dot product,c2(x) = (x21; x22;p2x1x2): (20)For this example, it is easy to verify that (x1; x2)(y1; y2)> 2 = (x21; x22;p2x1x2)(y21 ; y22;p2y1y2)>= c2(x)c2(y)>: (21)In general, the\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 114
                            }
                        ],
                        "text": "To develop this technique, we made use of a kernelmethod which so far only had been used in super-vised learning (Vapnik, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 256
                            }
                        ],
                        "text": "\u2026of using this database.9If the linear classi er trained on the componentshad not been an SV machine, this might look di erent| SV machines are known to possess high general-ization ability for high{dimensional data, due to theirbuilt{in capacity control (Vapnik, 1995). stage of feature extraction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7138354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8213dbed4db44e113af3ed17d6dad57471a0c048",
            "isKey": true,
            "numCitedBy": 38755,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?."
            },
            "slug": "The-Nature-of-Statistical-Learning-Theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics for Engineering and Information Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16729445"
                        ],
                        "name": "R. Courant",
                        "slug": "R.-Courant",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Courant",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Courant"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 102
                            }
                        ],
                        "text": "B.2 Kernels Corresponding to DotProducts in Another SpaceMercer's theorem of functional analysis (e.g.Courant & Hilbert, 1953) gives the conditions un-der which we can construct the mapping fromthe Eigenfunction decomposition of k."
                    },
                    "intents": []
                }
            ],
            "corpusId": 201873134,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "f46c989d49028abc1187fdf4c8cd45f45c4d29db",
            "isKey": false,
            "numCitedBy": 317,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "2008; John Wiley & Sons, 2008; Methods of Mathematical Physics, Volume 1; 575 pages; 3527617221, 9783527617227; Richard Courant, D. Hilbert; Since the first volume of this work came out in Germany in 1924, this book, together with its second volume, has remained standard in the field. Courant and Hilbert's treatment restores the historically deep connections between physical intuition and mathematical development, providing the reader with a unified approach to mathematical physics. The present volume represents Richard Courant's second and final revision of 1953. file download wodi.pdf"
            },
            "slug": "Methods-of-mathematical-physics,-Volume-I-Courant",
            "title": {
                "fragments": [],
                "text": "Methods of mathematical physics, Volume I"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716902"
                        ],
                        "name": "Marti A. Hearst",
                        "slug": "Marti-A.-Hearst",
                        "structuredName": {
                            "firstName": "Marti",
                            "lastName": "Hearst",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marti A. Hearst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206447772,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "455d9a4ff96561d543acbcb2aa81d6cd8fcd20df",
            "isKey": false,
            "numCitedBy": 2523,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "My first exposure to Support Vector Machines came this spring when heard Sue Dumais present impressive results on text categorization using this analysis technique. This issue's collection of essays should help familiarize our readers with this interesting new racehorse in the Machine Learning stable. Bernhard Scholkopf, in an introductory overview, points out that a particular advantage of SVMs over other learning algorithms is that it can be analyzed theoretically using concepts from computational learning theory, and at the same time can achieve good performance when applied to real problems. Examples of these real-world applications are provided by Sue Dumais, who describes the aforementioned text-categorization problem, yielding the best results to date on the Reuters collection, and Edgar Osuna, who presents strong results on application to face detection. Our fourth author, John Platt, gives us a practical guide and a new technique for implementing the algorithm efficiently."
            },
            "slug": "Trends-&-Controversies:-Support-Vector-Machines-Hearst",
            "title": {
                "fragments": [],
                "text": "Trends & Controversies: Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This issue's collection of essays should help familiarize readers with this interesting new racehorse in the Machine Learning stable, and give a practical guide and a new technique for implementing the algorithm efficiently."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Intell. Syst."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2545803"
                        ],
                        "name": "M. Aizerman",
                        "slug": "M.-Aizerman",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Aizerman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Aizerman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60493317,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3caf34c1c86633b6e80dca29e3cb2b6367a0f93",
            "isKey": false,
            "numCitedBy": 1692,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Theoretical-Foundations-of-the-Potential-Function-Aizerman",
            "title": {
                "fragments": [],
                "text": "Theoretical Foundations of the Potential Function Method in Pattern Recognition Learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113457855"
                        ],
                        "name": "Harriet D. Hudson",
                        "slug": "Harriet-D.-Hudson",
                        "structuredName": {
                            "firstName": "Harriet",
                            "lastName": "Hudson",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Harriet D. Hudson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 231987499,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "d926d7fa49e12115ae2dced9e1889fea8c99c2de",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "From American Optical Company, Southbiidge, Massachusetts: Annual Report for 1951. From American Petroleum Institute, New York City: Quarterly, Winter, 1951-52, Spring, Summer, Autumn, 1952; American Petroleum Institute Directory, 1952; Proceedings, Thirty-first Annual Meeting: Section I, General Sessions; Section II, Marketing; Section V, Transportation; Proceedings, Seventeenth Midyear Meeting, Division of Refining, Refining; American Petroleum Institute\u2014What it is, What it does; Security for our Oil Age. From Davison Publishing Company, Ridge wood, New Jersey: Davison's Rayon and Silk Trades, 1952; Davison's Textile Blue Book, July, 1952. From Georgia Historical Society, Savannah: The Georgia Historical Quarterly, currently. From Historical Society of Western Pennsylvania, Pittsburgh: Western Pennsylvania Historical Magazine, currently. From National Archives, Washington, D. C : National Archives Accessions, No. 47, No. 49; Annual Report on the National Archives and Records Service, June SO, 1951; Preliminary Inventories, Nos. 41, 42, 43. From National Provisioner, Chicago, Illinois: The National Provisioner, weekly issues and convention issues: The \"Significant Sixty\"\u2014A Historical Report on the Progress and Development of the Meat Packing Industry, 1891-1951. From Oregon Historical Society, Portland: Oregon Historical Quarterly currently. From Philosophical Society of Texas, Dallas: Proceedings, for 1951. From Rhode Island Historical Society, Providence: Rhode Island History, currently. From Southern Pine Association, New Orleans, Louisiana: Weekly Trade Barometer and Supplement, currently. From State Historical Society of Missouri, Columbia: Missouri Historical Review, quarterly. From State Street Trust Company, Boston, Massachusetts: Yankee Ship Sailing Cards, Vol. 3."
            },
            "slug": "Acknowledgments-Hudson",
            "title": {
                "fragments": [],
                "text": "Acknowledgments"
            },
            "venue": {
                "fragments": [],
                "text": "Bulletin of the Business Historical Society"
            },
            "year": 1952
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101655384"
                        ],
                        "name": "K. Karhunen",
                        "slug": "K.-Karhunen",
                        "structuredName": {
                            "firstName": "Kari",
                            "lastName": "Karhunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Karhunen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 282
                            }
                        ],
                        "text": "\u2026= Z fx(u) fy(u) du: (65)We can then de ne kernels of the typek(x;y) := (fx fy)d: (66)(These kernels can also be used if our observa-tions are already given as functions, as is usu-ally the case for the variant of PCA which is re-ferred to as the Karhunen{Lo eve{Transformation;see Karhunen 1946)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "1) in the context of polynomial approximation, is (x y)d = (Cd(x); Cd(y)); (19) where Cd maps x to the vector Cd(x) whose entries are all possible n-th degree ordered products of the entries of x."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 118738283,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8ab83ad89b2dedd642246d68ded065fec63ffa79",
            "isKey": false,
            "numCitedBy": 469,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Zur-Spektraltheorie-stochastischer-prozesse-Karhunen",
            "title": {
                "fragments": [],
                "text": "Zur Spektraltheorie stochastischer prozesse"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1946
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52301815"
                        ],
                        "name": "N. S. Barnett",
                        "slug": "N.-S.-Barnett",
                        "structuredName": {
                            "firstName": "Norman",
                            "lastName": "Barnett",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. S. Barnett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1997442"
                        ],
                        "name": "S. Dragomir",
                        "slug": "S.-Dragomir",
                        "structuredName": {
                            "firstName": "Sever",
                            "lastName": "Dragomir",
                            "middleNames": [
                                "Silvestru"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dragomir"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 135
                            }
                        ],
                        "text": "For us, this has two useful consequences: rst, we can consider the equivalent equation ( (xk) V) = ( (xk) CV) for all k = 1; : : : ;M; (7) and second, there exist coe cients i (i = 1; : : : ;M ) such that V = M Xi=1 i (xi): (8) Combining (7) and (8), we get M Xi=1 i( (xk) (xi)) = 1 M M Xi=1 i( (xk) M Xj=1 (xj))( (xj) (xi)) for all k = 1; : : : ;M: (9) De ning an M M matrix K by Kij := ( (xi) (xj)); (10) this reads M K = K2 ; (11) where denotes the column vector with entries 1; : : : ; M ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 461,
                                "start": 452
                            }
                        ],
                        "text": "This includes the abovekernels as special cases. can split the domain of integration into r smallregions, thus gettingk(x;y) = rXi=1 ki(x;y); (71)and then introduce local d-th order correlations byusing k(x;y) =Xi ki(x;y)d: (72)Of course, the corresponding kernels can also bewritten in terms of the original image vectors,splitting them into subvectors before raising thesubvector{kernels to the d-th power.17B.7 Constructing Kernels from otherKernelsCombining Kernels."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 86
                            }
                        ],
                        "text": "The number of components extracted thendetermines the size of of the rst hidden layer.Combining (24) with the Support Vector decisionfunction (Vapnik, 1995), we thus get machines ofthe typef(x) = sgn X\u0300i=1 iK2(~g(xi); ~g(x)) + b!"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 104
                            }
                        ],
                        "text": "However, in that case, wecan speed up the extraction by a method analogu-ous to a technique proposed by Burges (1996) inthe context of Support Vector machines."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 220
                            }
                        ],
                        "text": "For us, thishas two useful consequences: rst, we can considerthe equivalent equation ( (xk) V) = ( (xk) CV) for all k = 1; : : : ;M;(7)and second, there exist coe cients i (i =1; : : : ;M ) such thatV = MXi=1 i (xi): (8)Combining (7) and (8), we get MXi=1 i( (xk) (xi)) =1M MXi=1 i( (xk) MXj=1 (xj))( (xj) (xi))for all k = 1; : : : ;M: (9)De ning an M M matrix K byKij := ( (xi) (xj)); (10)this reads M K = K2 ; (11)where denotes the column vector with entries 1; : : : ; M ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 173
                            }
                        ],
                        "text": "In the case k(x;y) = (x y)2 it is possi-ble to get an exact expansion with at most N vec-tors zj (N being the dimension of the input space)by solving an Eigenvalue problem (Burges, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 42
                            }
                        ],
                        "text": "Even though this gen-eral fact was known (Burges, 1996), the machinelearning community has made little use of it, theexception being Support Vector machines (Vap-nik, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 116073922,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "98e84e80e7126805de225b263813bfb2cf596a26",
            "isKey": true,
            "numCitedBy": 8203,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "vii"
            },
            "slug": "Private-communication-Barnett-Dragomir",
            "title": {
                "fragments": [],
                "text": "Private communication"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "From left to right, the polynomial degree in the kernel (22) increases from 1 to 4; from top to bottom, the rst 3 Eigenvectors are shown (in order of decreasing Eigenvalue size)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 110
                            }
                        ],
                        "text": "1 Polynomial Kernels and Higher Order Correlations Consider the mappings corresponding to kernels of the form (22): suppose the monomials xi1xi2 : : :xid are written such that i1 i2 : : : id."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "polynomial kernels of degree d (22), we are taking into account d{th order statistics."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": "In Appendix B, we give some examples of kernels other than (22) which may be used."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 333,
                                "start": 329
                            }
                        ],
                        "text": "For instance (Vapnik, 1995), if x = (x1; x2), then C2(x) = (x21; x22; x1x2; x2x1), or, yielding the same value of the dot product, c2(x) = (x21; x22;p2x1x2): (20) For this example, it is easy to verify that (x1; x2)(y1; y2)> 2 = (x21; x22;p2x1x2)(y2 1 ; y2 2;p2y1y2)> = c2(x)c2(y)>: (21) In general, the function k(x;y) = (x y)d (22) corresponds to a dot product in the space of d-th order monomials of the input coordinates."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 173
                            }
                        ],
                        "text": "4 Table 2: Test error rates on the USPS handwritten digit database for linear Support Vector machines trained on nonlinear principal components extracted by PCA with kernel (22), for degrees 1 through 7."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 160
                            }
                        ],
                        "text": "2 Table 1: Test error rates on the MPI chair database for linear Support Vector machines trained on nonlinear principal components extracted by PCA with kernel (22), for degrees 1 through 7."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "Thus, using kernels of the form (22) is our only way to take into account higher{ order statistics without a combinatorial explosion of time complexity."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On optimal nonlinear associa-  tive recall"
            },
            "venue": {
                "fragments": [],
                "text": "Biological Cybernetics, 19:201{  209,"
            },
            "year": 1975
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 145
                            }
                        ],
                        "text": "The latter lives in a possibly very high{dimensional space: even though we will identify terms like x1x2 and x2x1 into one coordinate of F as in (20), the dimensionality of F , the image of RN under cd, still is (N+p 1)! p!(N 1)! and thus grows like Np."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 150
                            }
                        ],
                        "text": "\u2026that should not map all observa-tions to zero, then such a p will always exist.3Note that in our derivation we could have usedthe known result (e.g. Kirby & Sirovich, 1990) thatPCA can be carried out on the dot product matrix(xi xj)ij instead of (1), however, for the sake of clarityand\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 158
                            }
                        ],
                        "text": "For instance (Vapnik, 1995), if x = (x1; x2), then C2(x) = (x21; x22; x1x2; x2x1), or, yielding the same value of the dot product, c2(x) = (x21; x22;p2x1x2): (20) For this example, it is easy to verify that (x1; x2)(y1; y2)> 2 = (x21; x22;p2x1x2)(y2 1 ; y2 2;p2y1y2)> = c2(x)c2(y)>: (21) In general, the function k(x;y) = (x y)d (22) corresponds to a dot product in the space of d-th order monomials of the input coordinates."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "(20)), arising from the fact that di erent combinations of indices occur with di erent frequencies, are largest for i1 < i2 < : : : < id (let us assume here that the input dimensionality is not smaller than the polynomial degree d): in that case, we have a coe cient of pd!."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Application of the  Karhunen{Lo eve procedure for the character-  ization of human faces"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on  Pattern Analysis and Machine Intelligence,  PAMI-12(1):103{108, January"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 173
                            }
                        ],
                        "text": "In the case k(x;y) = (x y)2 it is possi-ble to get an exact expansion with at most N vec-tors zj (N being the dimension of the input space)by solving an Eigenvalue problem (Burges, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 104
                            }
                        ],
                        "text": "However, in that case, wecan speed up the extraction by a method analogu-ous to a technique proposed by Burges (1996) inthe context of Support Vector machines."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 42
                            }
                        ],
                        "text": "Even though this gen-eral fact was known (Burges, 1996), the machinelearning community has made little use of it, theexception being Support Vector machines (Vap-nik, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 353,
                                "start": 350
                            }
                        ],
                        "text": "For us, this has two useful consequences: rst, we can consider the equivalent equation ( (xk) V) = ( (xk) CV) for all k = 1; : : : ;M; (7) and second, there exist coe cients i (i = 1; : : : ;M ) such that V = M Xi=1 i (xi): (8) Combining (7) and (8), we get M Xi=1 i( (xk) (xi)) = 1 M M Xi=1 i( (xk) M Xj=1 (xj))( (xj) (xi)) for all k = 1; : : : ;M: (9) De ning an M M matrix K by Kij := ( (xi) (xj)); (10) this reads M K = K2 ; (11) where denotes the column vector with entries 1; : : : ; M ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sch\u007f  olkopf. Improv-  ing the accuracy and speed of support vector  learning machines. In Advances in Neural In-  formation"
            },
            "venue": {
                "fragments": [],
                "text": "Processing Systems"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 130
                            }
                        ],
                        "text": "2).6.2 Object RecognitionIn this set of experiments, we used the MPIdatabase of images of realistically rendered 3{Dchair models (Blanz et al., 1996).6 It containsdownsampled (16 16) snapshots of 25 chairs,taken from the upper half of the viewing sphere;for an example image, see Fig."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 149
                            }
                        ],
                        "text": "As Cv = 1 M PMj=1(xj v)xj, all solutions v must lie in the span of x1 : : :xM , hence (2) is equivalent to (xk v) = (xk Cv) for all k = 1; : : : ;M: (3) The remainder of this section is devoted to a straightforward translation to a nonlinear scenario, in order to prepare the ground for the method proposed in the present paper."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 62
                            }
                        ],
                        "text": "9\nSupport Vector machines, which on this data setachieved 8% (Blanz et al. 1996).6.3 Character RecognitionTo validate the above results on a widely usedpattern recognition benchmark database, we re-peated the same experiments on the US postalservice (USPS) database of handwritten digitscollected\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "view{based object recognition al-  gorithms using realistic 3d models"
            },
            "venue": {
                "fragments": [],
                "text": "Arti-  cial Neural Networks | ICANN'96,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 115
                            }
                        ],
                        "text": "Rather than giving a full review of this eldhere, we brie y describe just three approaches,and refer the reader to Diamantaras & Kung(1996) for more details."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 270
                            }
                        ],
                        "text": "\u2026inrepresenting the observations by the rst qprincipal components is minimal the principal components are uncorrelated the representation entropy is minimized the rst q principal components have maxi-mal mutual information with respect to theinputsFor more details, see Diamantaras & Kung (1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 129
                            }
                        ],
                        "text": "Note that K is positive semide nite, which can be seen by noticing that it equals ( (x1); : : : ; (xM))> ( (x1); : : : ; (xM )); (13) which implies that for all X 2 F , (X KX) = k( (x1); : : : ; (xM))Xk2 0: (14) Consequently, K's Eigenvalues will be nonnegative, and will exactly give the solutions M of Eq."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 222
                            }
                        ],
                        "text": "If we train itto reproduce the input values as outputs (i.e. useit in an autoassociative mode), then the hiddenunit activations form a lower{dimensional repre-sentation of the data, closely related to PCA (seefor instance Diamantaras & Kung, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Princi-  pal Component Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Wiley, New  York,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 120
                            }
                        ],
                        "text": "If F is high{dimensional,we would like to be able to nd a closed form ex-pression for k which can be e ciently computed.Aizerman et al. (1964) consider the possibility ofchoosing k a priori, without being directly con-cerned with the corresponding mapping into F ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 233,
                                "start": 230
                            }
                        ],
                        "text": "Using the covariance matrix in F , C = 1 M M Xj=1 (xj) (xj)>; (5) 1More precisely, the covariance matrix is de ned as the expectation of xx>; for convenience, we shall use the same term to refer to the maximum likelihood estimate (1) of the covariance matrix from a nite sample."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 98
                            }
                        ],
                        "text": "Compared to other techniques for nonlinear feature extraction, kernel PCA has the advantages that (1) it does not require nonlinear optimization, but just the solution of an Eigenvalue problem, and (2) by the possibility to use di erent kernels, it comprises a fairly general class of nonlinearities that can be used."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 99
                            }
                        ],
                        "text": "Kirby & Sirovich, 1990) that PCA can be carried out on the dot product matrix (xi xj)ij instead of (1), however, for the sake of clarity and extendability (in Appendix A, we shall consider the case where the data must be centered in F ), we gave a detailed derivation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 171
                            }
                        ],
                        "text": "2 PCA in Feature Spaces Given a set of M centered observations xk, k = 1; : : : ;M , xk 2 RN ,PMk=1 xk = 0, PCA diagonalizes the covariance matrix1 C = 1 M M Xj=1 xjx>j : (1) To do this, one has to solve the Eigenvalue equation v = Cv (2) for Eigenvalues 0 and v 2 RNnf0g."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theoretical foundations of the po-  tential function method in pattern recogni-  tion learning"
            },
            "venue": {
                "fragments": [],
                "text": "Automation and Remote Con-  trol,"
            },
            "year": 1964
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 61
                            }
                        ],
                        "text": "This is done by minimizing the squared di erence = kV ~ Vk2: (26) The crucial point is that this also can be done without explicitly dealing with the possibly high{ dimensional space F ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 189
                            }
                        ],
                        "text": "(8)) by another vector~V = mXj=1 j (zj); (25)where m < ` is chosen a priori according to the de-sired speed{up, and zj 2 RN ; j = 1; ;m. Thisis done by minimizing the squared di erence = kV ~Vk2: (26)The crucial point is that this also can be donewithout explicitly dealing with the possibly high{dimensional space F ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 87
                            }
                        ],
                        "text": "4: we could parametrize our set of desired input variables and run the minimization of (26) only over those parameters."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "E cient  pattern recognition using a new transforma-  tion distance"
            },
            "venue": {
                "fragments": [],
                "text": "S. J. Hanson, J. D. Cowan,  and C. L. Giles, editors, Advances in Neural  Information Processing Systems 5, San Ma-  teo, CA,"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 176
                            }
                        ],
                        "text": "The solution to this problem, which will be described in the following section, builds on the fact that we exclusively need to compute dot products between mapped patterns (in (10) and (17)); we never need the mapped patterns explicitly."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 406,
                                "start": 402
                            }
                        ],
                        "text": "For us, this has two useful consequences: rst, we can consider the equivalent equation ( (xk) V) = ( (xk) CV) for all k = 1; : : : ;M; (7) and second, there exist coe cients i (i = 1; : : : ;M ) such that V = M Xi=1 i (xi): (8) Combining (7) and (8), we get M Xi=1 i( (xk) (xi)) = 1 M M Xi=1 i( (xk) M Xj=1 (xj))( (xj) (xi)) for all k = 1; : : : ;M: (9) De ning an M M matrix K by Kij := ( (xi) (xj)); (10) this reads M K = K2 ; (11) where denotes the column vector with entries 1; : : : ; M ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 132
                            }
                        ],
                        "text": "In summary, the following steps were necessary to compute the principal components: rst, compute the dot product matrix K de ned by (10);3 second, compute its Eigenvectors and normalize them in F ; third, compute projections of a test point onto the Eigenvectors by (17)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "(10)) Kij = (k(xi;xj))ij: (23) Next, we solve (12) by diagonalizing K, and normalize the Eigenvector expansion coe cients n by requiring Eq."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 114
                            }
                        ],
                        "text": "To assess the utility of the components, wetrained a linear Support Vector classi er (Vapnik& Chervonenkis, 1979; Cortes & Vapnik, 1995) onthe classi cation task.7 Table 1 summarizes our ndings: in all cases, nonlinear components as ex-tracted by polynomial kernels (cf. Eq."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Support vector net-  works"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning, 20:273 { 297,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 115
                            }
                        ],
                        "text": "Rather than giving a full review of this eldhere, we brie y describe just three approaches,and refer the reader to Diamantaras & Kung(1996) for more details."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 110
                            }
                        ],
                        "text": "Rather than giving a full review of this field here, we briefly describe five approaches and refer readers to Diamantaras and Kung (1996) for more details."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 270
                            }
                        ],
                        "text": "\u2026inrepresenting the observations by the rst qprincipal components is minimal the principal components are uncorrelated the representation entropy is minimized the rst q principal components have maxi-mal mutual information with respect to theinputsFor more details, see Diamantaras & Kung (1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 222
                            }
                        ],
                        "text": "If we train itto reproduce the input values as outputs (i.e. useit in an autoassociative mode), then the hiddenunit activations form a lower{dimensional repre-sentation of the data, closely related to PCA (seefor instance Diamantaras & Kung, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Principal component neural networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2150947925"
                        ],
                        "name": "W. N. Wapnik",
                        "slug": "W.-N.-Wapnik",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Wapnik",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. N. Wapnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2150946676"
                        ],
                        "name": "A. J. Tscherwonenkis",
                        "slug": "A.-J.-Tscherwonenkis",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Tscherwonenkis",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Tscherwonenkis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 246218030,
            "fieldsOfStudy": [],
            "id": "406bb8942eca3b2eafe141847b76ce56d68ef5e8",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Theorie-der-Zeichenerkennung-Wapnik-Tscherwonenkis",
            "title": {
                "fragments": [],
                "text": "Theorie der Zeichenerkennung"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16729445"
                        ],
                        "name": "R. Courant",
                        "slug": "R.-Courant",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Courant",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Courant"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6234493"
                        ],
                        "name": "D. Hilbert",
                        "slug": "D.-Hilbert",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hilbert",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hilbert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 102
                            }
                        ],
                        "text": "B.2 Kernels Corresponding to DotProducts in Another SpaceMercer's theorem of functional analysis (e.g.Courant & Hilbert, 1953) gives the conditions un-der which we can construct the mapping fromthe Eigenfunction decomposition of k."
                    },
                    "intents": []
                }
            ],
            "corpusId": 125547342,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "1f5ba2142aa78a34a4077dafb337caea0aefaa04",
            "isKey": false,
            "numCitedBy": 865,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Methods-of-Mathematical-Physics,-Vol.-I-Courant-Hilbert",
            "title": {
                "fragments": [],
                "text": "Methods of Mathematical Physics, Vol. I"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1954
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "98415342"
                        ],
                        "name": "G. Reuter",
                        "slug": "G.-Reuter",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Reuter",
                            "middleNames": [
                                "E.",
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Reuter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 125617136,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "be86151293961999ca211ee164d6c6359aaa4e26",
            "isKey": false,
            "numCitedBy": 287,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "LINEAR-OPERATORS-PART-II-(SPECTRAL-THEORY)-Reuter",
            "title": {
                "fragments": [],
                "text": "LINEAR OPERATORS PART II (SPECTRAL THEORY)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Mercer\u2019s theorem of functional analysis (e.g.,  Courant & Hilbert, 1953 ) gives conditions under which we can construct the mapping 8 from the eigenfunction decomposition of k .I fk is the continuous kernel of an integral operatorK : L2! L2,.K f/.y/D R k.x; y/ f.x/ dx, which is positive, that is,"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 222394623,
            "fieldsOfStudy": [],
            "id": "eabb2f4d4b9f767475cd5dc37d70c32196545a4c",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Methods of Mathematical Physics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1947
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "79783680"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2099637865"
                        ],
                        "name": "Mozer",
                        "slug": "Mozer",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Mozer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mozer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054400760"
                        ],
                        "name": "M. Jordan",
                        "slug": "M.-Jordan",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Jordan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2214848"
                        ],
                        "name": "T. Petsche",
                        "slug": "T.-Petsche",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Petsche",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Petsche"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60518954,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "271c040ea880abc2470f72690ed89bc3d8a11a2c",
            "isKey": false,
            "numCitedBy": 213,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Improving-the-accuracy-and-speed-of-support-vector-Burges-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "Improving the accuracy and speed of support vector learning machines"
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 1997"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682548"
                        ],
                        "name": "J. Buhmann",
                        "slug": "J.-Buhmann",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Buhmann",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Buhmann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 188
                            }
                        ],
                        "text": "(if F is in nite{dimensional, we think of (xj) (xj)> as the linear operator which maps X 2 F to (xj)( (xj) X)) we now have to nd Eigenvalues 0 and Eigenvectors V 2 Fnf0g satisfying V = CV (6) By the same argument as above, the solutions V lie in the span of (x1); : : : ; (xM )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 130
                            }
                        ],
                        "text": "As an example, weconsider k{means, but the same reasoning holdsfor other algorithms (for an overview of clusteringtechniques, see Buhmann, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14093690,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0907628b49ee027ae860cb145ceab3e196bfe21a",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Data-clustering-and-learning-Buhmann",
            "title": {
                "fragments": [],
                "text": "Data clustering and learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61480753,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "edb62d05f8eeaa7e1921c6c25c544935a2b6b131",
            "isKey": false,
            "numCitedBy": 414,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Theory-of-Pattern-Recognition-Amari",
            "title": {
                "fragments": [],
                "text": "A Theory of Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1968
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theory of Pattern Recognition in Russian]. Nauka, Moscow"
            },
            "venue": {
                "fragments": [],
                "text": "Theorie der Zeichenerkennung"
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 130
                            }
                        ],
                        "text": "As an example, weconsider k{means, but the same reasoning holdsfor other algorithms (for an overview of clusteringtechniques, see Buhmann, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Data clustering and learning The Handbook of Brain Theory and Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Data clustering and learning The Handbook of Brain Theory and Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 147
                            }
                        ],
                        "text": "Combining (24) with the Support Vector decision function (Vapnik, 1995), we thus get machines of the type f(x) = sgn X\u0300i=1 iK2(~g(xi); ~g(x)) + b! (30) with~g(x)j = Vj ; (x) = M Xk=1 jkK1(xk;x): (31) Here, the expansion coe cients i are computed by a standard Support Vector Machine."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The-  ory of Pattern Recognition [in Russian"
            },
            "venue": {
                "fragments": [],
                "text": " Nauka, Moscow,"
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 37
                            }
                        ],
                        "text": "Initiated by the pioneer-ing work of Oja (1982), a number of unsupervisedneural{network type algorithms computing prin-cipal components have been proposed."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 287,
                                "start": 283
                            }
                        ],
                        "text": "For instance (Vapnik, 1995), if x = (x1; x2), then C2(x) = (x21; x22; x1x2; x2x1), or, yielding the same value of the dot product, c2(x) = (x21; x22;p2x1x2): (20) For this example, it is easy to verify that (x1; x2)(y1; y2)> 2 = (x21; x22;p2x1x2)(y2 1 ; y2 2;p2y1y2)> = c2(x)c2(y)>: (21) In general, the function k(x;y) = (x y)d (22) corresponds to a dot product in the space of d-th order monomials of the input coordinates."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A simpli ed neuron model as a prin-  cipal component analyzer"
            },
            "venue": {
                "fragments": [],
                "text": "J. Math. Biology,  15:267 { 273,"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 154
                            }
                        ],
                        "text": "We shall now describe this computation in another dot product space F , which is related to the input space by a possibly nonlinear map : RN ! F; x 7! X: (4) Note that F , which we will refer to as the feature space, could have an arbitrarily large, possibly innite, dimensionality."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Vapnik. A  training algorithm for optimal margin classi-  ers"
            },
            "venue": {
                "fragments": [],
                "text": "In Fifth Annual Workshop on Compu-  tational Learning Theory, Pittsburgh,"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 102
                            }
                        ],
                        "text": "B.2 Kernels Corresponding to DotProducts in Another SpaceMercer's theorem of functional analysis (e.g.Courant & Hilbert, 1953) gives the conditions un-der which we can construct the mapping fromthe Eigenfunction decomposition of k."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 433,
                                "start": 429
                            }
                        ],
                        "text": "For us, this has two useful consequences: rst, we can consider the equivalent equation ( (xk) V) = ( (xk) CV) for all k = 1; : : : ;M; (7) and second, there exist coe cients i (i = 1; : : : ;M ) such that V = M Xi=1 i (xi): (8) Combining (7) and (8), we get M Xi=1 i( (xk) (xi)) = 1 M M Xi=1 i( (xk) M Xj=1 (xj))( (xj) (xi)) for all k = 1; : : : ;M: (9) De ning an M M matrix K by Kij := ( (xi) (xj)); (10) this reads M K = K2 ; (11) where denotes the column vector with entries 1; : : : ; M ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Methods of Math-  ematical Physics, volume 1"
            },
            "venue": {
                "fragments": [],
                "text": "Interscience Pub-  lishers, Inc, New York,"
            },
            "year": 1953
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Flexible discriminant analysis"
            },
            "venue": {
                "fragments": [],
                "text": "JASA"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Burges . Simpli ed support vector decision rules"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "(10)) Kij = (k(xi;xj))ij: (23) Next, we solve (12) by diagonalizing K, and normalize the Eigenvector expansion coe cients n by requiring Eq."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nagy"
            },
            "venue": {
                "fragments": [],
                "text": "Functional Analy-  sis. Frederick Ungar Publishing Co.,"
            },
            "year": 1955
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theory of Pattern Recognition in Russian"
            },
            "venue": {
                "fragments": [],
                "text": "Theory of Pattern Recognition in Russian"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "were supported by grants from the Studienstiftung des deutschen Volkes. B. S. thanks the GMD First for hospitality during two visits"
            },
            "venue": {
                "fragments": [],
                "text": "were supported by grants from the Studienstiftung des deutschen Volkes. B. S. thanks the GMD First for hospitality during two visits"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theorie der Zeichenerkennung, A k ademie{Verlag"
            },
            "venue": {
                "fragments": [],
                "text": "Theorie der Zeichenerkennung, A k ademie{Verlag"
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Received December"
            },
            "venue": {
                "fragments": [],
                "text": "Received December"
            },
            "year": 1996
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 26,
            "methodology": 25
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 63,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/Nonlinear-Component-Analysis-as-a-Kernel-Eigenvalue-Sch\u00f6lkopf-Smola/3f600e6c6cf93e78c9e6e690443d6d22c4bf18b9?sort=total-citations"
}