{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145016534"
                        ],
                        "name": "J. Moody",
                        "slug": "J.-Moody",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Moody",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Moody"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2319258"
                        ],
                        "name": "C. Darken",
                        "slug": "C.-Darken",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Darken",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Darken"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 138
                            }
                        ],
                        "text": "This correspondence is assumed in models that use competitive learning as a preprocessing stage witl.in an associative network (Moody and Darken 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 31251383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e7c4f513f24c3b82a1138b9f22ed87ed00cbe76",
            "isKey": false,
            "numCitedBy": 4527,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a network architecture which uses a single internal layer of locally-tuned processing units to learn both classification tasks and real-valued function approximations (Moody and Darken 1988). We consider training such networks in a completely supervised manner, but abandon this approach in favor of a more computationally efficient hybrid learning method which combines self-organized and supervised learning. Our networks learn faster than backpropagation for two reasons: the local representations ensure that only a few units respond to any given input, thus reducing computational overhead, and the hybrid learning rules are linear rather than nonlinear, thus leading to faster convergence. Unlike many existing methods for data analysis, our network architecture and learning rules are truly adaptive and are thus appropriate for real-time use."
            },
            "slug": "Fast-Learning-in-Networks-of-Locally-Tuned-Units-Moody-Darken",
            "title": {
                "fragments": [],
                "text": "Fast Learning in Networks of Locally-Tuned Processing Units"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This work proposes a network architecture which uses a single internal layer of locally-tuned processing units to learn both classification tasks and real-valued function approximations (Moody and Darken 1988)."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802785"
                        ],
                        "name": "S. Nowlan",
                        "slug": "S.-Nowlan",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Nowlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nowlan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 71
                            }
                        ],
                        "text": ", the likelihood) of generating the output vectors in the training set (Nowlan 1990a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 227
                            }
                        ],
                        "text": "\u201cSoft\u201d competitive learning modifies the weights (and also the variances and the mixing proportions) so as to increase the product of the probabilities (i.e., the likelihood) of generating the output vectors in the training set (Nowlan 1990a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17487287,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "57c4baa5528ba805fc27eee86613c99503978fed",
            "isKey": false,
            "numCitedBy": 175,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "One popular class of unsupervised algorithms are competitive algorithms. In the traditional view of competition, only one competitor, the winner, adapts for any given case. I propose to view competitive adaptation as attempting to fit a blend of simple probability generators (such as gaussians) to a set of data-points. The maximum likelihood fit of a model of this type suggests a \"softer\" form of competition, in which all competitors adapt in proportion to the relative probability that the input came from each competitor. I investigate one application of the soft competitive model, placement of radial basis function centers for function interpolation, and show that the soft model can give better performance with little additional computational cost."
            },
            "slug": "Maximum-Likelihood-Competitive-Learning-Nowlan",
            "title": {
                "fragments": [],
                "text": "Maximum Likelihood Competitive Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work proposes to view competitive adaptation as attempting to fit a blend of simple probability generators to a set of data-points, and investigates one application of the soft competitive model, placement of radial basis function centers for function interpolation, and shows that the soft model can give better performance with little additional computational cost."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 7035291,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ca97e1668e305fb719845f84a05a62dfb946a5d",
            "isKey": false,
            "numCitedBy": 578,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Very rarely are training data evenly distributed in the input space. Local learning algorithms attempt to locally adjust the capacity of the training system to the properties of the training set in each area of the input space. The family of local learning algorithms contains known methods, like the k-nearest neighbors method (kNN) or the radial basis function networks (RBF), as well as new algorithms. A single analysis models some aspects of these algorithms. In particular, it suggests that neither kNN or RBF, nor nonlocal classifiers, achieve the best compromise between locality and capacity. A careful control of these parameters in a simple local learning algorithm has provided a performance breakthrough for an optical character recognition problem. Both the error rate and the rejection performance have been significantly improved."
            },
            "slug": "Local-Learning-Algorithms-Bottou-Vapnik",
            "title": {
                "fragments": [],
                "text": "Local Learning Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A single analysis suggests that neither kNN or RBF, nor nonlocal classifiers, achieve the best compromise between locality and capacity."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50333420"
                        ],
                        "name": "S. Becker",
                        "slug": "S.-Becker",
                        "structuredName": {
                            "firstName": "Suzanna",
                            "lastName": "Becker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Becker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 41706532,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f065b631b2df7964d308c8a649fb05e9c6c91f67",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "We have previously described an unsupervised learning procedure that discovers spatially coherent properties of the world by maximizing the information that parameters extracted from different parts of the sensory input convey about some common underlying cause. When given random dot stereograms of curved surfaces, this procedure learns to extract surface depth because that is the property that is coherent across space. It also learns how to interpolate the depth at one location from the depths at nearby locations (Becker and Hinton 1992b). In this paper, we propose two new models that handle surfaces with discontinuities. The first model attempts to detect cases of discontinuities and reject them. The second model develops a mixture of expert interpolators. It learns to detect the locations of discontinuities and to invoke specialized, asymmetric interpolators that do not cross the discontinuities."
            },
            "slug": "Learning-Mixture-Models-of-Spatial-Coherence-Becker-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning Mixture Models of Spatial Coherence"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Two new models that handle surfaces with discontinuities are proposed that develop a mixture of expert interpolators and specialized, asymmetric interpolators that do not cross the discontinUities."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144215175"
                        ],
                        "name": "R. Jacobs",
                        "slug": "R.-Jacobs",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Jacobs",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jacobs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 9302687,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3972cbda528a8ae76386aa317f2ee686d10eb65",
            "isKey": false,
            "numCitedBy": 601,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel modular connectionist architecture is presented in which the networks composing the architecture compete to learn the training patterns. An outcome of the competition is that different networks learn different training patterns and, thus, learn to compute different functions. The architecture performs task decomposition in the sense that it learns to partition a task into two or more functionally independent tasks and allocates distinct networks to learn each task. In addition, the architecture tends to allocate to each task the network whose topology is most appropriate to that task. The architecture's performance on \u201cwhat\u201d and \u201cwhere\u201d vision tasks is presented and compared with the performance of two multilayer networks. Finally, it is noted that function decomposition is an underconstrained problem, and, thus, different modular architectures may decompose a function in different ways. A desirable decomposition can be achieved if the architecture is suitably restricted in the types of functions that it can compute. Finding appropriate restrictions is possible through the application of domain knowledge. A strength of the modular architecture is that its structure is well suited for incorporating domain knowledge."
            },
            "slug": "Task-Decomposition-Through-Competition-in-a-Modular-Jacobs-Jordan",
            "title": {
                "fragments": [],
                "text": "Task Decomposition Through Competition in a Modular Connectionist Architecture: The What and Where Vision Tasks"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "A novel modular connectionist architecture is presented in which the networks composing the architecture compete to learn the training patterns, and an outcome of the competition is that different networks learn different training patterns and, thus, learn to compute different functions."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2976268"
                        ],
                        "name": "D. Cohn",
                        "slug": "D.-Cohn",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Cohn",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cohn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6170752,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "08627b368e6b7d142cfcece74d52d21d48cc64a6",
            "isKey": false,
            "numCitedBy": 411,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-Network-Exploration-Using-Optimal-Experiment-Cohn",
            "title": {
                "fragments": [],
                "text": "Neural Network Exploration Using Optimal Experiment Design"
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35191677"
                        ],
                        "name": "J. Hampshire",
                        "slug": "J.-Hampshire",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hampshire",
                            "middleNames": [
                                "B."
                            ],
                            "suffix": "II"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hampshire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 39104087,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "af06d65387266a3fe35973cbf77593229049bb10",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present the Meta-Pi network, a multinetwork connectionist classifier that forms distributed low-level knowledge representations for robust pattern recognition, given random feature vectors generated by multiple statistically distinct sources. They illustrate how the Meta-Pi paradigm implements an adaptive Bayesian maximum a posteriori classifier. They also demonstrate its performance in the context of multispeaker phoneme recognition in which the Meta-Pi superstructure combines speaker-dependent time-delay neural network (TDNN) modules to perform multispeaker /b,d,g/ phoneme recognition with speaker-dependent error rates of 2%. Finally, the authors apply the Meta-Pi architecture to a limited source-independent recognition task, illustrating its discrimination of a novel source. They demonstrate that it can adapt to the novel source (speaker), given five adaptation examples of each of the three phonemes. >"
            },
            "slug": "The-Meta-Pi-Network:-Building-Distributed-Knowledge-Hampshire-Waibel",
            "title": {
                "fragments": [],
                "text": "The Meta-Pi Network: Building Distributed Knowledge Representations for Robust Multisource Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The authors present the Meta-Pi network, a multinetwork connectionist classifier that forms distributed low-level knowledge representations for robust pattern recognition, given random feature vectors generated by multiple statistically distinct sources."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1807117"
                        ],
                        "name": "T. Sanger",
                        "slug": "T.-Sanger",
                        "structuredName": {
                            "firstName": "Terence",
                            "lastName": "Sanger",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sanger"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6340189,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ae91b5245ce209220a2c7bb411f77cc41b80ef80",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "I describe a new algorithm for approximating continuous functions in high-dimensional input spaces. The algorithm builds a tree-structured network of variable size, which is determined both by the distribution of the input data and by the function to be approximated. Unlike other tree-structured algorithms, learning occurs through completely local mechanisms and the weights and structure are modified incrementally as data arrives. Efficient computation in the tree structure takes advantage of the potential for low-order dependencies between the output and the individual dimensions of the input. This algorithm is related to the ideas behind k-d trees (Bentley 1975), CART (Breiman et al. 1984), and MARS (Friedman 1988). I present an example that predicts future values of the Mackey-Glass differential delay equation."
            },
            "slug": "A-Tree-Structured-Algorithm-for-Reducing-in-with-Sanger",
            "title": {
                "fragments": [],
                "text": "A Tree-Structured Algorithm for Reducing Computation in Networks with Separable Basis Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A new algorithm for approximating continuous functions in high-dimensional input spaces and an example that predicts future values of the Mackey-Glass differential delay equation are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143642477"
                        ],
                        "name": "T. Keerthi",
                        "slug": "T.-Keerthi",
                        "structuredName": {
                            "firstName": "Thiya",
                            "lastName": "Keerthi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Keerthi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723632"
                        ],
                        "name": "Balaraman Ravindran",
                        "slug": "Balaraman-Ravindran",
                        "structuredName": {
                            "firstName": "Balaraman",
                            "lastName": "Ravindran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Balaraman Ravindran"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3639517,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0f612ead06443edce1d161e1d5539c1964de86f0",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 120,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper gives a compact, self{contained tutorial survey of reinforcement learning, a tool that is increasingly nding application in the development o f i n telligent dynamic systems. Research on reinforcement learning during the past decade has led to the development of a variety of useful algorithms. This paper surveys the literature and presents the algorithms in a cohesive framework."
            },
            "slug": "A-Tutorial-Survey-of-Reinforcement-Learn-Keerthi-Ravindran",
            "title": {
                "fragments": [],
                "text": "A Tutorial Survey of Reinforcement Learn"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A compact, self-contained tutorial survey of reinforcement learning, a tool that is increasingly nding application in the development of telligent dynamic systems, is given."
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144215175"
                        ],
                        "name": "R. Jacobs",
                        "slug": "R.-Jacobs",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Jacobs",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jacobs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 8880054,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0cff31a05ef41d9fdc355c34bdc7ed8d76118a00",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Methodologies for designing piecewise control laws, such as gain scheduling, are useful because they circumvent the problem of determining a fixed global model of the plant dynamics. Instead, the dynamics are approximated using local models that vary with the plant's operating point. We describe a multi-network, or modular, connectionist architecture that learns to perform control tasks using a piecewise control strategy. The architecture's networks compete to learn the training patterns. As a result, a plant's parameter space is partitioned into a number of regions, and a different network learns a control law in each region."
            },
            "slug": "A-Modular-Connectionist-Architecture-For-Learning-Jacobs-Jordan",
            "title": {
                "fragments": [],
                "text": "A Modular Connectionist Architecture For Learning Piecewise Control Strategies"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work describes a multi-network, or modular, connectionist architecture that learns to perform control tasks using a piecewise control strategy, where the architecture's networks compete to learn the training patterns."
            },
            "venue": {
                "fragments": [],
                "text": "1991 American Control Conference"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 12561523,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "84cdfa79e6eb9bf9e625e3af38d9f968df18a880",
            "isKey": false,
            "numCitedBy": 212,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Since the usual approaches to cooperative computation in networks of neuron-like computating elements do not assume that network components have any \"preferences\", they do not make substantive contact with game theoretic concepts, despite their use of some of the same terminology. In the approach presented here, however, each network component, or adaptive element, is a self-interested agent that prefers some inputs over others and \"works\" toward obtaining the most highly preferred inputs. Here we describe an adaptive element that is robust enough to learn to cooperate with other elements like itself in order to further its self-interests. It is argued that some of the longstanding problems concerning adaptation and learning by networks might be solvable by this form of cooperativity, and computer simulation experiments are described that show how networks of self-interested components that are sufficiently robust can solve rather difficult learning problems. We then place the approach in its proper historical and theoretical perspective through comparison with a number of related algorithms. A secondary aim of this article is to suggest that beyond what is explicitly illustrated here, there is a wealth of ideas from game theory and allied disciplines such as mathematical economics that can be of use in thinking about cooperative computation in both nervous systems and man-made systems."
            },
            "slug": "Learning-by-statistical-cooperation-of-neuron-like-Barto",
            "title": {
                "fragments": [],
                "text": "Learning by statistical cooperation of self-interested neuron-like computing elements."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is argued that some of the longstanding problems concerning adaptation and learning by networks might be solvable by this form of cooperativity, and computer simulation experiments are described that show how networks of self-interested components that are sufficiently robust can solve rather difficult learning problems."
            },
            "venue": {
                "fragments": [],
                "text": "Human neurobiology"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145652961"
                        ],
                        "name": "L. Xu",
                        "slug": "L.-Xu",
                        "structuredName": {
                            "firstName": "Lei",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Xu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 21916035,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f36c68be10572eae956a173f7fd14d4fc5faf88",
            "isKey": false,
            "numCitedBy": 339,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Least-mean-square-error-reconstruction-principle-Xu",
            "title": {
                "fragments": [],
                "text": "Least mean square error reconstruction principle for self-organizing neural-nets"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144215175"
                        ],
                        "name": "R. Jacobs",
                        "slug": "R.-Jacobs",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Jacobs",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jacobs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 7213718,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "c903c5b656ac3019cbba14a518e0ae74b92f3725",
            "isKey": false,
            "numCitedBy": 127,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "A fundamental observation in the neurosciences is that the brain is a modular system in which different regions perform different tasks. Recent evidence, however, raises questions about the accuracy of this characterization with respect to neo-nates. One possible interpretation of this evidence is that certain aspects of the modular organization of the adult brain arise developmentally. To explore this hypothesis we wish to characterize the computational principles that underlie the development of modular systems. In previous work we have considered computational schemes that allow a learning system to discover the modular structure that is present in the environment (Jacobs, Jordan, & Barto, 1991). In the current paper we present a complementary approach in which the development of modularity is due to an architectural bias in the learner. In particular, we examine the computational consequences of a simple architectural bias toward short-range connections. We present simulations that show that systems that learn under the influence of such a bias have a number of desirable properties, including a tendency to decompose tasks into subtasks, to decouple the dynamics of recurrent subsystems, and to develop location-sensitive internal representations. Furthermore, the system's units develop local receptive and projective fields, and the system develops characteristics that are typically associated with topographic maps."
            },
            "slug": "Computational-Consequences-of-a-Bias-toward-Short-Jacobs-Jordan",
            "title": {
                "fragments": [],
                "text": "Computational Consequences of a Bias toward Short Connections"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper presents simulations that show that systems that learn under the influence of an architectural bias have a number of desirable properties, including a tendency to decompose tasks into subtasks, to decouple the dynamics of recurrent subsystems, and to develop location-sensitive internal representations."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Cognitive Neuroscience"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690163"
                        ],
                        "name": "G. McLachlan",
                        "slug": "G.-McLachlan",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "McLachlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. McLachlan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2410937"
                        ],
                        "name": "K. Basford",
                        "slug": "K.-Basford",
                        "structuredName": {
                            "firstName": "Kaye",
                            "lastName": "Basford",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Basford"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 30
                            }
                        ],
                        "text": "In the statistics literature (McLachlan and Basford 1988), the 11~ are called \u201cmixing proportions.\u201d"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "picking hidden unit I, so the I+ are constrained to sum to 1. In the statistics literature ( McLachlan and Basford 1988 ), the 11~ are called \u201cmixing proportions.\u201d \u201cSoft\u201d competitive learning modifies the weights (and also the variances and the mixing proportions) so as to increase the product of the probabilities (i.e., the likelihood) of generating the output vectors in the training set (Nowlan 1990a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 119405289,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "034a70c9fbe8e0075f5a5b3a7b06bdf7d3cab4a1",
            "isKey": false,
            "numCitedBy": 2074,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "General Introduction Introduction History of Mixture Models Background to the General Classification Problem Mixture Likelihood Approach to Clustering Identifiability Likelihood Estimation for Mixture Models via EM Algorithm Start Values for EMm Algorithm Properties of Likelihood Estimators for Mixture Models Information Matrix for Mixture Models Tests for the Number of Components in a Mixture Partial Classification of the Data Classification Likelihood Approach to Clustering Mixture Models with Normal Components Likelihood Estimation for a Mixture of Normal Distribution Normal Homoscedastic Components Asymptotic Relative Efficiency of the Mixture Likelihood Approach Expected and Observed Information Matrices Assessment of Normality for Component Distributions: Partially Classified Data Assessment of Typicality: Partially Classified Data Assessment of Normality and Typicality: Unclassified Data Robust Estimation for Mixture Models Applications of Mixture Models to Two-Way Data Sets Introduction Clustering of Hemophilia Data Outliers in Darwin's Data Clustering of Rare Events Latent Classes of Teaching Styles Estimation of Mixing Proportions Introduction Likelihood Estimation Discriminant Analysis Estimator Asymptotic Relative Efficiency of Discriminant Analysis Estimator Moment Estimators Minimum Distance Estimators Case Study Homogeneity of Mixing Proportions Assessing the Performance of the Mixture Likelihood Approach to Clustering Introduction Estimators of the Allocation Rates Bias Correction of the Estimated Allocation Rates Estimated Allocation Rates of Hemophilia Data Estimated Allocation Rates for Simulated Data Other Methods of Bias Corrections Bias Correction for Estimated Posterior Probabilities Partitioning of Treatment Means in ANOVA Introduction Clustering of Treatment Means by the Mixture Likelihood Approach Fitting of a Normal Mixture Model to a RCBD with Random Block Effects Some Other Methods of Partitioning Treatment Means Example 1 Example 2 Example 3 Example 4 Mixture Likelihood Approach to the Clustering of Three-Way Data Introduction Fitting a Normal Mixture Model to Three-Way Data Clustering of Soybean Data Multidimensional Scaling Approach to the Analysis of Soybean Data References Appendix"
            },
            "slug": "Mixture-models-:-inference-and-applications-to-McLachlan-Basford",
            "title": {
                "fragments": [],
                "text": "Mixture models : inference and applications to clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The Mixture Likelihood Approach to Clustering and the Case Study Homogeneity of Mixing Proportions Assessing the Performance of the Mixture likelihood approach toClustering."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2546518"
                        ],
                        "name": "D. Plaut",
                        "slug": "D.-Plaut",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Plaut",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Plaut"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 2633285,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "8c815ce591604555eafcf82f040199c6a7cf902c",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Computational-approaches-to-cognition:-top-down-McClelland-Plaut",
            "title": {
                "fragments": [],
                "text": "Computational approaches to cognition: top-down approaches"
            },
            "venue": {
                "fragments": [],
                "text": "Current Opinion in Neurobiology"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144472878"
                        ],
                        "name": "G. E. Peterson",
                        "slug": "G.-E.-Peterson",
                        "structuredName": {
                            "firstName": "Gordon",
                            "lastName": "Peterson",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. E. Peterson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32757098"
                        ],
                        "name": "H. L. Barney",
                        "slug": "H.-L.-Barney",
                        "structuredName": {
                            "firstName": "Harold",
                            "lastName": "Barney",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. L. Barney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 186
                            }
                        ],
                        "text": "The data consisted of the first and second formants of the vowels Ii], [I], [al, and [A] (usually denoted [11]) from 75 speakers (males, females, and children) uttered in a hVd context (Peterson and Barney 1952)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 33784988,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "970d43f4f3bb68bd11cd3e16bba758287a3ee9e1",
            "isKey": false,
            "numCitedBy": 3012,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Relationships between a listener's identification of a spoken vowel and its properties as revealed from acoustic measurement of its sound wave have been a subject of study by many investigators. Both the utterance and the identification of a vowel depend upon the language and dialectal backgrounds and the vocal and auditory characteristics of the individuals concerned. The purpose of this paper is to discuss some of the control methods that have been used in the evaluation of these effects in a vowel study program at Bell Telephone Laboratories. The plan of the study, calibration of recording and measuring equipment, and methods for checking the performance of both speakers and listeners are described. The methods are illustrated from results of tests involving some 76 speakers and 70 listeners."
            },
            "slug": "Control-Methods-Used-in-a-Study-of-the-Vowels-Peterson-Barney",
            "title": {
                "fragments": [],
                "text": "Control Methods Used in a Study of the Vowels"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Control methods used in the evaluation of effects of language and dialectal backgrounds and vocal and auditory characteristics of the individuals concerned in a vowel study program at Bell Telephone Laboratories are discussed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1951
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403924665"
                        ],
                        "name": "A. Karmiloff-Smith",
                        "slug": "A.-Karmiloff-Smith",
                        "structuredName": {
                            "firstName": "Annette",
                            "lastName": "Karmiloff-Smith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Karmiloff-Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37700983"
                        ],
                        "name": "A. Clark",
                        "slug": "A.-Clark",
                        "structuredName": {
                            "firstName": "Andy",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Clark"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 144814433,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "2d2c85078ee46f9ca908a4603f5959d09692b0f4",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "En reponse a tous les commentaires qu'a suscites leur article \u00abThe Cognizer's Innards\u00bb, les AA. se proposent de delimiter et de preciser les themes majeurs du debat sur la redescription representationnelle, en se referant a Dennett"
            },
            "slug": "What's-Special-about-the-Development-of-the-Human-Karmiloff-Smith-Clark",
            "title": {
                "fragments": [],
                "text": "What's Special about the Development of the Human Mind/Brain?"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2721662"
                        ],
                        "name": "W. Bechtel",
                        "slug": "W.-Bechtel",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Bechtel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Bechtel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 145768099,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "27447729158324f2a7baee308a9ed5276b7949af",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "L'A. reconnait l'insuffisance du connexionnisme de premier ordre pour rendre compte des processus cognitifs et souleve l'ambiguite de la presentation de la redescription representationnelle de A. Clark et A. Karmiloff-Smith"
            },
            "slug": "The-Path-Beyond-First\u2010Order-Connectionism-Bechtel",
            "title": {
                "fragments": [],
                "text": "The Path Beyond First\u2010Order Connectionism"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 115843015,
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "International Journal of Pattern Recognition and Artificial Intelligence Signature Segmentation from Machine Printed Documents Using Contextual Information --manuscript Draft-- Full Title: Signature Segmentation from Machine Printed Documents Using Contextual Information Signature Segmentation from "
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10802653"
                        ],
                        "name": "Francis F. Chen",
                        "slug": "Francis-F.-Chen",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Chen",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Francis F. Chen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 118333192,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "829fd6191cce31d31971af223ccd81e6d1ecf820",
            "isKey": false,
            "numCitedBy": 584,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The theory of turbulent transport of parallel momentum and ion heat by the interaction of stochastic magnetic fields and turbulence is presented. Attention is focused on determining the kinetic stress and the compressive energy flux. A critical parameter is identified as the ratio of the turbulent scattering rate to the rate of parallel acoustic dispersion. For the parameter large, the kinetic stress takes the form of a viscous stress. For the parameter small, the quasilinear residual stress is recovered. In practice, the viscous stress is the relevant form, and the quasilinear limit is not observable. This is the principal prediction of this paper. A simple physical picture is developed and shown to recover the results of the detailed analysis."
            },
            "slug": "Plasma-Physics-and-Controlled-Fusion-Chen",
            "title": {
                "fragments": [],
                "text": "Plasma Physics and Controlled Fusion"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Marcus FreanConnectionist Architectures: Optimization"
            },
            "venue": {
                "fragments": [],
                "text": "Marcus FreanConnectionist Architectures: Optimization"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Feyo de AzevedoHybrid Modelling of a PHA Production Process Using Modular Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Feyo de AzevedoHybrid Modelling of a PHA Production Process Using Modular Neural Networks"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "MohanModular neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "MohanModular neural networks"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 0
                            }
                        ],
                        "text": "Hampshire and Waibel (1989) have described a system of this kind that can be used when the division into subtasks is known prior to training, and Jacobs et al. (1990) have described a related system that learns how to allocate cases to experts."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The mefa - pi network : Building distribufed knowledge representations for robust pattern recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "CottrellChapter 10 Prosopagnosia in modular neural network models 121"
            },
            "venue": {
                "fragments": [],
                "text": "CottrellChapter 10 Prosopagnosia in modular neural network models 121"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Biswanath BhattacharyaReferences . [CrossRef]"
            },
            "venue": {
                "fragments": [],
                "text": "Biswanath BhattacharyaReferences . [CrossRef]"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Received 27 July"
            },
            "venue": {
                "fragments": [],
                "text": "Received 27 July"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sergios Theodoridis, Konstantinos KoutroumbasNonlinear Classifiers 121-211"
            },
            "venue": {
                "fragments": [],
                "text": "Sergios Theodoridis, Konstantinos KoutroumbasNonlinear Classifiers 121-211"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Yair BartalDivide-and-Conquer Methods"
            },
            "venue": {
                "fragments": [],
                "text": "Yair BartalDivide-and-Conquer Methods"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Geoffrey HintonArtificial Intelligence: Neural Networks . [CrossRef]"
            },
            "venue": {
                "fragments": [],
                "text": "Geoffrey HintonArtificial Intelligence: Neural Networks . [CrossRef]"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning piecewise control strategies in a modular connectionist architecture, in preparation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Competing experts : An experimental investigation of associative mixture models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "International Journal of Neural Systems"
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Neural Systems"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Lewis BottKnowledge selection in category learning 39"
            },
            "venue": {
                "fragments": [],
                "text": "Lewis BottKnowledge selection in category learning 39"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mixture Models: Znference and Applications to Clustering. Marcel Dekker"
            },
            "venue": {
                "fragments": [],
                "text": "Mixture Models: Znference and Applications to Clustering. Marcel Dekker"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 29
                            }
                        ],
                        "text": "In the statistics literature (McLachlan and Basford 1988), the 11~ are called \u201cmixing proportions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 30
                            }
                        ],
                        "text": "In the statistics literature (McLachlan and Basford 1988), the 11~ are called \u201cmixing proportions.\u201d"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mixture Models: Inference rind"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "International Journal of Neural Systems"
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Neural Systems"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Robi PolikarPattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Robi PolikarPattern Recognition"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Task decomposition through"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "International Journal of Neural Systems"
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Neural Systems"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Feyo de AzevedoKnowledge based modular networks for process modelling and control"
            },
            "venue": {
                "fragments": [],
                "text": "Feyo de AzevedoKnowledge based modular networks for process modelling and control"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "OliveiraHybrid modular mechanistic/ANN modelling of a wastewater phosphorus removal process"
            },
            "venue": {
                "fragments": [],
                "text": "OliveiraHybrid modular mechanistic/ANN modelling of a wastewater phosphorus removal process"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Marco MagginiOptimal learning in artificial neural networks: A theoretical view 2, 1-51"
            },
            "venue": {
                "fragments": [],
                "text": "Marco MagginiOptimal learning in artificial neural networks: A theoretical view 2, 1-51"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 227
                            }
                        ],
                        "text": "\u201cSoft\u201d competitive learning modifies the weights (and also the variances and the mixing proportions) so as to increase the product of the probabilities (i.e., the likelihood) of generating the output vectors in the training set (Nowlan 1990a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 110
                            }
                        ],
                        "text": "The mixture of experts model was evaluated on a speaker independent, four-class, vowel discrimination problem (Nowlan 1990b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Competing experts: A n experimental investigation of associative mixture models"
            },
            "venue": {
                "fragments": [],
                "text": "Tech. Rep. CRG-TR-90-5, University of Toronto, Toronto, Canada."
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Chapter 8 Design issues \u2014 Neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Chapter 8 Design issues \u2014 Neural networks"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 0
                            }
                        ],
                        "text": "Hampshire and Waibel (1989) have described a system of this kind that can be used when the division into subtasks is known prior to training, and Jacobs et al. (1990) have described a related system that learns how to allocate cases to experts."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 59
                            }
                        ],
                        "text": "Communicated by Jeffrey Elman\nAdaptive Mixtures of Local- Experts\nRobert A. Jacobs Michael I. Jordan Department of Braill nmf Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, MA 02139 USA\nSteven J. Nowlan Geoffrey E. Hinton Deparfmeut of Computer Science, University of Toronto,\u2026"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Meta-Pi network: Building distributed knowledge representations for robust pattern recognition"
            },
            "venue": {
                "fragments": [],
                "text": "The Meta-Pi network: Building distributed knowledge representations for robust pattern recognition"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nabney . 2001 . Dynamic local models for segmentation and prediction of financial time series"
            },
            "venue": {
                "fragments": [],
                "text": "The European Journal of Finance"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mixture Models: Inference rind Applications to Clustering. Marcel Dekker"
            },
            "venue": {
                "fragments": [],
                "text": "Mixture Models: Inference rind Applications to Clustering. Marcel Dekker"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Colin CampbellConstructive learning techniques for designing neural network systems 3"
            },
            "venue": {
                "fragments": [],
                "text": "Colin CampbellConstructive learning techniques for designing neural network systems 3"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning piecewise control strategies in a modular connectionist architecture"
            },
            "venue": {
                "fragments": [],
                "text": "Learning piecewise control strategies in a modular connectionist architecture"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 227
                            }
                        ],
                        "text": "\u201cSoft\u201d competitive learning modifies the weights (and also the variances and the mixing proportions) so as to increase the product of the probabilities (i.e., the likelihood) of generating the output vectors in the training set (Nowlan 1990a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Competing experts: An experimental investigation of associative"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning piecewise control strategies in a modular connectionist architecture , in preparation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning piecewise control strategies"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "International Journal of Computational Intelligence and Applications"
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computational Intelligence and Applications"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "BishopModeling Wind Direction from Satellite Scatterometer Data 295-301"
            },
            "venue": {
                "fragments": [],
                "text": "BishopModeling Wind Direction from Satellite Scatterometer Data 295-301"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 8,
            "methodology": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 55,
        "totalPages": 6
    },
    "page_url": "https://www.semanticscholar.org/paper/Adaptive-Mixtures-of-Local-Experts-Jacobs-Jordan/c8d90974c3f3b40fa05e322df2905fc16204aa56?sort=total-citations"
}