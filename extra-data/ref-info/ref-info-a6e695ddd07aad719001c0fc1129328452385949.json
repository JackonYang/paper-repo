{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753110"
                        ],
                        "name": "M. Huiskes",
                        "slug": "M.-Huiskes",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Huiskes",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Huiskes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2463875"
                        ],
                        "name": "B. Thomee",
                        "slug": "B.-Thomee",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Thomee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Thomee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731570"
                        ],
                        "name": "M. Lew",
                        "slug": "M.-Lew",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lew",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lew"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 123
                            }
                        ],
                        "text": "The initial 2008 release [5] included 25,000 images with ground truth annotations for 29 concepts, while the 2010 release [6] contained one million images without these annotations, but including visual features."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14761697,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3cfdd7b0dacef10a1578c0e6ed55aa3f3129a437",
            "isKey": false,
            "numCitedBy": 352,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "The MIR Flickr collection consists of 25000 high-quality photographic images of thousands of Flickr users, made available under the Creative Commons license. The database includes all the original user tags and EXIF metadata. Additionally, detailed and accurate annotations are provided for topics corresponding to the most prominent visual concepts in the user tag data. The rich metadata allow for a wide variety of image retrieval benchmarking scenarios.\n In this paper, we provide an overview of the various strategies that were devised for automatic visual concept detection using the MIR Flickr collection. In particular we discuss results from various experiments in combining social data and low-level content-based descriptors to improve the accuracy of visual concept classifiers. Additionally, we present retrieval results obtained by relevance feedback methods, demonstrating (i) how their performance can be enhanced using features based on visual concept classifiers, and (ii) how their performance, based on small samples, can be measured relative to their large sample classifier counterparts.\n Additionally, we identify a number of promising trends and ideas in visual concept detection. To keep the MIR Flickr collection up-to-date on these developments, we have formulated two new initiatives to extend the original image collection. First, the collection will be extended to one million Creative Commons Flickr images. Second, a number of state-of-the-art content-based descriptors will be made available for the entire collection."
            },
            "slug": "New-trends-and-ideas-in-visual-concept-detection:-Huiskes-Thomee",
            "title": {
                "fragments": [],
                "text": "New trends and ideas in visual concept detection: the MIR flickr retrieval evaluation initiative"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper provides an overview of the various strategies that were devised for automatic visual concept detection using the MIR Flickr collection, and discusses results from various experiments in combining social data and low-level content-based descriptors to improve the accuracy of visual concept classifiers."
            },
            "venue": {
                "fragments": [],
                "text": "MIR '10"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144078686"
                        ],
                        "name": "Tat-Seng Chua",
                        "slug": "Tat-Seng-Chua",
                        "structuredName": {
                            "firstName": "Tat-Seng",
                            "lastName": "Chua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tat-Seng Chua"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8053308"
                        ],
                        "name": "Jinhui Tang",
                        "slug": "Jinhui-Tang",
                        "structuredName": {
                            "firstName": "Jinhui",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jinhui Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48043335"
                        ],
                        "name": "Richang Hong",
                        "slug": "Richang-Hong",
                        "structuredName": {
                            "firstName": "Richang",
                            "lastName": "Hong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richang Hong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145537762"
                        ],
                        "name": "Haojie Li",
                        "slug": "Haojie-Li",
                        "structuredName": {
                            "firstName": "Haojie",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haojie Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114940115"
                        ],
                        "name": "Zhiping Luo",
                        "slug": "Zhiping-Luo",
                        "structuredName": {
                            "firstName": "Zhiping",
                            "lastName": "Luo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhiping Luo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2515436"
                        ],
                        "name": "Yantao Zheng",
                        "slug": "Yantao-Zheng",
                        "structuredName": {
                            "firstName": "Yantao",
                            "lastName": "Zheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yantao Zheng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 15
                            }
                        ],
                        "text": "M M\n] 5\nM ar\n2 01\n5\n2 NUS-WIDE [7] released 269,648 images along with semiautomatically obtained ground truth annotations for 81 concepts, such as events, scenes and people, organized in a multi-level hierarchy."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6483070,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b80a580a6f2eca77524302acd944fd6edf0a0611",
            "isKey": false,
            "numCitedBy": 2109,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a web image dataset created by NUS's Lab for Media Search. The dataset includes: (1) 269,648 images and the associated tags from Flickr, with a total of 5,018 unique tags; (2) six types of low-level features extracted from these images, including 64-D color histogram, 144-D color correlogram, 73-D edge direction histogram, 128-D wavelet texture, 225-D block-wise color moments extracted over 5x5 fixed grid partitions, and 500-D bag of words based on SIFT descriptions; and (3) ground-truth for 81 concepts that can be used for evaluation. Based on this dataset, we highlight characteristics of Web image collections and identify four research issues on web image annotation and retrieval. We also provide the baseline results for web image annotation by learning from the tags using the traditional k-NN algorithm. The benchmark results indicate that it is possible to learn effective models from sufficiently large image dataset to facilitate general image retrieval."
            },
            "slug": "NUS-WIDE:-a-real-world-web-image-database-from-of-Chua-Tang",
            "title": {
                "fragments": [],
                "text": "NUS-WIDE: a real-world web image database from National University of Singapore"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The benchmark results indicate that it is possible to learn effective models from sufficiently large image dataset to facilitate general image retrieval and four research issues on web image annotation and retrieval are identified."
            },
            "venue": {
                "fragments": [],
                "text": "CIVR '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50976489"
                        ],
                        "name": "Jaeyoung Choi",
                        "slug": "Jaeyoung-Choi",
                        "structuredName": {
                            "firstName": "Jaeyoung",
                            "lastName": "Choi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jaeyoung Choi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2463875"
                        ],
                        "name": "B. Thomee",
                        "slug": "B.-Thomee",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Thomee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Thomee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797144"
                        ],
                        "name": "G. Friedland",
                        "slug": "G.-Friedland",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Friedland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Friedland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48749954"
                        ],
                        "name": "Liangliang Cao",
                        "slug": "Liangliang-Cao",
                        "structuredName": {
                            "firstName": "Liangliang",
                            "lastName": "Cao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liangliang Cao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36845351"
                        ],
                        "name": "Karl S. Ni",
                        "slug": "Karl-S.-Ni",
                        "structuredName": {
                            "firstName": "Karl",
                            "lastName": "Ni",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karl S. Ni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1772549"
                        ],
                        "name": "Damian Borth",
                        "slug": "Damian-Borth",
                        "structuredName": {
                            "firstName": "Damian",
                            "lastName": "Borth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Damian Borth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2532460"
                        ],
                        "name": "Benjamin Elizalde",
                        "slug": "Benjamin-Elizalde",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Elizalde",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Elizalde"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1773341"
                        ],
                        "name": "L. Gottlieb",
                        "slug": "L.-Gottlieb",
                        "structuredName": {
                            "firstName": "Luke",
                            "lastName": "Gottlieb",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gottlieb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152762350"
                        ],
                        "name": "C. Carrano",
                        "slug": "C.-Carrano",
                        "structuredName": {
                            "firstName": "Carmen",
                            "lastName": "Carrano",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Carrano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157171"
                        ],
                        "name": "R. Pearce",
                        "slug": "R.-Pearce",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Pearce",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Pearce"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40429372"
                        ],
                        "name": "Douglas N. Poland",
                        "slug": "Douglas-N.-Poland",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Poland",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Douglas N. Poland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17295690,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "620c1a82d9e313c6f804d45b027fb68b582afe08",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The Placing Task is a yearly challenge offered by the MediaEval Multimedia Benchmarking Initiative that requires participants to develop algorithms that automatically predict the geo-location of social media videos and images. We introduce a recent development of a new standardized web-scale geo-tagged dataset for Placing Task 2014, which contains 5.5 million photos and 35,000 videos. This standardized benchmark with a large persistent dataset allows research community to easily evaluate new algorithms and to analyze their performance with respect to the state-of-the-art approaches. We discuss the characteristics of this year's Placing Task along with the description of the new dataset components and how they were collected."
            },
            "slug": "The-Placing-Task:-A-Large-Scale-Geo-Estimation-for-Choi-Thomee",
            "title": {
                "fragments": [],
                "text": "The Placing Task: A Large-Scale Geo-Estimation Challenge for Social-Media Videos and Images"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A recent development of a new standardized web-scale geo-tagged dataset for Placing Task 2014, which contains 5.5 million photos and 35,000 videos, allows research community to easily evaluate new algorithms and to analyze their performance with respect to the state-of-the-art approaches."
            },
            "venue": {
                "fragments": [],
                "text": "GeoMM '14"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2923708"
                        ],
                        "name": "Julia Bernd",
                        "slug": "Julia-Bernd",
                        "structuredName": {
                            "firstName": "Julia",
                            "lastName": "Bernd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Julia Bernd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1772549"
                        ],
                        "name": "Damian Borth",
                        "slug": "Damian-Borth",
                        "structuredName": {
                            "firstName": "Damian",
                            "lastName": "Borth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Damian Borth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2532460"
                        ],
                        "name": "Benjamin Elizalde",
                        "slug": "Benjamin-Elizalde",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Elizalde",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Elizalde"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797144"
                        ],
                        "name": "G. Friedland",
                        "slug": "G.-Friedland",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Friedland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Friedland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2076299806"
                        ],
                        "name": "Heather Gallagher",
                        "slug": "Heather-Gallagher",
                        "structuredName": {
                            "firstName": "Heather",
                            "lastName": "Gallagher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Heather Gallagher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1773341"
                        ],
                        "name": "L. Gottlieb",
                        "slug": "L.-Gottlieb",
                        "structuredName": {
                            "firstName": "Luke",
                            "lastName": "Gottlieb",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gottlieb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48788686"
                        ],
                        "name": "Adam L. Janin",
                        "slug": "Adam-L.-Janin",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Janin",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam L. Janin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2538663"
                        ],
                        "name": "Sara Karabashlieva",
                        "slug": "Sara-Karabashlieva",
                        "structuredName": {
                            "firstName": "Sara",
                            "lastName": "Karabashlieva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sara Karabashlieva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32718585"
                        ],
                        "name": "J. Takahashi",
                        "slug": "J.-Takahashi",
                        "structuredName": {
                            "firstName": "Jocelyn",
                            "lastName": "Takahashi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Takahashi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054742836"
                        ],
                        "name": "Jennifer Won",
                        "slug": "Jennifer-Won",
                        "structuredName": {
                            "firstName": "Jennifer",
                            "lastName": "Won",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jennifer Won"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "nts develop algorithms for estimating the geographic location where a photo or video was taken, is currently based on our dataset. To support research in multimedia event detection the YLI-MED corpus [1] was recently introduced, which consists of 50,000 handpicked videos from the YFCC100M that belong to events similar to those dened in the TRECVID MED challenge. Approximately 2,000 videos were categ"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16087155,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6e35585eb37ee8a1de60a10a56a3183af480e214",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "The YLI Multimedia Event Detection corpus is a public-domain index of videos with annotations and computed features, specialized for research in multimedia event detection (MED), i.e., automatically identifying what's happening in a video by analyzing the audio and visual content. The videos indexed in the YLI-MED corpus are a subset of the larger YLI feature corpus, which is being developed by the International Computer Science Institute and Lawrence Livermore National Laboratory based on the Yahoo Flickr Creative Commons 100 Million (YFCC100M) dataset. The videos in YLI-MED are categorized as depicting one of ten target events, or no target event, and are annotated for additional attributes like language spoken and whether the video has a musical score. The annotations also include degree of annotator agreement and average annotator confidence scores for the event categorization of each video. Version 1.0 of YLI-MED includes 1823 \"positive\" videos that depict the target events and 48,138 \"negative\" videos, as well as 177 supplementary videos that are similar to event videos but are not positive examples. Our goal in producing YLI-MED is to be as open about our data and procedures as possible. This report describes the procedures used to collect the corpus; gives detailed descriptive statistics about the corpus makeup (and how video attributes affected annotators' judgments); discusses possible biases in the corpus introduced by our procedural choices and compares it with the most similar existing dataset, TRECVID MED's HAVIC corpus; and gives an overview of our future plans for expanding the annotation effort."
            },
            "slug": "The-YLI-MED-Corpus:-Characteristics,-Procedures,-Bernd-Borth",
            "title": {
                "fragments": [],
                "text": "The YLI-MED Corpus: Characteristics, Procedures, and Plans"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The procedures used to collect the corpus are described; detailed descriptive statistics about the corpus makeup are given (and how video attributes affected annotators' judgments); possible biases in the corpus introduced by the authors' procedural choices are discussed; it is compared with the most similar existing dataset, TRECVID MED's HAVIC corpus; and gives an overview of the future plans for expanding the annotation effort."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33493200"
                        ],
                        "name": "Tsung-Yi Lin",
                        "slug": "Tsung-Yi-Lin",
                        "structuredName": {
                            "firstName": "Tsung-Yi",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tsung-Yi Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145854440"
                        ],
                        "name": "M. Maire",
                        "slug": "M.-Maire",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Maire",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Maire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48966748"
                        ],
                        "name": "James Hays",
                        "slug": "James-Hays",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Hays",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Hays"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699161"
                        ],
                        "name": "C. L. Zitnick",
                        "slug": "C.-L.-Zitnick",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Zitnick",
                            "middleNames": [
                                "Lawrence"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Zitnick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "Microsoft COCO [10] dataset contains 91 object categories, where 82 of them are associated with more than 5,000 labeled instances."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14113767,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71b7178df5d2b112d07e45038cb5637208659ff7",
            "isKey": false,
            "numCitedBy": 19789,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model."
            },
            "slug": "Microsoft-COCO:-Common-Objects-in-Context-Lin-Maire",
            "title": {
                "fragments": [],
                "text": "Microsoft COCO: Common Objects in Context"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding by gathering images of complex everyday scenes containing common objects in their natural context."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153302678"
                        ],
                        "name": "Jia Deng",
                        "slug": "Jia-Deng",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144847596"
                        ],
                        "name": "Wei Dong",
                        "slug": "Wei-Dong",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Dong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Dong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2040091191"
                        ],
                        "name": "Li-Jia Li",
                        "slug": "Li-Jia-Li",
                        "structuredName": {
                            "firstName": "Li-Jia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li-Jia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "94451829"
                        ],
                        "name": "K. Li",
                        "slug": "K.-Li",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 65
                            }
                        ],
                        "text": "The dataset featured in a yearly challenge held between 2006 and 2012."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 57246310,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b47265245e8db53a553049dcb27ed3e495fd625",
            "isKey": false,
            "numCitedBy": 27420,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called \u201cImageNet\u201d, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond."
            },
            "slug": "ImageNet:-A-large-scale-hierarchical-image-database-Deng-Dong",
            "title": {
                "fragments": [],
                "text": "ImageNet: A large-scale hierarchical image database"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new database called \u201cImageNet\u201d is introduced, a large-scale ontology of images built upon the backbone of the WordNet structure, much larger in scale and diversity and much more accurate than the current image datasets."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40599257"
                        ],
                        "name": "Jianxiong Xiao",
                        "slug": "Jianxiong-Xiao",
                        "structuredName": {
                            "firstName": "Jianxiong",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianxiong Xiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1865091"
                        ],
                        "name": "Krista A. Ehinger",
                        "slug": "Krista-A.-Ehinger",
                        "structuredName": {
                            "firstName": "Krista",
                            "lastName": "Ehinger",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Krista A. Ehinger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48966748"
                        ],
                        "name": "James Hays",
                        "slug": "James-Hays",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Hays",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Hays"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143868587"
                        ],
                        "name": "A. Oliva",
                        "slug": "A.-Oliva",
                        "structuredName": {
                            "firstName": "Aude",
                            "lastName": "Oliva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Oliva"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 85
                            }
                        ],
                        "text": "In contrast to the ImageNet dataset, COCO has fewer categories but more instances per category."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 10224573,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c9a6c7bfe831f2b154deac4409c35633c63ef326",
            "isKey": false,
            "numCitedBy": 262,
            "numCiting": 71,
            "paperAbstract": {
                "fragments": [],
                "text": "Progress in scene understanding requires reasoning about the rich and diverse visual environments that make up our daily experience. To this end, we propose the Scene Understanding database, a nearly exhaustive collection of scenes categorized at the same level of specificity as human discourse. The database contains 908 distinct scene categories and 131,072 images. Given this data with both scene and object labels available, we perform in-depth analysis of co-occurrence statistics and the contextual relationship. To better understand this large scale taxonomy of scene categories, we perform two human experiments: we quantify human scene recognition accuracy, and we measure how typical each image is of its assigned scene category. Next, we perform computational experiments: scene recognition with global image features, indoor versus outdoor classification, and \u201cscene detection,\u201d in which we relax the assumption that one image depicts only one scene category. Finally, we relate human experiments to machine performance and explore the relationship between human and machine recognition errors and the relationship between image \u201ctypicality\u201d and machine recognition accuracy."
            },
            "slug": "SUN-Database:-Exploring-a-Large-Collection-of-Scene-Xiao-Ehinger",
            "title": {
                "fragments": [],
                "text": "SUN Database: Exploring a Large Collection of Scene Categories"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The Scene Understanding database is proposed, a nearly exhaustive collection of scenes categorized at the same level of specificity as human discourse that contains 908 distinct scene categories and 131,072 images."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2821130"
                        ],
                        "name": "David J. Crandall",
                        "slug": "David-J.-Crandall",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Crandall",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Crandall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2612372"
                        ],
                        "name": "L. Backstrom",
                        "slug": "L.-Backstrom",
                        "structuredName": {
                            "firstName": "Lars",
                            "lastName": "Backstrom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Backstrom"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713089"
                        ],
                        "name": "D. Huttenlocher",
                        "slug": "D.-Huttenlocher",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Huttenlocher",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Huttenlocher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3371403"
                        ],
                        "name": "J. Kleinberg",
                        "slug": "J.-Kleinberg",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Kleinberg",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kleinberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2106641,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1a4dc773c76b016542206375eef7d8fe79fb7cd4",
            "isKey": false,
            "numCitedBy": 858,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate how to organize a large collection of geotagged photos, working with a dataset of about 35 million images collected from Flickr. Our approach combines content analysis based on text tags and image data with structural analysis based on geospatial data. We use the spatial distribution of where people take photos to define a relational structure between the photos that are taken at popular places. We then study the interplay between this structure and the content, using classification methods for predicting such locations from visual, textual and temporal features of the photos. We find that visual and temporal features improve the ability to estimate the location of a photo, compared to using just textual features. We illustrate using these techniques to organize a large photo collection, while also revealing various interesting properties about popular cities and landmarks at a global scale."
            },
            "slug": "Mapping-the-world's-photos-Crandall-Backstrom",
            "title": {
                "fragments": [],
                "text": "Mapping the world's photos"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work uses the spatial distribution of where people take photos to define a relational structure between the photos that are taken at popular places, and finds that visual and temporal features improve the ability to estimate the location of a photo, compared to using just textual features."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753110"
                        ],
                        "name": "M. Huiskes",
                        "slug": "M.-Huiskes",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Huiskes",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Huiskes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731570"
                        ],
                        "name": "M. Lew",
                        "slug": "M.-Lew",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lew",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lew"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 26
                            }
                        ],
                        "text": "The initial 2008 release [5] included 25,000 images with ground truth annotations for 29 concepts, while the 2010 release [6] contained one million images without these annotations, but including visual features."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14040310,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f79131806747fce087d0fe73d0867cc621547b2a",
            "isKey": false,
            "numCitedBy": 1225,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "In most well known image retrieval test sets, the imagery typically cannot be freely distributed or is not representative of a large community of users. In this paper we present a collection for the MIR community comprising 25000 images from the Flickr website which are redistributable for research purposes and represent a real community of users both in the image content and image tags. We have extracted the tags and EXIF image metadata, and also make all of these publicly available. In addition we discuss several challenges for benchmarking retrieval and classification methods."
            },
            "slug": "The-MIR-flickr-retrieval-evaluation-Huiskes-Lew",
            "title": {
                "fragments": [],
                "text": "The MIR flickr retrieval evaluation"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper presents a collection for the MIR community comprising 25000 images from the Flickr website which are redistributable for research purposes and represent a real community of users both in the image content and image tags."
            },
            "venue": {
                "fragments": [],
                "text": "MIR '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056091"
                        ],
                        "name": "M. Everingham",
                        "slug": "M.-Everingham",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Everingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Everingham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143648071"
                        ],
                        "name": "S. Eslami",
                        "slug": "S.-Eslami",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Eslami",
                            "middleNames": [
                                "M.",
                                "Ali"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Eslami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 164
                            }
                        ],
                        "text": "Three smaller versions of the dataset were also released to accommodate those without sufficient computational machinery at their disposal to handle the entire collection."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207252270,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "616b246e332573af1f4859aa91440280774c183a",
            "isKey": false,
            "numCitedBy": 3770,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes (VOC) challenge consists of two components: (i)\u00a0a publicly available dataset of images together with ground truth annotation and standardised evaluation software; and (ii)\u00a0an annual competition and workshop. There are five challenges: classification, detection, segmentation, action classification, and person layout. In this paper we provide a review of the challenge from 2008\u20132012. The paper is intended for two audiences: algorithm designers, researchers who want to see what the state of the art is, as measured by performance on the VOC datasets, along with the limitations and weak points of the current generation of algorithms; and, challenge designers, who want to see what we as organisers have learnt from the process and our recommendations for the organisation of future challenges. To analyse the performance of submitted algorithms on the VOC datasets we introduce a number of novel evaluation methods: a bootstrapping method for determining whether differences in the performance of two algorithms are significant or not; a normalised average precision so that performance can be compared across classes with different proportions of positive instances; a clustering method for visualising the performance across multiple algorithms so that the hard and easy images can be identified; and the use of a joint classifier over the submitted algorithms in order to measure their complementarity and combined performance. We also analyse the community\u2019s progress through time using the methods of Hoiem et al. (Proceedings of European Conference on Computer Vision, 2012) to identify the types of occurring errors. We conclude the paper with an appraisal of the aspects of the challenge that worked well, and those that could be improved in future challenges."
            },
            "slug": "The-Pascal-Visual-Object-Classes-Challenge:-A-Everingham-Eslami",
            "title": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes Challenge: A Retrospective"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A review of the Pascal Visual Object Classes challenge from 2008-2012 and an appraisal of the aspects of the challenge that worked well, and those that could be improved in future challenges."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680223"
                        ],
                        "name": "A. Smeaton",
                        "slug": "A.-Smeaton",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Smeaton",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Smeaton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143645506"
                        ],
                        "name": "P. Over",
                        "slug": "P.-Over",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Over",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Over"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740640"
                        ],
                        "name": "Wessel Kraaij",
                        "slug": "Wessel-Kraaij",
                        "structuredName": {
                            "firstName": "Wessel",
                            "lastName": "Kraaij",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wessel Kraaij"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 176
                            }
                        ],
                        "text": "Using an online annotation tool people have been invited to draw the outlines of objects visible in the images, so far producing over 325,000 annotations for 5,650 object categories."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7677566,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3cb1f624b56eca597d65d27b8da7c4f2cd4b8531",
            "isKey": false,
            "numCitedBy": 1398,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "The TREC Video Retrieval Evaluation (TRECVid)is an international benchmarking activity to encourage research in video information retrieval by providing a large test collection, uniform scoring procedures, and a forum for organizations 1 interested in comparing their results. TRECVid completed its fifth annual cycle at the end of 2005 and in 2006 TRECVid will involve almost 70 research organizations, universities and other consortia. Throughout its existence, TRECVid has benchmarked both interactive and automatic/manual searching for shots from within a video corpus,automatic detection of a variety of semantic and low-level video features, shot boundary detection and the detection of story boundaries in broadcast TV news. This paper will give an introduction to information retrieval (IR) evaluation from both a user and a system perspective, high-lighting that system evaluation is by far the most prevalent type of evaluation carried out. We also include a summary of TRECVid as an example of a system evaluation bench-marking campaign and this allows us to discuss whether such campaigns are a good thing or a bad thing. There are arguments for and against these campaigns and we present some of them in the paper concluding that on balance they have had a very positive impact on research progress."
            },
            "slug": "Evaluation-campaigns-and-TRECVid-Smeaton-Over",
            "title": {
                "fragments": [],
                "text": "Evaluation campaigns and TRECVid"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An introduction to information retrieval (IR) evaluation from both a user and a system perspective is given, high-lighting that system evaluation is by far the most prevalent type of evaluation carried out."
            },
            "venue": {
                "fragments": [],
                "text": "MIR '06"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2040091191"
                        ],
                        "name": "Li-Jia Li",
                        "slug": "Li-Jia-Li",
                        "structuredName": {
                            "firstName": "Li-Jia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li-Jia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1360466,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eed4e6967c7a96e4cc2c590db40269cd97c8c98e",
            "isKey": false,
            "numCitedBy": 431,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Given an image, we propose a hierarchical generative model that classifies the overall scene, recognizes and segments each object component, as well as annotates the image with a list of tags. To our knowledge, this is the first model that performs all three tasks in one coherent framework. For instance, a scene of a `polo game' consists of several visual objects such as `human', `horse', `grass', etc. In addition, it can be further annotated with a list of more abstract (e.g. `dusk') or visually less salient (e.g. `saddle') tags. Our generative model jointly explains images through a visual model and a textual model. Visually relevant objects are represented by regions and patches, while visually irrelevant textual annotations are influenced directly by the overall scene class. We propose a fully automatic learning framework that is able to learn robust scene models from noisy Web data such as images and user tags from Flickr.com. We demonstrate the effectiveness of our framework by automatically classifying, annotating and segmenting images from eight classes depicting sport scenes. In all three tasks, our model significantly outperforms state-of-the-art algorithms."
            },
            "slug": "Towards-total-scene-understanding:-Classification,-Li-Socher",
            "title": {
                "fragments": [],
                "text": "Towards total scene understanding: Classification, annotation and segmentation in an automatic framework"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A fully automatic learning framework that is able to learn robust scene models from noisy Web data such as images and user tags from Flickr.com that significantly outperforms state-of-the-art algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 195908774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "isKey": false,
            "numCitedBy": 80960,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry."
            },
            "slug": "ImageNet-classification-with-deep-convolutional-Krizhevsky-Sutskever",
            "title": {
                "fragments": [],
                "text": "ImageNet classification with deep convolutional neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A large, deep convolutional neural network was trained to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes and employed a recently developed regularization method called \"dropout\" that proved to be very effective."
            },
            "venue": {
                "fragments": [],
                "text": "Commun. ACM"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830653"
                        ],
                        "name": "Noah Snavely",
                        "slug": "Noah-Snavely",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Snavely",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah Snavely"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679223"
                        ],
                        "name": "S. Seitz",
                        "slug": "S.-Seitz",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Seitz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Seitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717841"
                        ],
                        "name": "R. Szeliski",
                        "slug": "R.-Szeliski",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Szeliski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Szeliski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13385757,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5ebf37ce170f13a905f7feba9fb7096b49fb8b3",
            "isKey": false,
            "numCitedBy": 3202,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a system for interactively browsing and exploring large unstructured collections of photographs of a scene using a novel 3D interface. Our system consists of an image-based modeling front end that automatically computes the viewpoint of each photograph as well as a sparse 3D model of the scene and image to model correspondences. Our photo explorer uses image-based rendering techniques to smoothly transition between photographs, while also enabling full 3D navigation and exploration of the set of images and world geometry, along with auxiliary information such as overhead maps. Our system also makes it easy to construct photo tours of scenic or historic locations, and to annotate image details, which are automatically transferred to other relevant images. We demonstrate our system on several large personal photo collections as well as images gathered from Internet photo sharing sites."
            },
            "slug": "Photo-tourism:-exploring-photo-collections-in-3D-Snavely-Seitz",
            "title": {
                "fragments": [],
                "text": "Photo tourism: exploring photo collections in 3D"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "This work presents a system for interactively browsing and exploring large unstructured collections of photographs of a scene using a novel 3D interface that consists of an image-based modeling front end that automatically computes the viewpoint of each photograph and a sparse 3D model of the scene and image to model correspondences."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH '06"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39978391"
                        ],
                        "name": "Yangqing Jia",
                        "slug": "Yangqing-Jia",
                        "structuredName": {
                            "firstName": "Yangqing",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yangqing Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782282"
                        ],
                        "name": "Evan Shelhamer",
                        "slug": "Evan-Shelhamer",
                        "structuredName": {
                            "firstName": "Evan",
                            "lastName": "Shelhamer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Evan Shelhamer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7408951"
                        ],
                        "name": "Jeff Donahue",
                        "slug": "Jeff-Donahue",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Donahue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Donahue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3049736"
                        ],
                        "name": "Sergey Karayev",
                        "slug": "Sergey-Karayev",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Karayev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sergey Karayev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117314646"
                        ],
                        "name": "Jonathan Long",
                        "slug": "Jonathan-Long",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Long",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Long"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687120"
                        ],
                        "name": "S. Guadarrama",
                        "slug": "S.-Guadarrama",
                        "structuredName": {
                            "firstName": "Sergio",
                            "lastName": "Guadarrama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Guadarrama"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1799558,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6bdb186ec4726e00a8051119636d4df3b94043b5",
            "isKey": false,
            "numCitedBy": 13757,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (approx 2 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia."
            },
            "slug": "Caffe:-Convolutional-Architecture-for-Fast-Feature-Jia-Shelhamer",
            "title": {
                "fragments": [],
                "text": "Caffe: Convolutional Architecture for Fast Feature Embedding"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Multimedia"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3332698"
                        ],
                        "name": "T. Rattenbury",
                        "slug": "T.-Rattenbury",
                        "structuredName": {
                            "firstName": "Tye",
                            "lastName": "Rattenbury",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Rattenbury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144252282"
                        ],
                        "name": "Nathaniel Good",
                        "slug": "Nathaniel-Good",
                        "structuredName": {
                            "firstName": "Nathaniel",
                            "lastName": "Good",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nathaniel Good"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687465"
                        ],
                        "name": "Mor Naaman",
                        "slug": "Mor-Naaman",
                        "structuredName": {
                            "firstName": "Mor",
                            "lastName": "Naaman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mor Naaman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1351365,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e044622146b486bc3b1bd23c778b266762828624",
            "isKey": false,
            "numCitedBy": 540,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an approach for extracting semantics of tags, unstructured text-labels assigned to resources on the Web, based on each tag's usage patterns. In particular, we focus on the problem of extracting place and event semantics for tags that are assigned to photos on Flickr, a popular photo sharing website that supports time and location (latitude/longitude) metadata. We analyze two methods inspired by well-known burst-analysis techniques and one novel method: Scale-structure Identification. We evaluate the methods on a subset of Flickr data, and show that our Scale-structure Identification method outperforms the existing techniques. The approach and methods described in this work can be used in other domains such as geo-annotated web pages, where text terms can be extracted and associated with usage patterns."
            },
            "slug": "Towards-automatic-extraction-of-event-and-place-Rattenbury-Good",
            "title": {
                "fragments": [],
                "text": "Towards automatic extraction of event and place semantics from flickr tags"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "An approach for extracting semantics of tags, unstructured text-labels assigned to resources on the Web, based on each tag's usage patterns, and shows that the Scale-structure Identification method outperforms the existing techniques."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696082"
                        ],
                        "name": "C. Borgman",
                        "slug": "C.-Borgman",
                        "structuredName": {
                            "firstName": "Christine",
                            "lastName": "Borgman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Borgman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 103
                            }
                        ],
                        "text": "The question of sharing data for replication and growth has arisen several times in the past 30 years alone [13\u201315]\nand has been brought into discussion in ACM\u2019s SIGCHI [16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 26931636,
            "fieldsOfStudy": [
                "Political Science"
            ],
            "id": "f5383bc9398006c2e4466748be5d191da923790f",
            "isKey": false,
            "numCitedBy": 684,
            "numCiting": 126,
            "paperAbstract": {
                "fragments": [],
                "text": "We must all accept that science is data and that data are science, and thus provide for, and justify the need for the support of, much-improved data curation. (Hanson, Sugden, & Alberts) \n \nResearchers are producing an unprecedented deluge of data by using new methods and instrumentation. Others may wish to mine these data for new discoveries and innovations. However, research data are not readily available as sharing is common in only a few fields such as astronomy and genomics. Data sharing practices in other fields vary widely. Moreover, research data take many forms, are handled in many ways, using many approaches, and often are difficult to interpret once removed from their initial context. Data sharing is thus a conundrum. Four rationales for sharing data are examined, drawing examples from the sciences, social sciences, and humanities: (1) to reproduce or to verify research, (2) to make results of publicly funded research available to the public, (3) to enable others to ask new questions of extant data, and (4) to advance the state of research and innovation. These rationales differ by the arguments for sharing, by beneficiaries, and by the motivations and incentives of the many stakeholders involved. The challenges are to understand which data might be shared, by whom, with whom, under what conditions, why, and to what effects. Answers will inform data policy and practice. \u00a9 2012 Wiley Periodicals, Inc."
            },
            "slug": "The-conundrum-of-sharing-research-data-Borgman",
            "title": {
                "fragments": [],
                "text": "The conundrum of sharing research data"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Four rationales for sharing data are examined, drawing examples from the sciences, social sciences, and humanities: to reproduce or to verify research, to make results of publicly funded research available to the public, to enable others to ask new questions of extant data, and to advance the state of research and innovation."
            },
            "venue": {
                "fragments": [],
                "text": "J. Assoc. Inf. Sci. Technol."
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48966748"
                        ],
                        "name": "James Hays",
                        "slug": "James-Hays",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Hays",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Hays"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2061602,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b48d90cfebb8fbff29d161f6704d31b6909eb7ad",
            "isKey": false,
            "numCitedBy": 894,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Estimating geographic information from an image is an excellent, difficult high-level computer vision problem whose time has come. The emergence of vast amounts of geographically-calibrated image data is a great reason for computer vision to start looking globally - on the scale of the entire planet! In this paper, we propose a simple algorithm for estimating a distribution over geographic locations from a single image using a purely data-driven scene matching approach. For this task, we leverage a dataset of over 6 million GPS-tagged images from the Internet. We represent the estimated image location as a probability distribution over the Earthpsilas surface. We quantitatively evaluate our approach in several geolocation tasks and demonstrate encouraging performance (up to 30 times better than chance). We show that geolocation estimates can provide the basis for numerous other image understanding tasks such as population density estimation, land cover estimation or urban/rural classification."
            },
            "slug": "IM2GPS:-estimating-geographic-information-from-a-Hays-Efros",
            "title": {
                "fragments": [],
                "text": "IM2GPS: estimating geographic information from a single image"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes a simple algorithm for estimating a distribution over geographic locations from a single image using a purely data-driven scene matching approach and shows that geolocation estimates can provide the basis for numerous other image understanding tasks such as population density estimation, land cover estimation or urban/rural classification."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2278731"
                        ],
                        "name": "Allen H. Renear",
                        "slug": "Allen-H.-Renear",
                        "structuredName": {
                            "firstName": "Allen",
                            "lastName": "Renear",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Allen H. Renear"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "28774860"
                        ],
                        "name": "S. Sacchi",
                        "slug": "S.-Sacchi",
                        "structuredName": {
                            "firstName": "Simone",
                            "lastName": "Sacchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sacchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40942044"
                        ],
                        "name": "Karen M. Wickett",
                        "slug": "Karen-M.-Wickett",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Wickett",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karen M. Wickett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 51
                            }
                        ],
                        "text": "Datasets are critical for research and exploration [4] as, rather obviously, data is required for performing experiments, validating hypotheses, analyzing designs, and building applications."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52046307,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "78e650cec1325a0b2fa32eb51d9bc1698d73e628",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "The integration of heterogeneous data in varying formats and from diverse communities requires an improved understanding of the concept of a dataset, and of key related concepts, such as format, encoding, and version. Ultimately, a normative formal framework of such concepts will be needed to support the effective curation, integration, and use of shared multi-disciplinary scientific data. To prepare for the development of this framework we reviewed the definitions of dataset found in technical documentation and the scientific literature. Four basic features can be identified as common to most definitions: grouping, content, relatedness, and purpose. In this summary of our results we describe each of these features, indicating the directions a more formal analysis might take."
            },
            "slug": "Definitions-of-dataset-in-the-scientific-and-Renear-Sacchi",
            "title": {
                "fragments": [],
                "text": "Definitions of dataset in the scientific and technical literature"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "To prepare for the development of this framework, the definitions of dataset were reviewed and four basic features can be identified as common to most definitions: grouping, content, relatedness, and purpose."
            },
            "venue": {
                "fragments": [],
                "text": "ASIST"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32980371"
                        ],
                        "name": "Brent J. Hecht",
                        "slug": "Brent-J.-Hecht",
                        "structuredName": {
                            "firstName": "Brent",
                            "lastName": "Hecht",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brent J. Hecht"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217278"
                        ],
                        "name": "Lichan Hong",
                        "slug": "Lichan-Hong",
                        "structuredName": {
                            "firstName": "Lichan",
                            "lastName": "Hong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lichan Hong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3994427"
                        ],
                        "name": "B. Suh",
                        "slug": "B.-Suh",
                        "structuredName": {
                            "firstName": "Bongwon",
                            "lastName": "Suh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Suh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2226805"
                        ],
                        "name": "Ed H. Chi",
                        "slug": "Ed-H.-Chi",
                        "structuredName": {
                            "firstName": "Ed",
                            "lastName": "Chi",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ed H. Chi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15244950,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "51ec8194cff157eb712382d589174909e087acf8",
            "isKey": false,
            "numCitedBy": 525,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Little research exists on one of the most common, oldest, and most utilized forms of online social geographic information: the 'location' field found in most virtual community user profiles. We performed the first in-depth study of user behavior with regard to the location field in Twitter user profiles. We found that 34% of users did not provide real location information, frequently incorporating fake locations or sarcastic comments that can fool traditional geographic information tools. When users did input their location, they almost never specified it at a scale any more detailed than their city. In order to determine whether or not natural user behaviors have a real effect on the 'locatability' of users, we performed a simple machine learning experiment to determine whether we can identify a user's location by only looking at what that user tweets. We found that a user's country and state can in fact be determined easily with decent accuracy, indicating that users implicitly reveal location information, with or without realizing it. Implications for location-based services and privacy are discussed."
            },
            "slug": "Tweets-from-Justin-Bieber's-heart:-the-dynamics-of-Hecht-Hong",
            "title": {
                "fragments": [],
                "text": "Tweets from Justin Bieber's heart: the dynamics of the location field in user profiles"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The first in-depth study of user behavior with regard to the location field in Twitter user profiles found that a user's country and state can in fact be determined easily with decent accuracy, indicating that users implicitly reveal location information, with or without realizing it."
            },
            "venue": {
                "fragments": [],
                "text": "CHI"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3132886"
                        ],
                        "name": "J. Dijck",
                        "slug": "J.-Dijck",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Dijck",
                            "middleNames": [
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dijck"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Beyond archived collections, the photostreams of individuals represents many facets of recorded visual information, from remembering moments and storytelling to social communication and self-identity [19]. This presents a grand challenge of sensemaking and understanding digital archives from non-homogeneous sources. Photographers and curators alike have contributed to the larger collection of Creative"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62628482,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "7c193464cb16031fbbe31274415ef23e6d49eb82",
            "isKey": false,
            "numCitedBy": 372,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Taking photographs seems no longer primarily an act of memory intended to safeguard a family's pictorial heritage, but is increasingly becoming a tool for an individual's identity formation and communication. Digital cameras, cameraphones, photoblogs and other multipurpose devices are used to promote the use of images as the preferred idiom of a new generation of users. The aim of this article is to explore how technical changes (digitization) combined with growing insights in cognitive science and socio-cultural transformations have affected personal photography. The increased manipulation of photographic images may suit the individual's need for continuous self-remodelling and instant communication and bonding. However, that same manipulability may also lessen our grip on our images' future repurposing and reframing. Memory is not eradicated from digital multipurpose tools. Instead, the function of memory reappears in the networked, distributed nature of digital photographs, as most images are sent over the internet and stored in virtual space."
            },
            "slug": "Digital-Photography:-Communication,-Identity,-Dijck",
            "title": {
                "fragments": [],
                "text": "Digital Photography: Communication, Identity, Memory."
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "How technical changes (digitization) combined with growing insights in cognitive science and socio-cultural transformations have affected personal photography is explored."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8530169"
                        ],
                        "name": "Max L. Wilson",
                        "slug": "Max-L.-Wilson",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Wilson",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Max L. Wilson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2226805"
                        ],
                        "name": "Ed H. Chi",
                        "slug": "Ed-H.-Chi",
                        "structuredName": {
                            "firstName": "Ed",
                            "lastName": "Chi",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ed H. Chi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145142464"
                        ],
                        "name": "S. Reeves",
                        "slug": "S.-Reeves",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Reeves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Reeves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144095233"
                        ],
                        "name": "D. Coyle",
                        "slug": "D.-Coyle",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Coyle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Coyle"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 161
                            }
                        ],
                        "text": "The question of sharing data for replication and growth has arisen several times in the past 30 years alone [13\u201315]\nand has been brought into discussion in ACM\u2019s SIGCHI [16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 241361,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "293273196a4c372bf3912d2afcd8c61935b91412",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The replication or recreation of research is a core part of many disciplines. Yet unlike many other disciplines, like medicine, physics, or mathematics, we have almost no drive and barely any reason to consider investigating the work of other HCI researchers. Our community is driven to publish novel results in novel spaces using novel designs, and to keep up with evolving technology. Further, our community contains a broad spectrum of research styles, from those that would aim to investigate cultural phenomenon observed with ethnographic measures, to those who would validate or refute prior work with experimental methods. The aim of this workshop is to continue to facilitate a cultural shift towards our community naturally adopting replication techniques in situations that are considered worth investigating."
            },
            "slug": "RepliCHI:-the-workshop-II-Wilson-Chi",
            "title": {
                "fragments": [],
                "text": "RepliCHI: the workshop II"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The aim of this workshop is to continue to facilitate a cultural shift towards the community naturally adopting replication techniques in situations that are considered worth investigating."
            },
            "venue": {
                "fragments": [],
                "text": "CHI Extended Abstracts"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8582148"
                        ],
                        "name": "C. Ishimoto",
                        "slug": "C.-Ishimoto",
                        "structuredName": {
                            "firstName": "Cindy",
                            "lastName": "Ishimoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Ishimoto"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 41
                            }
                        ],
                        "text": "To Facebook alone more than 250 billion photos have been uploaded and on average it receives over 350 million new photos every day [1], while YouTube reports that 100 hours of video are uploaded every single minute [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 40226213,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "abcf3c67a369e122edf67b8338eaec596bd98a6c",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Focus-on-efficiency.-Ishimoto",
            "title": {
                "fragments": [],
                "text": "Focus on efficiency."
            },
            "venue": {
                "fragments": [],
                "text": "Hawaii dental journal"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145089198"
                        ],
                        "name": "G. Hughes",
                        "slug": "G.-Hughes",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hughes",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Hughes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 93
                            }
                        ],
                        "text": "The question of sharing data for replication and growth has arisen several times in the past 30 years alone [13\u201315]\nand has been brought into discussion in ACM\u2019s SIGCHI [16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 20744273,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "89395a983f50022249dd703e6890ce612a34284a",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Sharing-research-data-Hughes",
            "title": {
                "fragments": [],
                "text": "Sharing research data"
            },
            "venue": {
                "fragments": [],
                "text": "Emergency medicine Australasia : EMA"
            },
            "year": 2017
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 98
                            }
                        ],
                        "text": "The question of sharing data for replication and growth has arisen several times in the past 30 years alone [13\u201315]\nand has been brought into discussion in ACM\u2019s SIGCHI [16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "To share or not to share: publication and quality assurance of research data outputs"
            },
            "venue": {
                "fragments": [],
                "text": "Research Information Network, Tech. Rep"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Welcome the internet archive to the commons , \" https://blog.flickr.net/2014/08/29/welcome-the-internet- archive-to-the-commons"
            },
            "venue": {
                "fragments": [],
                "text": "Welcome the internet archive to the commons , \" https://blog.flickr.net/2014/08/29/welcome-the-internet- archive-to-the-commons"
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Yelp dataset challenge http://yelp.com/dataset challenge"
            },
            "venue": {
                "fragments": [],
                "text": "Yelp dataset challenge http://yelp.com/dataset challenge"
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Digital photography: communication, identity, memory Visual Communication"
            },
            "venue": {
                "fragments": [],
                "text": "Digital photography: communication, identity, memory Visual Communication"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 219
                            }
                        ],
                        "text": "These large online collections tell a larger story about the world around us, from consumer reviews [24] that speak to how people engage with the spaces around them to 500 years of scanned book photos and illustrations [25] that describe how concepts and objects have been visually depicted over time."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Welcome the internet archive to the commons"
            },
            "venue": {
                "fragments": [],
                "text": "https://blog.flickr.net/2014/08/29/welcome-the-internetarchive-to-the-commons/, August 2014, accessed March, 2015."
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 165
                            }
                        ],
                        "text": "A back of the envelope estimation reports 10% of all photos in the world were taken in the last 12 months, and that was calculated already more than three years ago [3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "How many photos have ever been taken?"
            },
            "venue": {
                "fragments": [],
                "text": "https://web.archive.org/web/20150203215607/http: //blog.1000memories.com/94-number-of-photos-ever-takendigital-and-analog-in-shoebox,"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 125
                            }
                        ],
                        "text": "To Facebook alone more than 250 billion photos have been uploaded and on average it receives over 350 million new photos every day [1], while YouTube reports that 100 hours of video are uploaded every single minute [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Youtube press statistics"
            },
            "venue": {
                "fragments": [],
                "text": "Youtube press statistics"
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 75
                            }
                        ],
                        "text": "A back of the envelope estimation reports 10% of all photos in the world were taken in the last 12 months, and that was calculated already more than three years ago [3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "How many photos have ever been taken? \" https://web.archive.org/web/20150203215607/http: //blog.1000memories.com/94-number-of-photos-ever-taken- digital-and-analog-in-shoebox"
            },
            "venue": {
                "fragments": [],
                "text": "How many photos have ever been taken? \" https://web.archive.org/web/20150203215607/http: //blog.1000memories.com/94-number-of-photos-ever-taken- digital-and-analog-in-shoebox"
            },
            "year": 2011
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 17,
            "methodology": 2,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 32,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/The-New-Data-and-New-Challenges-in-Multimedia-Thomee-Shamma/a6e695ddd07aad719001c0fc1129328452385949?sort=total-citations"
}